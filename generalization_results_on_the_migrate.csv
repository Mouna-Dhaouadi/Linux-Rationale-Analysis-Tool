text;predicted_decision;predicted_rationale
mm/vmscan: fix a bug calling wakeup_kswapd() with a wrong zone index;1;1
"With numa balancing on, when a numa system is running where a numa node
doesn't have its local memory so it has no managed zones, the following
oops has been observed";0;1
" It's because wakeup_kswapd() is called with a
wrong zone index, -1";0;0
" Fixed it by checking the index before calling
wakeup_kswapd()";1;1
"> BUG: unable to handle page fault for address: 00000000000033f3
> #PF: supervisor read access in kernel mode
> #PF: error_code(0x0000) - not-present page
> PGD 0 P4D 0
> Oops: 0000 [#1] PREEMPT SMP NOPTI
> CPU: 2 PID: 895 Comm: masim Not tainted 6.6.0-dirty #255
> Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS
>    rel-1.16.0-0-gd239552ce722-prebuilt.qemu.org 04/01/2014
> RIP: 0010:wakeup_kswapd (./linux/mm/vmscan.c:7812)
> Code: (omitted)
> RSP: 0000:ffffc90004257d58 EFLAGS: 00010286
> RAX: ffffffffffffffff RBX: ffff88883fff0480 RCX: 0000000000000003
> RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff88883fff0480
> RBP: ffffffffffffffff R08: ff0003ffffffffff R09: ffffffffffffffff
> R10: ffff888106c95540 R11: 0000000055555554 R12: 0000000000000003
> R13: 0000000000000000 R14: 0000000000000000 R15: ffff88883fff0940
> FS:  00007fc4b8124740(0000) GS:ffff888827c00000(0000) knlGS:0000000000000000
> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
> CR2: 00000000000033f3 CR3: 000000026cc08004 CR4: 0000000000770ee0
> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
> PKRU: 55555554";0;0
mm/migrate: page_add_anon_rmap() -> folio_add_anon_rmap_pte();1;0
Let's convert remove_migration_pte().;1;0
mm/migrate: page_add_file_rmap() -> folio_add_file_rmap_pte();1;1
Let's convert remove_migration_pte().;1;0
mm/rmap: introduce and use hugetlb_add_file_rmap();1;1
"hugetlb rmap handling differs quite a lot from ""ordinary"" rmap code";0;1
" For
example, hugetlb currently only supports entire mappings, and treats any
mapping as mapped using a single ""logical PTE""";0;0
" Let's move it out of the
way so we can overhaul our ""ordinary"" rmap";1;1
 implementation/interface;0;0
"Right now we're using page_dup_file_rmap() in some cases where ""ordinary""
rmap code would have used page_add_file_rmap()";1;1
" So let's introduce and
use hugetlb_add_file_rmap() instead";1;1
" We won't be adding a
""hugetlb_dup_file_rmap()"" functon for the fork() case, as it would be
doing the same: ""dup"" is just an optimization for ""add""";1;1
What remains is a single page_dup_file_rmap() call in fork() code;1;1
"Add sanity checks that we end up with the right folios in the right
functions.";0;1
mm/rmap: rename hugepage_add* to hugetlb_add*;1;0
"Patch series ""mm/rmap: interface overhaul"", v2";0;1
"This series overhauls the rmap interface, to get rid of the ""bool
compound"" / RMAP_COMPOUND parameter with the goal of making the interface
less error prone, more future proof, and more natural to extend to
""batching""";0;1
" Also, this converts the interface to always consume
folio+subpage, which speeds up operations on large folios";0;1
"Further, this series adds PTE-batching variants for 4 rmap functions,
whereby only folio_add_anon_rmap_ptes() is used for batching in this
series when PTE-remapping a PMD-mapped THP";1;0
" folio_remove_rmap_ptes(),
folio_try_dup_anon_rmap_ptes() and folio_dup_file_rmap_ptes() will soon
come in handy[1,2]";1;0
This series performs a lot of folio conversion along the way;1;1
" Most of the
added LOC in the diff are only due to documentation";1;0
"As we're moving to a pte/pmd interface where we clearly express the
mapping granularity we are dealing with, we first get the remainder of
hugetlb out of the way, as it is special and expected to remain special";0;1
"it treats everything as a ""single logical PTE"" and only currently allows
entire mappings";1;1
"Even if we'd ever support partial mappings, I strongly assume the
interface and implementation will still differ heavily: hopefull we can
avoid working on subpages/subpage mapcounts completely and only add a
""count"" parameter for them to enable batching";1;1
New (extended) hugetlb interface that operates on entire folio;0;1
"New ""ordinary"" interface for small folios / THP:";1;0
"folio_add_new_anon_rmap() will always map at the largest granularity
possible (currently, a single PMD to cover a PMD-sized THP)";1;0
" Could be
extended if ever required";0;0
"In the future, we might want ""_pud"" variants and eventually ""_pmds""
variants for batching";1;1
I ran some simple microbenchmarks on an Intel(R) Xeon(R) Silver 4210R;1;1
measuring munmap(), fork(), cow, MADV_DONTNEED on each PTE ..;1;0
" and PTE
remapping PMD-mapped THPs on 1 GiB of memory";0;0
For small folios, there is barely a change (< 1% improvement for me);0;0
For PTE-mapped THP:;0;0
mm: migrate: fix getting incorrect page mapping during page migration;1;1
When running stress-ng testing, we found below kernel crash after a few hours;0;0
"Unable to handle kernel NULL pointer dereference at virtual address 0000000000000000
pc : dentry_name+0xd8/0x224
lr : pointer+0x22c/0x370
sp : ffff800025f134c0
Call trace";0;1
"  dentry_name+0xd8/0x224
  pointer+0x22c/0x370
  vsnprintf+0x1ec/0x730
  vscnprintf+0x2c/0x60
  vprintk_store+0x70/0x234
  vprintk_emit+0xe0/0x24c
  vprintk_default+0x3c/0x44
  vprintk_func+0x84/0x2d0
  printk+0x64/0x88
  __dump_page+0x52c/0x530
  dump_page+0x14/0x20
  set_migratetype_isolate+0x110/0x224
  start_isolate_page_range+0xc4/0x20c
  offline_pages+0x124/0x474
  memory_block_offline+0x44/0xf4
  memory_subsys_offline+0x3c/0x70
  device_offline+0xf0/0x120
After analyzing the vmcore, I found this issue is caused by page migration";0;0
"The scenario is that, one thread is doing page migration, and we will use the
target page's ->mapping field to save 'anon_vma' pointer between page unmap and
page move, and now the target page is locked and refcount is 1";0;0
"Currently, there is another stress-ng thread performing memory hotplug,
attempting to offline the target page that is being migrated";0;1
"It discovers that
the refcount of this target page is 1, preventing the offline operation, thus
proceeding to dump the page";0;1
"However, page_mapping() of the target page may
return an incorrect file mapping to crash the system in dump_mapping(), since
the target page->mapping only saves 'anon_vma' pointer without setting
PAGE_MAPPING_ANON flag";1;0
There are seveval ways to fix this issue;0;1
"(1) Setting the PAGE_MAPPING_ANON flag for target page's ->mapping when saving
'anon_vma', but this can confuse PageAnon() for PFN walkers, since the target
page has not built mappings yet";1;1
"(2) Getting the page lock to call page_mapping() in __dump_page() to avoid crashing
the system, however, there are still some PFN walkers that call page_mapping()
without holding the page lock, such as compaction";0;1
"(3) Using target page->private field to save the 'anon_vma' pointer and 2 bits
page state, just as page->mapping records an anonymous page, which can remove
the page_mapping() impact for PFN walkers and also seems a simple way";1;1
"So I choose option 3 to fix this issue, and this can also fix other potential
issues for PFN walkers, such as compaction.";1;1
mm: migrate high-order folios in swap cache correctly;1;0
"Large folios occupy N consecutive entries in the swap cache instead of
using multi-index entries like the page cache";1;0
" However, if a large folio
is re-added to the LRU list, it can be migrated";0;0
" The migration code was
not aware of the difference between the swap cache and the page cache and
assumed that a single xas_store() would be sufficient";0;0
"This leaves potentially many stale pointers to the now-migrated folio in
the swap cache, which can lead to almost arbitrary data corruption in the
future";0;1
" This can also manifest as infinite loops with the RCU read lock
held.";1;1
fs: Rename mapping private members;1;1
"It is hard to find where mapping->private_lock, mapping->private_list and
mapping->private_data are used, due to private_XXX being a relatively
common name for variables and structure members in the kernel";0;1
" To fit
with other members of struct address_space, rename them all to have an
i_ prefix";1;0
 Tested with an allmodconfig build.;1;0
mm: Add AS_UNMOVABLE to mark mapping as completely unmovable;1;0
"Add an ""unmovable"" flag for mappings that cannot be migrated under any
circumstance";1;1
" KVM will use the flag for its upcoming GUEST_MEMFD support,
which will not support compaction/migration, at least not in the
foreseeable future";0;0
"Test AS_UNMOVABLE under folio lock as already done for the async
compaction/dirty folio case, as the mapping can be removed by truncation
while compaction is running";1;1
" To avoid having to lock every folio with a
mapping, assume/require that unmovable mappings are also unevictable, and
have mapping_set_unmovable() also set AS_UNEVICTABLE.";0;1
mm: migrate: record the mlocked page status to remove unnecessary lru drain;1;1
"When doing compaction, I found the lru_add_drain() is an obvious hotspot
when migrating pages";1;1
The distribution of this hotspot is as follows;0;0
"   - 18.75% compact_zone
      - 17.39% migrate_pages
         - 13.79% migrate_pages_batch
            - 11.66% migrate_folio_move
               - 7.02% lru_add_drain
                  + 7.02% lru_add_drain_cpu
               + 3.00% move_to_new_folio
                 1.23% rmap_walk
            + 1.92% migrate_folio_unmap
         + 3.20% migrate_pages_sync
      + 0.90% isolate_migratepages
The lru_add_drain() was added by commit c3096e6782b7 (""mm/migrate";1;0
"__unmap_and_move() push good newpage to LRU"") to drain the newpage to LRU
immediately, to help to build up the correct newpage->mlock_count in
remove_migration_ptes() for mlocked pages";1;1
" However, if there are no
mlocked pages are migrating, then we can avoid this lru drain operation,
especailly for the heavy concurrent scenarios";0;1
"So we can record the source pages' mlocked status in
migrate_folio_unmap(), and only drain the lru list when the mlocked status
is set in migrate_folio_move()";1;0
"In addition, the page was already isolated from lru when migrating, so
checking the mlocked status is stable by folio_test_mlocked() in
migrate_folio_unmap()";1;1
After this patch, I can see the hotpot of the lru_add_drain() is gone;1;1
"   - 9.41% migrate_pages_batch
      - 6.15% migrate_folio_move
         - 3.64% move_to_new_folio
            + 1.80% migrate_folio_extra
            + 1.70% buffer_migrate_folio
         + 1.41% rmap_walk
         + 0.62% folio_add_lru
      + 3.07% migrate_folio_unmap
Meanwhile, the compaction latency shows some improvements when running
thpscale";1;0
"                            base                   patched
Amean     fault-both-1      1131.22 (   0.00%)     1112.55 *   1.65%*
Amean     fault-both-3      2489.75 (   0.00%)     2324.15 *   6.65%*
Amean     fault-both-5      3257.37 (   0.00%)     3183.18 *   2.28%*
Amean     fault-both-7      4257.99 (   0.00%)     4079.04 *   4.20%*
Amean     fault-both-12     6614.02 (   0.00%)     6075.60 *   8.14%*
Amean     fault-both-18    10607.78 (   0.00%)     8978.86 *  15.36%*
Amean     fault-both-24    14911.65 (   0.00%)    11619.55 *  22.08%*
Amean     fault-both-30    14954.67 (   0.00%)    14925.66 *   0.19%*
Amean     fault-both-32    16654.87 (   0.00%)    15580.31 *   6.45%*";1;0
mm/migrate: add nr_split to trace_mm_migrate_pages stats.;1;0
"Add nr_split to trace_mm_migrate_pages for large folio (including THP)
split events.";1;0
mm/migrate: correct nr_failed in migrate_pages_sync();1;1
"nr_failed was missing the large folio splits from migrate_pages_batch()
and can cause a mismatch between migrate_pages() return value and the
number of not migrated pages, i.e., when the return value of
migrate_pages() is 0, there are still pages left in the from page list";0;0
"
It will happen when a non-PMD THP large folio fails to migrate due to
-ENOMEM and is split successfully but not all the split pages are not
migrated, migrate_pages_batch() would return non-zero, but
astats.nr_thp_split = 0";0;0
" nr_failed would be 0 and returned to the caller
of migrate_pages(), but the not migrated pages are left in the from page
list without being added back to LRU lists";0;0
"Fix it by adding a new nr_split counter for large folio splits and adding
it to nr_failed in migrate_page_sync() after migrate_pages_batch() is
done.";1;1
mm: migrate: use folio_xchg_last_cpupid() in folio_migrate_flags();1;0
"Convert to use folio_xchg_last_cpupid() in folio_migrate_flags(), also
directly use folio_nid() instead of page_to_nid(&folio->page).";1;0
hugetlb: memcg: account hugetlb-backed memory in memory controller;1;0
"Currently, hugetlb memory usage is not acounted for in the memory
controller, which could lead to memory overprotection for cgroups with
hugetlb-backed memory";0;1
 This has been observed in our production system;0;1
"For instance, here is one of our usecases: suppose there are two 32G
containers";1;1
" The machine is booted with hugetlb_cma=6G, and each container
may or may not use up to 3 gigantic page, depending on the workload within
it";0;0
 The rest is anon, cache, slab, etc;1;0
" We can set the hugetlb cgroup
limit of each cgroup to 3G to enforce hugetlb fairness";1;0
" But it is very
difficult to configure memory.max to keep overall consumption, including
anon, cache, slab etc";0;0
 fair;0;0
"What we have had to resort to is to constantly poll hugetlb usage and
readjust memory.max";0;0
" Similar procedure is done to other memory limits
(memory.low for e.g)";0;1
 However, this is rather cumbersome and buggy;0;1
"
Furthermore, when there is a delay in memory limits correction, (for e.g
when hugetlb usage changes within consecutive runs of the userspace
agent), the system could be in an over/underprotected state";0;1
"This patch rectifies this issue by charging the memcg when the hugetlb
folio is utilized, and uncharging when the folio is freed (analogous to
the hugetlb controller)";1;0
" Note that we do not charge when the folio is
allocated to the hugetlb pool, because at this point it is not owned by
any memcg";0;1
Some caveats to consider;1;0
    controller;1;0
"As stated above, hugetlb folios are only charged towards
    the memory controller when it is used";1;0
"Host overcommit management
    has to consider it when configuring hard limits";0;0
"    happen even if the hugetlb pool still has pages (but the cgroup
    limit is hit and reclaim attempt fails)";0;0
    reclaim protection;1;0
"low, min limits tuning must take into account
    hugetlb memory";1;1
"    be tracked by the memory controller (even if cgroup v2 is remounted
    later on).";1;0
mm/migrate: remove unused mm argument from do_move_pages_to_node;1;1
This function does not actively use the mm_struct, it can be removed.;1;1
Merge mm-hotfixes-stable into mm-stable to pick up depended-upon changes.;1;1
;0;0
mm/filemap: remove hugetlb special casing in filemap.c;1;0
"Remove special cased hugetlb handling code within the page cache by
changing the granularity of ->index to the base page size rather than the
huge page size";1;1
" The motivation of this patch is to reduce complexity
within the filemap code while also increasing performance by removing
branches that are evaluated on every page cache lookup";1;1
"To support the change in index, new wrappers for hugetlb page cache
interactions are added";1;0
" These wrappers perform the conversion to a linear
index which is now expected by the page cache for huge pages";0;1
"========================= PERFORMANCE ======================================
Perf was used to check the performance differences after the patch";0;0
"
Overall the performance is similar to mainline with a very small larger
overhead that occurs in __filemap_add_folio() and
hugetlb_add_to_page_cache()";1;0
" This is because of the larger overhead that
occurs in xa_load() and xa_store() as the xarray is now using more entries
to store hugetlb folios in the page cache";0;1
"Timing
aarch64
    2MB Page Size
        6.5-rc3 + this patch";1;0
"            [root@sidhakum-ol9-1 hugepages]# time fallocate -l 700GB test.txt
            real    1m49.568s
            user    0m0.000s
            sys     1m49.461s
        6.5-rc3";1;0
"            [root]# time fallocate -l 700GB test.txt
            real    1m47.495s
            user    0m0.000s
            sys     1m47.370s
    1GB Page Size
        6.5-rc3 + this patch";1;0
"            [root@sidhakum-ol9-1 hugepages1G]# time fallocate -l 700GB test.txt
            real    1m47.024s
            user    0m0.000s
            sys     1m46.921s
        6.5-rc3";1;0
"            [root@sidhakum-ol9-1 hugepages1G]# time fallocate -l 700GB test.txt
            real    1m44.551s
            user    0m0.000s
            sys     1m44.438s
    2MB Page Size
        6.5-rc3 + this patch";1;0
"            [root@sidhakum-ol9-2 hugepages]# time fallocate -l 100GB test.txt
            real    0m22.383s
            user    0m0.000s
            sys     0m22.255s
        6.5-rc3";1;0
"            [opc@sidhakum-ol9-2 hugepages]$ time sudo fallocate -l 100GB /dev/hugepages/test.txt
            real    0m22.735s
            user    0m0.038s
            sys     0m22.567s
    1GB Page Size
        6.5-rc3 + this patch";1;0
"            [root@sidhakum-ol9-2 hugepages1GB]# time fallocate -l 100GB test.txt
            real    0m25.786s
            user    0m0.001s
            sys     0m25.589s
        6.5-rc3";1;0
"            [root@sidhakum-ol9-2 hugepages1G]# time fallocate -l 100GB test.txt
            real    0m33.454s
            user    0m0.001s
            sys     0m33.193s
aarch64";0;0
"    workload - fallocate a 700GB file backed by huge pages
    6.5-rc3 + this patch";1;0
        2MB Page Size;1;0
"            --100.00%--__arm64_sys_fallocate
                          ksys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                          |--95.04%--__pi_clear_page
                          |--3.57%--clear_huge_page
                          |          |--2.63%--rcu_all_qs
                          |           --0.91%--__cond_resched
                           --0.67%--__cond_resched
            0.17%     0.00%             0  fallocate  [kernel.vmlinux]       [k] hugetlb_add_to_page_cache
            0.14%     0.10%            11  fallocate  [kernel.vmlinux]       [k] __filemap_add_folio
    6.5-rc3
        2MB Page Size";1;0
"                --100.00%--__arm64_sys_fallocate
                          ksys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                          |--94.91%--__pi_clear_page
                          |--4.11%--clear_huge_page
                          |          |--3.00%--rcu_all_qs
                          |           --1.10%--__cond_resched
                           --0.59%--__cond_resched
            0.08%     0.01%             1  fallocate  [kernel.kallsyms]  [k] hugetlb_add_to_page_cache
            0.05%     0.03%             3  fallocate  [kernel.kallsyms]  [k] __filemap_add_folio
    workload - fallocate a 100GB file backed by huge pages
    6.5-rc3 + this patch";1;1
        2MB Page Size;1;0
"            hugetlbfs_fallocate
            --99.57%--clear_huge_page
                --98.47%--clear_page_erms
                    --0.53%--asm_sysvec_apic_timer_interrupt
            0.04%     0.04%             1  fallocate  [kernel.kallsyms]     [k] xa_load
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] hugetlb_add_to_page_cache
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] __filemap_add_folio
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] xas_store
    6.5-rc3
        2MB Page Size";1;0
"                --99.93%--__x64_sys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                           --99.38%--clear_huge_page
                                     |--98.40%--clear_page_erms
                                      --0.59%--__cond_resched
            0.03%     0.03%             1  fallocate  [kernel.kallsyms]  [k] __filemap_add_folio
========================= TESTING ======================================
This patch passes libhugetlbfs tests and LTP hugetlb tests
    Done executing testcases";1;1
"    LTP Version:  20220527-178-g2761a81c4
page migration was also tested using Mike Kravetz's test program.[8]";1;0
mm/migrate: fix do_pages_move for compat pointers;1;1
do_pages_move does not handle compat pointers for the page list;1;0
"
correctly";0;0
" Add in_compat_syscall check and appropriate get_user fetch
when iterating the page list";1;0
"It makes the syscall in compat mode (32-bit userspace, 64-bit kernel)
work the same way as the native 32-bit syscall again, restoring the
behavior before my broken commit 5b1b561ba73c (""mm: simplify
compat_sys_move_pages"")";1;1
"More specifically, my patch moved the parsing of the 'pages' array from
the main entry point into do_pages_stat(), which left the syscall
working correctly for the 'stat' operation (nodes = NULL), while the
'move' operation (nodes != NULL) is now missing the conversion and
interprets 'pages' as an array of 64-bit pointers instead of the
intended 32-bit userspace pointers";1;1
"It is possible that nobody noticed this bug because the few
applications that actually call move_pages are unlikely to run in
compat mode because of their large memory requirements, but this
clearly fixes a user-visible regression and should have been caught by
ltp.";0;1
mm: migrate: remove isolated variable in add_page_for_migration();1;1
"Directly check the return of isolate_hugetlb() and folio_isolate_lru() to
remove isolated variable, also setup err = -EBUSY in advance before
isolation, and update err only when successfully queued for migration,
which could help us to unify and simplify code a bit.";1;1
mm: migrate: remove PageHead() check for HugeTLB in add_page_for_migration();1;0
"There is some different between hugeTLB and THP behave when passed the
address of a tail page, for THP, it will migrate the entire THP page, but
for HugeTLB, it will return -EACCES, or -ENOENT before commit e66f17ff7177
(""mm/hugetlb: take page table lock in follow_huge_pmd()""),
  -EACCES The page is mapped by multiple processes and can be moved
	  only if MPOL_MF_MOVE_ALL is specified";0;0
  -ENOENT The page is not present;0;1
"But when check manual[1], both of the two errnos are not suitable, it is
better to keep the same behave between hugetlb and THP when passed the
address of a tail page, so let's just remove the PageHead() check for
HugeTLB.";1;1
mm: migrate: use a folio in add_page_for_migration();1;0
Use a folio in add_page_for_migration() to save compound_head() calls.;1;0
mm: migrate: use __folio_test_movable();1;1
Use __folio_test_movable(), no need to convert from folio to page again.;1;0
mm: migrate: convert migrate_misplaced_page() to migrate_misplaced_folio();1;0
"At present, numa balance only support base page and PMD-mapped THP, but we
will expand to support to migrate large folio/pte-mapped THP in the
future, it is better to make migrate_misplaced_page() to take a folio
instead of a page, and rename it to migrate_misplaced_folio(), it is a
preparation, also this remove several compound_head() calls.";1;1
mm: migrate: convert numamigrate_isolate_page() to numamigrate_isolate_folio();1;0
"Rename numamigrate_isolate_page() to numamigrate_isolate_folio(), then
make it takes a folio and use folio API to save compound_head() calls.";1;0
mm: migrate: remove THP mapcount check in numamigrate_isolate_page();1;1
"The check of THP mapped by multiple processes was introduced by commit
04fa5d6a6547 (""mm: migrate: check page_count of THP before migrating"") and
refactor by commit 340ef3902cf2 (""mm: numa: cleanup flow of transhuge page
migration""), which is out of date, since migrate_misplaced_page() is now
using the standard migrate_pages() for small pages and THPs, the reference
count checking is in folio_migrate_mapping(), so let's remove the special
check for THP.";1;1
mm: migrate: remove PageTransHuge check in numamigrate_isolate_page();1;1
"Patch series ""mm: migrate: more folio conversion and unification"", v3";1;1
"Convert more migrate functions to use a folio, it is also a preparation
for large folio migration support when balancing numa";0;1
This patch (of 8);1;0
"The assert VM_BUG_ON_PAGE(order && !PageTransHuge(page), page) is not very
useful,
   1) for a tail/base page, order = 0, for a head page, the order > 0 &&
      PageTransHuge() is true
   2) there is a PageCompound() check and only base page is handled in
      do_numa_page(), and do_huge_pmd_numa_page() only handle PMD-mapped
   3) even though the page is a tail page, isolate_lru_page() will post
      a warning, and fail to isolate the page
   4) if large folio/pte-mapped THP migration supported in the future,
      we could migrate the entire folio if numa fault on a tail page
so just remove the check.";1;1
mm/rmap: pass folio to hugepage_add_anon_rmap();1;0
"Let's pass a folio; we are always mapping the entire thing.";1;0
mm: hugetlb: add huge page size param to set_huge_pte_at();1;0
"Patch series ""Fix set_huge_pte_at() panic on arm64"", v2";1;1
"This series fixes a bug in arm64's implementation of set_huge_pte_at(),
which can result in an unprivileged user causing a kernel panic";0;1
" The
problem was triggered when running the new uffd poison mm selftest for
HUGETLB memory";0;0
" This test (and the uffd poison feature) was merged for
v6.5-rc7";0;0
"Ideally, I'd like to get this fix in for v6.6 and I've cc'ed stable
(correctly this time) to get it backported to v6.5, where the issue first
showed up";1;1
"Description of Bug
arm64's huge pte implementation supports multiple huge page sizes, some of
which are implemented in the page table with multiple contiguous entries";1;0
"
So set_huge_pte_at() needs to work out how big the logical pte is, so that
it can also work out how many physical ptes (or pmds) need to be written";0;1
"
It previously did this by grabbing the folio out of the pte and querying
its size";1;0
However, there are cases when the pte being set is actually a swap entry;0;0
"
But this also used to work fine, because for huge ptes, we only ever saw
migration entries and hwpoison entries";0;1
" And both of these types of swap
entries have a PFN embedded, so the code would grab that and everything
still worked out";0;0
"But over time, more calls to set_huge_pte_at() have been added that set
swap entry types that do not embed a PFN";0;0
" And this causes the code to go
bang";0;1
" The triggering case is for the uffd poison test, commit
99aa77215ad0 (""selftests/mm: add uffd unit test for UFFDIO_POISON""), which
causes a PTE_MARKER_POISONED swap entry to be set, coutesey of commit
8a13897fb0da (""mm: userfaultfd: support UFFDIO_POISON for hugetlbfs"") -
added in v6.5-rc7";1;0
" Although review shows that there are other call sites
that set PTE_MARKER_UFFD_WP (which also has no PFN), these don't trigger
on arm64 because arm64 doesn't support UFFD WP";0;1
"If CONFIG_DEBUG_VM is enabled, we do at least get a BUG(), but otherwise,
it will dereference a bad pointer in page_folio()";1;1
"    static inline struct folio *hugetlb_swap_entry_to_folio(swp_entry_t entry)
The simplest fix would have been to revert the dodgy cleanup commit
18f3962953e4 (""mm: hugetlb: kill set_huge_swap_pte_at()""), but since
things have moved on, this would have required an audit of all the new
set_huge_pte_at() call sites to see if they should be converted to
set_huge_swap_pte_at()";1;1
" As per the original intent of the change, it
would also leave us open to future bugs when people invariably get it
wrong and call the wrong helper";0;1
So instead, I've added a huge page size parameter to set_huge_pte_at();1;0
"
This means that the arm64 code has the size in all cases";1;1
" It's a bigger
change, due to needing to touch the arches that implement the function,
but it is entirely mechanical, so in my view, low risk";0;1
"I've compile-tested all touched arches; arm64, parisc, powerpc, riscv,
s390, sparc (and additionally x86_64)";1;0
" I've additionally booted and run
mm selftests against arm64, where I observe the uffd poison test is fixed,
and there are no other regressions";1;0
This patch (of 2);1;0
"In order to fix a bug, arm64 needs to be told the size of the huge page
for which the pte is being set in set_huge_pte_at()";1;0
" Provide for this by
adding an `unsigned long sz` parameter to the function";0;0
" This follows the
same pattern as huge_pte_clear()";1;1
"This commit makes the required interface modifications to the core mm as
well as all arches that implement this function (arm64, parisc, powerpc,
riscv, s390, sparc)";0;1
" The actual arm64 bug will be fixed in a separate
commit";1;0
No behavioral changes intended.;1;1
migrate: use folio_set_bh() instead of set_bh_page();1;0
This function was converted before folio_set_bh() existed;0;0
" Catch up to
the new API.";0;1
mm: merge folio_has_private()/filemap_release_folio() call pairs;0;1
"Patch series ""mm, netfs, fscache: Stop read optimisation when folio
removed from pagecache"", v7";0;0
"This fixes an optimisation in fscache whereby we don't read from the cache
for a particular file until we know that there's data there that we don't
have in the pagecache";0;1
" The problem is that I'm no longer using PG_fscache
(aka PG_private_2) to indicate that the page is cached and so I don't get
a notification when a cached page is dropped from the pagecache";1;1
"The first patch merges some folio_has_private() and
filemap_release_folio() pairs and introduces a helper,
folio_needs_release(), to indicate if a release is required";1;0
The second patch is the actual fix;1;1
" Following Willy's suggestions[1], it
adds an AS_RELEASE_ALWAYS flag to an address_space that will make
filemap_release_folio() always call ->release_folio(), even if
PG_private/PG_private_2 aren't set";0;1
" folio_needs_release() is altered to
add a check for this";0;1
This patch (of 2);1;0
Make filemap_release_folio() check folio_has_private();1;1
" Then, in most
cases, where a call to folio_has_private() is immediately followed by a
call to filemap_release_folio(), we can get rid of the test in the pair";0;1
"There are a couple of sites in mm/vscan.c that this can't so easily be
done";0;1
" In shrink_folio_list(), there are actually three cases (something
different is done for incompletely invalidated buffers), but
filemap_release_folio() elides two of them";0;1
"In shrink_active_list(), we don't have have the folio lock yet, so the
check allows us to avoid locking the page unnecessarily";1;1
"A wrapper function to check if a folio needs release is provided for those
places that still need to do it in the mm/ directory";1;1
" This will acquire
additional parts to the condition in a future patch";0;0
"After this, the only remaining caller of folio_has_private() outside of
mm/ is a check in fuse.";0;1
fs: add CONFIG_BUFFER_HEAD;1;0
"Add a new config option that controls building the buffer_head code, and
select it from all file systems and stacking drivers that need it";1;1
"For the block device nodes and alternative iomap based buffered I/O path
is provided when buffer_head support is not enabled, and iomap needs a
a small tweak to define the IOMAP_F_BUFFER_HEAD flag to 0 to not call
into the buffer_head code when it doesn't exist";0;1
Otherwise this is just Kconfig and ifdef changes.;1;1
mm: Make pte_mkwrite() take a VMA;1;0
"The x86 Shadow stack feature includes a new type of memory called shadow
stack";1;1
"This shadow stack memory has some unusual properties, which requires
some core mm changes to function properly";0;0
"One of these unusual properties is that shadow stack memory is writable,
but only in limited ways";0;1
"These limits are applied via a specific PTE
bit combination";1;0
"Nevertheless, the memory is writable, and core mm code
will need to apply the writable permissions in the typical paths that
call pte_mkwrite()";0;0
"Future patches will make pte_mkwrite() take a VMA, so
that the x86 implementation of it can know whether to create regular
writable or shadow stack mappings";1;1
But there are a couple of challenges to this;0;0
"Modifying the signatures of
each arch pte_mkwrite() implementation would be error prone because some
are generated with macros and would need to be re-implemented";1;0
"Also, some
pte_mkwrite() callers operate on kernel memory without a VMA";1;0
So this can be done in a three step process;1;1
"First pte_mkwrite() can be
renamed to pte_mkwrite_novma() in each arch, with a generic pte_mkwrite()
added that just calls pte_mkwrite_novma()";1;0
"Next callers without a VMA can
be moved to pte_mkwrite_novma()";0;1
"And lastly, pte_mkwrite() and all callers
can be changed to take/pass a VMA";1;0
"Previous work pte_mkwrite() renamed pte_mkwrite_novma() and converted
callers that don't have a VMA were to use pte_mkwrite_novma()";0;0
"So now
change pte_mkwrite() to take a VMA and change the remaining callers to
pass a VMA";1;0
Apply the same changes for pmd_mkwrite();1;1
No functional change.;1;0
mm: remove unnecessary pagevec includes;1;1
"These files no longer need pagevec.h, mostly due to function declarations
being moved out of it.";0;1
mm: fix shmem THP counters on migration;1;1
"The per node numa_stat values for shmem don't change on page migration for
THP";0;0
  grep shmem /sys/fs/cgroup/machine.slice/.../memory.numa_stat;1;0
"    shmem N0=1092616192 N1=10485760
    shmem_thp N0=1092616192 N1=10485760
  migratepages 9181 0 1";0;0
"    shmem N0=0 N1=1103101952
    shmem_thp N0=1092616192 N1=10485760
Fix that by updating shmem_thp counters likewise to shmem counters on page
migration.";0;1
mm: ptep_get() conversion;1;0
"Convert all instances of direct pte_t* dereferencing to instead use
ptep_get() helper";1;0
" This means that by default, the accesses change from a
C dereference to a READ_ONCE()";1;1
" This is technically the correct thing to
do since where pgtables are modified by HW (for access/dirty) they are
volatile and therefore we should always ensure READ_ONCE() semantics";1;1
"But more importantly, by always using the helper, it can be overridden by
the architecture to fully encapsulate the contents of the pte";0;1
" Arch code
is deliberately not converted, as the arch code knows best";1;0
" It is
intended that arch code (arm64) will override the default with its own
implementation that can (e.g.) hide certain bits from the core code, or
determine young/dirty status by mixing in state from another source";1;1
Conversion was done using Coccinelle;1;0
"// $ make coccicheck \
//          COCCI=ptepget.cocci \
//          SPFLAGS=""--include-headers"" \
//          MODE=patch
virtual patch
@ depends on patch @
+ ptep_get(v)
Then reviewed and hand-edited to avoid multiple unnecessary calls to
ptep_get(), instead opting to store the result of a single call in a
variable, where it is correct to do so";1;1
" This aims to negate any cost of
READ_ONCE() and will benefit arch-overrides that may be more complex";0;0
"Included is a fix for an issue in an earlier version of this patch that
was pointed out by kernel test robot";0;1
" The issue arose because config
MMU=n elides definition of the ptep helper functions, including
ptep_get()";0;1
" HUGETLB_PAGE=n configs still define a simple
huge_ptep_clear_flush() for linking purposes, which dereferences the ptep";0;1
"So when both configs are disabled, this caused a build error because
ptep_get() is not defined";1;1
" Fix by continuing to do a direct dereference
when MMU=n";1;1
" This is safe because for this config the arch code cannot be
trying to virtualize the ptes because none of the ptep helpers are
defined.";1;1
mm/various: give up if pte_offset_map[_lock]() fails;1;1
"Following the examples of nearby code, various functions can just give up
if pte_offset_map() or pte_offset_map_lock() fails";1;0
" And there's no need
for a preliminary pmd_trans_unstable() or other such check, since such
cases are now safely handled inside.";1;1
mm/migrate: remove cruft from migration_entry_wait()s;1;0
"migration_entry_wait_on_locked() does not need to take a mapped pte
pointer, its callers can do the unmap first";0;0
" Annotate it with
__releases(ptl) to reduce sparse warnings";0;1
Fold __migration_entry_wait_huge() into migration_entry_wait_huge();1;0
" Fold
__migration_entry_wait() into migration_entry_wait(), preferring the
tighter pte_offset_map_lock() to pte_offset_map() and pte_lockptr().";0;0
mm: convert migrate_pages() to work on folios;1;0
"Almost all of the callers & implementors of migrate_pages() were already
converted to use folios";0;1
" compaction_alloc() & compaction_free() are
trivial to convert a part of this patch and not worth splitting out.";0;0
migrate_pages_batch: simplify retrying and failure counting of large folios;1;1
"After recent changes to the retrying and failure counting in
migrate_pages_batch(), it was found that it's unnecessary to count
retrying and failure for normal, large, and THP folios separately";0;1
"
Because we don't use retrying and failure number of large folios directly";1;1
"So, in this patch, we simplified retrying and failure counting of large
folios via counting retrying and failure of normal and large folios
together";0;1
 This results in the reduced line number;0;1
"Previously, in migrate_pages_batch we need to track whether the source
folio is large/THP before splitting";0;0
" So is_large is used to cache
folio_test_large() result";0;1
" Now, we don't need that variable any more
because we don't count retrying and failure of large folios (only counting
that of THP folios)";1;1
" So, in this patch, is_large is removed to simplify
the code";1;1
This is just code cleanup, no functionality changes are expected.;1;1
migrate_pages: avoid blocking for IO in MIGRATE_SYNC_LIGHT;0;1
"The MIGRATE_SYNC_LIGHT mode is intended to block for things that will
finish quickly but not for things that will take a long time";0;1
" Exactly how
long is too long is not well defined, but waits of tens of milliseconds is
likely non-ideal";0;1
"When putting a Chromebook under memory pressure (opening over 90 tabs on a
4GB machine) it was fairly easy to see delays waiting for some locks in
the kcompactd code path of > 100 ms";0;0
" While the laptop wasn't amazingly
usable in this state, it was still limping along and this state isn't
something artificial";0;1
" Sometimes we simply end up with a lot of memory
pressure";1;1
"Putting the same Chromebook under memory pressure while it was running
Android apps (though not stressing them) showed a much worse result (NOTE";0;1
this was on a older kernel but the codepaths here are similar);0;0
" Android
apps on ChromeOS currently run from a 128K-block, zlib-compressed,
loopback-mounted squashfs disk";0;0
" If we get a page fault from something
backed by the squashfs filesystem we could end up holding a folio lock
while reading enough from disk to decompress 128K (and then decompressing
it using the somewhat slow zlib algorithms)";0;1
" That reading goes through
the ext4 subsystem (because it's a loopback mount) before eventually
ending up in the block subsystem";0;1
 This extra jaunt adds extra overhead;1;1
"
Without much work I could see cases where we ended up blocked on a folio
lock for over a second";0;1
" With more extreme memory pressure I could see up
to 25 seconds";0;0
"We considered adding a timeout in the case of MIGRATE_SYNC_LIGHT for the
two locks that were seen to be slow [1] and that generated much
discussion";1;1
" After discussion, it was decided that we should avoid waiting
for the two locks during MIGRATE_SYNC_LIGHT if they were being held for
IO";1;1
 We'll continue with the unbounded wait for the more full SYNC modes;1;0
"With this change, I couldn't see any slow waits on these locks with my
previous testcases";1;1
"NOTE: The reason I stated digging into this originally isn't because some
benchmark had gone awry, but because we've received in-the-field crash
reports where we have a hung task waiting on the page lock (which is the
equivalent code path on old kernels)";0;0
" While the root cause of those
crashes is likely unrelated and won't be fixed by this patch, analyzing
those crash reports did point out these very long waits seemed like
something good to fix";0;1
" With this patch we should no longer hang waiting
on these locks, but presumably the system will still be in a bad shape and
hang somewhere else.";1;1
mm: don't check VMA write permissions if the PTE/PMD indicates write permissions;1;0
"Staring at the comment ""Recheck VMA as permissions can change since
migration started"" in remove_migration_pte() can result in confusion,
because if the source PTE/PMD indicates write permissions, then there
should be no need to check VMA write permissions when restoring migration
entries or PTE-mapping a PMD";1;1
"Commit d3cb8bf6081b (""mm: migrate: Close race between migration completion
and mprotect"") introduced the maybe_mkwrite() handling in
remove_migration_pte() in 2014, stating that a race between mprotect() and
migration finishing would be possible, and that we could end up with a
writable PTE that should be readable";0;1
"However, mprotect() code first updates vma->vm_flags / vma->vm_page_prot
and then walks the page tables to (a) set all present writable PTEs to
read-only and (b) convert all writable migration entries to readable
migration entries";1;0
" While walking the page tables and modifying the
entries, migration code has to grab the PT locks to synchronize against
concurrent page table modifications";0;0
"Assuming migration would find a writable migration entry (while holding
the PT lock) and replace it with a writable present PTE, surely mprotect()
code didn't stumble over the writable migration entry yet (converting it
into a readable migration entry) and would instead wait for the PT lock to
convert the now present writable PTE into a read-only PTE";1;0
" As mprotect()
didn't finish yet, the behavior is just like migration didn't happen: a
writable PTE will be converted to a read-only PTE";1;0
"So it's fine to rely on the writability information in the source PTE/PMD
and not recheck against the VMA as long as we're holding the PT lock to
synchronize with anyone who concurrently wants to downgrade write
permissions (like mprotect()) by first adjusting vma->vm_flags /
vma->vm_page_prot to then walk over the page tables to adjust the page
table entries";0;0
"Running test cases that should reveal such races -- mprotect(PROT_READ)
racing with page migration or THP splitting -- for multiple hours did not
reveal an issue with this cleanup.";0;1
migrate_pages_batch: fix statistics for longterm pin retry;1;1
"In commit fd4a7ac32918 (""mm: migrate: try again if THP split is failed due
to page refcnt""), if the THP splitting fails due to page reference count,
we will retry to improve migration successful rate";1;1
" But the failed
splitting is counted as migration failure and migration retry, which will
cause duplicated failure counting";0;1
" So, in this patch, this is fixed via
undoing the failure counting if we decide to retry";1;1
" The patch is tested
via failure injection.";1;1
"mm/migrate: revert ""mm/migrate: fix wrongly apply write bit after mkdirty on sparc64""";1;1
"This reverts commit 96a9c287e25d (""mm/migrate: fix wrongly apply write bit
after mkdirty on sparc64"")";1;1
"Now that sparc64 mkdirty handling is fixed and no longer sets a PTE/PMD
writable that shouldn't be writable, let's revert the temporary fix";1;1
The mkdirty mm selftest still passes with this change on sparc64;1;0
"Note that loongarch handling was fixed in commit bf2f34a506e6 (""LoongArch:";1;0
mm/migrate: drop pte_mkhuge() in remove_migration_pte();1;0
"Since the following commit, arch_make_huge_pte() should be used directly
in generic memory subsystem as a platform provided page table helper,
instead of pte_mkhuge()";1;1
" This just drops pte_mkhuge() from
remove_migration_pte(), which has now become redundant";1;1
"'commit 16785bd77431 (""mm: merge pte_mkhuge() call into arch_make_huge_pte()"")'";1;0
mm: Introduce untagged_addr_remote();1;1
"untagged_addr() removes tags/metadata from the address and brings it to
the canonical form";1;1
The helper is implemented on arm64 and sparc;1;1
"Both of
them do untagging based on global rules";0;0
"However, Linear Address Masking (LAM) on x86 introduces per-process
settings for untagging";0;0
"As a result, untagged_addr() is now only
suitable for untagging addresses for the current proccess";1;1
"The new helper untagged_addr_remote() has to be used when the address
targets remote process";1;1
"It requires the mmap lock for target mm to be
taken.";0;0
migrate_pages: try migrate in batch asynchronously firstly;0;1
"When we have locked more than one folios, we cannot wait the lock or bit
(e.g., page lock, buffer head lock, writeback bit) synchronously";1;1
"
Otherwise deadlock may be triggered";0;1
" This make it hard to batch the
synchronous migration directly";1;1
"This patch re-enables batching synchronous migration via trying to migrate
in batch asynchronously firstly";1;1
" And any folios that are failed to be
migrated asynchronously will be migrated synchronously one by one";0;0
"Test shows that this can restore the TLB flushing batching performance for
synchronous migration effectively.";0;1
migrate_pages: move split folios processing out of migrate_pages_batch();0;0
To simplify the code logic and reduce the line number.;1;1
migrate_pages: fix deadlock in batched migration;1;1
"Patch series ""migrate_pages: fix deadlock in batched synchronous
migration"", v2";1;1
Two deadlock bugs were reported for the migrate_pages() batching series;0;1
"
Thanks Hugh and Pengfei";1;1
" Analysis shows that if we have locked some other
folios except the one we are migrating, it's not safe in general to wait
synchronously, for example, to wait the writeback to complete or wait to
lock the buffer head";0;0
"So 1/3 fixes the deadlock in a simple way, where the batching support for
the synchronous migration is disabled";1;1
" The change is straightforward and
easy to be understood";0;0
" While 3/3 re-introduce the batching for
synchronous migration via trying to migrate asynchronously in batch
optimistically, then fall back to migrate synchronously one by one for
fail-to-migrate folios";0;1
" Test shows that this can restore the TLB flushing
batching performance for synchronous migration effectively";0;1
This patch (of 3);1;0
Two deadlock bugs were reported for the migrate_pages() batching series;0;1
"
Thanks Hugh and Pengfei!  For example, in the following deadlock trace
snippet,
 INFO: task kworker/u4:0:9 blocked for more than 147 seconds";0;1
"       Not tainted 6.2.0-rc4-kvm+ #1314
 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";1;0
" task:kworker/u4:0    state:D stack:0     pid:9     ppid:2      flags:0x00004000
 Workqueue: loop4 loop_rootcg_workfn
  INFO: task repro:1023 blocked for more than 147 seconds";0;1
"       Not tainted 6.2.0-rc4-kvm+ #1314
 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";1;0
" task:repro           state:D stack:0     pid:1023  ppid:360    flags:0x00004004
 Call Trace";0;0
"  <TASK>
  __schedule+0x43b/0xd00
  schedule+0x6a/0xf0
  io_schedule+0x4a/0x80
  folio_wait_bit_common+0x1b5/0x4e0
  ? compaction_alloc+0x77/0x1150
  ? __pfx_wake_page_function+0x10/0x10
  folio_wait_bit+0x30/0x40
  folio_wait_writeback+0x2e/0x1e0
  migrate_pages_batch+0x555/0x1ac0
  ? __pfx_compaction_alloc+0x10/0x10
  ? __pfx_compaction_free+0x10/0x10
  ? __this_cpu_preempt_check+0x17/0x20
  ? lock_is_held_type+0xe6/0x140
  migrate_pages+0x100e/0x1180
  ? __pfx_compaction_free+0x10/0x10
  ? __pfx_compaction_alloc+0x10/0x10
  compact_zone+0xe10/0x1b50
  ? lock_is_held_type+0xe6/0x140
  ? check_preemption_disabled+0x80/0xf0
  compact_node+0xa3/0x100
  ? __sanitizer_cov_trace_const_cmp8+0x1c/0x30
  ? _find_first_bit+0x7b/0x90
  sysctl_compaction_handler+0x5d/0xb0
  proc_sys_call_handler+0x29d/0x420
  proc_sys_write+0x2b/0x40
  vfs_write+0x3a3/0x780
  ksys_write+0xb7/0x180
  __x64_sys_write+0x26/0x30
  do_syscall_64+0x3b/0x90
  entry_SYSCALL_64_after_hwframe+0x72/0xdc
 RIP: 0033:0x7f3a2471f59d
 RSP: 002b:00007ffe567f7288 EFLAGS: 00000217 ORIG_RAX: 0000000000000001
 RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f3a2471f59d
 RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000005
 RBP: 00007ffe567f72a0 R08: 0000000000000010 R09: 0000000000000010
 R10: 0000000000000010 R11: 0000000000000217 R12: 00000000004012e0
 R13: 00007ffe567f73e0 R14: 0000000000000000 R15: 0000000000000000
  </TASK>
The page migration task has held the lock of the shmem folio A, and is
waiting the writeback of the folio B of the file system on the loop block
device to complete";1;0
" While the loop worker task which writes back the
folio B is waiting to lock the shmem folio A, because the folio A backs
the folio B in the loop device";1;0
 Thus deadlock is triggered;0;1
"In general, if we have locked some other folios except the one we are
migrating, it's not safe to wait synchronously, for example, to wait the
writeback to complete or wait to lock the buffer head";0;0
"To fix the deadlock, in this patch, we avoid to batch the page migration
except for MIGRATE_ASYNC mode";1;1
" In MIGRATE_ASYNC mode, synchronous waiting
is avoided";1;0
The fix can be improved further;1;1
 We will do that as soon as possible.;0;0
mm: avoid gcc complaint about pointer casting;1;1
"The migration code ends up temporarily stashing information of the wrong
type in unused fields of the newly allocated destination folio";1;1
" That
all works fine, but gcc does complain about the pointer type mis-use";1;1
    mm/migrate.c: In function ‘__migrate_folio_extract’;0;1
"    mm/migrate.c:1050:20: note: randstruct: casting between randomized structure pointer types (ssa): ‘struct anon_vma’ and ‘struct address_space’
and gcc is actually right to complain since it really doesn't understand
that this is a very temporary special case where this is ok";0;1
"This could be fixed in different ways by just obfuscating the assignment
sufficiently that gcc doesn't see what is going on, but the truly
""proper C"" way to do this is by explicitly using a union";0;1
"Using unions for type conversions like this is normally hugely ugly and
syntactically nasty, but this really is one of the few cases where we
want to make it clear that we're not doing type conversion, we're really
re-using the value bit-for-bit just using another type";1;1
"IOW, this should not become a common pattern, but in this one case using
that odd union is probably the best way to document to the compiler what
is conceptually going on here";1;1
"[ Side note: there are valid cases where we convert pointers to other
  pointer types, notably the whole ""folio vs page"" situation, where the
  types actually have fundamental commonalities";0;1
"  The fact that the gcc note is limited to just randomized structures
  means that we don't see equivalent warnings for those cases, but it
  migth also mean that we miss other cases where we do play these kinds
  of dodgy games, and this kind of explicit conversion might be a good
  idea";0;1
"]
I verified that at least for an allmodconfig build on x86-64, this
generates the exact same code, apart from line numbers and assembler
comment changes";1;1
"Fixes: 64c8902ed441 (""migrate_pages: split unmap_and_move() to _unmap() and _move()"")";0;1
mm: change to return bool for isolate_movable_page();0;0
"Now the isolate_movable_page() can only return 0 or -EBUSY, and no users
will care about the negative return value, thus we can convert the
isolate_movable_page() to return a boolean value to make the code more
clear when checking the movable page isolation state";1;1
No functional changes intended.;1;0
mm: hugetlb: change to return bool for isolate_hugetlb();0;0
"Now the isolate_hugetlb() only returns 0 or -EBUSY, and most users did not
care about the negative value, thus we can convert the isolate_hugetlb()
to return a boolean value to make code more clear when checking the
hugetlb isolation state";1;1
" Moreover converts 2 users which will consider
the negative value returned by isolate_hugetlb()";0;1
No functional changes intended.;1;0
mm: change to return bool for isolate_lru_page();1;0
"The isolate_lru_page() can only return 0 or -EBUSY, and most users did not
care about the negative error of isolate_lru_page(), except one user in
add_page_for_migration()";1;0
" So we can convert the isolate_lru_page() to
return a boolean value, which can help to make the code more clear when
checking the return value of isolate_lru_page()";1;1
Also convert all users' logic of checking the isolation state;1;1
No functional changes intended.;1;0
mm/migrate: fix wrongly apply write bit after mkdirty on sparc64;1;1
"Nick Bowler reported another sparc64 breakage after the young/dirty
persistent work for page migration (per ""Link:"" below)";0;1
" That's after a
similar report [2]";0;0
"It turns out page migration was overlooked, and it wasn't failing before
because page migration was not enabled in the initial report test
environment";0;1
"David proposed another way [2] to fix this from sparc64 side, but that
patch didn't land somehow";1;1
" Neither did I check whether there's any other
arch that has similar issues";0;0
"Let's fix it for now as simple as moving the write bit handling to be
after dirty, like what we did before";1;1
"Note: this is based on mm-unstable, because the breakage was since 6.1 and
we're at a very late stage of 6.2 (-rc8), so I assume for this specific
case we should target this at 6.3.";1;1
migrate_pages: move THP/hugetlb migration support check to simplify code;0;1
This is a code cleanup patch, no functionality change is expected;1;1
" After
the change, the line number reduces especially in the long
migrate_pages_batch().";0;1
migrate_pages: batch flushing TLB;0;0
"The TLB flushing will cost quite some CPU cycles during the folio
migration in some situations";0;1
" For example, when migrate a folio of a
process with multiple active threads that run on multiple CPUs";1;0
" After
batching the _unmap and _move in migrate_pages(), the TLB flushing can be
batched easily with the existing TLB flush batching mechanism";0;0
" This patch
implements that";1;0
We use the following test case to test the patch;0;1
"On a 2-socket Intel server,
- Run pmbench memory accessing benchmark
- Run `migratepages` to migrate pages of pmbench between node 0 and
  node 1 back and forth";0;0
"With the patch, the TLB flushing IPI reduces 99.1% during the test and the
number of pages migrated successfully per second increases 291.7%";0;1
"Haoxin helped to test the patchset on an ARM64 server with 128 cores, 2
NUMA nodes";0;0
" Test results show that the page migration performance
increases up to 78%";1;1
NOTE: TLB flushing is batched only for normal folios, not for THP folios;0;0
"
Because the overhead of TLB flushing for THP folios is much lower than
that for normal folios (about 1/512 on x86 platform).";0;0
migrate_pages: share more code between _unmap and _move;0;1
"This is a code cleanup patch to reduce the duplicated code between the
_unmap and _move stages of migrate_pages()";1;1
" No functionality change is
expected.";1;1
migrate_pages: move migrate_folio_unmap();0;0
Just move the position of the functions;1;0
" There's no any functionality
change";1;0
" This is to make it easier to review the next patch via putting
code near its position in the next patch.";1;1
migrate_pages: batch _unmap and _move;0;0
"In this patch the _unmap and _move stage of the folio migration is
batched";1;0
" That for, previously, it is,
  for each folio
    _unmap()
    _move()
Now, it is,
  for each folio
    _unmap()
  for each folio
    _move()
Based on this, we can batch the TLB flushing and use some hardware
accelerator to copy folios between batched _unmap and batched _move
stages.";0;1
migrate_pages: split unmap_and_move() to _unmap() and _move();0;0
This is a preparation patch to batch the folio unmapping and moving;1;1
"In this patch, unmap_and_move() is split to migrate_folio_unmap() and
migrate_folio_move()";1;1
" So, we can batch _unmap() and _move() in different
loops later";0;1
" To pass some information between unmap and move, the
original unused dst->mapping and dst->private are used.";0;1
migrate_pages: restrict number of pages to migrate in batch;0;1
"This is a preparation patch to batch the folio unmapping and moving for
non-hugetlb folios";1;1
"If we had batched the folio unmapping, all folios to be migrated would be
unmapped before copying the contents and flags of the folios";1;0
" If the
folios that were passed to migrate_pages() were too many in unit of pages,
the execution of the processes would be stopped for too long time, thus
too long latency";0;1
" For example, migrate_pages() syscall will call
migrate_pages() with all folios of a process";0;0
" To avoid this possible
issue, in this patch, we restrict the number of pages to be migrated to be
no more than HPAGE_PMD_NR";1;1
" That is, the influence is at the same level of
THP migration.";0;1
migrate_pages: separate hugetlb folios migration;1;1
"This is a preparation patch to batch the folio unmapping and moving for
the non-hugetlb folios";1;1
" Based on that we can batch the TLB shootdown
during the folio migration and make it possible to use some hardware
accelerator for the folio copying";0;1
"In this patch the hugetlb folios and non-hugetlb folios migration is
separated in migrate_pages() to make it easy to change the non-hugetlb
folios migration implementation.";1;1
migrate_pages: organize stats with struct migrate_pages_stats;0;0
"Patch series ""migrate_pages(): batch TLB flushing"", v5";0;0
"Now, migrate_pages() migrates folios one by one, like the fake code as
follows,
  for each folio
    unmap
    flush TLB
    copy
    restore map
If multiple folios are passed to migrate_pages(), there are opportunities
to batch the TLB flushing and copying";1;0
" That is, we can change the code to
something as follows,
  for each folio
    unmap
  for each folio
    flush TLB
  for each folio
    copy
  for each folio
    restore map
The total number of TLB flushing IPI can be reduced considerably";0;0
" And we
may use some hardware accelerator such as DSA to accelerate the folio
copying";0;1
"So in this patch, we refactor the migrate_pages() implementation and
implement the TLB flushing batching";1;1
" Base on this, hardware accelerated
folio copying can be implemented";0;1
"If too many folios are passed to migrate_pages(), in the naive batched
implementation, we may unmap too many folios at the same time";1;1
" The
possibility for a task to wait for the migrated folios to be mapped again
increases";0;0
 So the latency may be hurt;1;1
" To deal with this issue, the max
number of folios be unmapped in batch is restricted to no more than
HPAGE_PMD_NR in the unit of page";1;1
" That is, the influence is at the same
level of THP migration";0;1
"We use the following test to measure the performance impact of the
patchset,
On a 2-socket Intel server,
 - Run pmbench memory accessing benchmark
 - Run `migratepages` to migrate pages of pmbench between node 0 and
   node 1 back and forth";0;0
"With the patch, the TLB flushing IPI reduces 99.1% during the test and
the number of pages migrated successfully per second increases 291.7%";0;1
"Xin Hao helped to test the patchset on an ARM64 server with 128 cores,
2 NUMA nodes";0;0
" Test results show that the page migration performance
increases up to 78%";1;1
This patch (of 9);1;0
"Define struct migrate_pages_stats to organize the various statistics in
migrate_pages()";1;0
" This makes it easier to collect and consume the
statistics in multiple functions";0;1
" This will be needed in the following
patches in the series.";1;1
mm/migrate: convert putback_movable_pages() to use folios;1;0
"Removes 6 calls to compound_head(), and replaces putback_movable_page()
with putback_movable_folio() as well.";1;1
mm/migrate: convert isolate_movable_page() to use folios;1;0
"Removes 6 calls to compound_head() and prepares the function to take in a
folio instead of page argument.";1;1
mm/migrate: add folio_movable_ops();1;1
"folio_movable_ops() does the same as page_movable_ops() except uses folios
instead of pages";0;0
" This function will help make folio conversions in
migrate.c more readable.";0;1
mm/hugetlb: convert putback_active_hugepage to take in a folio;1;0
"Convert putback_active_hugepage() to folio_putback_active_hugetlb(), this
removes one user of the Huge Page macros which take in a page";1;0
" The
callers in migrate.c are also cleaned up by being able to directly use the
src and dst folio variables.";0;1
mm/hugetlb: convert alloc_migrate_huge_page to folios;1;0
"Change alloc_huge_page_nodemask() to alloc_hugetlb_folio_nodemask() and
alloc_migrate_huge_page() to alloc_migrate_hugetlb_folio()";0;0
" Both
functions now return a folio rather than a page.";0;0
mm/hugetlb: convert isolate_hugetlb to folios;1;0
"Patch series ""continue hugetlb folio conversion"", v3";1;1
"This series continues the conversion of core hugetlb functions to use
folios";0;0
"This series converts many helper funtions in the hugetlb fault
path";0;1
"This is in preparation for another series to convert the hugetlb
fault code paths to operate on folios";0;0
This patch (of 8);1;0
"Convert isolate_hugetlb() to take in a folio and convert its callers to
pass a folio";1;0
" Use page_folio() to convert the callers to use a folio is
safe as isolate_hugetlb() operates on a head page.";1;1
mm: mlock: update the interface to use folios;1;1
"Update the mlock interface to accept folios rather than pages, bringing
the interface in line with the internal implementation";0;1
"munlock_vma_page() still requires a page_folio() conversion, however this
is consistent with the existent mlock_vma_page() implementation and a
product of rmap still dealing in pages rather than folios.";1;1
mm/hugetlb: move swap entry handling into vma lock when faulted;1;0
"In hugetlb_fault(), there used to have a special path to handle swap entry
at the entrance using huge_pte_offset()";1;0
" That's unsafe because
huge_pte_offset() for a pmd sharable range can access freed pgtables if
without any lock to protect the pgtable from being freed after pmd
unshare";0;0
"Here the simplest solution to make it safe is to move the swap handling to
be after the vma lock being held";1;1
" We may need to take the fault mutex on
either migration or hwpoison entries now (also the vma lock, but that's
really needed), however neither of them is hot path";0;1
"Note that the vma lock cannot be released in hugetlb_fault() when the
migration entry is detected, because in migration_entry_wait_huge() the
pgtable page will be used again (by taking the pgtable lock), so that also
so that it must be called with vma read lock held, and properly release
the lock in __migration_entry_wait_huge().";1;1
mm: export buffer_migrate_folio_norefs();0;0
"Ext4 needs this function to allow safe migration for journalled data
pages.";1;1
mm/migrate.c: stop using 0 as NULL pointer;0;0
mm/migrate.c:1198:24: warning: Using plain integer as NULL pointer;0;0
migrate: convert migrate_pages() to use folios;1;0
"Quite straightforward, the page functions are converted to corresponding
folio functions";0;0
 Same for comments;0;1
THP specific code are converted to be large folio.;0;0
migrate: convert unmap_and_move() to use folios;1;0
"Patch series ""migrate: convert migrate_pages()/unmap_and_move() to use
folios"", v2";1;0
"The conversion is quite straightforward, just replace the page API to the
corresponding folio API";1;1
" migrate_pages() and unmap_and_move() mostly work
with folios (head pages) only";0;0
This patch (of 2);1;0
"Quite straightforward, the page functions are converted to corresponding
folio functions";0;0
 Same for comments.;0;1
"Revert ""mm: migration: fix the FOLL_GET failure on following huge page""";1;1
"Revert commit 831568214883 (""mm: migration: fix the FOLL_GET failure on
following huge page""), since after commit 1a6baaa0db73 (""s390/hugetlb";1;1
"switch to generic version of follow_huge_pud()"") and commit 57a196a58421
(""hugetlb: simplify hugetlb handling in follow_page_mask"") were merged,
now all the following huge page routines can support FOLL_GET operation.";1;1
mm/hugetlb: convert move_hugetlb_state() to folios;1;0
"Clean up unmap_and_move_huge_page() by converting move_hugetlb_state() to
take in folios.";0;0
mm/migrate: make isolate_movable_page() skip slab pages;1;0
"In the next commit we want to rearrange struct slab fields to allow a larger
rcu_head";1;1
"Afterwards, the page->mapping field will overlap with SLUB's ""struct
list_head slab_list"", where the value of prev pointer can become LIST_POISON2,
which is 0x122 + POISON_POINTER_DELTA";1;1
" Unfortunately the bit 1 being set can
confuse PageMovable() to be a false positive and cause a GPF as reported by lkp
To fix this, make isolate_movable_page() skip pages with the PageSlab flag set";1;1
"This is a bit tricky as we need to add memory barriers to SLAB and SLUB's page
allocation and freeing, and their counterparts to isolate_movable_page()";1;1
Based on my RFC from [2];0;0
"Added a comment update from Matthew's variant in [3]
and, as done there, moved the PageSlab checks to happen before trying to take
the page lock.";0;0
mm: migrate: try again if THP split is failed due to page refcnt;1;1
"When creating a virtual machine, we will use memfd_create() to get a file
descriptor which can be used to create share memory mappings using the
mmap function, meanwhile the mmap() will set the MAP_POPULATE flag to
allocate physical pages for the virtual machine";1;0
"When allocating physical pages for the guest, the host can fallback to
allocate some CMA pages for the guest when over half of the zone's free
memory is in the CMA area";0;1
"In guest os, when the application wants to do some data transaction with
DMA, our QEMU will call VFIO_IOMMU_MAP_DMA ioctl to do longterm-pin and
create IOMMU mappings for the DMA pages";1;1
" However, when calling
VFIO_IOMMU_MAP_DMA ioctl to pin the physical pages, we found it will be
failed to longterm-pin sometimes";1;1
"After some invetigation, we found the pages used to do DMA mapping can
contain some CMA pages, and these CMA pages will cause a possible failure
of the longterm-pin, due to failed to migrate the CMA pages";0;1
" The reason
of migration failure may be temporary reference count or memory allocation
failure";0;1
" So that will cause the VFIO_IOMMU_MAP_DMA ioctl returns error,
which makes the application failed to start";1;1
"I observed one migration failure case (which is not easy to reproduce) is
that, the 'thp_migration_fail' count is 1 and the 'thp_split_page_failed'
count is also 1";1;0
"That means when migrating a THP which is in CMA area, but can not allocate
a new THP due to memory fragmentation, so it will split the THP";1;1
" However
THP split is also failed, probably the reason is temporary reference count
of this THP";0;1
" And the temporary reference count can be caused by dropping
page caches (I observed the drop caches operation in the system), but we
can not drop the shmem page caches due to they are already dirty at that
time";1;1
"Especially for THP split failure, which is caused by temporary reference
count, we can try again to mitigate the failure of migration in this case
according to previous discussion [1].";0;1
mm/hugetlb: add folio_hstate();1;1
Helper function to retrieve hstate information from a hugetlb folio.;0;1
mm: migrate: fix return value if all subpages of THPs are migrated successfully;1;1
"During THP migration, if THPs are not migrated but they are split and all
subpages are migrated successfully, migrate_pages() will still return the
number of THP pages that were not migrated";0;1
" This will confuse the callers
of migrate_pages()";1;0
" For example, the longterm pinning will failed though
all pages are migrated successfully";0;1
"Thus we should return 0 to indicate that all pages are migrated in this
case";0;1
mm/memory.c: fix race when faulting a device private page;1;1
"Patch series ""Fix several device private page reference counting issues"",
This series aims to fix a number of page reference counting issues in
drivers dealing with device private ZONE_DEVICE pages";1;1
" These result in
use-after-free type bugs, either from accessing a struct page which no
longer exists because it has been removed or accessing fields within the
struct page which are no longer valid because the page has been freed";1;1
During normal usage it is unlikely these will cause any problems;0;0
" However
without these fixes it is possible to crash the kernel from userspace";0;1
"
These crashes can be triggered either by unloading the kernel module or
unbinding the device from the driver prior to a userspace task exiting";0;0
"
In modules such as Nouveau it is also possible to trigger some of these
issues by explicitly closing the device file-descriptor prior to the task
exiting and then accessing device private memory";0;1
This involves some minor changes to both PowerPC and AMD GPU code;1;1
"
Unfortunately I lack hardware to test either of those so any help there
would be appreciated";0;1
" The changes mimic what is done in for both Nouveau
and hmm-tests though so I doubt they will cause problems";1;1
This patch (of 8);1;0
"When the CPU tries to access a device private page the migrate_to_ram()
callback associated with the pgmap for the page is called";0;1
" However no
reference is taken on the faulting page";0;0
" Therefore a concurrent migration
of the device private page can free the page and possibly the underlying
pgmap";0;1
" This results in a race which can crash the kernel due to the
migrate_to_ram() function pointer becoming invalid";0;1
" It also means drivers
can't reliably read the zone_device_data field because the page may have
been freed with memunmap_pages()";0;1
"Close the race by getting a reference on the page while holding the ptl to
ensure it has not been freed";0;1
" Unfortunately the elevated reference count
will cause the migration required to handle the fault to fail";1;1
" To avoid
this failure pass the faulting page into the migrate_vma functions so that
if an elevated reference count is found it can be checked to see if it's
expected or not.";0;1
mm: convert page_get_anon_vma() to folio_get_anon_vma();1;0
"With all callers now passing in a folio, rename the function and convert
all callers";1;0
" Removes a couple of calls to compound_head() and a reference
to page->mapping.";0;1
migrate: convert unmap_and_move_huge_page() to use folios;1;0
"Saves several calls to compound_head() and removes a couple of uses of
page->lru.";0;1
migrate: convert __unmap_and_move() to use folios;1;0
Removes a lot of calls to compound_head();1;0
" Also remove a VM_BUG_ON that
can never trigger as the PageAnon bit is the bottom bit of page->mapping.";1;1
mm: fix the handling Non-LRU pages returned by follow_page;1;1
"The handling Non-LRU pages returned by follow_page() jumps directly, it
doesn't call put_page() to handle the reference count, since 'FOLL_GET'
flag for follow_page() has get_page() called";1;1
" Fix the zone device page
check by handling the page reference count correctly before returning";1;1
"And as David reviewed, ""device pages are never PageKsm pages""";0;0
" Drop this
zone device page check for break_ksm()";1;1
"Since the zone device page can't be a transparent huge page, so drop the
redundant zone device page check for split_huge_pages_pid()";1;1
 (by Miaohe);0;0
mm/demotion: update node_is_toptier to work with memory tiers;0;1
"With memory tier support we can have memory only NUMA nodes in the top
tier from which we want to avoid promotion tracking NUMA faults";1;1
" Update
node_is_toptier to work with memory tiers";0;1
" All NUMA nodes are by default
top tier nodes";0;0
" With lower(slower) memory tiers added we consider all
memory tiers above a memory tier having CPU NUMA nodes as a top memory
tier";0;1
mm/demotion: build demotion targets based on explicit memory tiers;1;1
"This patch switch the demotion target building logic to use memory tiers
instead of NUMA distance";1;1
" All N_MEMORY NUMA nodes will be placed in the
default memory tier and additional memory tiers will be added by drivers
like dax kmem";0;0
"This patch builds the demotion target for a NUMA node by looking at all
memory tiers below the tier to which the NUMA node belongs";0;0
" The closest
node in the immediately following memory tier is used as a demotion
target";0;0
"Since we are now only building demotion target for N_MEMORY NUMA nodes the
CPU hotplug calls are removed in this patch.";0;0
mm/demotion: move memory demotion related code;1;1
This moves memory demotion related code to mm/memory-tiers.c;0;1
" No
functional change in this patch.";1;0
mm: migrate: do not retry 10 times for the subpages of fail-to-migrate THP;1;1
"If THP is failed to migrate due to -ENOSYS or -ENOMEM case, the THP will
be split, and the subpages of fail-to-migrate THP will be tried to migrate
again, so we should not account the retry counter in the second loop,
since we already accounted 'nr_thp_failed' in the first loop";1;1
"Moreover we also do not need retry 10 times for -EAGAIN case for the
subpages of fail-to-migrate THP in the second loop, since we already
regarded the THP as migration failure, and save some migration time (for
the worst case, will try 512 * 10 times) according to previous discussion";1;1
migrate_pages(): fix failure counting for retry;1;1
"After 10 retries, we will give up and the remaining pages will be counted
as failure in nr_failed and nr_thp_failed";1;1
" We should count the failure in
nr_failed_pages too";1;1
 This is done in this patch.;1;0
migrate_pages(): fix failure counting for THP splitting;1;1
If THP is failed to be migrated, it may be split and retry;1;1
" But after
splitting, the head page will be left in ""from"" list, although THP
migration failure has been counted already";0;1
" If the head page is failed to
be migrated too, the failure will be counted twice incorrectly";1;1
" So this
is fixed in this patch via moving the head page of THP after splitting to
""thp_split_pages"" too.";1;1
migrate_pages(): fix failure counting for THP on -ENOSYS;1;1
"If THP or hugetlbfs page migration isn't supported, unmap_and_move() or
unmap_and_move_huge_page() will return -ENOSYS";0;0
" For THP, splitting will
be tried, but if splitting doesn't succeed, the THP will be left in ""from""
list wrongly";1;0
" If some other pages are retried, the THP migration failure
will counted again";0;1
" This is fixed via moving the failure THP from ""from""
to ""ret_pages""";1;1
"Another issue of the original code is that the unsupported failure
processing isn't consistent between THP and hugetlbfs page";0;1
" Make them
consistent in this patch to make the code easier to be understood too.";1;1
migrate_pages(): fix failure counting for THP subpages retrying;1;1
"If THP is failed to be migrated for -ENOSYS and -ENOMEM, the THP will be
split into thp_split_pages, and after other pages are migrated, pages in
thp_split_pages will be migrated with no_subpage_counting == true, because
its failure have been counted already";0;1
" If some pages in thp_split_pages
are retried during migration, we should not count their failure if
no_subpage_counting == true too";1;1
" This is done this patch to fix the
failure counting for THP subpages retrying.";1;1
migrate_pages(): fix THP failure counting for -ENOMEM;1;1
"In unmap_and_move(), if the new THP cannot be allocated, -ENOMEM will be
returned, and migrate_pages() will try to split the THP unless ""reason"" is
MR_NUMA_MISPLACED (that is, nosplit == true)";0;1
" But when nosplit == true,
the THP migration failure will not be counted";0;1
"This is incorrect, so in this patch, the THP migration failure will be
counted for -ENOMEM regardless of nosplit is true or false";0;1
" The nr_failed
counting isn't fixed because it's not used";1;1
" Added some comments for it
per Baolin's suggestion.";1;1
migrate_pages(): remove unnecessary list_safe_reset_next();1;1
"Before commit b5bade978e9b (""mm: migrate: fix the return value of
migrate_pages()""), the tail pages of THP will be put in the ""from""
list directly";1;1
" So one of the loop cursors (page2) needs to be reset,
as is done in try_split_thp() via list_safe_reset_next()";1;1
" But after
the commit, the tail pages of THP will be put in a dedicated
list (thp_split_pages)";1;0
" That is, the ""from"" list will not be changed
during splitting";0;0
" So, it's unnecessary to call list_safe_reset_next()
anymore";1;1
This is a code cleanup, no functionality changes are expected.;1;1
migrate: fix syscall move_pages() return value for failure;1;1
"Patch series ""migrate_pages(): fix several bugs in error path"", v3";1;1
"During review the code of migrate_pages() and build a test program for
it";1;1
" Several bugs in error path are identified and fixed in this
series";0;0
"Most patches are tested via
- Apply error-inject.patch in Linux kernel
- Compile test-migrate.c (with -lnuma)
- Test with test-migrate.sh
error-inject.patch, test-migrate.c, and test-migrate.sh are as below";1;0
"It turns out that error injection is an important tool to fix bugs in
error path";0;1
This patch (of 8);1;0
"The return value of move_pages() syscall is incorrect when counting
the remaining pages to be migrated";0;0
" For example, for the following
test program,
 #define _GNU_SOURCE
 #include <stdbool.h>
 #include <stdio.h>
 #include <string.h>
 #include <stdlib.h>
 #include <errno.h>
 #include <fcntl.h>
 #include <sys/uio.h>
 #include <sys/mman.h>
 #include <sys/types.h>
 #include <unistd.h>
 #include <numaif.h>
 #include <numa.h>
 #ifndef MADV_FREE
 #endif
 #define ONE_MB		(1024 * 1024)
 #define MAP_SIZE	(16 * ONE_MB)
 #define THP_SIZE	(2 * ONE_MB)
 #define THP_MASK	(THP_SIZE - 1)
 #define ERR_EXIT_ON(cond, msg)					\
	 do {							\
		 int __cond_in_macro = (cond);			\
		 if (__cond_in_macro)				\
			 error_exit(__cond_in_macro, (msg));	\
	 } while (0)
 void error_msg(int ret, int nr, int *status, const char *msg)
	 fprintf(stderr, ""Error: %s, ret : %d, error: %s\n"",
	 for (i = 0; i < nr; i++)
 void error_exit(int ret, const char *msg)
 void prepare()
	 addr = mmap(NULL, MAP_SIZE, PROT_READ | PROT_WRITE,
 void test_migrate()
 int main(int argc, char *argv[])
The output of the current kernel is,
Error: move 1 page, ret : 0, error: Success
status: 1
Error: move 2 pages, page 1 not mapped, ret : 0, error: Success
status: 1 -14
Error: move 2 pages, ret : 0, error: Success
status: 1 1
Error: move 2 pages, page 1 to node 0, ret : 0, error: Success
status: 1 0
Make page 0 cannot be migrated";0;0
"Error: move 1 page, ret : 0, error: Success
status: 1024
Error: move 2 pages, page 1 not mapped, ret : 1, error: Success
status: 1024 -14
Error: move 2 pages, ret : 0, error: Success
status: 1024 1024
Error: move 2 pages, page 1 to node 0, ret : 1, error: Success
status: 1024 1024
While the expected output is,
Error: move 1 page, ret : 0, error: Success
status: 1
Error: move 2 pages, page 1 not mapped, ret : 0, error: Success
status: 1 -14
Error: move 2 pages, ret : 0, error: Success
status: 1 1
Error: move 2 pages, page 1 to node 0, ret : 0, error: Success
status: 1 0
Make page 0 cannot be migrated";0;0
"Error: move 1 page, ret : 1, error: Success
status: 1024
Error: move 2 pages, page 1 not mapped, ret : 1, error: Success
status: 1024 -14
Error: move 2 pages, ret : 1, error: Success
status: 1024 1024
Error: move 2 pages, page 1 to node 0, ret : 2, error: Success
status: 1024 1024
Fix this via correcting the remaining pages counting";0;1
" With the fix,
the output for the test program as above is expected.";1;1
mm: remember young/dirty bit for page migrations;0;1
"When page migration happens, we always ignore the young/dirty bit settings
in the old pgtable, and marking the page as old in the new page table
using either pte_mkold() or pmd_mkold(), and keeping the pte clean";1;1
"That's fine from functional-wise, but that's not friendly to page reclaim
because the moving page can be actively accessed within the procedure";0;1
"
Not to mention hardware setting the young bit can bring quite some
overhead on some systems, e.g";1;1
" x86_64 needs a few hundreds nanoseconds to
set the bit";1;0
" The same slowdown problem to dirty bits when the memory is
first written after page migration happened";0;1
"Actually we can easily remember the A/D bit configuration and recover the
information after the page is migrated";0;1
" To achieve it, define a new set
of bits in the migration swap offset field to cache the A/D bits for old
pte";1;0
" Then when removing/recovering the migration entry, we can recover
the A/D bits even if the page changed";1;1
"One thing to mention is that here we used max_swapfile_size() to detect
how many swp offset bits we have, and we'll only enable this feature if we
know the swp offset is big enough to store both the PFN value and the A/D
bits";1;1
 Otherwise the A/D bits are dropped like before.;0;0
memory tiering: hot page selection with hint page fault latency;1;0
"Patch series ""memory tiering: hot page selection"", v4";1;1
"To optimize page placement in a memory tiering system with NUMA balancing,
the hot pages in the slow memory nodes need to be identified";0;0
"
Essentially, the original NUMA balancing implementation selects the mostly
recently accessed (MRU) pages to promote";1;0
" But this isn't a perfect
algorithm to identify the hot pages";1;1
" Because the pages with quite low
access frequency may be accessed eventually given the NUMA balancing page
table scanning period could be quite long (e.g";0;1
 60 seconds);0;1
" So in this
patchset, we implement a new hot page identification algorithm based on
the latency between NUMA balancing page table scanning and hint page
fault";1;1
 Which is a kind of mostly frequently accessed (MFU) algorithm;0;1
"In NUMA balancing memory tiering mode, if there are hot pages in slow
memory node and cold pages in fast memory node, we need to promote/demote
hot/cold pages between the fast and cold memory nodes";0;1
A choice is to promote/demote as fast as possible;1;0
" But the CPU cycles and
memory bandwidth consumed by the high promoting/demoting throughput will
hurt the latency of some workload because of accessing inflating and slow
memory bandwidth contention";0;1
"A way to resolve this issue is to restrict the max promoting/demoting
throughput";0;1
 It will take longer to finish the promoting/demoting;0;1
" But
the workload latency will be better";0;1
" This is implemented in this patchset
as the page promotion rate limit mechanism";1;1
"The promotion hot threshold is workload and system configuration
dependent";0;0
" So in this patchset, a method to adjust the hot threshold
automatically is implemented";1;1
" The basic idea is to control the number of
the candidate promotion pages to match the promotion rate limit";0;1
"We used the pmbench memory accessing benchmark tested the patchset on a
2-socket server system with DRAM and PMEM installed";1;0
" The test results are
as follows,
		pmbench score		promote rate
		 (accesses/s)			MB/s
base		  146887704.1		       725.6
hot selection     165695601.2		       544.0
rate limit	  162814569.8		       165.2
auto adjustment	  170495294.0                  136.9
From the results above,
With hot page selection patch [1/3], the pmbench score increases about
12.8%, and promote rate (overhead) decreases about 25.0%, compared with
base kernel";0;1
"With rate limit patch [2/3], pmbench score decreases about 1.7%, and
promote rate decreases about 69.6%, compared with hot page selection
patch";1;1
"With threshold auto adjustment patch [3/3], pmbench score increases about
4.7%, and promote rate decrease about 17.1%, compared with rate limit
patch";0;0
"Baolin helped to test the patchset with MySQL on a machine which contains
1 DRAM node (30G) and 1 PMEM node (126G)";0;0
"sysbench /usr/share/sysbench/oltp_read_write.lua \
--tables=200 \
--table-size=1000000 \
--report-interval=10 \
--threads=16 \
--time=120
The tps can be improved about 5%";0;0
This patch (of 3);1;0
"To optimize page placement in a memory tiering system with NUMA balancing,
the hot pages in the slow memory node need to be identified";0;0
" Essentially,
the original NUMA balancing implementation selects the mostly recently
accessed (MRU) pages to promote";1;0
" But this isn't a perfect algorithm to
identify the hot pages";1;1
" Because the pages with quite low access frequency
may be accessed eventually given the NUMA balancing page table scanning
period could be quite long (e.g";0;1
 60 seconds);0;1
" The most frequently
accessed (MFU) algorithm is better";0;0
So, in this patch we implemented a better hot page selection algorithm;1;1
"
Which is based on NUMA balancing page table scanning and hint page fault
as follows,
- When the page tables of the processes are scanned to change PTE/PMD
  to be PROT_NONE, the current time is recorded in struct page as scan
  time";1;1
- When the page is accessed, hint page fault will occur;0;0
" The scan
  time is gotten from the struct page";1;0
" And The hint page fault
  latency is defined as
    hint page fault time - scan time
The shorter the hint page fault latency of a page is, the higher the
probability of their access frequency to be higher";1;0
" So the hint page
fault latency is a better estimation of the page hot/cold";1;1
It's hard to find some extra space in struct page to hold the scan time;1;0
"
Fortunately, we can reuse some bits used by the original NUMA balancing";0;0
"NUMA balancing uses some bits in struct page to store the page accessing
CPU and PID (referring to page_cpupid_xchg_last())";0;1
" Which is used by the
multi-stage node selection algorithm to avoid to migrate pages shared
accessed by the NUMA nodes back and forth";0;1
" But for pages in the slow
memory node, even if they are shared accessed by multiple NUMA nodes, as
long as the pages are hot, they need to be promoted to the fast memory
node";0;1
" So the accessing CPU and PID information are unnecessary for the
slow memory pages";1;1
" We can reuse these bits in struct page to record the
scan time";0;1
 For the fast memory pages, these bits are used as before;0;0
"For the hot threshold, the default value is 1 second, which works well in
our performance test";1;1
" All pages with hint page fault latency < hot
threshold will be considered hot";1;0
It's hard for users to determine the hot threshold;0;0
" So we don't provide a
kernel ABI to set it, just provide a debugfs interface for advanced users
to experiment";0;0
" We will continue to work on a hot threshold automatic
adjustment mechanism";0;1
"The downside of the above method is that the response time to the workload
hot spot changing may be much longer";0;0
" For example,
- A previous cold memory area becomes hot
- The hint page fault will be triggered";0;0
" But the hint page fault
  latency isn't shorter than the hot threshold";1;0
" So the pages will
  not be promoted";0;0
"- When the memory area is scanned again, maybe after a scan period,
  the hint page fault latency measured will be shorter than the hot
  threshold and the pages will be promoted";1;0
"To mitigate this, if there are enough free space in the fast memory node,
the hot threshold will not be used, all pages will be promoted upon the
hint page fault for fast response";1;0
"Thanks Zhong Jiang reported and tested the fix for a bug when disabling
memory tiering mode dynamically.";1;1
mm: migration: fix the FOLL_GET failure on following huge page;1;1
"Not all huge page APIs support FOLL_GET option, so move_pages() syscall
will fail to get the page node information for some huge pages";1;1
"Like x86 on linux 5.19 with 1GB huge page API follow_huge_pud(), it will
return NULL page for FOLL_GET when calling move_pages() syscall with the
NULL 'nodes' parameter, the 'status' parameter has '-2' error in array";0;1
Note: follow_huge_pud() now supports FOLL_GET in linux 6.0.;1;0
fs: Remove aops->migratepage();1;0
With all users converted to migrate_folio(), remove this operation.;0;0
hugetlb: Convert to migrate_folio;1;0
This involves converting migrate_huge_page_move_mapping();1;1
" We also need a
folio variant of hugetlb_set_page_subpool(), but that's for a later patch.";0;0
mm/migrate: Add filemap_migrate_folio();1;1
"There is nothing iomap-specific about iomap_migratepage(), and it fits
a pattern used by several other filesystems, so move it to mm/migrate.c,
convert it to be filemap_migrate_folio() and convert the iomap filesystems
to use it.";1;0
mm/migrate: Convert migrate_page() to migrate_folio();1;0
Convert all callers to pass a folio;1;0
" Most have the folio
already available";0;0
" Switch all users from aops->migratepage to
aops->migrate_folio";0;0
 Also turn the documentation into kerneldoc.;1;1
mm/migrate: Convert expected_page_refs() to folio_expected_refs();0;1
"Now that both callers have a folio, convert this function to
take a folio & rename it.";1;0
mm/migrate: Convert buffer_migrate_page() to buffer_migrate_folio();1;0
"Use a folio throughout __buffer_migrate_folio(), add kernel-doc for
buffer_migrate_folio() and buffer_migrate_folio_norefs(), move their
declarations to buffer.h and switch all filesystems that have wired
them up.";1;0
mm/migrate: Convert writeout() to take a folio;1;0
Use a folio throughout this function.;0;0
mm/migrate: Convert fallback_migrate_page() to fallback_migrate_folio();1;0
Use a folio throughout;1;1
" migrate_page() will be converted to
migrate_folio() later.";0;1
fs: Add aops->migrate_folio;1;0
Provide a folio-based replacement for aops->migratepage;0;0
" Update the
documentation to document migrate_folio instead of migratepage.";1;1
mm: Convert all PageMovable users to movable_operations;1;0
"These drivers are rather uncomfortably hammered into the
address_space_operations hole";0;1
" They aren't filesystems and don't behave
like filesystems";1;1
" They just need their own movable_operations structure,
which we can point to directly from page->mapping.";1;1
mm: handling Non-LRU pages returned by vm_normal_pages;1;0
"With DEVICE_COHERENT, we'll soon have vm_normal_pages() return
device-managed anonymous pages that are not LRU pages";0;0
" Although they
behave like normal pages for purposes of mapping in CPU page, and for COW";0;0
They do not support LRU lists, NUMA migration or THP;1;1
"Callers to follow_page() currently don't expect ZONE_DEVICE pages,
however, with DEVICE_COHERENT we might now return ZONE_DEVICE";0;1
" Check for
ZONE_DEVICE pages in applicable users of follow_page() as well.";0;0
mm/migration: fix potential pte_unmap on an not mapped pte;1;1
"__migration_entry_wait and migration_entry_wait_on_locked assume pte is
always mapped from caller";1;0
" But this is not the case when it's called from
migration_entry_wait_huge and follow_huge_pmd";0;0
" Add a hugetlbfs variant
that calls hugetlb_migration_entry_wait(ptep == NULL) to fix this issue.";0;0
mm/migration: return errno when isolate_huge_page failed;0;1
We might fail to isolate huge page due to e.g;0;1
" the page is under
migration which cleared HPageMigratable";0;1
" We should return errno in this
case rather than always return 1 which could confuse the user, i.e";1;1
" the
caller might think all of the memory is migrated while the hugetlb page is
left behind";1;1
" We make the prototype of isolate_huge_page consistent with
isolate_lru_page as suggested by Huang Ying and rename isolate_huge_page
to isolate_hugetlb as suggested by Muchun to improve the readability.";1;1
mm/migration: remove unneeded lock page and PageMovable check;1;1
"When non-lru movable page was freed from under us, __ClearPageMovable must
have been done";0;1
" So we can remove unneeded lock page and PageMovable check
here";1;1
" Also free_pages_prepare() will clear PG_isolated for us, so we can
further remove ClearPageIsolated as suggested by David.";1;1
mm: Clear page->private when splitting or migrating a page;0;1
"In our efforts to remove uses of PG_private, we have found folios with
the private flag clear and folio->private not-NULL";0;1
" That is the root
cause behind 642d51fb0775 (""ceph: check folio PG_private bit instead
of folio->private"")";1;1
" It can also affect a few other filesystems that
haven't yet reported a problem";0;1
"compaction_alloc() can return a page with uninitialised page->private,
and rather than checking all the callers of migrate_pages(), just zero
page->private after calling get_new_page()";1;1
" Similarly, the tail pages
from split_huge_page() may also have an uninitialised page->private.";0;1
mm/migrate: convert move_to_new_page() into move_to_new_folio();1;0
Pass in the folios that we already have in each caller;1;0
" Saves a
lot of calls to compound_head().";0;0
mm: convert sysfs input to bool using kstrtobool();1;0
Sysfs input conversion to corrosponding bool value e.g;0;0
" ""false"" or ""0"" to
false, ""true"" or ""1"" to true are currently handled through strncmp at
multiple places";0;1
 Use kstrtobool() to convert sysfs input to bool value.;1;0
fs: Change try_to_free_buffers() to take a folio;1;0
"All but two of the callers already have a folio; pass a folio into
try_to_free_buffers()";1;0
" This removes the last user of cancel_dirty_page()
so remove that wrapper function too.";1;1
mm: remember exclusively mapped anonymous pages with PG_anon_exclusive;1;0
"Let's mark exclusively mapped anonymous pages with PG_anon_exclusive as
exclusive, and use that information to make GUP pins reliable and stay
consistent with the page mapped into the page table even if the page table
entry gets write-protected";1;1
"With that information at hand, we can extend our COW logic to always reuse
anonymous pages that are exclusive";0;1
" For anonymous pages that might be
shared, the existing logic applies";0;1
"As already documented, PG_anon_exclusive is usually only expressive in
combination with a page table entry";0;0
 Especially PTE vs;0;0
" PMD-mapped
anonymous pages require more thought, some examples: due to mremap() we
can easily have a single compound page PTE-mapped into multiple page
tables exclusively in a single process -- multiple page table locks apply";1;1
"Further, due to MADV_WIPEONFORK we might not necessarily write-protect
all PTEs, and only some subpages might be pinned";1;0
" Long story short: once
PTE-mapped, we have to track information about exclusivity per sub-page,
but until then, we can just track it for the compound page in the head
page and not having to update a whole bunch of subpages all of the time
for a simple PMD mapping of a THP";1;1
"For simplicity, this commit mostly talks about ""anonymous pages"", while
it's for THP actually ""the part of an anonymous folio referenced via a
page table entry""";0;1
"To not spill PG_anon_exclusive code all over the mm code-base, we let the
anon rmap code to handle all PG_anon_exclusive logic it can easily handle";1;1
"If a writable, present page table entry points at an anonymous (sub)page,
that (sub)page must be PG_anon_exclusive";1;0
" If GUP wants to take a reliably
pin (FOLL_PIN) on an anonymous page references via a present page table
entry, it must only pin if PG_anon_exclusive is set for the mapped
(sub)page";1;1
"This commit doesn't adjust GUP, so this is only implicitly handled for
FOLL_WRITE, follow-up commits will teach GUP to also respect it for
FOLL_PIN without FOLL_WRITE, to make all GUP pins of anonymous pages fully
reliable";1;1
"Whenever an anonymous page is to be shared (fork(), KSM), or when
temporarily unmapping an anonymous page (swap, migration), the relevant
PG_anon_exclusive bit has to be cleared to mark the anonymous page
possibly shared";1;0
 Clearing will fail if there are GUP pins on the page;1;0
  share it;1;1
" fork() protects against concurrent GUP using the PT lock and
  the src_mm->write_protect_seq";0;1
  will fail, For migration this means, migration will fail early;0;1
" All
  three cases protect against concurrent GUP using the PT lock and a
  proper clear/invalidate+flush of the relevant page table entry";0;1
"This fixes memory corruptions reported for FOLL_PIN | FOLL_WRITE, when a
pinned page gets mapped R/O and the successive write fault ends up
replacing the page instead of reusing it";1;1
" It improves the situation for
O_DIRECT/vmsplice/..";0;1
" that still use FOLL_GET instead of FOLL_PIN, if
fork() is *not* involved, however swapout and fork() are still
problematic";1;1
" Properly using FOLL_PIN instead of FOLL_GET for these GUP
users will fix the issue for them";1;1
I;0;0
"Details about basic handling
I.1";0;0
"Fresh anonymous pages
page_add_new_anon_rmap() and hugepage_add_new_anon_rmap() will mark the
given page exclusive via __page_set_anon_rmap(exclusive=1)";1;0
" As that is
the mechanism fresh anonymous pages come into life (besides migration code
where we copy the page->mapping), all fresh anonymous pages will start out
as exclusive";1;0
I.2;1;0
"COW reuse handling of anonymous pages
When a COW handler stumbles over a (sub)page that's marked exclusive, it
simply reuses it";1;1
" Otherwise, the handler tries harder under page lock to
detect if the (sub)page is exclusive and can be reused";0;1
" If exclusive,
page_move_anon_rmap() will mark the given (sub)page exclusive";1;0
"Note that hugetlb code does not yet check for PageAnonExclusive(), as it
still uses the old COW logic that is prone to the COW security issue
because hugetlb code cannot really tolerate unnecessary/wrong COW as huge
pages are a scarce resource";0;1
I.3;1;0
"Migration handling
try_to_migrate() has to try marking an exclusive anonymous page shared via
page_try_share_anon_rmap()";1;1
" If it fails because there are GUP pins on the
page, unmap fails";1;0
" migrate_vma_collect_pmd() and
__split_huge_pmd_locked() are handled similarly";1;0
Writable migration entries implicitly point at shared anonymous pages;1;0
"
For readable migration entries that information is stored via a new
""readable-exclusive"" migration entry, specific to anonymous pages";1;0
"When restoring a migration entry in remove_migration_pte(), information
about exlusivity is detected via the migration entry type, and
RMAP_EXCLUSIVE is set accordingly for
page_add_anon_rmap()/hugepage_add_anon_rmap() to restore that information";1;0
I.4;0;0
"Swapout handling
try_to_unmap() has to try marking the mapped page possibly shared via
page_try_share_anon_rmap()";0;1
" If it fails because there are GUP pins on the
page, unmap fails";1;0
 For now, information about exclusivity is lost;0;1
" In
the future, we might want to remember that information in the swap entry
in some cases, however, it requires more thought, care, and a way to store
that information in swap entries";0;1
I.5;1;0
"Swapin handling
do_swap_page() will never stumble over exclusive anonymous pages in the
swap cache, as try_to_migrate() prohibits that";1;1
" do_swap_page() always has
to detect manually if an anonymous page is exclusive and has to set
RMAP_EXCLUSIVE for page_add_anon_rmap() accordingly";1;0
I.6;1;0
"THP handling
__split_huge_pmd_locked() has to move the information about exclusivity
from the PMD to the PTEs";0;0
"a) In case we have a readable-exclusive PMD migration entry, simply
   insert readable-exclusive PTE migration entries";1;0
"b) In case we have a present PMD entry and we don't want to freeze
   (""convert to migration entries""), simply forward PG_anon_exclusive to
   all sub-pages, no need to temporarily clear the bit";1;0
"c) In case we have a present PMD entry and want to freeze, handle it
   similar to try_to_migrate(): try marking the page shared first";1;1
" In
   case we fail, we ignore the ""freeze"" instruction and simply split
   ordinarily";1;0
" try_to_migrate() will properly fail because the THP is
   still mapped via PTEs";0;1
"When splitting a compound anonymous folio (THP), the information about
exclusivity is implicitly handled via the migration entries: no need to
replicate PG_anon_exclusive manually";1;0
I.7;0;0
" fork() handling fork() handling is relatively easy, because
PG_anon_exclusive is only expressive for some page table entry types";0;1
"a) Present anonymous pages
page_try_dup_anon_rmap() will mark the given subpage shared -- which will
fail if the page is pinned";1;1
" If it failed, we have to copy (or PTE-map a
PMD to handle it on the PTE level)";1;1
"Note that device exclusive entries are just a pointer at a PageAnon()
page";1;0
" fork() will first convert a device exclusive entry to a present
page table and handle it just like present anonymous pages";1;1
"b) Device private entry
Device private entries point at PageAnon() pages that cannot be mapped
directly and, therefore, cannot get pinned";0;1
"page_try_dup_anon_rmap() will mark the given subpage shared, which cannot
fail because they cannot get pinned";1;1
"c) HW poison entries
PG_anon_exclusive will remain untouched and is stale -- the page table
entry is just a placeholder after all";0;0
"d) Migration entries
Writable and readable-exclusive entries are converted to readable entries";0;0
possibly shared;1;0
I.8;1;0
"mprotect() handling
mprotect() only has to properly handle the new readable-exclusive
migration entry";1;1
"When write-protecting a migration entry that points at an anonymous page,
remember the information about exclusivity via the ""readable-exclusive""
migration entry type";1;1
II;0;0
"Migration and GUP-fast
Whenever replacing a present page table entry that maps an exclusive
anonymous page by a migration entry, we have to mark the page possibly
shared and synchronize against GUP-fast by a proper clear/invalidate+flush
to make the following scenario impossible";0;0
1;0;0
"try_to_migrate() places a migration entry after checking for GUP pins
   and marks the page possibly shared";1;1
2;1;0
"GUP-fast pins the page due to lack of synchronization
3";0;0
"fork() converts the ""writable/readable-exclusive"" migration entry into a
   readable migration entry
4";0;0
"Migration fails due to the GUP pin (failing to freeze the refcount)
5";1;0
Migration entries are restored;0;0
"PG_anon_exclusive is lost
-> We have a pinned page that is not marked exclusive anymore";0;1
"Note that we move information about exclusivity from the page to the
migration entry as it otherwise highly overcomplicates fork() and
PTE-mapping a THP";0;1
III;0;0
"Swapout and GUP-fast
Whenever replacing a present page table entry that maps an exclusive
anonymous page by a swap entry, we have to mark the page possibly shared
and synchronize against GUP-fast by a proper clear/invalidate+flush to
make the following scenario impossible";0;0
1;0;0
"try_to_unmap() places a swap entry after checking for GUP pins and
   clears exclusivity information on the page";0;1
2;1;0
GUP-fast pins the page due to lack of synchronization;0;0
-> We have a pinned page that is not marked exclusive anymore;0;1
"If we'd ever store information about exclusivity in the swap entry,
similar to migration handling, the same considerations as in II would
apply";1;1
 This is future work.;0;0
mm/rmap: pass rmap flags to hugepage_add_anon_rmap();1;0
"Let's prepare for passing RMAP_EXCLUSIVE, similarly as we do for
page_add_anon_rmap() now";1;0
" RMAP_COMPOUND is implicit for hugetlb pages and
ignored.";0;0
mm/rmap: remove do_page_add_anon_rmap();1;0
..;0;0
and instead convert page_add_anon_rmap() to accept flags;1;0
"Passing flags instead of bools is usually nicer either way, and we want to
more often also pass RMAP_EXCLUSIVE in follow up patches when detecting
that an anonymous page is exclusive: for example, when restoring an
anonymous page from a writable migration entry";1;1
"This is a preparation for marking an anonymous page inside
page_add_anon_rmap() as exclusive when RMAP_EXCLUSIVE is passed.";1;1
mm/rmap: split page_dup_rmap() into page_dup_file_rmap() and page_try_dup_anon_rmap();0;1
..;0;0
" and move the special check for pinned pages into
page_try_dup_anon_rmap() to prepare for tracking exclusive anonymous pages
via a new pageflag, clearing it only after making sure that there are no
GUP pins on the anonymous page";1;1
"We really only care about pins on anonymous pages, because they are prone
to getting replaced in the COW handler once mapped R/O";0;1
" For !anon pages
in cow-mappings (!VM_SHARED && VM_MAYWRITE) we shouldn't really care about
that, at least not that I could come up with an example";0;1
"Let's drop the is_cow_mapping() check from page_needs_cow_for_dma(), as we
know we're dealing with anonymous pages";1;1
" Also, drop the handling of
pinned pages from copy_huge_pud() and add a comment if ever supporting
anonymous pages on the PUD level";1;1
"This is a preparation for tracking exclusivity of anonymous pages in the
rmap code, and disallowing marking a page shared (-> failing to duplicate)
if there are GUP pins on a page.";0;0
mm: untangle config dependencies for demote-on-reclaim;1;0
"At the time demote-on-reclaim was introduced, it was tied to
CONFIG_HOTPLUG_CPU + CONFIG_MIGRATE, but that is not really accurate";0;1
"The only two things we need to depend on are CONFIG_NUMA + CONFIG_MIGRATE,
so clean this up";1;1
" Furthermore, we only register the hotplug memory
notifier when the system has CONFIG_MEMORY_HOTPLUG.";1;0
mm: migrate: simplify the refcount validation when migrating hugetlb mapping;1;1
"There is no need to validate the hugetlb page's refcount before trying to
freeze the hugetlb page's expected refcount, instead we can just rely on
the page_ref_freeze() to simplify the validation";1;1
"Moreover we are always under the page lock when migrating the hugetlb page
mapping, which means nowhere else can remove it from the page cache, so we
can remove the xas_load() validation under the i_pages lock.";0;1
mm/migration: fix possible do_pages_stat_array racing with memory offline;1;1
"When follow_page peeks a page, the page could be migrated and then be
offlined while it's still being used by the do_pages_stat_array()";0;0
" Use
FOLL_GET to hold the page refcnt to fix this potential race.";0;1
mm/migration: fix potential invalid node access for reclaim-based migration;1;1
"If we failed to setup hotplug state callbacks for mm/demotion:online in
some corner cases, node_demotion will be left uninitialized";1;1
" Invalid node
might be returned from the next_demotion_node() when doing reclaim-based
migration";0;1
 Use kcalloc to allocate node_demotion to fix the issue.;0;1
mm/migration: fix potential page refcounts leak in migrate_pages;1;1
"In -ENOMEM case, there might be some subpages of fail-to-migrate THPs left
in thp_split_pages list";0;1
" We should move them back to migration list so
that they could be put back to the right list by the caller otherwise the
page refcnt will be leaked here";1;1
" Also adjust nr_failed and nr_thp_failed
accordingly to make vm events account more accurate.";1;1
mm/migration: remove some duplicated codes in migrate_pages;1;0
Remove the duplicated codes in migrate_pages to simplify the code;1;1
" Minor
readability improvement";0;1
 No functional change intended.;1;0
mm/migration: avoid unneeded nodemask_t initialization;1;1
"Avoid unneeded next_pass and this_pass initialization as they're always
set before using to save possible cpu cycles when there are plenty of
nodes in the system.";1;0
mm/migration: use helper macro min in do_pages_stat;1;0
"We could use helper macro min to help set the chunk_nr to simplify the
code.";0;1
mm/migration: use helper function vma_lookup() in add_page_for_migration;1;1
"We could use helper function vma_lookup() to lookup the needed vma to
simplify the code.";0;1
mm/migration: remove unneeded local variable page_lru;1;1
"We can use page_is_file_lru() directly to help account the isolated pages
to simplify the code a bit.";1;1
mm/migration: remove unneeded local variable mapping_locked;1;1
"Patch series ""A few cleanup and fixup patches for migration"", v2";1;1
"This series contains a few patches to remove unneeded variables, jump
label and use helper to simplify the code";1;1
" Also we fix some bugs such as
page refcounts leak , invalid node access and so on";1;1
" More details can be
found in the respective changelogs";0;1
This patch (of 11);1;0
When mapping_locked is true, TTU_RMAP_LOCKED is always set to ttu;0;1
" We can
check ttu instead so mapping_locked can be removed";1;1
" And ttu is either 0
or TTU_RMAP_LOCKED now";0;1
 Change '|=' to '=' to reflect this.;0;0
mm/vmscan: make sure wakeup_kswapd with managed zone;1;0
wakeup_kswapd() only wake up kswapd when the zone is managed;1;0
For two callers of wakeup_kswapd(), they are node perspective;0;1
If we picked up a !managed zone, this is not we expected;1;1
This patch makes sure we pick up a managed zone for wakeup_kswapd();1;1
" And
it also use managed_zone in migrate_balanced_pgdat() to get the proper
zone.";1;0
mm/migrate: Use a folio in migrate_misplaced_transhuge_page();1;0
Unify alloc_misplaced_dst_page() and alloc_misplaced_dst_page_thp();0;1
Removes an assumption that compound pages are HPAGE_PMD_ORDER.;1;0
mm/migrate: Use a folio in alloc_migration_target();1;1
"This removes an assumption that a large folio is HPAGE_PMD_ORDER
as well as letting us remove the call to prep_transhuge_page()
and a few hidden calls to compound_head().";1;1
mm/munlock: protect the per-CPU pagevec by a local_lock_t;0;1
"The access to mlock_pvec is protected by disabling preemption via
get_cpu_var() or implicit by having preemption disabled by the caller
(in mlock_page_drain() case)";0;1
" This breaks on PREEMPT_RT since
folio_lruvec_lock_irq() acquires a sleeping lock in this section";0;1
"Create struct mlock_pvec which consits of the local_lock_t and the
pagevec";1;0
 Acquire the local_lock() before accessing the per-CPU pagevec;0;1
"Replace mlock_page_drain() with a _local() version which is invoked on
the local CPU and acquires the local_lock_t and a _remote() version
which uses the pagevec from a remote CPU which offline.";0;1
mm/migration: add trace events for base page and HugeTLB migrations;1;0
This adds two trace events for base page and HugeTLB page migrations;1;1
"These events, closely follow the implementation details like setting and
removing of PTE migration entries, which are essential operations for
migration";1;0
" The new CREATE_TRACE_POINTS in <mm/rmap.c> covers both
<events/migration.h> and <events/tlb.h> based trace events";1;0
" Hence drop
redundant CREATE_TRACE_POINTS from other places which could have otherwise
conflicted during build.";1;0
mm: only re-generate demotion targets when a numa node changes its N_CPU state;0;0
"Abhishek reported that after patch [1], hotplug operations are taking
roughly double the expected time";1;1
" [2]
The reason behind is that the CPU callbacks that
migrate_on_reclaim_init() sets always call set_migration_target_nodes()
whenever a CPU is brought up/down";0;1
"But we only care about numa nodes going from having cpus to become
cpuless, and vice versa, as that influences the demotion_target order";0;0
"We do already have two CPU callbacks (vmstat_cpu_online() and
vmstat_cpu_dead()) that check exactly that, so get rid of the CPU
callbacks in migrate_on_reclaim_init() and only call
numa node change its N_CPU state.";0;1
NUMA balancing: optimize page placement for memory tiering system;0;0
"With the advent of various new memory types, some machines will have
multiple types of memory, e.g";0;1
 DRAM and PMEM (persistent memory);1;1
" The
memory subsystem of these machines can be called memory tiering system,
because the performance of the different types of memory are usually
different";0;0
"In such system, because of the memory accessing pattern changing etc,
some pages in the slow memory may become hot globally";1;1
" So in this
patch, the NUMA balancing mechanism is enhanced to optimize the page
placement among the different memory types according to hot/cold
dynamically";1;1
"In a typical memory tiering system, there are CPUs, fast memory and slow
memory in each physical NUMA node";0;0
" The CPUs and the fast memory will be
put in one logical node (called fast memory node), while the slow memory
will be put in another (faked) logical node (called slow memory node)";0;0
"That is, the fast memory is regarded as local while the slow memory is
regarded as remote";0;1
" So it's possible for the recently accessed pages in
the slow memory node to be promoted to the fast memory node via the
existing NUMA balancing mechanism";0;0
"The original NUMA balancing mechanism will stop to migrate pages if the
free memory of the target node becomes below the high watermark";0;0
" This
is a reasonable policy if there's only one memory type";1;1
" But this makes
the original NUMA balancing mechanism almost do not work to optimize
page placement among different memory types";0;1
 Details are as follows;1;0
"It's the common cases that the working-set size of the workload is
larger than the size of the fast memory nodes";1;0
" Otherwise, it's
unnecessary to use the slow memory at all";0;1
" So, there are almost always
no enough free pages in the fast memory nodes, so that the globally hot
pages in the slow memory node cannot be promoted to the fast memory
node";0;0
" To solve the issue, we have 2 choices as follows,
a";1;0
"Ignore the free pages watermark checking when promoting hot pages
   from the slow memory node to the fast memory node";0;1
" This will
   create some memory pressure in the fast memory node, thus trigger
   the memory reclaiming";1;1
" So that, the cold pages in the fast memory
   node will be demoted to the slow memory node";1;0
b;1;0
"Define a new watermark called wmark_promo which is higher than
   wmark_high, and have kswapd reclaiming pages until free pages reach
   such watermark";1;0
" The scenario is as follows: when we want to promote
   hot-pages from a slow memory to a fast memory, but fast memory's free
   pages would go lower than high watermark with such promotion, we wake
   up kswapd with wmark_promo watermark in order to demote cold pages and
   free us up some space";1;1
" So, next time we want to promote hot-pages we
   might have a chance of doing so";0;1
"The choice ""a"" may create high memory pressure in the fast memory node";1;0
"If the memory pressure of the workload is high, the memory pressure
may become so high that the memory allocation latency of the workload
is influenced, e.g";0;0
 the direct reclaiming may be triggered;0;0
"The choice ""b"" works much better at this aspect";0;1
" If the memory
pressure of the workload is high, the hot pages promotion will stop
earlier because its allocation watermark is higher than that of the
normal memory allocation";0;0
" So in this patch, choice ""b"" is implemented";1;0
A new zone watermark (WMARK_PROMO) is added;1;1
" Which is larger than the
high watermark and can be controlled via watermark_scale_factor";1;0
"In addition to the original page placement optimization among sockets,
the NUMA balancing mechanism is extended to be used to optimize page
placement according to hot/cold among different memory types";0;0
" So the
sysctl user space interface (numa_balancing) is extended in a backward
compatible way as follow, so that the users can enable/disable these
functionality individually";1;0
The sysctl is converted from a Boolean value to a bits field;0;1
" The
definition of the flags is,
- 0: NUMA_BALANCING_DISABLED
- 1: NUMA_BALANCING_NORMAL
- 2: NUMA_BALANCING_MEMORY_TIERING
We have tested the patch with the pmbench memory accessing benchmark
with the 80:20 read/write ratio and the Gauss access address
distribution on a 2 socket Intel server with Optane DC Persistent
Memory Model";0;0
" The test results shows that the pmbench score can
improve up to 95.9%";0;1
Thanks Andrew Morton to help fix the document format error.;1;1
NUMA Balancing: add page promotion counter;1;0
"Patch series ""NUMA balancing: optimize memory placement for memory tiering system"", v13
With the advent of various new memory types, some machines will have
multiple types of memory, e.g";0;0
 DRAM and PMEM (persistent memory);1;1
" The
memory subsystem of these machines can be called memory tiering system,
because the performance of the different types of memory are different";0;0
"After commit c221c0b0308f (""device-dax: ""Hotplug"" persistent memory for
use like normal RAM""), the PMEM could be used as the cost-effective
volatile memory in separate NUMA nodes";0;0
" In a typical memory tiering
system, there are CPUs, DRAM and PMEM in each physical NUMA node";0;0
" The
CPUs and the DRAM will be put in one logical node, while the PMEM will
be put in another (faked) logical node";1;0
"To optimize the system overall performance, the hot pages should be
placed in DRAM node";0;0
" To do that, we need to identify the hot pages in
the PMEM node and migrate them to DRAM node via NUMA migration";0;0
"In the original NUMA balancing, there are already a set of existing
mechanisms to identify the pages recently accessed by the CPUs in a node
and migrate the pages to the node";0;0
" So we can reuse these mechanisms to
build the mechanisms to optimize the page placement in the memory
tiering system";0;1
 This is implemented in this patchset;0;1
At the other hand, the cold pages should be placed in PMEM node;0;1
" So, we
also need to identify the cold pages in the DRAM node and migrate them
to PMEM node";1;1
"In commit 26aa2d199d6f (""mm/migrate: demote pages during reclaim""), a
mechanism to demote the cold DRAM pages to PMEM node under memory
pressure is implemented";0;1
" Based on that, the cold DRAM pages can be
demoted to PMEM node proactively to free some memory space on DRAM node
to accommodate the promoted hot PMEM pages";0;1
" This is implemented in this
patchset too";1;1
"We have tested the solution with the pmbench memory accessing benchmark
with the 80:20 read/write ratio and the Gauss access address
distribution on a 2 socket Intel server with Optane DC Persistent Memory
Model";0;0
" The test results shows that the pmbench score can improve up to
This patch (of 3)";0;1
In a system with multiple memory types, e.g;1;0
" DRAM and PMEM, the CPU
and DRAM in one socket will be put in one NUMA node as before, while
the PMEM will be put in another NUMA node as described in the
description of the commit c221c0b0308f (""device-dax: ""Hotplug""
persistent memory for use like normal RAM"")";0;1
" So, the NUMA balancing
mechanism will identify all PMEM accesses as remote access and try to
promote the PMEM pages to DRAM";1;0
"To distinguish the number of the inter-type promoted pages from that of
the inter-socket migrated pages";0;1
 A new vmstat count is added;1;0
" The
counter is per-node (count in the target node)";1;1
" So this can be used to
identify promotion imbalance among the NUMA nodes.";0;1
mm/migrate: fix race between lock page and clear PG_Isolated;1;1
"When memory is tight, system may start to compact memory for large
continuous memory demands";0;0
" If one process tries to lock a memory page
that is being locked and isolated for compaction, it may wait a long time
or even forever";0;0
" This is because compaction will perform non-atomic
PG_Isolated clear while holding page lock, this may overwrite PG_waiters
set by the process that can't obtain the page lock and add itself to the
waiting queue to wait for the lock to be unlocked";0;1
"  CPU1                            CPU2
  lock_page(page); (successful)
                                  lock_page(); (failed)
  __ClearPageIsolated(page);      SetPageWaiters(page) (may be overwritten)
The solution is to not perform non-atomic operation on page flags while
holding page lock.";0;1
mm,migrate: fix establishing demotion target;1;1
"In commit ac16ec835314 (""mm: migrate: support multiple target nodes
demotion""), after the first demotion target node is found, we will
continue to check the next candidate obtained via find_next_best_node()";0;1
This is to find all demotion target nodes with same NUMA distance;0;0
" But
one side effect of find_next_best_node() is that the candidate node
returned will be set in ""used"" parameter, even if the candidate node isn't
passed in the following NUMA distance checking, the candidate node will
not be used as demotion target node for the following nodes";0;1
" For example,
for system as follows,
node distances";0;1
"node   0   1   2   3
when we establish demotion target node for node 0, in the first round node
2 is added to the demotion target node set";0;0
" Then in the second round,
node 3 is checked and failed because distance(0, 3) > distance(0, 2)";0;0
" But
node 3 is set in ""used"" nodemask too";0;0
" When we establish demotion target
node for node 1, there is no available node";0;0
" This is wrong, node 3 should
be set as the demotion target of node 1";0;1
"To fix this, if the candidate node is failed to pass the distance
checking, it will be cleared in ""used"" nodemask";1;1
" So that it can be used
for the following node";1;1
"The bug can be reproduced and fixed with this patch on a 2 socket server
machine with DRAM and PMEM.";1;1
mm/fs: delete PF_SWAPWRITE;1;0
"PF_SWAPWRITE has been redundant since v3.2 commit ee72886d8ed5 (""mm";0;1
"vmscan: do not writeback filesystem pages in direct reclaim"")";0;0
"Coincidentally, NeilBrown's current patch ""remove inode_congested()""
deletes may_write_to_inode(), which appeared to be the one function which
took notice of PF_SWAPWRITE";1;0
" But if you study the old logic, and the
conditions under which may_write_to_inode() was called, you discover that
flag and function have been pointless for a decade.";0;0
mm: remove unneeded local variable follflags;1;1
"We can pass FOLL_GET | FOLL_DUMP to follow_page directly to simplify the
code a bit in add_page_for_migration and split_huge_pages_pid.";1;1
mm: replace multiple dcache flush with flush_dcache_folio();1;1
Simplify the code by using flush_dcache_folio().;1;1
mm: fix missing cache flush for all tail pages of compound page;1;1
"The D-cache maintenance inside move_to_new_page() only consider one
page, there is still D-cache maintenance issue for tail pages of
compound page (e.g";1;0
THP or HugeTLB);0;0
"THP migration is only enabled on x86_64, ARM64 and powerpc, while
powerpc and arm64 need to maintain the consistency between I-Cache and
D-Cache, which depends on flush_dcache_page() to maintain the
consistency between I-Cache and D-Cache";0;1
"But there is no issues on arm64 and powerpc since they already considers
the compound page cache flushing in their icache flush function";0;1
"HugeTLB migration is enabled on arm, arm64, mips, parisc, powerpc,
riscv, s390 and sh, while arm has handled the compound page cache flush
in flush_dcache_page(), but most others do not";0;1
In theory, the issue exists on many architectures;0;0
" Fix this by not
using flush_dcache_folio() since it is not backportable.";1;1
mm/gup: follow_pfn_pte(): -EEXIST cleanup;1;1
"Remove a quirky special case from follow_pfn_pte(), and adjust its
callers to match";1;0
 Caller changes include;1;0
"__get_user_pages(): Regardless of any FOLL_* flags, get_user_pages() and
its variants should handle PFN-only entries by stopping early, if the
caller expected **pages to be filled in";0;1
" This makes for a more reliable
API, as compared to the previous approach of skipping over such entries
(and thus leaving them silently unwritten)";0;1
"move_pages(): squash the -EEXIST error return from follow_page() into
-EFAULT, because -EFAULT is listed in the man page, whereas -EEXIST is
not.";0;0
mm/rmap: Convert rmap_walk() to take a folio;1;0
"This ripples all the way through to every calling and called function
from rmap.";1;0
mm/migrate: Convert remove_migration_ptes() to folios;1;0
Convert the implementation and all callers.;1;0
mm/rmap: Convert try_to_migrate() to folios;1;0
"Convert the callers to pass a folio and the try_to_migrate_one()
worker to use a folio throughout";1;0
" Fixes an assumption that a
folio must be <= PMD size.";1;1
mm: Convert page_vma_mapped_walk to work on PFNs;1;0
"page_mapped_in_vma() really just wants to walk one page, but as the
code stands, if passed the head page of a compound page, it will
walk every page in the compound page";1;1
" Extract pfn/nr_pages/pgoff
from the struct page early, so they can be overridden by
page_mapped_in_vma().";1;0
mm: Add DEFINE_PAGE_VMA_WALK and DEFINE_FOLIO_VMA_WALK;1;1
"Instead of declaring a struct page_vma_mapped_walk directly,
use these helpers to allow us to transition to a PFN approach in the
following patches.";1;1
mm: move the migrate_vma_* device migration code into its own file;1;0
"Split the code used to migrate to and from ZONE_DEVICE memory from
migrate.c into a new file.";0;1
mm: refactor the ZONE_DEVICE handling in migrate_vma_pages;1;0
"Make the flow a little more clear and prepare for adding a new
ZONE_DEVICE memory type.";1;1
mm: refactor the ZONE_DEVICE handling in migrate_vma_insert_page;1;1
"Make the flow a little more clear and prepare for adding a new
ZONE_DEVICE memory type.";1;1
mm: remove the extra ZONE_DEVICE struct page refcount;1;0
"ZONE_DEVICE struct pages have an extra reference count that complicates
the code for put_page() and several places in the kernel that need to
check the reference count to see that a page is not being used (gup,
compaction, migration, etc.)";0;0
"Clean up the code so the reference count
doesn't need to be treated specially for ZONE_DEVICE pages";1;1
"Note that this excludes the special idle page wakeup for fsdax pages,
which still happens at refcount 1";1;0
" This is a separate issue and will
be sorted out later";0;0
" Given that only fsdax pages require the
notifiacation when the refcount hits 1 now, the PAGEMAP_OPS Kconfig
symbol can go away and be replaced with a FS_DAX check for this hook
in the put_page fastpath";1;1
Based on an earlier patch from Ralph Campbell <rcampbell@nvidia.com>.;0;0
mm/munlock: page migration needs mlock pagevec drained;1;1
"Page migration of a VM_LOCKED page tends to fail, because when the old
page is unmapped, it is put on the mlock pagevec with raised refcount,
which then fails the freeze";1;1
"At first I thought this would be fixed by a local mlock_page_drain() at
the upper rmap_walk() level - which would have nicely batched all the
munlocks of that page; but tests show that the task can too easily move
to another cpu, leaving pagevec residue behind which fails the migration";0;1
"So try_to_migrate_one() drain the local pagevec after page_remove_rmap()
from a VM_LOCKED vma; and do the same in try_to_unmap_one(), whose
TTU_IGNORE_MLOCK users would want the same treatment; and do the same
in remove_migration_pte() - not important when successfully inserting
a new page, but necessary when hoping to retry after failure";1;1
"Any new pagevec runs the risk of adding a new way of stranding, and we
might discover other corners where mlock_page_drain() or lru_add_drain()
would now help.";1;1
mm/migrate: __unmap_and_move() push good newpage to LRU;1;0
"Compaction, NUMA page movement, THP collapse/split, and memory failure
do isolate unevictable pages from their ""LRU"", losing the record of
mlock_count in doing so (isolators are likely to use page->lru for their
own private lists, so mlock_count has to be presumed lost)";1;1
"That's unfortunate, and we should put in some work to correct that: one
can imagine a function to build up the mlock_count again - but it would
require i_mmap_rwsem for read, so be careful where it's called";1;1
" Or
page_referenced_one() and try_to_unmap_one() might do that extra work";0;1
"But one place that can very easily be improved is page migration's
__unmap_and_move(): a small adjustment to where the successful new page
is put back on LRU, and its mlock_count (if any) is built back up by
remove_migration_ptes().";1;1
mm/munlock: rmap call mlock_vma_page() munlock_vma_page();1;0
"Add vma argument to mlock_vma_page() and munlock_vma_page(), make them
inline functions which check (vma->vm_flags & VM_LOCKED) before calling
mlock_page() and munlock_page() in mm/mlock.c";1;0
"Add bool compound to mlock_vma_page() and munlock_vma_page(): this is
because we have understandable difficulty in accounting pte maps of THPs,
and if passed a PageHead page, mlock_page() and munlock_page() cannot
tell whether it's a pmd map to be counted or a pte map to be ignored";0;1
"Add vma arg to page_add_file_rmap() and page_remove_rmap(), like the
others, and use that to call mlock_vma_page() at the end of the page
adds, and munlock_vma_page() at the end of page_remove_rmap() (end or
beginning? unimportant, but end was easier for assertions in testing)";1;1
No page lock is required (although almost all adds happen to hold it);1;0
"delete the ""Serialize with page migration"" BUG_ON(!PageLocked(page))s";0;0
"Certainly page lock did serialize with page migration, but I'm having
difficulty explaining why that was ever important";0;1
"Mlock accounting on THPs has been hard to define, differed between anon
and file, involved PageDoubleMap in some places and not others, required
clear_page_mlock() at some points";0;0
" Keep it simple now: just count the
pmds and ignore the ptes, there is no reason for ptes to undo pmd mlocks";1;0
"page_add_new_anon_rmap() callers unchanged: they have long been calling
lru_cache_add_inactive_or_unevictable(), which does its own VM_LOCKED
handling (it also checks for not VM_SPECIAL: I think that's overcautious,
and inconsistent with other checks, that mmap_region() already prevents
VM_LOCKED on VM_SPECIAL; but haven't quite convinced myself to change it).";0;0
mm/migrate.c: rework migration_entry_wait() to not take a pageref;0;0
This fixes the FIXME in migrate_vma_check_page();1;1
"Before migrating a page migration code will take a reference and check
there are no unexpected page references, failing the migration if there
are";1;0
" When a thread faults on a migration entry it will take a temporary
reference to the page to wait for the page to become unlocked signifying
the migration entry has been removed";0;1
"This reference is dropped just prior to waiting on the page lock,
however the extra reference can cause migration failures so it is
desirable to avoid taking it";1;1
"As migration code already has a reference to the migrating page an extra
reference to wait on PG_locked is unnecessary so long as the reference
can't be dropped whilst setting up the wait";1;1
"When faulting on a migration entry the ptl is taken to check the
migration entry";0;0
" Removing a migration entry also requires the ptl, and
migration code won't drop its page reference until after the migration
entry has been removed";1;0
" Therefore retaining the ptl of a migration
entry is sufficient to ensure the page has a reference";0;0
" Reworking
migration_entry_wait() to hold the ptl until the wait setup is complete
means the extra page reference is no longer needed.";0;0
mm/migrate: remove redundant variables used in a for-loop;1;1
"The variable addr is being set and incremented in a for-loop but not
actually being used";0;0
" It is redundant and so addr and also variable
start can be removed.";1;1
mm/migrate: move node demotion code to near its user;0;0
"Now, node_demotion and next_demotion_node() are placed between
__unmap_and_move() and unmap_and_move()";0;0
 This hurts code readability;0;1
So move them near their users in the file;1;0
" There's no functionality
change in this patch.";1;0
mm: migrate: add more comments for selecting target node randomly;1;1
"As Yang Shi suggested [1], it will be helpful to explain why we should
select target node randomly now if there are multiple target nodes.";0;1
mm: migrate: support multiple target nodes demotion;1;1
"We have some machines with multiple memory types like below, which have
one fast (DRAM) memory node and two slow (persistent memory) memory
nodes";0;0
" According to current node demotion policy, if node 0 fills up,
its memory should be migrated to node 1, when node 1 fills up, its
memory will be migrated to node 2: node 0 -> node 1 -> node 2 ->stop";0;0
"But this is not efficient and suitbale memory migration route for our
machine with multiple slow memory nodes";1;1
" Since the distance between
node 0 to node 1 and node 0 to node 2 is equal, and memory migration
between slow memory nodes will increase persistent memory bandwidth
greatly, which will hurt the whole system's performance";0;0
"Thus for this case, we can treat the slow memory node 1 and node 2 as a
whole slow memory region, and we should migrate memory from node 0 to
node 1 and node 2 if node 0 fills up";0;1
"This patch changes the node_demotion data structure to support multiple
target nodes, and establishes the migration path to support multiple
target nodes with validating if the node distance is the best or not";1;0
"  available: 3 nodes (0-2)
  node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
  node 0 size: 62153 MB
  node 0 free: 55135 MB
  node 1 cpus";0;0
"  node 1 size: 127007 MB
  node 1 free: 126930 MB
  node 2 cpus";0;0
"  node 2 size: 126968 MB
  node 2 free: 126878 MB
  node distances";0;0
  node   0   1   2;0;0
mm: migrate: correct the hugetlb migration stats;1;1
"Correct the migration stats for hugetlb with using compound_nr() instead
of thp_nr_pages(), meanwhile change 'nr_failed_pages' to record the
number of normal pages failed to migrate, including THP and hugetlb, and
'nr_succeeded' will record the number of normal pages migrated
successfully.";0;1
mm: migrate: fix the return value of migrate_pages();1;1
"Patch series ""Improve the migration stats""";1;1
"According to talk with Zi Yan [1], this patch set changes the return
value of migrate_pages() to avoid returning a number which is larger
than the number of pages the users tried to migrate by move_pages()
syscall";0;1
" Also fix the hugetlb migration stats and migration stats in
trace_mm_compaction_migratepages().";1;1
mm: change page type prior to adding page table entry;1;1
"Patch series ""page table check"", v3";1;1
"Ensure that some memory corruptions are prevented by checking at the
time of insertion of entries into user page tables that there is no
illegal sharing";0;0
We have recently found a problem [1] that existed in kernel since 4.14;0;1
"The problem was caused by broken page ref count and led to memory
leaking from one process into another";0;0
" The problem was accidentally
detected by studying a dump of one process and noticing that one page
contains memory that should not belong to this process";0;1
"There are some other page->_refcount related problems that were recently
fixed: [2], [3] which potentially could also lead to illegal sharing";0;1
"In addition to hardening refcount [4] itself, this work is an attempt to
prevent this class of memory corruption issues";1;1
"It uses a simple state machine that is independent from regular MM logic
to check for illegal sharing at time pages are inserted and removed from
page tables.";0;1
mm: Use multi-index entries in the page cache;1;1
We currently store large folios as 2^N consecutive entries;0;0
" While this
consumes rather more memory than necessary, it also turns out to be buggy";1;1
"A writeback operation which starts within a tail page of a dirty folio will
not write back the folio as the xarray's dirty bit is only set on the
head index";1;1
" With multi-index entries, the dirty bit will be found no
matter where in the folio the operation starts";1;1
"This does end up simplifying the page cache slightly, although not as
much as I had hoped.";0;1
filemap: Add folio_put_wait_locked();1;1
"Convert all three callers of put_and_wait_on_page_locked() to
folio_put_wait_locked()";1;0
 This shrinks the kernel overall by 19 bytes;0;0
"filemap_update_page() shrinks by 19 bytes while __migration_entry_wait()
is unchanged";1;1
" folio_put_wait_locked() is 14 bytes smaller than
put_and_wait_on_page_locked(), but pmd_migration_entry_wait() grows by
14 bytes";1;0
" It removes the assumption from pmd_migration_entry_wait()
that pages cannot be larger than a PMD (which is true today, but
may be interesting to explore in the future).";0;0
mm/migrate.c: remove MIGRATE_PFN_LOCKED;1;1
"MIGRATE_PFN_LOCKED is used to indicate to migrate_vma_prepare() that a
source page was already locked during migrate_vma_collect()";1;1
" If it
wasn't then the a second attempt is made to lock the page";0;1
" However if
the first attempt failed it's unlikely a second attempt will succeed,
and the retry adds complexity";1;1
" So clean this up by removing the retry
and MIGRATE_PFN_LOCKED flag";1;1
"Destination pages are also meant to have the MIGRATE_PFN_LOCKED flag
set, but nothing actually checks that.";0;0
mm: migrate: simplify the file-backed pages validation when migrating its mapping;1;1
"There is no need to validate the file-backed page's refcount before
trying to freeze the page's expected refcount, instead we can rely on
the folio_ref_freeze() to validate if the page has the expected refcount
before migrating its mapping";1;1
"Moreover we are always under the page lock when migrating the page
mapping, which means nowhere else can remove it from the page cache, so
we can remove the xas_load() validation under the i_pages lock.";0;1
mm: migrate: make demotion knob depend on migration;1;0
The memory demotion needs to call migrate_pages() to do the jobs;0;1
" And
it is controlled by a knob, however, the knob doesn't depend on
CONFIG_MIGRATION";0;1
" The knob could be truned on even though MIGRATION is
disabled, this will not cause any crash since migrate_pages() would just
return -ENOSYS";0;1
" But it is definitely not optimal to go through demotion
path then retry regular swap every time";1;0
"And it doesn't make too much sense to have the knob visible to the users
when !MIGRATION";0;1
" Move the related code from mempolicy.[h|c] to
migrate.[h|c].";0;1
mm/migrate: fix CPUHP state to update node demotion order;1;1
The node demotion order needs to be updated during CPU hotplug;0;1
" Because
whether a NUMA node has CPU may influence the demotion order";0;1
" The
update function should be called during CPU online/offline after the
node_states[N_CPU] has been updated";0;1
" That is done in
CPUHP_AP_ONLINE_DYN during CPU online and in CPUHP_MM_VMSTAT_DEAD during
CPU offline";0;1
" But in commit 884a6e5d1f93 (""mm/migrate: update node
demotion order on hotplug events""), the function to update node demotion
order is called in CPUHP_AP_ONLINE_DYN during CPU online/offline";0;1
" This
doesn't satisfy the order requirement";0;1
"For example, there are 4 CPUs (P0, P1, P2, P3) in 2 sockets (P0, P1 in S0
and P2, P3 in S1), the demotion order is
 - S0 -> NUMA_NO_NODE
 - S1 -> NUMA_NO_NODE
After P2 and P3 is offlined, because S1 has no CPU now, the demotion
order should have been changed to
 - S0 -> S1
 - S1 -> NO_NODE
but it isn't changed, because the order updating callback for CPU
hotplug doesn't see the new nodemask";0;1
" After that, if P1 is offlined,
the demotion order is changed to the expected order as above";0;1
"So in this patch, we added CPUHP_AP_MM_DEMOTION_ONLINE and
CPUHP_MM_DEMOTION_DEAD to be called after CPUHP_AP_ONLINE_DYN and
CPUHP_MM_VMSTAT_DEAD during CPU online and offline, and register the
update function on them.";0;0
mm/migrate: add CPU hotplug to demotion #ifdef;1;1
"Once upon a time, the node demotion updates were driven solely by memory
hotplug events";1;0
" But now, there are handlers for both CPU and memory
hotplug";0;1
However, the #ifdef around the code checks only memory hotplug;0;0
" A
system that has HOTPLUG_CPU=y but MEMORY_HOTPLUG=n would miss CPU
hotplug events";0;1
Update the #ifdef around the common code;1;1
" Add memory and CPU-specific
#ifdefs for their handlers";1;1
" These memory/CPU #ifdefs avoid unused
function warnings when their Kconfig option is off.";0;1
mm/migrate: optimize hotplug-time demotion order updates;1;1
"Patch series ""mm/migrate: 5.15 fixes for automatic demotion"", v2";0;1
"This contains two fixes for the ""automatic demotion"" code which was
merged into 5.15";1;1
   suppressing any real action on irrelevant hotplug events;1;1
   is disabled;0;0
This patch (of 2);1;0
"== tl;dr ==
Automatic demotion opted for a simple, lazy approach to handling hotplug
events";1;1
 This noticeably slows down memory hotplug[1];0;0
" Optimize away
updates to the demotion order when memory hotplug events should have no
effect";1;0
This has no effect on CPU hotplug;0;1
" There is no known problem on the CPU
side and any work there will be in a separate series";0;1
"== Background ==
Automatic demotion is a memory migration strategy to ensure that new
allocations have room in faster memory tiers on tiered memory systems";1;1
"The kernel maintains an array (node_demotion[]) to drive these
migrations";0;0
"The node_demotion[] path is calculated by starting at nodes with CPUs
and then ""walking"" to nodes with memory";0;0
" Only hotplug events which
online or offline a node with memory (N_ONLINE) or CPUs (N_CPU) will
actually affect the migration order";0;1
"== Problem ==
However, the current code is lazy";0;1
" It completely regenerates the
migration order on *any* CPU or memory hotplug event";0;1
" The logic was
that these events are extremely rare and that the overhead from
indiscriminate order regeneration is minimal";0;0
"Part of the update logic involves a synchronize_rcu(), which is a pretty
big hammer";1;1
" Its overhead was large enough to be detected by some 0day
tests that watch memory hotplug performance[1]";0;0
"== Solution ==
Add a new helper (node_demotion_topo_changed()) which can differentiate
between superfluous and impactful hotplug events";1;1
" Skip the expensive
update operation for superfluous events";1;0
"== Aside: Locking ==
It took me a few moments to declare the locking to be safe enough for
node_demotion_topo_changed() to work";1;1
" It all hinges on the memory
hotplug lock";1;0
During memory hotplug events, 'mem_hotplug_lock' is held for write;0;1
"This ensures that two memory hotplug events can not be called
simultaneously";0;1
"CPU hotplug has a similar lock (cpuhp_state_mutex) which also provides
mutual exclusion between CPU hotplug events";1;1
" In addition, the demotion
code acquire and hold the mem_hotplug_lock for read during its CPU
hotplug handlers";0;0
" This provides mutual exclusion between the demotion
memory hotplug callbacks and the CPU hotplug callbacks";0;1
"This effectively allows treating the migration target generation code to
act as if it is single-threaded.";1;0
mm/migrate: Add folio_migrate_copy();1;1
"This is the folio equivalent of migrate_page_copy(), which is retained
as a wrapper for filesystems which are not yet converted to folios";1;0
Also convert copy_huge_page() to folio_copy().;1;0
mm/migrate: Add folio_migrate_flags();1;0
Turn migrate_page_states() into a wrapper around folio_migrate_flags();1;0
"Also convert two functions only called from folio_migrate_flags() to
be folio-based";0;0
" ksm_migrate_page() becomes folio_migrate_ksm() and
copy_page_owner() becomes folio_copy_owner()";1;0
" folio_migrate_flags()
alone shrinks by two thirds -- 1967 bytes down to 642 bytes.";0;0
mm/migrate: Add folio_migrate_mapping();1;1
"Reimplement migrate_page_move_mapping() as a wrapper around
folio_migrate_mapping()";1;0
 Saves 193 bytes of kernel text.;1;0
mm/memcg: Convert mem_cgroup_migrate() to take folios;1;0
Convert all callers of mem_cgroup_migrate() to call page_folio() first;1;0
They all look like they're using head pages already, but this proves it.;1;0
mm/memcg: Convert mem_cgroup_charge() to take a folio;1;0
"Convert all callers of mem_cgroup_charge() to call page_folio() on the
page they're currently passing in";1;0
" Many of them will be converted to
use folios themselves soon.";0;1
compat: remove some compat entry points;1;0
"These are all handled correctly when calling the native system call entry
point, so remove the special cases.";1;1
mm: simplify compat_sys_move_pages;1;1
"The compat move_pages() implementation uses compat_alloc_user_space() for
converting the pointer array";0;0
" Moving the compat handling into the
function itself is a bit simpler and lets us avoid the
compat_alloc_user_space() call.";1;1
mm: migrate: change to use bool type for 'page_was_mapped';1;1
"Change to use bool type for 'page_was_mapped' variable making it more
readable.";1;1
mm: migrate: fix the incorrect function name in comments;1;1
"since commit a98a2f0c8ce1 (""mm/rmap: split migration into its own
function""), the migration ptes establishment has been split into a
separate try_to_migrate() function, thus update the related comments.";0;1
mm: migrate: introduce a local variable to get the number of pages;1;1
"Use thp_nr_pages() instead of compound_nr() to get the number of pages for
THP page, meanwhile introducing a local variable 'nr_pages' to avoid
getting the number of pages repeatedly.";0;1
mm/migrate: correct kernel-doc notation;1;0
"Use the expected ""Return:"" format to prevent a kernel-doc warning";1;1
mm/migrate.c:1157: warning: Excess function parameter 'returns' description in 'next_demotion_node';0;1
mm/migrate: enable returning precise migrate_pages() success count;1;0
"Under normal circumstances, migrate_pages() returns the number of pages
migrated";0;1
 In error conditions, it returns an error code;1;0
" When returning
an error code, there is no way to know how many pages were migrated or not
migrated";0;1
"Make migrate_pages() return how many pages are demoted successfully for
all cases, including when encountering errors";1;1
" Page reclaim behavior will
depend on this in subsequent patches.";1;1
mm/migrate: update node demotion order on hotplug events;1;1
"Reclaim-based migration is attempting to optimize data placement in memory
based on the system topology";0;0
" If the system changes, so must the
migration ordering";1;1
The implementation is conceptually simple and entirely unoptimized;1;1
" On
any memory or CPU hotplug events, assume that a node was added or removed
and recalculate all migration targets";1;1
" This ensures that the
node_demotion[] array is always ready to be used in case the new reclaim
mode is enabled";1;1
"This recalculation is far from optimal, most glaringly that it does not
even attempt to figure out the hotplug event would have some *actual*
effect on the demotion order";0;0
" But, given the expected paucity of hotplug
events, this should be fine.";1;1
mm/numa: automatically generate node migration order;0;1
"Patch series ""Migrate Pages in lieu of discard"", v11";0;1
"We're starting to see systems with more and more kinds of memory such as
Intel's implementation of persistent memory";0;1
Let's say you have a system with some DRAM and some persistent memory;1;0
"Today, once DRAM fills up, reclaim will start and some of the DRAM
contents will be thrown out";1;1
" Allocations will, at some point, start
falling over to the slower persistent memory";1;1
That has two nasty properties;0;1
" First, the newer allocations can end up in
the slower persistent memory";1;1
" Second, reclaimed data in DRAM are just
discarded even if there are gobs of space in persistent memory that could
be used";1;0
This patchset implements a solution to these problems;0;1
" At the end of the
reclaim process in shrink_page_list() just before the last page refcount
is dropped, the page is migrated to persistent memory instead of being
dropped";0;0
"While I've talked about a DRAM/PMEM pairing, this approach would function
in any environment where memory tiers exist";1;1
This is not perfect;0;1
" It ""strands"" pages in slower memory and never brings
them back to fast DRAM";0;1
" Huang Ying has follow-on work which repurposes
NUMA balancing to promote hot pages back to DRAM";0;0
"This is also all based on an upstream mechanism that allows persistent
memory to be onlined and used as if it were volatile";1;0
"With that, the DRAM and PMEM in each socket will be represented as 2
separate NUMA nodes, with the CPUs sit in the DRAM node";1;0
" So the
general inter-NUMA demotion mechanism introduced in the patchset can
migrate the cold DRAM pages to the PMEM node";1;1
We have tested the patchset with the postgresql and pgbench;1;1
" On a
2-socket server machine with DRAM and PMEM, the kernel with the patchset
can improve the score of pgbench up to 22.1% compared with that of the
DRAM only + disk case";0;1
" This comes from the reduced disk read throughput
(which reduces up to 70.8%)";1;0
"== Open Issues ==
   to DRAM can be demoted to PMEM whenever they opt in to this
   new mechanism";0;1
" A cgroup-level API to opt-in or opt-out of
   these migrations will likely be required as a follow-on";0;0
   since it no longer necessarily involves I/O;1;1
" get_scan_count()
   for instance says: ""If we have no swap space, do not bother
   scanning anon pages""
This patch (of 9)";1;0
"Prepare for the kernel to auto-migrate pages to other memory nodes with a
node migration table";0;0
" This allows creating single migration target for
each NUMA node to enable the kernel to do NUMA page migrations instead of
simply discarding colder pages";1;1
" A node with no target is a ""terminal
node"", so reclaim acts normally there";0;0
" The migration target does not
fundamentally _need_ to be a single node, but this implementation starts
there to limit complexity";0;1
"When memory fills up on a node, memory contents can be automatically
migrated to another node";0;1
" The biggest problems are knowing when to
migrate and to where the migration should be targeted";0;0
"The most straightforward way to generate the ""to where"" list would be to
follow the page allocator fallback lists";0;0
" Those lists already tell us if
memory is full where to look next";1;1
" It would also be logical to move
memory in that order";1;0
"But, the allocator fallback lists have a fatal flaw: most nodes appear in
all the lists";0;0
" This would potentially lead to migration cycles (A->B,
B->A, A->B, ...)";0;1
"Instead of using the allocator fallback lists directly, keep a separate
node migration ordering";1;1
" But, reuse the same data used to generate page
allocator fallback in the first place: find_next_best_node()";0;0
"This means that the firmware data used to populate node distances
essentially dictates the ordering for now";0;1
" It should also be
architecture-neutral since all NUMA architectures have a working
find_next_best_node()";1;1
"RCU is used to allow lock-less read of node_demotion[] and prevent
demotion cycles been observed";0;1
" If multiple reads of node_demotion[] are
performed, a single rcu_read_lock() must be held over all reads to ensure
no cycles are observed";1;0
 Details are as follows;1;0
"=== What does RCU provide? ===
Imagine a simple loop which walks down the demotion path looking
for the last node";1;1
The initial values are;1;0
and are updated to;1;0
What guarantees that the cycle is not observed;1;1
"and would loop forever?
With RCU, a rcu_read_lock/unlock() can be placed around the loop";1;1
" Since
the write side does a synchronize_rcu(), the loop that observed the old
contents is known to be complete before the synchronize_rcu() has
completed";0;0
"RCU, combined with disable_all_migrate_targets(), ensures that the old
migration state is not visible by the time __set_migration_target_nodes()
is called";0;0
"=== What does READ_ONCE() provide? ===
READ_ONCE() forbids the compiler from merging or reordering successive
reads of node_demotion[]";1;0
" This ensures that any updates are *eventually*
observed";1;1
Consider the above loop again;1;0
" The compiler could theoretically read the
entirety of node_demotion[] into local storage (registers) and never go
back to memory, and *permanently* observe bad values for node_demotion[]";0;0
"Note: RCU does not provide any universal compiler-ordering
guarantees";1;0
This code is unused for now;0;1
" It will be called later in the
series.";0;0
mm/migrate: fix NR_ISOLATED corruption on 64-bit;1;1
"Similar to commit 2da9f6305f30 (""mm/vmscan: fix NR_ISOLATED_FILE
corruption on 64-bit"") avoid using unsigned int for nr_pages";1;1
" With
unsigned int type the large unsigned int converts to a large positive
signed long";0;1
"Symptoms include CMA allocations hanging forever due to
alloc_contig_range->...->isolate_migratepages_block waiting forever in
""while (unlikely(too_many_isolated(pgdat)))"".";1;1
mm: Make copy_huge_page() always available;1;0
"Rewrite copy_huge_page() and move it into mm/util.c so it's always
available";1;0
" Fixes an exposure of uninitialised memory on configurations
with HUGETLB and UFFD enabled and MIGRATION disabled";0;1
"Fixes: 8cc5fcbb5be8 (""mm, hugetlb: fix racy resv_huge_pages underflow on UFFDIO_COPY"")";1;1
mm: rename migrate_pgmap_owner;1;1
"MMU notifier ranges have a migrate_pgmap_owner field which is used by
drivers to store a pointer";0;0
" This is subsequently used by the driver
callback to filter MMU_NOTIFY_MIGRATE events";1;0
" Other notifier event types
can also benefit from this filtering, so rename the 'migrate_pgmap_owner'
field to 'owner' and create a new notifier initialisation function to
initialise this field.";1;1
mm/rmap: split migration into its own function;0;0
"Migration is currently implemented as a mode of operation for
try_to_unmap_one() generally specified by passing the TTU_MIGRATION flag
or in the case of splitting a huge anonymous page TTU_SPLIT_FREEZE";1;1
"However it does not have much in common with the rest of the unmap
functionality of try_to_unmap_one() and thus splitting it into a separate
function reduces the complexity of try_to_unmap_one() making it more
readable";0;1
"Several simplifications can also be made in try_to_migrate_one() based on
the following observations";0;1
 - All users of TTU_MIGRATION also set TTU_IGNORE_MLOCK;1;0
 - No users of TTU_MIGRATION ever set TTU_IGNORE_HWPOISON;1;0
 - No users of TTU_MIGRATION ever set TTU_BATCH_FLUSH;1;0
"TTU_SPLIT_FREEZE is a special case of migration used when splitting an
anonymous page";1;0
" This is most easily dealt with by calling the correct
function from unmap_page() in mm/huge_memory.c - either try_to_migrate()
for PageAnon or try_to_unmap().";0;1
mm/swapops: rework swap entry manipulation code;1;0
"Both migration and device private pages use special swap entries that are
manipluated by a range of inline functions";0;0
" The arguments to these are
somewhat inconsistent so rework them to remove flag type arguments and to
make the arguments similar for both read and write entry creation.";1;1
mm: remove special swap entry functions;1;0
"Patch series ""Add support for SVM atomics in Nouveau"", v11";1;1
"Introduction
Some devices have features such as atomic PTE bits that can be used to
implement atomic access to system memory";0;1
" To support atomic operations to
a shared virtual memory page such a device needs access to that page which
is exclusive of the CPU";0;1
" This series introduces a mechanism to
temporarily unmap pages granting exclusive access to a device";0;0
"These changes are required to support OpenCL atomic operations in Nouveau
to shared virtual memory (SVM) regions allocated with the
CL_MEM_SVM_ATOMICS clSVMAlloc flag";0;1
" A more complete description of the
OpenCL SVM feature is available at
OpenCL_API.html#_shared_virtual_memory ";0;0
"Implementation
Exclusive device access is implemented by adding a new swap entry type
(SWAP_DEVICE_EXCLUSIVE) which is similar to a migration entry";1;0
" The main
difference is that on fault the original entry is immediately restored by
the fault handler instead of waiting";0;0
"Restoring the entry triggers calls to MMU notifers which allows a device
driver to revoke the atomic access permission from the GPU prior to the
CPU finalising the entry";0;1
"Patches
Patches 1 & 2 refactor existing migration and device private entry
functions";1;1
"Patches 3 & 4 rework try_to_unmap_one() by splitting out unrelated
functionality into separate functions - try_to_migrate_one() and
try_to_munlock_one()";0;0
Patch 5 renames some existing code but does not introduce functionality;1;0
Patch 6 is a small clean-up to swap entry handling in copy_pte_range();1;1
"Patch 7 contains the bulk of the implementation for device exclusive
memory";1;1
"Patch 8 contains some additions to the HMM selftests to ensure everything
works as expected";1;1
Patch 9 is a cleanup for the Nouveau SVM implementation;1;1
"Patch 10 contains the implementation of atomic access for the Nouveau
driver";1;1
"Testing
This has been tested with upstream Mesa 21.1.0 and a simple OpenCL program
which checks that GPU atomic accesses to system memory are atomic";1;1
"Without this series the test fails as there is no way of write-protecting
the page mapping which results in the device clobbering CPU writes";0;1
" For
reference the test is available at
Further testing has been performed by adding support for testing exclusive
access to the hmm-tests kselftests";0;0
This patch (of 10);1;0
"Remove multiple similar inline functions for dealing with different types
of special swap entries";1;0
"Both migration and device private swap entries use the swap offset to
store a pfn";1;0
" Instead of multiple inline functions to obtain a struct page
for each swap entry type use a common function pfn_swap_entry_to_page()";1;0
"Also open-code the various entry_to_pfn() functions as this results is
shorter code that is easier to understand.";0;1
mm: migrate: check mapcount for THP instead of refcount;1;1
"The generic migration path will check refcount, so no need check refcount
here";1;1
" But the old code actually prevents from migrating shared THP
(mapped by multiple processes), so bail out early if mapcount is > 1 to
keep the behavior.";1;1
mm: migrate: don't split THP for misplaced NUMA page;1;0
"The old behavior didn't split THP if migration is failed due to lack of
memory on the target node";0;1
" But the THP migration does split THP, so keep
the old behavior for misplaced NUMA page migration.";0;0
mm: migrate: account THP NUMA migration counters correctly;1;1
"Now both base page and THP NUMA migration is done via
migrate_misplaced_page(), keep the counters correctly for THP.";1;1
mm: thp: refactor NUMA fault handling;1;0
"When the THP NUMA fault support was added THP migration was not supported
yet";0;0
 So the ad hoc THP migration was implemented in NUMA fault handling;1;0
"Since v4.14 THP migration has been supported so it doesn't make too much
sense to still keep another THP migration implementation rather than using
the generic migration code";1;1
"This patch reworks the NUMA fault handling to use generic migration
implementation to migrate misplaced page";1;1
 There is no functional change;1;0
"After the refactor the flow of NUMA fault handling looks just like its
PTE counterpart";0;0
"  Acquire ptl
  Prepare for migration (elevate page refcount)
  Release ptl
  Isolate page from lru and elevate page refcount
  Migrate the misplaced THP
If migration fails just restore the old normal PMD";1;0
"In the old code anon_vma lock was needed to serialize THP migration
against THP split, but since then the THP code has been reworked a lot, it
seems anon_vma lock is not required anymore to avoid the race";0;1
"The page refcount elevation when holding ptl should prevent from THP
split";1;1
"Use migrate_misplaced_page() for both base page and THP NUMA hinting fault
and remove all the dead and duplicate code.";0;0
mm: migrate: fix missing update page_private to hugetlb_page_subpool;1;1
"Since commit d6995da31122 (""hugetlb: use page.private for hugetlb specific
page flags"") converts page.private for hugetlb specific page flags";0;0
" We
should use hugetlb_page_subpool() to get the subpool pointer instead of
page_private()";1;1
This 'could' prevent the migration of hugetlb pages;1;1
" page_private(hpage)
is now used for hugetlb page specific flags";0;1
" At migration time, the only
flag which could be set is HPageVmemmapOptimized";0;0
" This flag will only be
set if the new vmemmap reduction feature is enabled";1;1
" In addition,
!page_mapping() implies an anonymous mapping";1;1
" So, this will prevent
migration of hugetb pages in anonymous mappings if the vmemmap reduction
feature is enabled";1;1
"In addition, that if statement checked for the rare race condition of a
page being migrated while in the process of being freed";0;1
" Since that check
is now wrong, we could leak hugetlb subpool usage counts";0;1
The commit forgot to update it in the page migration routine;0;0
 So fix it.;1;1
mm, hugetlb: fix racy resv_huge_pages underflow on UFFDIO_COPY;1;1
"On UFFDIO_COPY, if we fail to copy the page contents while holding the
hugetlb_fault_mutex, we will drop the mutex and return to the caller after
allocating a page that consumed a reservation";1;1
" In this case there may be
a fault that double consumes the reservation";0;0
" To handle this, we free the
allocated page, fix the reservations, and allocate a temporary hugetlb
page and return that to the caller";1;1
" When the caller does the copy outside
of the lock, we again check the cache, and allocate a page consuming the
reservation, and copy over the contents";0;1
Test;0;0
"Hacked the code locally such that resv_huge_pages underflows produce
a warning and the copy_huge_page_from_user() always fails, then";1;0
"./tools/testing/selftests/vm/userfaultfd hugetlb_shared 10
        2 /tmp/kokonut_test/huge/userfaultfd_test && echo test success
./tools/testing/selftests/vm/userfaultfd hugetlb 10
	2 /tmp/kokonut_test/huge/userfaultfd_test && echo test success
Both tests succeed and produce no warnings";0;1
"After the
test runs number of free/resv hugepages is correct.";0;1
mm/hugetlb: change parameters of arch_make_huge_pte();1;0
"Patch series ""Subject: [PATCH v2 0/5] Implement huge VMAP and VMALLOC on powerpc 8xx"", v2";1;0
This series implements huge VMAP and VMALLOC on powerpc 8xx;0;1
Powerpc 8xx has 4 page sizes;0;0
"- 16k
- 512k
At the time being, vmalloc and vmap only support huge pages which are
leaf at PMD level";0;0
"Here the PMD level is 4M, it doesn't correspond to any supported
page size";1;1
"For now, implement use of 16k and 512k pages which is done
at PTE level";1;0
"Support of 8M pages will be implemented later, it requires use of
hugepd tables";0;0
To allow this, the architecture provides two functions;1;0
"- arch_vmap_pte_range_map_size() which tells vmap_pte_range() what
page size to use";1;0
"A stub returning PAGE_SIZE is provided when the
architecture doesn't provide this function";1;0
what page shift to use for a given area size;1;0
"A stub returning
PAGE_SHIFT is provided when the architecture doesn't provide this
function";0;0
This patch (of 5);1;0
At the time being, arch_make_huge_pte() has the following prototype;0;0
"  pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
vma is used to get the pages shift or size";1;0
vma is also used on Sparc to get vm_flags;1;0
page is not used;0;0
writable is not used;0;0
"In order to use this function without a vma, replace vma by shift and
flags";1;1
 Also remove the used parameters.;1;0
mm: hugetlb: alloc the vmemmap pages associated with each HugeTLB page;0;0
"When we free a HugeTLB page to the buddy allocator, we need to allocate
the vmemmap pages associated with it";0;0
" However, we may not be able to
allocate the vmemmap pages when the system is under memory pressure";0;0
" In
this case, we just refuse to free the HugeTLB page";1;1
" This changes behavior
in some corner cases as listed below";1;1
 1) Failing to free a huge page triggered by the user (decrease nr_pages);0;0
    User needs to try again later;0;1
 2) Failing to free a surplus huge page when freed by the application;0;0
    Try again later when freeing a huge page next time;0;1
" 3) Failing to dissolve a free huge page on ZONE_MOVABLE via
    offline_pages()";0;0
"    This can happen when we have plenty of ZONE_MOVABLE memory, but
    not enough kernel memory to allocate vmemmmap pages";0;1
" We may even
    be able to migrate huge page contents, but will not be able to
    dissolve the source huge page";0;1
" This will prevent an offline
    operation and is unfortunate as memory offlining is expected to
    succeed on movable zones";1;1
" Users that depend on memory hotplug
    to succeed for movable zones should carefully consider whether the
    memory savings gained from this feature are worth the risk of
    possibly not being able to offline memory in certain situations";0;0
" 4) Failing to dissolve a huge page on CMA/ZONE_MOVABLE via
    alloc_contig_range() - once we have that handling in place";1;1
"Mainly
    affects CMA and virtio-mem";1;1
    Similar to 3);1;0
virito-mem will handle migration errors gracefully;0;1
"    CMA might be able to fallback on other free areas within the CMA
    region";0;1
Vmemmap pages are allocated from the page freeing context;0;0
" In order for
those allocations to be not disruptive (e.g";0;1
" trigger oom killer)
__GFP_NORETRY is used";1;0
" hugetlb_lock is dropped for the allocation because
a non sleeping allocation would be too fragile and it could fail too
easily under memory pressure";1;1
" GFP_ATOMIC or other modes to access memory
reserves is not used because we want to prevent consuming reserves under
heavy hugetlb freeing.";0;1
mm/migrate: use vma_lookup() in do_pages_stat_array();1;1
"will return NULL if the address is not within any VMA, the start address
no longer needs to be validated.";1;1
mm, thp: use head page in __migration_entry_wait();1;1
"We notice that hung task happens in a corner but practical scenario when
CONFIG_PREEMPT_NONE is enabled, as follows";1;0
"Process 0                       Process 1                     Process 2..Inf
split_huge_page_to_list
    unmap_page
        split_huge_pmd_address
                                __migration_entry_wait(head)
                                                              __migration_entry_wait(tail)
    remap_page (roll back)
        remove_migration_ptes
            rmap_walk_anon
                cond_resched
Where __migration_entry_wait(tail) is occurred in kernel space, e.g.,
copy_to_user in fstat, which will immediately fault again without
rescheduling, and thus occupy the cpu fully";0;1
"When there are too many processes performing __migration_entry_wait on
tail page, remap_page will never be done after cond_resched";0;0
"This makes __migration_entry_wait operate on the compound head page,
thus waits for remap_page to complete, whether the THP is split
successfully or roll back";0;0
"Note that put_and_wait_on_page_locked helps to drop the page reference
acquired with get_page_unless_zero, as soon as the page is on the wait
queue, before actually waiting";1;1
" So splitting the THP is only prevented
for a brief interval.";1;0
mm: fix typos in comments;1;1
"Fix ~94 single-word typos in locking code comments, plus a few
very obvious grammar mistakes.";1;1
mm: cma: add trace events for CMA alloc perf testing;1;1
"Add cma and migrate trace events to enable CMA allocation performance to
be measured via ftrace.";1;0
"Revert ""mm: migrate: skip shared exec THP for NUMA balancing""";1;0
This reverts commit c77c5cbafe549eb330e8909861a3e16cbda2c848;1;0
"Since commit c77c5cbafe54 (""mm: migrate: skip shared exec THP for NUMA
balancing""), the NUMA balancing would skip shared exec transhuge page";0;0
But this enhancement is not suitable for transhuge page;0;1
" Because it's
required that page_mapcount() must be 1 due to no migration pte dance is
done here";0;0
" On the other hand, the shared exec transhuge page will leave
the migrate_misplaced_page() with pte entry untouched and page locked";0;1
"Thus pagefault for NUMA will be triggered again and deadlock occurs when
we start waiting for the page lock held by ourselves";0;1
Yang Shi said;0;1
" ""Thanks for catching this";0;0
"By relooking the code I think the other
  important reason for removing this is
  migrate_misplaced_transhuge_page() actually can't see shared exec
  file THP at all since page_lock_anon_vma_read() is called before
  and if page is not anonymous page it will just restore the PMD
  without migrating anything";0;1
"  The pages for private mapped file vma may be anonymous pages due to
  COW but they can't be THP so it won't trigger THP numa fault at all";0;1
"I
  think this is why no bug was reported";0;1
"I overlooked this in the first
  place.""";1;0
mm/migrate.c: use helper migrate_vma_collect_skip() in migrate_vma_collect_hole();1;1
"It's more recommended to use helper function migrate_vma_collect_skip() to
skip the unexpected case and it also helps remove some duplicated codes";1;1
"Move migrate_vma_collect_skip() above migrate_vma_collect_hole() to avoid
compiler warning.";1;1
mm/migrate.c: fix potential indeterminate pte entry in migrate_vma_insert_page();1;1
"If the zone device page does not belong to un-addressable device memory,
the variable entry will be uninitialized and lead to indeterminate pte
entry ultimately";0;1
 Fix this unexpected case and warn about it.;1;1
mm/migrate.c: remove unnecessary rc != MIGRATEPAGE_SUCCESS check in 'else' case;1;1
"It's guaranteed that in the 'else' case of the rc == MIGRATEPAGE_SUCCESS
check, rc does not equal to MIGRATEPAGE_SUCCESS";0;0
" Remove this unnecessary
check.";1;1
mm/migrate.c: make putback_movable_page() static;1;0
"Patch series ""Cleanup and fixup for mm/migrate.c"", v3";0;1
"This series contains cleanups to remove unnecessary VM_BUG_ON_PAGE and rc
!= MIGRATEPAGE_SUCCESS check";0;1
" Also use helper function to remove some
duplicated codes";1;1
" What's more, this fixes potential deadlock in NUMA
balancing shared exec THP case and so on";0;1
" More details can be found in
the respective changelogs";0;1
This patch (of 5);1;0
"The putback_movable_page() is just called by putback_movable_pages() and
we know the page is locked and both PageMovable() and PageIsolated() is
checked right before calling putback_movable_page()";0;1
" So we make it static
and remove all the 3 VM_BUG_ON_PAGE().";1;0
mm: replace migrate_[prep|finish] with lru_cache_[disable|enable];1;1
"Currently, migrate_[prep|finish] is merely a wrapper of
lru_cache_[disable|enable]";1;1
" There is not much to gain from having
additional abstraction";0;0
"Use lru_cache_[disable|enable] instead of migrate_[prep|finish], which
would be more descriptive";1;1
"note: migrate_prep_local in compaction.c changed into lru_add_drain to
avoid CPU schedule cost with involving many other CPUs to keep old
behavior.";1;1
mm: disable LRU pagevec during the migration temporarily;1;0
LRU pagevec holds refcount of pages until the pagevec are drained;1;0
" It
could prevent migration since the refcount of the page is greater than
the expection in migration logic";1;1
" To mitigate the issue, callers of
migrate_pages drains LRU pagevec via migrate_prep or lru_add_drain_all
before migrate_pages call";1;0
"However, it's not enough because pages coming into pagevec after the
draining call still could stay at the pagevec so it could keep
preventing page migration";1;1
" Since some callers of migrate_pages have
retrial logic with LRU draining, the page would migrate at next trail
but it is still fragile in that it doesn't close the fundamental race
between upcoming LRU pages into pagvec and migration so the migration
failure could cause contiguous memory allocation failure in the end";0;1
"To close the race, this patch disables lru caches(i.e, pagevec) during
ongoing migration until migrate is done";1;0
"Since it's really hard to reproduce, I measured how many times
migrate_pages retried with force mode(it is about a fallback to a sync
migration) with below debug code";1;1
"int migrate_pages(struct list_head *from, new_page_t get_new_page,
The test was repeating android apps launching with cma allocation in
background every five seconds";0;1
" Total cma allocation count was about 500
during the testing";1;1
" With this patch, the dump_page count was reduced
from 400 to 30";1;1
"The new interface is also useful for memory hotplug which currently
drains lru pcp caches after each migration failure";0;1
" This is rather
suboptimal as it has to disrupt others running during the operation";0;1
With the new interface the operation happens only once;1;0
" This is also in
line with pcp allocator cache which are disabled for the offlining as
well.";0;0
mm/page_alloc: combine __alloc_pages and __alloc_pages_nodemask;1;1
"There are only two callers of __alloc_pages() so prune the thicket of
alloc_page variants by combining the two functions together";0;0
" Current
callers of __alloc_pages() simply add an extra 'NULL' parameter and
current callers of __alloc_pages_nodemask() call __alloc_pages() instead.";1;0
mm: memcg: add swapcache stat for memcg v2;1;0
This patch adds swapcache stat for the cgroup v2;1;1
" The swapcache
represents the memory that is accounted against both the memory and the
swap limit of the cgroup";1;0
" The main motivation behind exposing the
swapcache stat is for enabling users to gracefully migrate from cgroup
v1's memsw counter to cgroup v2's memory and swap counters";1;0
"Cgroup v1's memsw limit allows users to limit the memory+swap usage of a
workload but without control on the exact proportion of memory and swap";1;1
"Cgroup v2 provides separate limits for memory and swap which enables more
control on the exact usage of memory and swap individually for the
workload";1;0
"With some little subtleties, the v1's memsw limit can be switched with the
sum of the v2's memory and swap limits";1;0
" However the alternative for memsw
usage is not yet available in cgroup v2";0;0
" Exposing per-cgroup swapcache
stat enables that alternative";1;1
" Adding the memory usage and swap usage and
subtracting the swapcache will approximate the memsw usage";1;0
" This will
help in the transparent migration of the workloads depending on memsw
usage and limit to v2' memory and swap counters";1;1
"The reasons these applications are still interested in this approximate
memsw usage are: (1) these applications are not really interested in two
separate memory and swap usage metrics";1;1
" A single usage metric is more
simple to use and reason about for them";1;1
"(2) The memsw usage metric hides the underlying system's swap setup from
the applications";1;0
" Applications with multiple instances running in a
datacenter with heterogeneous systems (some have swap and some don't) will
keep seeing a consistent view of their usage.";1;0
mm/filemap: pass a sleep state to put_and_wait_on_page_locked;1;0
"This is prep work for the next patch, but I think at least one of the
current callers would prefer a killable sleep to an uninterruptible one.";0;0
mm: migrate: do not migrate HugeTLB page whose refcount is one;1;1
"All pages isolated for the migration have an elevated reference count and
therefore seeing a reference count equal to 1 means that the last user of
the page has dropped the reference and the page has became unused and
there doesn't make much sense to migrate it anymore";0;1
"This has been done for regular pages and this patch does the same for
hugetlb pages";1;1
" Although the likelihood of the race is rather small for
hugetlb pages it makes sense the two code paths in sync.";0;1
mm: fix numa stats for thp migration;1;1
"Currently the kernel is not correctly updating the numa stats for
NR_FILE_PAGES and NR_SHMEM on THP migration";1;1
 Fix that;1;1
"For NR_FILE_DIRTY and NR_ZONE_WRITE_PENDING, although at the moment
there is no need to handle THP migration as kernel still does not have
write support for file THP but to be more future proof, this patch adds
the THP support for those stats as well.";1;1
mm: memcg: fix memcg file_dirty numa stat;1;1
"The kernel updates the per-node NR_FILE_DIRTY stats on page migration
but not the memcg numa stats";0;1
"That was not an issue until recently the commit 5f9a4f4a7096 (""mm";0;0
"memcontrol: add the missing numa_stat interface for cgroup v2"") exposed
numa stats for the memcg";1;0
So fix the file_dirty per-memcg numa stat.;1;1
mm: fix some spelling mistakes in comments;1;1
Fix some spelling mistakes in comments;1;1
"	udpate ==> update
	succesful ==> successful
	exmaple ==> example
	unneccessary ==> unnecessary
	stoping ==> stopping
	uknown ==> unknown";0;1
mm: migrate: remove unused parameter in migrate_vma_insert_page();1;1
"""dst"" parameter to migrate_vma_insert_page() is not used anymore.";0;1
mm: migrate: return -ENOSYS if THP migration is unsupported;1;1
"In the current implementation unmap_and_move() would return -ENOMEM if THP
migration is unsupported, then the THP will be split";0;0
" If split is failed
just exit without trying to migrate other pages";0;1
" It doesn't make too much
sense since there may be enough free memory to migrate other pages and
there may be a lot base pages on the list";0;1
Return -ENOSYS to make consistent with hugetlb;1;0
" And if THP split is
failed just skip and try other pages on the list";1;1
Just skip the whole list and exit when free memory is really low.;0;0
mm: migrate: clean up migrate_prep{_local};1;1
"The migrate_prep{_local} never fails, so it is pointless to have return
value and check the return value.";0;1
mm: migrate: skip shared exec THP for NUMA balancing;1;0
The NUMA balancing skip shared exec base page;1;0
" Since
CONFIG_READ_ONLY_THP_FOR_FS was introduced, there are probably shared exec
THP, so skip such THPs for NUMA balancing as well";0;0
"And Willy's regular filesystem THP support patches could create shared
exec THP wven without that config";1;1
"In addition, the page_is_file_lru() is used to tell if the page is file
cache or not, but it filters out shmem page";1;1
" It sounds like a typical
usecase by putting executables in shmem to achieve performance gain via
using shmem-THP, so it sounds worth skipping migration for such case too.";1;1
mm: migrate: simplify the logic for handling permanent failure;1;1
"!MIGRATEPAGE_SUCCESS, the page would be put back to LRU or proper list if
it is non-LRU movable page";1;1
" But, the callers always call
putback_movable_pages() to put the failed pages back later on, so it seems
not very efficient to put every single page back immediately, and the code
looks convoluted";0;1
"Put the failed page on a separate list, then splice the list to migrate
list when all pages are tried";1;0
" It is the caller's responsibility to call
putback_movable_pages() to handle failures";0;1
" This also makes the code
simpler and more readable";1;1
After the change the rules are;1;0
"               back
The from list would be empty iff all pages are migrated successfully, it
was not so before";0;1
 This has no impact to current existing callsites.;1;1
mm: truncate_complete_page() does not exist any more;1;1
"Patch series ""mm: misc migrate cleanup and improvement"", v3";1;1
This patch (of 5);1;0
"The commit 9f4e41f4717832e (""mm: refactor truncate_complete_page()"")
refactored truncate_complete_page(), and it is not existed anymore,
correct the comment in vmscan and migrate to avoid confusion.";1;1
mm/migrate.c: optimize migrate_vma_pages() mmu notifier;1;1
"When migrating a zero page or pte_none() anonymous page to device private
memory, migrate_vma_setup() will initialize the src[] array with a NULL
PFN";0;1
" This lets the device driver allocate device private memory and clear
it instead of DMAing a page of zeros over the device bus";0;1
"Since the source page didn't exist at the time, no struct page was locked
nor a migration PTE inserted into the CPU page tables";0;0
" The actual PTE
insertion happens in migrate_vma_pages() when it tries to insert the
device private struct page PTE into the CPU page tables";1;0
"migrate_vma_pages() has to call the mmu notifiers again since another
device could fault on the same page before the page table locks are
acquired";0;1
"Allow device drivers to optimize the invalidation similar to
migrate_vma_setup() by calling mmu_notifier_range_init() which sets struct
mmu_notifier_range event type to MMU_NOTIFY_MIGRATE and the
migrate_pgmap_owner field.";0;1
mm/migrate.c: fix comment spelling;1;1
"The word in the comment is misspelled, it should be ""include"".";1;1
mm/rmap: always do TTU_IGNORE_ACCESS;1;0
"Since commit 369ea8242c0f (""mm/rmap: update to new mmu_notifier semantic
v2""), the code to check the secondary MMU's page table access bit is
broken for !(TTU_IGNORE_ACCESS) because the page is unmapped from the
secondary MMU's page table before the check";0;1
" More specifically for those
secondary MMUs which unmap the memory in
mmu_notifier_invalidate_range_start() like kvm";0;0
"However memory reclaim is the only user of !(TTU_IGNORE_ACCESS) or the
absence of TTU_IGNORE_ACCESS and it explicitly performs the page table
access check before trying to unmap the page";0;1
" So, at worst the reclaim
will miss accesses in a very short window if we remove page table access
check in unmapping code";1;1
"There is an unintented consequence of !(TTU_IGNORE_ACCESS) for the memcg
reclaim";0;0
" From memcg reclaim the page_referenced() only account the
accesses from the processes which are in the same memcg of the target page
but the unmapping code is considering accesses from all the processes, so,
decreasing the effectiveness of memcg reclaim";1;0
"The simplest solution is to always assume TTU_IGNORE_ACCESS in unmapping
code.";0;1
hugetlbfs: fix anon huge page migration race;1;1
"Qian Cai reported the following BUG in [1]
  LTP: starting move_pages12
  BUG: unable to handle page fault for address: ffffffffffffffe0
  RIP: 0010:anon_vma_interval_tree_iter_first+0xa2/0x170 avc_start_pgoff at mm/interval_tree.c:63
  Hugh Dickins diagnosed this as a migration bug caused by code introduced
to use i_mmap_rwsem for pmd sharing synchronization";1;0
" Specifically, the
routine unmap_and_move_huge_page() is always passing the TTU_RMAP_LOCKED
flag to try_to_unmap() while holding i_mmap_rwsem";0;1
" This is wrong for
anon pages as the anon_vma_lock should be held in this case";1;1
" Further
analysis suggested that i_mmap_rwsem was not required to he held at all
when calling try_to_unmap for anon pages as an anon page could never be
part of a shared pmd mapping";0;1
"Discussion also revealed that the hack in hugetlb_page_mapping_lock_write
to drop page lock and acquire i_mmap_rwsem is wrong";1;0
" There is no way to
keep mapping valid while dropping page lock";1;1
This patch does the following;1;0
" - Do not take i_mmap_rwsem and set TTU_RMAP_LOCKED for anon pages when
   calling try_to_unmap";1;0
 - Remove the hacky code in hugetlb_page_mapping_lock_write;1;0
"The routine
   will now simply do a 'trylock' while still holding the page lock";1;0
"If
   the trylock fails, it will return NULL";1;0
"This could impact the
   callers";0;1
"    - migration calling code will receive -EAGAIN and retry up to the
      hard coded limit (10)";1;0
    - memory error code will treat the page as BUSY;1;0
"This will force
      killing (SIGKILL) instead of SIGBUS any mapping tasks";1;1
"   Do note that this change in behavior only happens when there is a
   race";1;0
"None of the standard kernel testing suites actually hit this
   race, but it is possible.";0;1
mm/migrate: avoid possible unnecessary process right check in kernel_move_pages();1;1
"There is no need to check if this process has the right to modify the
specified process when they are same";0;1
" And we could also skip the security
hook call if a process is modifying its own pages";1;1
" Add helper function to
handle these.";1;0
mm,hwpoison: rework soft offline for in-use pages;1;0
This patch changes the way we set and handle in-use poisoned pages;1;1
" Until
now, poisoned pages were released to the buddy allocator, trusting that
the checks that take place at allocation time would act as a safe net and
would skip that page";1;1
"This has proved to be wrong, as we got some pfn walkers out there, like
compaction, that all they care is the page to be in a buddy freelist";0;1
"Although this might not be the only user, having poisoned pages in the
buddy allocator seems a bad idea as we should only have free pages that
are ready and meant to be used as such";0;1
"Before explaining the taken approach, let us break down the kind of pages
we can soft offline";1;1
"- Anonymous THP (after the split, they end up being 4K pages)
- Hugetlb
- Order-0 pages (that can be either migrated or invalited)
  - If they are clean and unmapped page cache pages, we invalidate
    then by means of invalidate_inode_page()";0;0
  - If they are mapped/dirty, we do the isolate-and-migrate dance;0;1
Either way, do not call put_page directly from those paths;1;1
" Instead, we
keep the page and send it to page_handle_poison to perform the right
handling";1;1
page_handle_poison sets the HWPoison flag and does the last put_page;0;0
"Down the chain, we placed a check for HWPoison page in
free_pages_prepare, that just skips any poisoned page, so those pages
do not end up in any pcplist/freelist";1;1
"After that, we set the refcount on the page to 1 and we increment
the poisoned pages counter";0;0
"If we see that the check in free_pages_prepare creates trouble, we can
always do what we do for free pages";0;1
"  - wait until the page hits buddy's freelists
  - take it off, and flag it
The downside of the above approach is that we could race with an
allocation, so by the time we  want to take the page off the buddy, the
page has been already allocated so we cannot soft offline it";0;1
But the user could always retry it;1;1
"  - We isolate-and-migrate them
After the migration has been successful, we call dissolve_free_huge_page,
and we set HWPoison on the page if we succeed";0;0
Hugetlb has a slightly different handling though;0;1
"While for non-hugetlb pages we cared about closing the race with an
allocation, doing so for hugetlb pages requires quite some additional
and intrusive code (we would need to hook in free_huge_page and some other
places)";0;1
"So I decided to not make the code overly complicated and just fail
normally if the page we allocated in the meantime";1;1
We can always build on top of this;0;1
"As a bonus, because of the way we handle now in-use pages, we no longer
need the put-as-isolation-migratetype dance, that was guarding for poisoned
pages to end up in pcplists.";1;1
mm/migrate: remove obsolete comment about device public;1;1
"Device public memory never had an in tree consumer and was removed in
commit 25b2995a35b6 (""mm: remove MEMORY_DEVICE_PUBLIC support"")";1;0
" Delete
the obsolete comment.";1;1
mm/migrate: remove cpages-- in migrate_vma_finalize();1;1
"The variable struct migrate_vma->cpages is only used in
migrate_vma_setup()";1;1
" There is no need to decrement it in
migrate_vma_finalize() since it is never checked.";1;1
mm/migrate: correct thp migration stats;1;1
"PageTransHuge returns true for both thp and hugetlb, so thp stats was
counting both thp and hugetlb migrations";1;1
" Exclude hugetlb migration by
setting is_thp variable right";0;1
Clean up thp handling code too when we are there;1;1
"Fixes: 1a5bae25e3cf (""mm/vmstat: add events for THP migration without split"")";1;1
bdi: replace BDI_CAP_NO_{WRITEBACK,ACCT_DIRTY} with a single flag;1;1
"Replace the two negative flags that are always used together with a
single positive flag that indicates the writeback capability instead
of two related non-capabilities";1;1
" Also remove the pointless wrappers
to just check the flag.";1;1
mm: migration of hugetlbfs page skip memcg;1;0
"hugetlbfs pages do not participate in memcg: so although they do find most
of migrate_page_states() useful, it would be better if they did not call
into mem_cgroup_migrate() - where Qian Cai reported that LTP's
move_pages12 triggers the warning in Alex Shi's prospective commit
""mm/memcg: warning on !memcg after readahead page charged"".";1;1
mm/migrate: preserve soft dirty in remove_migration_pte();1;0
"The code to remove a migration PTE and replace it with a device private
PTE was not copying the soft dirty bit from the migration entry";1;1
" This
could lead to page contents not being marked dirty when faulting the page
back from device private memory.";0;1
mm/migrate: remove unnecessary is_zone_device_page() check;1;1
"Patch series ""mm/migrate: preserve soft dirty in remove_migration_pte()""";1;1
"I happened to notice this from code inspection after seeing Alistair
Popple's patch (""mm/rmap: Fixup copying of soft dirty and uffd ptes"")";1;1
This patch (of 2);1;0
"The check for is_zone_device_page() and is_device_private_page() is
unnecessary since the latter is sufficient to determine if the page is a
device private page";0;1
 Simplify the code for easier reading.;1;1
mm/rmap: fixup copying of soft dirty and uffd ptes;0;1
"During memory migration a pte is temporarily replaced with a migration
swap pte";1;0
" Some pte bits from the existing mapping such as the soft-dirty
and uffd write-protect bits are preserved by copying these to the
temporary migration swap pte";0;1
"However these bits are not stored at the same location for swap and
non-swap ptes";1;0
" Therefore testing these bits requires using the
appropriate helper function for the given pte type";1;0
"Unfortunately several code locations were found where the wrong helper
function is being used to test soft_dirty and uffd_wp bits which leads to
them getting incorrectly set or cleared during page-migration";0;1
Fix these by using the correct tests based on pte type;1;1
"Fixes: a5430dda8a3a (""mm/migrate: support un-addressable ZONE_DEVICE page in migration"")
Fixes: 8c3328f1f36a (""mm/migrate: migrate_vma() unmap page from vma while collecting pages"")
Fixes: f45ec5ff16a7 (""userfaultfd: wp: support swap and page migration"")";0;1
mm/migrate: fixup setting UFFD_WP flag;1;1
"Commit f45ec5ff16a75 (""userfaultfd: wp: support swap and page migration"")
introduced support for tracking the uffd wp bit during page migration";0;0
"However the non-swap PTE variant was used to set the flag for zone device
private pages which are a type of swap page";0;0
"This leads to corruption of the swap offset if the original PTE has the
uffd_wp flag set";1;0
"Fixes: f45ec5ff16a75 (""userfaultfd: wp: support swap and page migration"")";1;1
mm: replace hpage_nr_pages with thp_nr_pages;1;1
"The thp prefix is more frequently used than hpage and we should be
consistent between the various functions.";1;1
mm/mempolicy: use a standard migration target allocation callback;1;0
There is a well-defined migration target allocation callback;1;1
 Use it.;1;1
mm/migrate: introduce a standard migration target allocation function;1;0
There are some similar functions for migration target allocation;0;0
" Since
there is no fundamental difference, it's better to keep just one rather
than keeping all variants";1;1
" This patch implements base migration target
allocation function";0;0
" In the following patches, variants will be converted
to use this function";1;0
"Changes should be mechanical, but, unfortunately, there are some
differences";0;1
" First, some callers' nodemask is assgined to NULL since NULL
nodemask will be considered as all available nodes, that is,
&node_states[N_MEMORY]";1;1
" Second, for hugetlb page allocation, gfp_mask is
redefined as regular hugetlb allocation gfp_mask plus __GFP_THISNODE if
user provided gfp_mask has it";1;0
" This is because future caller of this
function requires to set this node constaint";0;1
" Lastly, if provided nodeid
is NUMA_NO_NODE, nodeid is set up to the node where migration source
lives";0;1
 It helps to remove simple wrappers for setting up the nodeid;1;1
"Note that PageHighmem() call in previous function is changed to open-code
""is_highmem_idx()"" since it provides more readability.";0;1
mm/migrate: clear __GFP_RECLAIM to make the migration callback consistent with regular THP allocations;0;1
"new_page_nodemask is a migration callback and it tries to use a common gfp
flags for the target page allocation whether it is a base page or a THP";0;0
The later only adds GFP_TRANSHUGE to the given mask;0;0
" This results in the
allocation being slightly more aggressive than necessary because the
resulting gfp mask will contain also __GFP_RECLAIM_KSWAPD";0;1
" THP
allocations usually exclude this flag to reduce over eager background
reclaim during a high THP allocation load which has been seen during large
mmaps initialization";1;1
" There is no indication that this is a problem for
migration as well but theoretically the same might happen when migrating
large mappings to a different node";0;1
" Make the migration callback
consistent with regular THP allocations.";1;1
mm/hugetlb: unify migration callbacks;1;0
"There is no difference between two migration callback functions,
alloc_huge_page_node() and alloc_huge_page_nodemask(), except
__GFP_THISNODE handling";0;1
" It's redundant to have two almost similar
functions in order to handle this flag";0;1
" So, this patch tries to remove
one by introducing a new argument, gfp_mask, to
alloc_huge_page_nodemask()";1;0
"After introducing gfp_mask argument, it's caller's job to provide correct
gfp_mask";0;1
" So, every callsites for alloc_huge_page_nodemask() are changed
to provide gfp_mask";1;0
"Note that it's safe to remove a node id check in alloc_huge_page_node()
since there is no caller passing NUMA_NO_NODE as a node id.";1;1
mm/migrate: move migration helper from .h to .c;1;0
It's not performance sensitive function;0;0
 Move it to .c;1;0
" This is a
preparation step for future change.";0;0
mm/migrate.c: delete duplicated word;0;1
"Drop the repeated word ""and"".";1;0
mm/vmstat: add events for THP migration without split;1;0
"Add following new vmstat events which will help in validating THP
migration without split";1;1
" Statistics reported through these new VM events
will help in performance debugging";0;1
1;0;0
"THP_MIGRATION_SUCCESS
2";1;0
"THP_MIGRATION_FAILURE
3";1;1
"THP_MIGRATION_SPLIT
In addition, these new events also update normal page migration statistics
appropriately via PGMIGRATE_SUCCESS and PGMIGRATE_FAILURE";1;1
" While here,
this updates current trace event 'mm_migrate_pages' to accommodate now
available THP statistics.";1;1
mm/migrate: optimize migrate_vma_setup() for holes;1;1
"Patch series ""mm/migrate: optimize migrate_vma_setup() for holes""";1;1
"A simple optimization for migrate_vma_*() when the source vma is not an
anonymous vma and a new test case to exercise it";1;1
This patch (of 2);1;0
"When migrating system memory to device private memory, if the source
address range is a valid VMA range and there is no memory or a zero page,
the source PFN array is marked as valid but with no PFN";0;1
"This lets the device driver allocate private memory and clear it, then
insert the new device private struct page into the CPU's page tables when
migrate_vma_pages() is called";0;1
" migrate_vma_pages() only inserts the new
page if the VMA is an anonymous range";1;1
"There is no point in telling the device driver to allocate device private
memory and then not migrate the page";0;1
" Instead, mark the source PFN array
entries as not migrating to avoid this overhead.";1;0
mm/vmscan: protect the workingset on anonymous LRU;1;1
"In current implementation, newly created or swap-in anonymous page is
started on active list";1;0
" Growing active list results in rebalancing
active/inactive list so old pages on active list are demoted to inactive
list";1;0
 Hence, the page on active list isn't protected at all;1;0
Following is an example of this situation;0;1
Assume that 50 hot pages on active list;1;0
" Numbers denote the number of
pages on active/inactive list (active | inactive)";1;0
1;0;0
"50 hot pages on active list
50(h) | 0
2";1;0
"workload: 50 newly created (used-once) pages
50(uo) | 50(h)
3";1;0
"workload: another 50 newly created (used-once) pages
50(uo) | 50(uo), swap-out 50(h)
This patch tries to fix this issue";1;1
" Like as file LRU, newly created or
swap-in anonymous pages will be inserted to the inactive list";1;1
" They are
promoted to active list if enough reference happens";1;0
" This simple
modification changes the above example as following";1;1
1;0;0
"50 hot pages on active list
50(h) | 0
2";1;0
"workload: 50 newly created (used-once) pages
50(h) | 50(uo)
3";1;0
"workload: another 50 newly created (used-once) pages
50(h) | 50(uo), swap-out 50(uo)
As you can see, hot pages on active list would be protected";0;0
"Note that, this implementation has a drawback that the page cannot be
promoted and will be swapped-out if re-access interval is greater than the
size of inactive list but less than the size of total(active+inactive)";1;0
"To solve this potential issue, following patch will apply workingset
detection similar to the one that's already applied to file LRU.";1;1
mm/migrate: fix migrate_pgmap_owner w/o CONFIG_MMU_NOTIFIER;1;1
"On x86_64, when CONFIG_MMU_NOTIFIER is not set/enabled, there is a
compiler error";0;1
   mm/migrate.c: In function 'migrate_vma_collect';0;1
   mm/migrate.c:2481:7: error: 'struct mmu_notifier_range' has no member named 'migrate_pgmap_owner';1;1
mm/notifier: add migration invalidation type;1;1
"which flushes all device private page mappings whether or not a page is
being migrated to/from device private memory";0;1
"In order to not disrupt device mappings that are not being migrated, shift
the responsibility for clearing device private mappings to the device
driver and leave CPU page table unmapping handled by
migrate_vma_setup()";0;1
"To support this, the caller of migrate_vma_setup() should always set
struct migrate_vma::pgmap_owner to a non NULL value that matches the
device private page->pgmap->owner";0;1
"This value is then passed to the struct
mmu_notifier_range with a new event type which the driver's invalidation
function can use to avoid device MMU invalidations.";1;1
mm/migrate: add a flags parameter to migrate_vma;1;1
"The src_owner field in struct migrate_vma is being used for two purposes,
it acts as a selection filter for which types of pages are to be migrated
and it identifies device private pages owned by the caller";0;1
"Split this into separate parameters so the src_owner field can be used
just to identify device private pages owned by the caller of
migrate_vma_setup()";1;1
"Rename the src_owner field to pgmap_owner to reflect it is now used only
to identify which device private pages to migrate.";1;0
Raise gcc version requirement to 4.9;0;1
"I realize that we fairly recently raised it to 4.8, but the fact is, 4.9
is a much better minimum version to target";1;1
"We have a number of workarounds for actual bugs in pre-4.9 gcc versions
(including things like internal compiler errors on ARM), but we also
have some syntactic workarounds for lacking features";1;1
"In particular, raising the minimum to 4.9 means that we can now just
assume _Generic() exists, which is likely the much better replacement
for a lot of very convoluted built-time magic with conditionals on
sizeof and/or __builtin_choose_expr() with same_type() etc";1;1
"Using _Generic also means that you will need to have a very recent
version of 'sparse', but thats easy to build yourself, and much less of
a hassle than some old gcc version can be";0;1
"The latest (in a long string) of reasons for minimum compiler version
upgrades was commit 5435f73d5c4a (""efi/x86: Fix build with gcc 4"")";1;0
"Ard points out that RHEL 7 uses gcc-4.8, but the people who stay back on
old RHEL versions persumably also don't build their own kernels anyway";1;1
"And maybe they should cross-built or just have a little side affair with
a newer compiler?";0;1
mmap locking API: convert mmap_sem comments;1;0
Convert comments that reference mmap_sem to reference mmap_lock instead.;1;0
mmap locking API: convert mmap_sem API comments;1;0
"Convert comments that reference old mmap_sem APIs to reference
corresponding new mmap locking APIs instead.";1;1
mmap locking API: use coccinelle to convert mmap_sem rwsem call sites;1;0
"This change converts the existing mmap_sem rwsem calls to use the new mmap
locking API instead";1;1
The change is generated using coccinelle with the following rule;1;0
// spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir ;1;0
"-init_rwsem
+mmap_init_lock
-down_write
+mmap_write_lock
-down_write_killable
+mmap_write_lock_killable
-down_write_trylock
+mmap_write_trylock
-up_write
+mmap_write_unlock
-downgrade_write
+mmap_write_downgrade
-down_read
+mmap_read_lock
-down_read_killable
+mmap_read_lock_killable
-down_read_trylock
+mmap_read_trylock
-up_read
+mmap_read_unlock
-(&mm->mmap_sem)
+(mm)";0;1
mm: memcontrol: delete unused lrucare handling;1;1
"Swapin faults were the last event to charge pages after they had already
been put on the LRU list";0;0
" Now that we charge directly on swapin, the
lrucare portion of the charge code is unused.";0;1
mm: memcontrol: convert anon and file-thp to new mem_cgroup_charge() API;1;0
"With the page->mapping requirement gone from memcg, we can charge anon and
file-thp pages in one single step, right after they're allocated";0;1
"This removes two out of three API calls - especially the tricky commit
step that needed to happen at just the right time between when the page is
""set up"" and when it's ""published"" - somewhat vague and fluid concepts
that varied by page type";0;1
" All we need is a freshly allocated page and a
memcg context to charge";1;1
v2: prevent double charges on pre-allocated hugepages in khugepaged;1;1
mm: memcontrol: switch to native NR_ANON_MAPPED counter;1;0
Memcg maintains a private MEMCG_RSS counter;0;0
" This divergence from the
generic VM accounting means unnecessary code overhead, and creates a
dependency for memcg that page->mapping is set up at the time of charging,
so that page types can be told apart";0;1
"Convert the generic accounting sites to mod_lruvec_page_state and friends
to maintain the per-cgroup vmstat counter of NR_ANON_MAPPED";1;0
" We use
lock_page_memcg() to stabilize page->mem_cgroup during rmap changes, the
same way we do for NR_FILE_MAPPED";1;1
"With the previous patch removing MEMCG_CACHE and the private NR_SHMEM
counter, this patch finally eliminates the need to have page->mapping set
up at charge time";1;1
" However, we need to have page->mem_cgroup set up by
the time rmap runs and does the accounting, so switch the commit and the
rmap callbacks around";1;0
v2: fix temporary accounting bug by switching rmap<->commit (Joonsoo);1;1
mm: memcontrol: switch to native NR_FILE_PAGES and NR_SHMEM counters;1;1
Memcg maintains private MEMCG_CACHE and NR_SHMEM counters;0;1
" This
divergence from the generic VM accounting means unnecessary code overhead,
and creates a dependency for memcg that page->mapping is set up at the
time of charging, so that page types can be told apart";0;1
"Convert the generic accounting sites to mod_lruvec_page_state and friends
to maintain the per-cgroup vmstat counters of NR_FILE_PAGES and NR_SHMEM";1;1
"we only need minimal tweaks of two mem_cgroup_migrate() calls to ensure
it's set up in time";0;1
"Then replace MEMCG_CACHE with NR_FILE_PAGES and delete the private
NR_SHMEM accounting sites.";1;1
mm: memcontrol: drop @compound parameter from memcg charging API;1;0
"The memcg charging API carries a boolean @compound parameter that tells
whether the page we're dealing with is a hugepage";0;0
"mem_cgroup_commit_charge() has another boolean @lrucare that indicates
whether the page needs LRU locking or not while charging";0;1
" The majority of
callsites know those parameters at compile time, which results in a lot of
naked ""false, false"" argument lists";0;1
" This makes for cryptic code and is a
breeding ground for subtle mistakes";0;1
"Thankfully, the huge page state can be inferred from the page itself and
doesn't need to be passed along";0;1
" This is safe because charging completes
before the page is published and somebody may split it";1;1
"Simplify the callsites by removing @compound, and let memcg infer the
state by using hpage_nr_pages() unconditionally";1;1
" That function does
PageTransHuge() to identify huge pages, which also helpfully asserts that
nobody passes in tail pages by accident";0;1
"The following patches will introduce a new charging API, best not to carry
over unnecessary weight.";1;1
mm/migrate.c: attach_page_private already does the get_page;1;0
"Just finished bisecting mmotm, to find why a test which used to take
four minutes now took more than an hour: the __buffer_migrate_page()
cleanup left behind a get_page() which attach_page_private() now does";1;1
"Fixes: cd0f37154443 (""mm/migrate.c: call detach_page_private to cleanup code"")";0;1
mm/migrate.c: call detach_page_private to cleanup code;0;1
We can cleanup code a little by call detach_page_private here.;1;1
fs: convert mpage_readpages to mpage_readahead;1;0
"Implement the new readahead aop and convert all callers (block_dev,
exfat, ext2, fat, gfs2, hpfs, isofs, jfs, nilfs2, ocfs2, omfs, qnx6,
reiserfs & udf)";1;0
The callers are all trivial except for GFS2 & OCFS2.;0;0
userfaultfd: wp: support swap and page migration;1;0
"For either swap and page migration, we all use the bit 2 of the entry to
identify whether this entry is uffd write-protected";1;1
" It plays a similar
role as the existing soft dirty bit in swap entries but only for keeping
the uffd-wp tracking for a specific PTE/PMD";1;0
"Something special here is that when we want to recover the uffd-wp bit
from a swap/migration entry to the PTE bit we'll also need to take care of
the _PAGE_RW bit and make sure it's cleared, otherwise even with the
_PAGE_UFFD_WP bit we can't trap it at all";1;1
In change_pte_range() we do nothing for uffd if the PTE is a swap entry;1;0
"That can lead to data mismatch if the page that we are going to write
protect is swapped out when sending the UFFDIO_WRITEPROTECT";0;1
" This patch
also applies/removes the uffd-wp bit even for the swap entries.";1;1
mm: code cleanup for MADV_FREE;1;1
"Some comments for MADV_FREE is revised and added to help people understand
the MADV_FREE code, especially the page flag, PG_swapbacked";1;1
" This makes
page_is_file_cache() isn't consistent with its comments";0;1
" So the function
is renamed to page_is_file_lru() to make them consistent again";1;1
" All these
are put in one patch as one logical change.";1;0
mm/migrate.c: migrate PG_readahead flag;1;1
Currently the migration code doesn't migrate PG_readahead flag;0;0
"Theoretically this would incur slight performance loss as the application
might have to ramp its readahead back up again";0;0
" Even though such problem
happens, it might be hidden by something else since migration is typically
triggered by compaction and NUMA balancing, any of which should be more
noticeable";0;1
"Migrate the flag after end_page_writeback() since it may clear PG_reclaim
flag, which is the same bit as PG_readahead, for the new page.";0;1
"mm/migrate.c: unify ""not queued for migration"" handling in do_pages_move()";0;0
It can currently happen that we store the status of a page twice;0;0
  target node is contained in the current interval;0;0
"Let's simplify the code and always call do_move_pages_to_node() in case we
did not queue a page for migration";1;1
" Note that pages that are already on
the target node are not added to the pagelist and are, therefore, ignored
by do_move_pages_to_node() - there is no functional change";0;0
The status of such a page is now only stored once;1;0
[david@redhat.com rephrase changelog];0;1
mm/migrate.c: check pagelist in move_pages_and_store_status();1;1
When pagelist is empty, it is not necessary to do the move and store;0;1
Also it consolidate the empty list check in one place.;1;1
mm/migrate.c: wrap do_move_pages_to_node() and store_status();0;0
"Usually, do_move_pages_to_node() and store_status() are used in
combination";0;0
 We have three similar call sites;0;0
"Let's provide a wrapper for both function calls -
move_pages_and_store_status - to make the calling code easier to maintain
and fix (as noted by Yang Shi, the return value handling of
do_move_pages_to_node() has a flaw)";1;1
[david@redhat.com rephrase changelog];0;1
mm/migrate.c: no need to check for i > start in do_pages_move();1;0
"Patch series ""cleanup on do_pages_move()"", v5";0;1
"The logic in do_pages_move() is a little mess for audience to read and has
some potential error on handling the return value";0;1
"Especially there are
three calls on do_move_pages_to_node() and store_status() with almost the
same form";0;1
"This patch set tries to make the code a little friendly for audience by
consolidate the calls";1;0
This patch (of 4);0;0
At this point, we always have i >= start;0;0
" If i == start, store_status()
will return 0";1;0
 So we can drop the check for i > start;1;0
[david@redhat.com rephrase changelog];0;1
hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization;1;0
"Patch series ""hugetlbfs: use i_mmap_rwsem for more synchronization"", v2";1;0
"While discussing the issue with huge_pte_offset [1], I remembered that
there were more outstanding hugetlb races";0;1
 These issues are;0;0
"1) For shared pmds, huge PTE pointers returned by huge_pte_alloc can become
   invalid via a call to huge_pmd_unshare by another thread";0;1
"2) hugetlbfs page faults can race with truncation causing invalid global
   reserve counts and state";0;1
"A previous attempt was made to use i_mmap_rwsem in this manner as
described at [2]";1;0
" However, those patches were reverted starting with [3]
due to locking issues";1;1
"To effectively use i_mmap_rwsem to address the above issues it needs to be
held (in read mode) during page fault processing";1;0
" However, during fault
processing we need to lock the page we will be adding";0;0
" Lock ordering
requires we take page lock before i_mmap_rwsem";0;0
" Waiting until after
taking the page lock is too late in the fault process for the
synchronization we want to do";1;1
"To address this lock ordering issue, the following patches change the lock
ordering for hugetlb pages";1;0
" This is not too invasive as hugetlbfs
processing is done separate from core mm in many places";1;1
" However, I don't
really like this idea";0;1
" Much ugliness is contained in the new routine
hugetlb_page_mapping_lock_write() of patch 1";0;0
"The only other way I can think of to address these issues is by catching
all the races";0;0
 After catching a race, cleanup, backout, retry ..;1;1
" etc,
as needed";1;1
" This can get really ugly, especially for huge page
reservations";0;1
" At one time, I started writing some of the reservation
backout code for page faults and it got so ugly and complicated I went
down the path of adding synchronization to avoid the races";1;0
" Any other
suggestions would be welcome.";0;0
mm: handle multiple owners of device private pages in migrate_vma;1;1
Add a new src_owner field to struct migrate_vma;1;1
" If the field is set,
only device private pages with page->pgmap->owner equal to that field are
migrated";0;1
" If the field is not set only ""normal"" pages are migrated";0;0
"Fixes: df6ad69838fc (""mm/device-public-memory: device memory cache coherent with CPU"")";0;1
mm: pagewalk: add 'depth' parameter to pte_hole;1;1
The pte_hole() callback is called at multiple levels of the page tables;0;0
"Code dumping the kernel page tables needs to know what at what depth the
missing entry is";1;1
 Add this is an extra parameter to pte_hole();1;1
" When the
depth isn't know (e.g";0;1
 processing a vma) then -1 is passed;0;0
"The depth that is reported is the actual level where the entry is missing
(ignoring any folding that is in place), i.e";0;1
" any levels where
PTRS_PER_P?D is set to 1 are ignored";1;0
"Note that depth starts at 0 for a PGD so that PUD/PMD/PTE retain their
natural numbers as levels 2/3/4.";1;0
mm/migrate: add stable check in migrate_vma_insert_page();1;1
migrate_vma_insert_page() closely follows the code in;1;1
"  __handle_mm_fault()
    handle_pte_fault()
      do_anonymous_page()
Add a call to check_stable_address_space() after locking the page table
entry before inserting a ZONE_DEVICE private zero page mapping similar
to page faulting a new anonymous page.";1;1
mm/migrate: clean up some minor coding style;1;1
"Fix some comment typos and coding style clean up in preparation for the
next patch";1;1
 No functional changes.;1;0
mm/migrate: remove useless mask of start address;1;1
"Addresses passed to walk_page_range() callback functions are already
page aligned and don't need to be masked with PAGE_MASK.";0;0
mm: move_pages: report the number of non-attempted pages;0;1
"Since commit a49bd4d71637 (""mm, numa: rework do_pages_move""), the
semantic of move_pages() has changed to return the number of
non-migrated pages if they were result of a non-fatal reasons (usually a
busy page)";0;1
"This was an unintentional change that hasn't been noticed except for LTP
tests which checked for the documented behavior";0;1
There are two ways to go around this change;0;0
" We can even get back to
the original behavior and return -EAGAIN whenever migrate_pages is not
able to migrate pages due to non-fatal reasons";0;1
" Another option would be
to simply continue with the changed semantic and extend move_pages
documentation to clarify that -errno is returned on an invalid input or
when migration simply cannot succeed (e.g";0;0
" -ENOMEM, -EBUSY) or the
number of pages that couldn't have been migrated due to ephemeral
reasons (e.g";0;1
 page is pinned or locked for other reasons);0;1
"This patch implements the second option because this behavior is in
place for some time without anybody complaining and possibly new users
depending on it";1;1
" Also it allows to have a slightly easier error
handling as the caller knows that it is worth to retry when err > 0";1;1
"But since the new semantic would be aborted immediately if migration is
failed due to ephemeral reasons, need include the number of
non-attempted pages in the return value too.";1;1
mm/migrate.c: also overwrite error when it is bigger than zero;1;1
"If we get here after successfully adding page to list, err would be 1 to
indicate the page is queued in the list";0;0
Current code has two problems;0;0
"    from do_move_pages_to_node() is set, the err1 is not returned since err
And these behaviors break the user interface.";0;1
mm: move_pages: return valid node id in status if the page is already on the target node;0;0
"Felix Abecassis reports move_pages() would return random status if the
pages are already on the target node by the below test program";0;0
"  int main(void)
		pages[i] = mmap(NULL, page_size, PROT_WRITE | PROT_READ,
				MAP_PRIVATE | MAP_POPULATE | MAP_ANONYMOUS,
	for (int i = 0; i < num_pages; ++i)
Then running the program would return nonsense status values";1;0
"  $ ./move_pages_bug
  move_pages: 0
  status[0] = 208
  status[1] = 208
  status[2] = 208
  status[3] = 208
  status[4] = 208
  status[5] = 208
  status[6] = 208
  status[7] = 208
This is because the status is not set if the page is already on the
target node, but move_pages() should return valid status as long as it
succeeds";0;1
 The valid status may be errno or node id;0;0
"We can't simply initialize status array to zero since the pages may be
not on node 0";0;1
" Fix it by updating status with node id which the page is
already on.";1;1
autonuma: fix watermark checking in migrate_balanced_pgdat();1;1
"When zone_watermark_ok() is called in migrate_balanced_pgdat() to check
migration target node, the parameter classzone_idx (for requested zone)
is specified as 0 (ZONE_DMA)";0;0
" But when allocating memory for autonuma
in alloc_misplaced_dst_page(), the requested zone from GFP flags is
ZONE_MOVABLE";0;0
 That is, the requested zone is different;0;0
" The size of
lowmem_reserve for the different requested zone is different";1;0
" And this
may cause some issues";0;1
"For example, in the zoneinfo of a test machine as below,
Node 0, zone    DMA32
  pages free     61592
        min      29
        low      454
        high     879
        spanned  1044480
        present  442306
        managed  425921
        protection: (0, 0, 62457, 62457, 62457)
The free page number of ZONE_DMA32 is greater than ""high watermark +
lowmem_reserve[ZONE_DMA]"", but less than ""high watermark +
lowmem_reserve[ZONE_MOVABLE]""";1;1
" And because __alloc_pages_node() in
alloc_misplaced_dst_page() requests ZONE_MOVABLE, the
zone_watermark_ok() on ZONE_DMA32 in migrate_balanced_pgdat() may always
return true";0;1
" So, autonuma may not stop even when memory pressure in
node 0 is heavy";0;0
"To fix the issue, ZONE_MOVABLE is used as parameter to call
zone_watermark_ok() in migrate_balanced_pgdat()";1;1
" This makes it same as
requested zone in alloc_misplaced_dst_page()";1;1
" So that
migrate_balanced_pgdat() returns false when memory pressure is heavy.";0;0
mm/migrate.c: handle freed page at the first place;1;1
"When doing migration if the freed page is met, we just return without
migrating it since it is pointless to migrate a freed page";0;1
" But, the
current code allocates target page unconditionally before handling freed
page, if the page is freed, the newly allocated will be just freed";0;1
" It
doesn't make too much sense and is just a waste of time although
migrating freed page is rare";1;1
"So, handle freed page at the before that to avoid unnecessary page
allocation and free.";1;1
mm: untag user pointers passed to memory syscalls;1;1
"This patch is a part of a series that extends kernel ABI to allow to pass
tagged user pointers (with the top byte set to something else other than
0x00) as syscall arguments";1;1
"This patch allows tagged pointers to be passed to the following memory
syscalls: get_mempolicy, madvise, mbind, mincore, mlock, mlock2, mprotect,
mremap, msync, munlock, move_pages";1;1
The mmap and mremap syscalls do not currently accept tagged addresses;0;0
"Architectures may interpret the tag as a background colour for the
corresponding vma.";1;0
mm/migrate.c: clean up useless code in migrate_vma_collect_pmd();1;1
Remove unused 'pfn' variable.;1;1
mm: page cache: store only head pages in i_pages;1;0
"Transparent Huge Pages are currently stored in i_pages as pointers to
consecutive subpages";0;0
" This patch changes that to storing consecutive
pointers to the head page in preparation for storing huge pages more
efficiently in i_pages";0;1
"Large parts of this are ""inspired"" by Kirill's patch
Kirill and Huang Ying contributed several fixes.";0;1
mm: introduce compound_nr();1;1
Replace 1 << compound_order(page) with compound_nr(page);1;0
" Minor
improvements in readability.";1;1
pagewalk: separate function pointers from iterator data;0;0
The mm_walk structure currently mixed data and code;0;0
" Split out the
operations vectors into a new mm_walk_ops structure, and while we are
changing the API also declare the mm_walk structure inside the
walk_page_range and walk_page_vma functions";1;1
Based on patch from Linus Torvalds.;0;0
mm: split out a new pagewalk.h header from mm.h;1;1
"Add a new header for the two handful of users of the walk_page_range /
walk_page_vma interface instead of polluting all users of mm.h with it.";1;1
mm: remove CONFIG_MIGRATE_VMA_HELPER;1;0
"CONFIG_MIGRATE_VMA_HELPER guards helpers that are required for proper
devic private memory support";1;1
" Remove the option and just check for
CONFIG_DEVICE_PRIVATE instead.";1;1
mm: remove the unused MIGRATE_PFN_DEVICE flag;1;1
"No one ever checks this flag, and we could easily get that information
from the page if needed.";1;1
mm: turn migrate_vma upside down;1;0
There isn't any good reason to pass callbacks to migrate_vma;1;0
" Instead
we can just export the three steps done by this function to drivers and
let them sequence the operation without callbacks";1;1
" This removes a lot
of boilerplate code as-is, and will allow the drivers to drastically
improve code flow and error handling further on.";1;1
mm/migrate.c: initialize pud_entry in migrate_vma();1;1
"When CONFIG_MIGRATE_VMA_HELPER is enabled, migrate_vma() calls
migrate_vma_collect() which initializes a struct mm_walk but didn't
initialize mm_walk.pud_entry";1;1
" (Found by code inspection) Use a C
structure initialization to make sure it is set to NULL.";1;0
mm: migrate: fix reference check race between __find_get_block() and migration;1;1
"buffer_migrate_page_norefs() can race with bh users in the following
way";0;1
"CPU1                                    CPU2
buffer_migrate_page_norefs()
  buffer_migrate_lock_buffers()
  checks bh refs
  spin_unlock(&mapping->private_lock)
                                        __find_get_block()
                                          spin_lock(&mapping->private_lock)
                                          grab bh ref
                                          spin_unlock(&mapping->private_lock)
  move page                               do bh work
This can result in various issues like lost updates to buffers (i.e";0;1
metadata corruption) or use after free issues for the old page;0;0
"This patch closes the race by holding mapping->private_lock while the
mapping is being moved to a new page";0;1
" Ordinarily, a reference can be
taken outside of the private_lock using the per-cpu BH LRU but the
references are checked and the LRU invalidated if necessary";1;1
" The
private_lock is held once the references are known so the buffer lookup
slow path will spin on the private_lock";1;0
" Between the page lock and
private_lock, it should be impossible for other references to be
acquired and updates to happen during the migration";0;1
"A user had reported data corruption issues on a distribution kernel with
a similar page migration implementation as mainline";1;1
" The data
corruption could not be reproduced with this patch applied";1;1
" A small
number of migration-intensive tests were run and no performance problems
were noted.";0;1
mm: migrate: remove unused mode argument;1;1
migrate_page_move_mapping() doesn't use the mode argument;1;0
" Remove it
and update callers accordingly.";1;1
"Revert ""mm: page cache: store only head pages in i_pages""";1;0
This reverts commit 5fd4ca2d84b249f0858ce28cf637cf25b61a398f;1;0
"Mikhail Gavrilov reports that it causes the VM_BUG_ON_PAGE() in
__delete_from_swap_cache() to trigger";0;1
"   page:ffffd6d34dff0000 refcount:1 mapcount:1 mapping:ffff97812323a689 index:0xfecec363
   anon
   flags: 0x17fffe00080034(uptodate|lru|active|swapbacked)
   raw: 0017fffe00080034 ffffd6d34c67c508 ffffd6d3504b8d48 ffff97812323a689
   raw: 00000000fecec363 0000000000000000 0000000100000000 ffff978433ace000
   page dumped because: VM_BUG_ON_PAGE(entry != page)
   page->mem_cgroup:ffff978433ace000
   ------------[ cut here ]------------
   kernel BUG at mm/swap_state.c:170!
   invalid opcode: 0000 [#1] SMP NOPTI
   CPU: 1 PID: 221 Comm: kswapd0 Not tainted 5.2.0-0.rc2.git0.1.fc31.x86_64 #1
   Hardware name: System manufacturer System Product Name/ROG STRIX X470-I GAMING, BIOS 2202 04/11/2019
   RIP: 0010:__delete_from_swap_cache+0x20d/0x240
   Code: 30 65 48 33 04 25 28 00 00 00 75 4a 48 83 c4 38 5b 5d 41 5c 41 5d 41 5e 41 5f c3 48 c7 c6 2f dc 0f 8a 48 89 c7 e8 93 1b fd ff <0f> 0b 48 c7 c6 a8 74 0f 8a e8 85 1b fd ff 0f 0b 48 c7 c6 a8 7d 0f
   RSP: 0018:ffffa982036e7980 EFLAGS: 00010046
   RAX: 0000000000000021 RBX: 0000000000000040 RCX: 0000000000000006
   RDX: 0000000000000000 RSI: 0000000000000086 RDI: ffff97843d657900
   RBP: 0000000000000001 R08: ffffa982036e7835 R09: 0000000000000535
   R10: ffff97845e21a46c R11: ffffa982036e7835 R12: ffff978426387120
   R13: 0000000000000000 R14: ffffd6d34dff0040 R15: ffffd6d34dff0000
   FS:  0000000000000000(0000) GS:ffff97843d640000(0000) knlGS:0000000000000000
   CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
   CR2: 00002cba88ef5000 CR3: 000000078a97c000 CR4: 00000000003406e0
   and it's not immediately obvious why it happens";0;1
" It's too late in the
rc cycle to do anything but revert for now.";0;1
mm: remove MEMORY_DEVICE_PUBLIC support;1;1
"The code hasn't been used since it was added to the tree, and doesn't
appear to actually be usable.";0;1
mm/mmu_notifier: use correct mmu_notifier events for each invalidation;1;0
"This updates each existing invalidation to use the correct mmu notifier
event that represent what is happening to the CPU page table";1;1
" See the
patch which introduced the events to see the rational behind this.";0;1
mm/mmu_notifier: contextual information for event triggering invalidation;0;0
"CPU page table update can happens for many reasons, not only as a result
of a syscall (munmap(), mprotect(), mremap(), madvise(), ...) but also as
a result of kernel activities (memory compression, reclaim, migration,
Users of mmu notifier API track changes to the CPU page table and take
specific action for them";0;1
" While current API only provide range of virtual
address affected by the change, not why the changes is happening";0;1
"This patchset do the initial mechanical convertion of all the places that
calls mmu_notifier_range_init to also provide the default MMU_NOTIFY_UNMAP
event as well as the vma if it is know (most invalidation happens against
a given vma)";1;1
" Passing down the vma allows the users of mmu notifier to
inspect the new vma page protection";1;1
"The MMU_NOTIFY_UNMAP is always the safe default as users of mmu notifier
should assume that every for the range is going away when that event
happens";1;1
" A latter patch do convert mm call path to use a more appropriate
events for each call";1;1
"This is done as 2 patches so that no call site is forgotten especialy
as it uses this following coccinelle patch";1;1
"static inline void mmu_notifier_range_init(struct mmu_notifier_range *I1,
+enum mmu_notifier_event event,
+unsigned flags,
+struct vm_area_struct *vma,
struct mm_struct *I2, unsigned long I3, unsigned long I4) { ..";1;0
"}
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, I1,
I1->vm_mm, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, VMA,
E2, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, VMA,
E2, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, NULL,
E2, E3, E4)
Applied with:";0;1
mm: page cache: store only head pages in i_pages;1;0
"Transparent Huge Pages are currently stored in i_pages as pointers to
consecutive subpages";0;0
" This patch changes that to storing consecutive
pointers to the head page in preparation for storing huge pages more
efficiently in i_pages";0;1
"Large parts of this are ""inspired"" by Kirill's patch";0;0
mm/migrate.c: add missing flush_dcache_page for non-mapped page migrate;1;1
"Our MIPS 1004Kc SoCs were seeing random userspace crashes with SIGILL
and SIGSEGV that could not be traced back to a userspace code bug";1;1
" They
had all the magic signs of an I/D cache coherency issue";1;0
"Now recently we noticed that the /proc/sys/vm/compact_memory interface
was quite efficient at provoking this class of userspace crashes";0;0
"Studying the code in mm/migrate.c there is a distinction made between
migrating a page that is mapped at the instant of migration and one that
is not mapped";0;1
 Our problem turned out to be the non-mapped pages;1;1
"For the non-mapped page the code performs a copy of the page content and
all relevant meta-data of the page without doing the required D-cache
maintenance";1;0
" This leaves dirty data in the D-cache of the CPU and on
the 1004K cores this data is not visible to the I-cache";1;1
" A subsequent
page-fault that triggers a mapping of the page will happily serve the
process with potentially stale code";0;0
"What about ARM then, this bug should have seen greater exposure? Well
ARM became immune to this flaw back in 2010, see commit c01778001a4f
(""ARM: 6379/1: Assume new page cache pages have dirty D-cache"")";0;1
"My proposed fix moves the D-cache maintenance inside move_to_new_page to
make it common for both cases.";1;1
mm/migrate.c: cleanup expected_page_refs();0;1
"Andrea has noted that page migration code propagates page_mapping(page)
through the whole migration stack down to migrate_page() function so it
seems stupid to then use page_mapping(page) in expected_page_refs()
instead of passed down 'mapping' argument";0;1
" I agree so let's make
expected_page_refs() more in line with the rest of the migration stack.";1;1
mm: fix some typos in mm directory;1;1
No functional change.;1;0
mm, migrate: immediately fail migration of a page with no migration handler;1;1
"Pages with no migration handler use a fallback handler which sometimes
works and sometimes persistently retries";1;1
" A historical example was
blockdev pages but there are others such as odd refcounting when
page->private is used";0;0
" These are retried multiple times which is
wasteful during compaction so this patch will fail migration faster
unless the caller specifies MIGRATE_SYNC";1;1
"This is not expected to help THP allocation success rates but it did
reduce latencies very slightly in some cases";0;1
"1-socket thpfioscale
                              noreserved-v2r15         failfast-v2r15
Amean     fault-both-1         0.00 (   0.00%)        0.00 *   0.00%*
Amean     fault-both-3      3839.67 (   0.00%)     3833.72 (   0.15%)
Amean     fault-both-5      5177.47 (   0.00%)     4967.15 (   4.06%)
Amean     fault-both-7      7245.03 (   0.00%)     7139.19 (   1.46%)
Amean     fault-both-12    11534.89 (   0.00%)    11326.30 (   1.81%)
Amean     fault-both-18    16241.10 (   0.00%)    16270.70 (  -0.18%)
Amean     fault-both-24    19075.91 (   0.00%)    19839.65 (  -4.00%)
Amean     fault-both-30    22712.11 (   0.00%)    21707.05 (   4.43%)
Amean     fault-both-32    21692.92 (   0.00%)    21968.16 (  -1.27%)
The 2-socket results are not materially different";0;0
" Scan rates are
similar as expected.";1;1
mm/hugetlb: distinguish between migratability and movability;1;1
"Patch series ""arm64/mm: Enable HugeTLB migration"", v4";1;1
"This patch series enables HugeTLB migration support for all supported
huge page sizes at all levels including contiguous bit implementation";1;0
"Following HugeTLB migration support matrix has been enabled with this
patch series";0;1
 All permutations have been tested except for the 16GB;1;0
"           CONT PTE    PMD    CONT PMD    PUD
  4K:         64K     2M         32M     1G
  16K:         2M    32M          1G
  64K:         2M   512M         16G
First the series adds migration support for PUD based huge pages";1;0
" It
then adds a platform specific hook to query an architecture if a given
huge page size is supported for migration while also providing a default
fallback option preserving the existing semantics which just checks for
(PMD|PUD|PGDIR)_SHIFT macros";1;1
" The last two patches enables HugeTLB
migration on arm64 and subscribe to this new platform specific hook by
defining an override";1;0
"The second patch differentiates between movability and migratability
aspects of huge pages and implements hugepage_movable_supported() which
can then be used during allocation to decide whether to place the huge
page in movable zone or not";0;0
This patch (of 5);1;0
"During huge page allocation it's migratability is checked to determine
if it should be placed under movable zones with GFP_HIGHUSER_MOVABLE";1;0
"But the movability aspect of the huge page could depend on other factors
than just migratability";0;1
" Movability in itself is a distinct property
which should not be tied with migratability alone";0;1
"This differentiates these two and implements an enhanced movability check
which also considers huge page size to determine if it is feasible to be
placed under a movable zone";1;1
" At present it just checks for gigantic pages
but going forward it can incorporate other enhanced checks.";1;1
hugetlbfs: fix races and page leaks during migration;1;1
hugetlb pages should only be migrated if they are 'active';1;1
" The
routines set/clear_page_huge_active() modify the active state of hugetlb
pages";1;0
"When a new hugetlb page is allocated at fault time, set_page_huge_active
is called before the page is locked";0;0
" Therefore, another thread could
race and migrate the page while it is being added to page table by the
fault code";0;1
" This race is somewhat hard to trigger, but can be seen by
strategically adding udelay to simulate worst case scheduling behavior";1;1
Depending on 'how' the code races, various BUG()s could be triggered;0;0
"To address this issue, simply delay the set_page_huge_active call until
after the page is successfully added to the page table";1;0
"Hugetlb pages can also be leaked at migration time if the pages are
associated with a file in an explicitly mounted hugetlbfs filesystem";0;0
"For example, consider a two node system with 4GB worth of huge pages
available";0;0
 A program mmaps a 2G file in a hugetlbfs filesystem;1;1
" It
then migrates the pages associated with the file from one node to
another";0;0
 When the program exits, huge page counts are as follows;1;1
"  node0
  1024    free_hugepages
  1024    nr_hugepages
  node1
  0       free_hugepages
  1024    nr_hugepages
  Filesystem                         Size  Used Avail Use% Mounted on
  nodev                              4.0G  2.0G  2.0G  50% /var/opt/hugepool
That is as expected";1;0
" 2G of huge pages are taken from the free_hugepages
counts, and 2G is the size of the file in the explicitly mounted
filesystem";1;0
 If the file is then removed, the counts become;1;1
"  node0
  1024    free_hugepages
  1024    nr_hugepages
  node1
  1024    free_hugepages
  1024    nr_hugepages
  Filesystem                         Size  Used Avail Use% Mounted on
  nodev                              4.0G  2.0G  2.0G  50% /var/opt/hugepool
Note that the filesystem still shows 2G of pages used, while there
actually are no huge pages in use";1;1
" The only way to 'fix' the filesystem
accounting is to unmount the filesystem
If a hugetlb page is associated with an explicitly mounted filesystem,
this information in contained in the page_private field";0;1
" At migration
time, this information is not preserved";0;1
" To fix, simply transfer
page_private from old to new page at migration time if necessary";1;1
"There is a related race with removing a huge page from a file and
migration";0;1
" When a huge page is removed from the pagecache, the
page_mapping() field is cleared, yet page_private remains set until the
page is actually freed by free_huge_page()";0;1
" A page could be migrated
while in this state";0;1
" However, since page_mapping() is not set the
hugetlbfs specific routine to transfer page_private is not called and we
leak the page count in the filesystem";0;1
To fix that, check for this condition before migrating a huge page;1;1
" If
the condition is detected, return EBUSY for the page.";0;1
mm: migrate: don't rely on __PageMovable() of newpage after unlocking it;1;0
"We had a race in the old balloon compaction code before b1123ea6d3b3
(""mm: balloon: use general non-lru movable page feature"") refactored it
that became visible after backporting 195a8c43e93d (""virtio-balloon";0;1
"deflate via a page list"") without the refactoring";0;0
"The bug existed from commit d6d86c0a7f8d (""mm/balloon_compaction";0;0
"redesign ballooned pages management"") till b1123ea6d3b3 (""mm: balloon";0;1
"use general non-lru movable page feature"")";1;1
" d6d86c0a7f8d
(""mm/balloon_compaction: redesign ballooned pages management"") was
backported to 3.12, so the broken kernels are stable kernels [3.12 -
There was a subtle race between dropping the page lock of the newpage in
__unmap_and_move() and checking for __is_movable_balloon_page(newpage)";0;1
"Just after dropping this page lock, virtio-balloon could go ahead and
deflate the newpage, effectively dequeueing it and clearing PageBalloon,
in turn making __is_movable_balloon_page(newpage) fail";1;1
"This resulted in dropping the reference of the newpage via
putback_lru_page(newpage) instead of put_page(newpage), leading to
page->lru getting modified and a !LRU page ending up in the LRU lists";1;1
"With 195a8c43e93d (""virtio-balloon: deflate via a page list"")
backported, one would suddenly get corrupted lists in
release_pages_balloon()";0;0
"- WARNING: CPU: 13 PID: 6586 at lib/list_debug.c:59 __list_del_entry+0xa1/0xd0
- list_del corruption";1;0
"prev->next should be ffffe253961090a0, but was dead000000000100
Nowadays this race is no longer possible, but it is hidden behind very
ugly handling of __ClearPageMovable() and __PageMovable()";0;1
"__ClearPageMovable() will not make __PageMovable() fail, only
PageMovable()";1;1
" So the new check (__PageMovable(newpage)) will still
hold even after newpage was dequeued by virtio-balloon";1;1
"If anybody would ever change that special handling, the BUG would be
introduced again";0;0
" So instead, make it explicit and use the information
of the original isolated page before migration";1;1
"This patch can be backported fairly easy to stable kernels (in contrast
to the refactoring).";1;1
mm: migrate: make buffer_migrate_page_norefs() actually succeed;1;1
"Currently, buffer_migrate_page_norefs() was constantly failing because
buffer_migrate_lock_buffers() grabbed reference on each buffer";0;0
" In
fact, there's no reason for buffer_migrate_lock_buffers() to grab any
buffer references as the page is locked during all our operation and
thus nobody can reclaim buffers from the page";1;1
"So remove grabbing of buffer references which also makes
buffer_migrate_page_norefs() succeed.";1;1
"hugetlbfs: revert ""use i_mmap_rwsem for more pmd sharing synchronization""";1;0
"This reverts b43a9990055958e70347c56f90ea2ae32c67334c
The reverted commit caused issues with migration and poisoning of anon
huge pages";0;0
" The LTP move_pages12 test will cause an ""unable to handle
kernel NULL pointer"" BUG would occur with stack similar to";0;1
"  RIP: 0010:down_write+0x1b/0x40
  The purpose of the reverted patch was to fix some long existing races
with huge pmd sharing";1;1
" It used i_mmap_rwsem for this purpose with the
idea that this could also be used to address truncate/page fault races
with another patch";0;1
" Further analysis has determined that i_mmap_rwsem
can not be used to address all these hugetlbfs synchronization issues";0;0
"Therefore, revert this patch while working an another approach to the
underlying issues.";1;1
mm: treewide: remove unused address argument from pte_alloc functions;1;1
"Patch series ""Add support for fast mremap""";1;1
"This series speeds up the mremap(2) syscall by copying page tables at
the PMD level even for non-THP systems";0;0
" There is concern that the extra
'address' argument that mremap passes to pte_alloc may do something
subtle architecture related in the future that may make the scheme not
work";0;0
" Also we find that there is no point in passing the 'address' to
pte_alloc since its unused";1;1
" This patch therefore removes this argument
tree-wide resulting in a nice negative diff as well";1;1
" Also ensuring
along the way that the enabled architectures do not do anything funky
with the 'address' argument that goes unnoticed by the optimization";1;0
Build and boot tested on x86-64;1;1
 Build tested on arm64;1;1
" The config
enablement patch for arm64 will be posted in the future after more
testing";1;1
The changes were obtained by applying the following Coccinelle script;0;0
(thanks Julia for answering all Coccinelle questions!);0;0
Following fix ups were done manually;1;1
"// Options: --include-headers --no-includes
// Note: I split the 'identifier fn' line, so if you are manually
// running it, please unsplit it so it runs for you";1;1
"virtual patch
@pte_alloc_func_def depends on patch exists@
identifier fn =~
 fn(..";1;1
"- , T2 E2
@pte_alloc_func_proto_noarg depends on patch exists@
@pte_alloc_func_proto depends on patch exists@
identifier fn =~
@pte_alloc_func_call depends on patch exists@
identifier fn =~
 fn(..";1;1
"-,  E2
@pte_alloc_macro depends on patch exists@
identifier fn =~
- #define fn(a, b, c) e
+ #define fn(a, b) e
- #define fn(a, b) e
+ #define fn(a) e";0;0
hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization;1;0
"While looking at BUGs associated with invalid huge page map counts, it was
discovered and observed that a huge pte pointer could become 'invalid' and
point to another task's page table";0;0
 Consider the following;1;0
"A task takes a page fault on a shared hugetlbfs file and calls
huge_pte_alloc to get a ptep";1;0
" Suppose the returned ptep points to a
shared pmd";0;0
Now, another task truncates the hugetlbfs file;1;1
" As part of truncation, it
unmaps everyone who has the file mapped";0;0
" If the range being truncated is
covered by a shared pmd, huge_pmd_unshare will be called";0;0
" For all but the
last user of the shared pmd, huge_pmd_unshare will clear the pud pointing
to the pmd";0;0
" If the task in the middle of the page fault is not the last
user, the ptep returned by huge_pte_alloc now points to another task's
page table or worse";1;0
" This leads to bad things such as incorrect page
map/reference counts or invalid memory references";0;1
To fix, expand the use of i_mmap_rwsem as follows;1;1
- i_mmap_rwsem is held in read mode whenever huge_pmd_share is called;1;0
"  huge_pmd_share is only called via huge_pte_alloc, so callers of
  huge_pte_alloc take i_mmap_rwsem before calling";1;0
" In addition, callers
  of huge_pte_alloc continue to hold the semaphore until finished with the
  ptep";0;0
"- i_mmap_rwsem is held in write mode whenever huge_pmd_unshare is
  called.";1;0
mm: migrate: drop unused argument of migrate_page_move_mapping();1;0
"All callers of migrate_page_move_mapping() now pass NULL for 'head'
argument";1;0
 Drop it.;1;0
mm: migrate: provide buffer_migrate_page_norefs();0;1
"Provide a variant of buffer_migrate_page() that also checks whether there
are no unexpected references to buffer heads";0;0
" This function will then be
safe to use for block device pages.";1;1
mm: migrate: move migrate_page_lock_buffers();1;0
"buffer_migrate_page() is the only caller of migrate_page_lock_buffers()
move it close to it and also drop the now unused stub for !CONFIG_BLOCK.";1;1
mm: migrate: lock buffers before migrate_page_move_mapping();0;0
"Lock buffers before calling into migrate_page_move_mapping() so that that
function doesn't have to know about buffers (which is somewhat unexpected
anyway) and all the buffer head logic is in buffer_migrate_page().";1;1
mm: migration: factor out code to compute expected number of page references;0;1
"Patch series ""mm: migrate: Fix page migration stalls for blkdev pages""";1;1
"This patchset deals with page migration stalls that were reported by our
customer due to a block device page that had a bufferhead that was in the
bh LRU cache";0;1
"The patchset modifies the page migration code so that bufferheads are
completely handled inside buffer_migrate_page() and then provides a new
migration helper for pages with buffer heads that is safe to use even for
block device pages and that also deals with bh lrus";1;1
This patch (of 6);1;1
"Factor out function to compute number of expected page references in
migrate_page_move_mapping()";0;1
" Note that we move hpage_nr_pages() and
page_has_private() checks from under xas_lock_irq() however this is safe
since we hold page lock.";0;1
mm/mmu_notifier: use structure for invalidate_range_start/end calls v2;0;0
"To avoid having to change many call sites everytime we want to add a
parameter use a structure to group all parameters for the mmu_notifier
invalidate_range_start/end cakks";1;1
 No functional changes with this patch.;1;0
mm: put_and_wait_on_page_locked() while page is migrated;1;0
"Waiting on a page migration entry has used wait_on_page_locked() all along
since 2006: but you cannot safely wait_on_page_locked() without holding a
reference to the page, and that extra reference is enough to make
migrate_page_move_mapping() fail with -EAGAIN, when a racing task faults
on the entry before migrate_page_move_mapping() gets there";0;1
"And that failure is retried nine times, amplifying the pain when trying to
migrate a popular page";0;1
" With a single persistent faulter, migration
sometimes succeeds; with two or three concurrent faulters, success becomes
much less likely (and the more the page was mapped, the worse the overhead
of unmapping and remapping it on each try)";0;1
"This is especially a problem for memory offlining, where the outer level
retries forever (or until terminated from userspace), because a heavy
refault workload can trigger an endless loop of migration failures";0;1
wait_on_page_locked() is the wrong tool for the job;0;1
David Herrmann (but was he the first?) noticed this issue in 2014;0;0
Tim Chen started a thread in August 2017 which appears relevant;0;0
"where Kan Liang went
on to implicate __migration_entry_wait()";0;1
"and the thread ended
up with the v4.14 commits: 2554db916586 (""sched/wait: Break up long wake
list walk"") 11a19c7b099f (""sched/wait: Introduce wakeup boomark in
wake_up_page_bit"")
Baoquan He reported ""Memory hotplug softlock issue"" 14 November 2018";1;0
"We have all assumed that it is essential to hold a page reference while
waiting on a page lock: partly to guarantee that there is still a struct
page when MEMORY_HOTREMOVE is configured, but also to protect against
reuse of the struct page going to someone who then holds the page locked
indefinitely, when the waiter can reasonably expect timely unlocking";0;1
"But in fact, so long as wait_on_page_bit_common() does the put_page(), and
is careful not to rely on struct page contents thereafter, there is no
need to hold a reference to the page while waiting on it";0;1
" That does mean
that this case cannot go back through the loop: but that's fine for the
page migration case, and even if used more widely, is limited by the ""Stop
walking if it's locked"" optimization in wake_page_function()";0;1
"Add interface put_and_wait_on_page_locked() to do this, using ""behavior""
enum in place of ""lock"" arg to wait_on_page_bit_common() to implement it";1;0
"No interruptible or killable variant needed yet, but they might follow: I
have a vague notion that reporting -EINTR should take precedence over
return from wait_on_page_bit_common() without knowing the page state, so
arrange it accordingly - but that may be nothing but pedantic";0;1
"__migration_entry_wait() still has to take a brief reference to the page,
prior to calling put_and_wait_on_page_locked(): but now that it is dropped
before waiting, the chance of impeding page migration is very much
reduced";1;0
" Should we perhaps disable preemption across this?
shrink_page_list()'s __ClearPageLocked(): that was a surprise!  This
survived a lot of testing before that showed up";0;0
" PageWaiters may have
been set by wait_on_page_bit_common(), and the reference dropped, just
before shrink_page_list() succeeds in freezing its last page reference: in
such a case, unlock_page() must be used";1;1
" Follow the suggestion from
Michal Hocko, just revert a978d6f52106 (""mm: unlockless reclaim"") now";1;1
"that optimization predates PageWaiters, and won't buy much these days; but
we can reinstate it for the !PageWaiters case if anyone notices";0;1
"It does raise the question: should vmscan.c's is_page_cache_freeable() and
__remove_mapping() now treat a PageWaiters page as if an extra reference
were held?  Perhaps, but I don't think it matters much, since
shrink_page_list() already had to win its trylock_page(), so waiters are
not very common there: I noticed no difference when trying the bigger
change, and it's surely not needed while put_and_wait_on_page_locked() is
only used for page migration.";1;1
mm: thp: relocate flush_cache_range() in migrate_misplaced_transhuge_page();0;0
"There should be no cache left by the time we overwrite the old transhuge
pmd with the new one";1;0
" It's already too late to flush through the virtual
address because we already copied the page data to the new physical
address";0;1
So flush the cache before the data copy;0;0
"Also delete the ""end"" variable to shutoff a ""unused variable"" warning on
x86 where flush_cache_range() is a noop.";0;1
mm: thp: fix mmu_notifier in migrate_misplaced_transhuge_page();1;1
"change_huge_pmd() after arming the numa/protnone pmd doesn't flush the TLB
right away";0;0
" do_huge_pmd_numa_page() flushes the TLB before calling
migrate_misplaced_transhuge_page()";0;0
" By the time do_huge_pmd_numa_page()
runs some CPU could still access the page through the TLB";0;1
"change_huge_pmd() before arming the numa/protnone transhuge pmd calls
mmu_notifier_invalidate_range_start()";0;0
" So there's no need of
sequence in migrate_misplaced_transhuge_page() too, because by the time
migrate_misplaced_transhuge_page() runs, the pmd mapping has already been
invalidated in the secondary MMUs";1;1
" It has to or if a secondary MMU can
still write to the page, the migrate_page_copy() would lose data";1;0
"However an explicit mmu_notifier_invalidate_range() is needed before
migrate_misplaced_transhuge_page() starts copying the data of the
transhuge page or the below can happen for MMU notifier users sharing the
primary MMU pagetables and only implementing ->invalidate_range";0;0
"CPU0		CPU1		GPU sharing linux pagetables using
                                only ->invalidate_range
				GPU secondary MMU writes to the page
				mapped by the transhuge pmd
change_pmd_range()
mmu..._range_start()
->invalidate_range_start() noop
change_huge_pmd()
set_pmd_at(numa/protnone)
pmd_unlock()
		do_huge_pmd_numa_page()
		CPU TLB flush globally (1)
		CPU cannot write to page
		migrate_misplaced_transhuge_page()
				GPU writes to the page..";0;1
"		migrate_page_copy()
				...GPU stops writing to the page
CPU TLB flush (2)
mmu..._range_end() (3)
->invalidate_range_stop() noop
->invalidate_range()
				GPU secondary MMU is invalidated
				and cannot write to the page anymore
				(too late)
Just like we need a CPU TLB flush (1) because the TLB flush (2) arrives
too late, we also need a mmu_notifier_invalidate_range() before calling
migrate_misplaced_transhuge_page(), because the ->invalidate_range() in
(3) also arrives too late";0;1
"This requirement is the result of the lazy optimization in
change_huge_pmd() that releases the pmd_lock without first flushing the
TLB and without first calling mmu_notifier_invalidate_range()";0;0
"Even converting the removed mmu_notifier_invalidate_range_only_end() into
a mmu_notifier_invalidate_range_end() would not have been enough to fix
this, because it run after migrate_page_copy()";0;1
"After the hugepage data copy is done migrate_misplaced_transhuge_page()
can proceed and call set_pmd_at without having to flush the TLB nor any
secondary MMUs because the secondary MMU invalidate, just like the CPU TLB
flush, has to happen before the migrate_page_copy() is called or it would
be a bug in the first place (and it was for drivers using
->invalidate_range())";0;0
KVM is unaffected because it doesn't implement ->invalidate_range();0;0
"The standard PAGE_SIZEd migrate_misplaced_page is less accelerated and
uses the generic migrate_pages which transitions the pte from
numa/protnone to a migration entry in try_to_unmap_one() and flushes TLBs
and all mmu notifiers there before copying the page.";0;0
mm: thp: fix MADV_DONTNEED vs migrate_misplaced_transhuge_page race condition;1;1
"Patch series ""migrate_misplaced_transhuge_page race conditions""";0;1
"Aaron found a new instance of the THP MADV_DONTNEED race against
pmdp_clear_flush* variants, that was apparently left unfixed";1;1
"While looking into the race found by Aaron, I may have found two more
issues in migrate_misplaced_transhuge_page";0;1
"These race conditions would not cause kernel instability, but they'd
corrupt userland data or leave data non zero after MADV_DONTNEED";1;1
"I did only minor testing, and I don't expect to be able to reproduce this
(especially the lack of ->invalidate_range before migrate_page_copy,
requires the latest iommu hardware or infiniband to reproduce)";0;1
" The last
patch is noop for x86 and it needs further review from maintainers of
archs that implement flush_cache_range() (not in CC yet)";1;1
"To avoid confusion, it's not the first patch that introduces the bug fixed
in the second patch, even before removing the
pmdp_huge_clear_flush_notify, that _notify suffix was called after
migrate_page_copy already run";0;1
This patch (of 3);1;0
"This is a corollary of ced108037c2aa (""thp: fix MADV_DONTNEED vs";1;1
" numa
balancing race""), 58ceeb6bec8 (""thp: fix MADV_DONTNEED vs";1;1
" MADV_FREE
race"") and 5b7abeae3af8c (""thp: fix MADV_DONTNEED vs clear soft dirty
race)";1;1
"When the above three fixes where posted Dave asked
but apparently this was missed";0;1
"The pmdp_clear_flush* in migrate_misplaced_transhuge_page() was introduced
in a54a407fbf7 (""mm: Close races between THP migration and PMD numa
clearing"")";0;1
"The important part of such commit is only the part where the page lock is
not released until the first do_huge_pmd_numa_page() finished disarming
the pagenuma/protnone";0;1
"The addition of pmdp_clear_flush() wasn't beneficial to such commit and
there's no commentary about such an addition either";0;1
"I guess the pmdp_clear_flush() in such commit was added just in case for
safety, but it ended up introducing the MADV_DONTNEED race condition found
by Aaron";1;0
"At that point in time nobody thought of such kind of MADV_DONTNEED race
conditions yet (they were fixed later) so the code may have looked more
robust by adding the pmdp_clear_flush()";1;1
"This specific race condition won't destabilize the kernel, but it can
confuse userland because after MADV_DONTNEED the memory won't be zeroed
out";1;1
This also optimizes the code and removes a superfluous TLB flush.;1;0
mm: workingset: tell cache transitions from workingset thrashing;1;0
"Refaults happen during transitions between workingsets as well as in-place
thrashing";1;0
" Knowing the difference between the two has a range of
applications, including measuring the impact of memory shortage on the
system performance, as well as the ability to smarter balance pressure
between the filesystem cache and the swap-backed workingset";1;1
"During workingset transitions, inactive cache refaults and pushes out
established active cache";1;0
" When that active cache isn't stale, however,
and also ends up refaulting, that's bonafide thrashing";1;1
"Introduce a new page flag that tells on eviction whether the page has been
active or not in its lifetime";1;1
" This bit is then stored in the shadow
entry, to classify refaults as transitioning or thrashing";0;1
"How many page->flags does this leave us with on 32-bit?
	20 bits are always page flags
	21 if you have an MMU
	23 with the zone bits for DMA, Normal, HighMem, Movable
	29 with the sparsemem section bits
	30 if PAE is enabled
	31 with this patch";0;1
So on 32-bit PAE, that leaves 1 bit for distinguishing two NUMA nodes;1;1
" If
that's not enough, the system can switch to discontigmem and re-gain the 6
or 7 sparsemem section bits.";1;1
mm: Convert page migration to XArray;1;0
;0;0
mm/migrate.c: split only transparent huge pages when allocation fails;0;0
split_huge_page_to_list() fails on HugeTLB pages;1;0
" I was experimenting
with moving 32MB contig HugeTLB pages on arm64 (with a debug patch
applied) and hit the following stack trace when the kernel crashed";1;0
"When unmap_and_move[_huge_page]() fails due to lack of memory, the
splitting should happen only for transparent huge pages not for HugeTLB
pages";0;0
 PageTransHuge() returns true for both THP and HugeTLB pages;0;0
"Hence the conditonal check should test PagesHuge() flag to make sure that
given pages is not a HugeTLB one.";1;1
mm, thp: fix mlocking THP page with migration enabled;1;1
A transparent huge page is represented by a single entry on an LRU list;1;1
"Therefore, we can only make unevictable an entire compound page, not
individual subpages";1;1
"If a user tries to mlock() part of a huge page, we want the rest of the
page to be reclaimable";1;0
"We handle this by keeping PTE-mapped huge pages on normal LRU lists: the
PMD on border of VM_LOCKED VMA will be split into PTE table";1;0
"Introduction of THP migration breaks[1] the rules around mlocking THP
pages";1;0
" If we had a single PMD mapping of the page in mlocked VMA, the
page will get mlocked, regardless of PTE mappings of the page";0;1
"For tmpfs/shmem it's easy to fix by checking PageDoubleMap() in
remove_migration_pmd()";1;1
Anon THP pages can only be shared between processes via fork();0;1
" Mlocked
page can only be shared if parent mlocked it before forking, otherwise CoW
will be triggered on mlock()";0;0
"For Anon-THP, we can fix the issue by munlocking the page on removing PTE
migration entry for the page";1;1
" PTEs for the page will always come after
mlocked PMD: rmap walks VMAs from oldest to newest";0;1
Test-case;0;1
"	int main(void)
		addr = mmap((void *)0x20000000UL, 2UL << 20, PROT_READ | PROT_WRITE,
	        mbind(addr, 2UL << 20, MPOL_PREFERRED | MPOL_F_RELATIVE_NODES,";0;0
mm, sched/numa: Remove rate-limiting of automatic NUMA balancing migration;1;0
"Rate limiting of page migrations due to automatic NUMA balancing was
introduced to mitigate the worst-case scenario of migrating at high
frequency due to false sharing or slowly ping-ponging between nodes";0;0
"Since then, a lot of effort was spent on correctly identifying these
pages and avoiding unnecessary migrations and the safety net may no longer
be required";0;1
"Jirka Hladky reported a regression in 4.17 due to a scheduler patch that
avoids spreading STREAM tasks wide prematurely";1;0
"However, once the task
was properly placed, it delayed migrating the memory due to rate limiting";0;0
Increasing the limit fixed the problem for him;0;0
"Currently, the limit is hard-coded and does not account for the real
capabilities of the hardware";0;0
"Even if an estimate was attempted, it would
not properly account for the number of memory controllers and it could
not account for the amount of bandwidth used for normal accesses";0;1
"Rather
than fudging, this patch simply eliminates the rate limiting";1;1
"However, Jirka reports that a STREAM configuration using multiple
processes achieved similar performance to 4.16";0;0
"In local tests, this patch
improved performance of STREAM relative to the baseline but it is somewhat
machine-dependent";1;1
"Most workloads show little or not performance difference
implying that there is not a heavily reliance on the throttling mechanism
and it is safe to remove";1;1
"STREAM on 2-socket machine
                         4.19.0-rc5             4.19.0-rc5
                         numab-v1r1       noratelimit-v1r1
MB/sec copy     43298.52 (   0.00%)    44673.38 (   3.18%)
MB/sec scale    30115.06 (   0.00%)    31293.06 (   3.91%)
MB/sec add      32825.12 (   0.00%)    34883.62 (   6.27%)
MB/sec triad    32549.52 (   0.00%)    34906.60 (   7.24%";1;0
mm/migrate: Use spin_trylock() while resetting rate limit;1;1
"Since this spinlock will only serialize the migrate rate limiting,
convert the spin_lock() to a spin_trylock()";1;0
"If another thread is updating, this
task can move on";0;1
"Specjbb2005 results (8 warehouses)
Higher bops are better
2 Socket - 2  Node Haswell - X86
JVMS  Prev    Current  %Change
2 Socket - 4 Node Power8 - PowerNV
JVMS  Prev    Current  %Change
2 Socket - 2  Node Power9 - PowerNV
JVMS  Prev    Current  %Change
4 Socket - 4  Node Power7 - PowerVM
JVMS  Prev     Current  %Change
Avoiding stretching of window intervals may be the reason for the
regression";0;1
Also code now uses READ_ONCE/WRITE_ONCE;1;0
"That may
also be hurting performance to some extent";1;1
Some events stats before and after applying the patch;0;0
"perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
Event                     Before          After
cs                        14,285,708      13,818,546
migrations                1,180,621       1,149,960
faults                    339,114         385,583
cache-misses              55,205,631,894  55,259,546,768
sched:sched_move_numa     843             2,257
sched:sched_stick_numa    6               9
sched:sched_swap_numa     219             512
migrate:mm_migrate_pages  365             2,225
vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
Event                   Before  After
numa_hint_faults        26907   72692
numa_hint_faults_local  24279   62270
numa_hit                239771  238762
numa_huge_pte_updates   0       48
numa_interleave         68      75
numa_local              239688  238676
numa_other              83      86
numa_pages_migrated     363     2225
numa_pte_updates        27415   98557
perf stats 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
Event                     Before          After
cs                        3,202,779       3,173,490
migrations                37,186          36,966
faults                    106,076         108,776
cache-misses              12,024,873,744  12,200,075,320
sched:sched_move_numa     931             1,264
sched:sched_stick_numa    0               0
sched:sched_swap_numa     1               0
migrate:mm_migrate_pages  637             899
vmstat 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
Event                   Before  After
numa_hint_faults        17409   21109
numa_hint_faults_local  14367   17120
numa_hit                73953   72934
numa_huge_pte_updates   20      42
numa_interleave         25      33
numa_local              73892   72866
numa_other              61      68
numa_pages_migrated     668     915
numa_pte_updates        27276   42326
perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
Event                     Before       After
cs                        8,474,013    8,312,022
migrations                254,934      231,705
faults                    320,506      310,242
cache-misses              110,580,458  402,324,573
sched:sched_move_numa     725          193
sched:sched_stick_numa    0            0
sched:sched_swap_numa     7            3
migrate:mm_migrate_pages  145          93
vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
Event                   Before  After
numa_hint_faults        22797   11838
numa_hint_faults_local  21539   11216
numa_hit                89308   90689
numa_huge_pte_updates   0       0
numa_interleave         865     1579
numa_local              88955   89634
numa_other              353     1055
numa_pages_migrated     149     92
numa_pte_updates        22930   12109
perf stats 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
Event                     Before     After
cs                        2,195,628  2,170,481
migrations                11,179     10,126
faults                    149,656    160,962
cache-misses              8,117,515  10,834,845
sched:sched_move_numa     49         10
sched:sched_stick_numa    0          0
sched:sched_swap_numa     0          0
migrate:mm_migrate_pages  5          2
vmstat 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
Event                   Before  After
numa_hint_faults        3577    403
numa_hint_faults_local  3476    358
numa_hit                26142   25898
numa_huge_pte_updates   0       0
numa_interleave         358     207
numa_local              26042   25860
numa_other              100     38
numa_pages_migrated     5       2
numa_pte_updates        3587    400
perf stats 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
Event                     Before           After
cs                        100,602,296      110,339,633
migrations                4,135,630        4,139,812
faults                    789,256          863,622
cache-misses              226,160,621,058  231,838,045,660
sched:sched_move_numa     1,366            2,196
sched:sched_stick_numa    16               33
sched:sched_swap_numa     374              544
migrate:mm_migrate_pages  1,350            2,469
vmstat 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
Event                   Before  After
numa_hint_faults        47857   85748
numa_hint_faults_local  39768   66831
numa_hit                240165  242213
numa_huge_pte_updates   0       0
numa_interleave         0       0
numa_local              240165  242211
numa_other              0       2
numa_pages_migrated     1224    2376
numa_pte_updates        48354   86233
perf stats 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
Event                     Before          After
cs                        58,515,496      59,331,057
migrations                564,845         552,019
faults                    245,807         266,586
cache-misses              73,603,757,976  73,796,312,990
sched:sched_move_numa     996             981
sched:sched_stick_numa    10              54
sched:sched_swap_numa     193             286
migrate:mm_migrate_pages  646             713
vmstat 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
Event                   Before  After
numa_hint_faults        13422   14807
numa_hint_faults_local  5619    5738
numa_hit                36118   36230
numa_huge_pte_updates   0       0
numa_interleave         0       0
numa_local              36116   36228
numa_other              2       2
numa_pages_migrated     616     703
numa_pte_updates        13374   14742";0;1
mm: soft-offline: close the race against page allocation;0;0
"A process can be killed with SIGBUS(BUS_MCEERR_AR) when it tries to
allocate a page that was just freed on the way of soft-offline";0;0
" This is
undesirable because soft-offline (which is about corrected error) is
less aggressive than hard-offline (which is about uncorrected error),
and we can make soft-offline fail and keep using the page for good
reason like ""system is busy.""
Two main changes of this patch are";1;1
- setting migrate type of the target page to MIGRATE_ISOLATE;1;1
"As done
  in free_unref_page_commit(), this makes kernel bypass pcplist when
  freeing the page";1;1
"So we can assume that the page is in freelist just
  after put_page() returns,
- setting PG_hwpoison on free page under zone->lock which protects
  freelists, so this allows us to avoid setting PG_hwpoison on a page
  that is decided to be allocated soon.";1;1
mm: fix race on soft-offlining free huge pages;1;1
"Patch series ""mm: soft-offline: fix race against page allocation""";1;1
"Xishi recently reported the issue about race on reusing the target pages
of soft offlining";0;1
" Discussion and analysis showed that we need make
sure that setting PG_hwpoison should be done in the right place under
zone->lock for soft offline";1;1
" 1/2 handles free hugepage's case, and 2/2
hanldes free buddy page's case";0;1
This patch (of 2);1;0
"There's a race condition between soft offline and hugetlb_fault which
causes unexpected process killing and/or hugetlb allocation failure";0;1
The process killing is caused by the following flow;0;0
"  CPU 0               CPU 1              CPU 2
  soft offline
    get_any_page
    // find the hugetlb is free
                      mmap a hugetlb file
                      page fault
                          hugetlb_fault
                            hugetlb_no_page
                              alloc_huge_page
                              // succeed
      soft_offline_free_page
      // set hwpoison flag
                                         mmap the hugetlb file
                                         page fault
                                             hugetlb_fault
                                               hugetlb_no_page
                                                 find_lock_page
                                                   return VM_FAULT_HWPOISON
                                           mm_fault_error
                                             do_sigbus
                                             // kill the process
The hugetlb allocation failure comes from the following flow";1;0
"  CPU 0                          CPU 1
                                 mmap a hugetlb file
                                 // reserve all free page but don't fault-in
  soft offline
    get_any_page
    // find the hugetlb is free
      soft_offline_free_page
      // set hwpoison flag
        dissolve_free_huge_page
        // fail because all free hugepages are reserved
                                 page fault
                                     hugetlb_fault
                                       hugetlb_no_page
                                         alloc_huge_page
                                             dequeue_huge_page_node_exact
                                             // ignore hwpoisoned hugepage
                                             // and finally fail due to no-mem
The root cause of this is that current soft-offline code is written based
on an assumption that PageHWPoison flag should be set at first to avoid
accessing the corrupted data";0;1
" This makes sense for memory_failure() or
hard offline, but does not for soft offline because soft offline is about
corrected (not uncorrected) error and is safe from data lost";1;1
" This patch
changes soft offline semantics where it sets PageHWPoison flag only after
containment of the error page completes successfully.";1;1
include/linux/compiler*.h: make compiler-*.h mutually exclusive;1;1
"Commit cafa0010cd51 (""Raise the minimum required gcc version to 4.6"")
recently exposed a brittle part of the build for supporting non-gcc
compilers";1;1
"Both Clang and ICC define __GNUC__, __GNUC_MINOR__, and
__GNUC_PATCHLEVEL__ for quick compatibility with code bases that haven't
added compiler specific checks for __clang__ or __INTEL_COMPILER";0;0
"This is brittle, as they happened to get compatibility by posing as a
certain version of GCC";0;1
" This broke when upgrading the minimal version
of GCC required to build the kernel, to a version above what ICC and
Clang claim to be";0;1
"Rather than always including compiler-gcc.h then undefining or
redefining macros in compiler-intel.h or compiler-clang.h, let's
separate out the compiler specific macro definitions into mutually
exclusive headers, do more proper compiler detection, and keep shared
definitions in compiler_types.h";1;1
"Fixes: cafa0010cd51 (""Raise the minimum required gcc version to 4.6"")";0;1
dax: remove VM_MIXEDMAP for fsdax and device dax;1;0
This patch is reworked from an earlier patch that Dan has posted;1;1
"VM_MIXEDMAP is used by dax to direct mm paths like vm_normal_page() that
the memory page it is dealing with is not typical memory from the linear
map";0;1
" The get_user_pages_fast() path, since it does not resolve the vma,
use that as a VM_MIXEDMAP replacement in some locations";0;0
" In the cases
where there is no pte to consult we fallback to using vma_is_dax() to
detect the VM_MIXEDMAP special case";0;1
"Now that we have explicit driver pfn_t-flag opt-in/opt-out for
get_user_pages() support for DAX we can stop setting VM_MIXEDMAP";0;1
" This
also means we no longer need to worry about safely manipulating vm_flags
in a future where we support dynamically changing the dax mode of a
file";1;1
"DAX should also now be supported with madvise_behavior(), vma_merge(),
and copy_page_range()";1;1
This patch has been tested against ndctl unit test;0;0
" It has also been
tested against xfstests commit: 625515d using fake pmem created by
memmap and no additional issues have been observed.";0;0
mm: migrate: fix double call of radix_tree_replace_slot();1;1
"radix_tree_replace_slot() is called twice for head page, it's obviously
a bug";1;1
 Let's fix it.;1;1
mm: enable thp migration for shmem thp;1;0
"My testing for the latest kernel supporting thp migration showed an
infinite loop in offlining the memory block that is filled with shmem
thps";0;0
" We can get out of the loop with a signal, but kernel should return
with failure in this case";0;1
"What happens in the loop is that scan_movable_pages() repeats returning
the same pfn without any progress";1;0
" That's because page migration always
fails for shmem thps";0;1
"In memory offline code, memory blocks containing unmovable pages should be
prevented from being offline targets by has_unmovable_pages() inside
start_isolate_page_range()";0;0
" So it's possible to change migratability for
non-anonymous thps to avoid the issue, but it introduces more complex and
thp-specific handling in migration code, so it might not good";1;1
"So this patch is suggesting to fix the issue by enabling thp migration for
shmem thp";1;1
" Both of anon/shmem thp are migratable so we don't need
precheck about the type of thps.";1;1
mm: fix do_pages_move status handling;1;1
"Li Wang has reported that LTP move_pages04 test fails with the current
tree";1;0
LTP move_pages04;1;0
"   TFAIL  :  move_pages04.c:143: status[1] is EPERM, expected EFAULT
The test allocates an array of two pages, one is present while the other
is not (resp";0;1
" backed by zero page) and it expects EFAULT for the second
page as the man page suggests";0;1
" We are reporting EPERM which doesn't make
any sense and this is a result of a bug from cf5f16b23ec9 (""mm: unclutter
THP migration"")";0;1
"do_pages_move tries to handle as many pages in one batch as possible so we
queue all pages with the same node target together and that corresponds to
[start, i] range which is then used to update status array";0;1
add_page_for_migration will correctly notice the zero (resp;1;0
" !present)
page and returns with EFAULT which gets written to the status";1;0
" But if
this is the last page in the array we do not update start and so the last
store_status after the loop will overwrite the range of the last batch
with NUMA_NO_NODE (which corresponds to EPERM)";1;0
"Fix this by simply bailing out from the last flush if the pagelist is
empty as there is clearly nothing more to do.";1;1
page cache: use xa_lock;1;1
"Remove the address_space ->tree_lock and use the xa_lock newly added to
the radix_tree_root";1;0
" Rename the address_space ->page_tree to ->i_pages,
since we don't really care that it's a tree.";1;1
mm: unclutter THP migration;1;1
"THP migration is hacked into the generic migration with rather
surprising semantic";0;0
" The migration allocation callback is supposed to
check whether the THP can be migrated at once and if that is not the
case then it allocates a simple page to migrate";1;1
" unmap_and_move then
fixes that up by spliting the THP into small pages while moving the head
page to the newly allocated order-0 page";0;1
" Remaning pages are moved to
the LRU list by split_huge_page";1;0
" The same happens if the THP allocation
fails";1;0
 This is really ugly and error prone [1];0;1
"I also believe that split_huge_page to the LRU lists is inherently wrong
because all tail pages are not migrated";0;1
" Some callers will just work
around that by retrying (e.g";0;0
 memory hotplug);1;1
" There are other pfn
walkers which are simply broken though";0;0
 e.g;0;0
"madvise_inject_error will
migrate head and then advances next pfn by the huge page size";1;1
"do_move_page_to_node_array, queue_pages_range (migrate_pages, mbind),
will simply split the THP before migration if the THP migration is not
supported then falls back to single page migration but it doesn't handle
tail pages if the THP migration path is not able to allocate a fresh THP
so we end up with ENOMEM and fail the whole migration which is a
questionable behavior";0;1
" Page compaction doesn't try to migrate large
pages so it should be immune";0;1
"This patch tries to unclutter the situation by moving the special THP
handling up to the migrate_pages layer where it actually belongs";1;1
" We
simply split the THP page into the existing list if unmap_and_move fails
with ENOMEM and retry";1;0
" So we will _always_ migrate all THP subpages and
specific migrate_pages users do not have to deal with this case in a
special way.";0;0
mm, migrate: remove reason argument from new_page_t;1;0
No allocation callback is using this argument anymore;1;0
" new_page_node
used to use this parameter to convey node_id resp";0;1
" migration error up
to move_pages code (do_move_page_to_node_array)";1;0
" The error status never
made it into the final status field and we have a better way to
communicate node id to the status field now";0;1
" All other allocation
callbacks simply ignored the argument so we can drop it finally.";1;1
mm, numa: rework do_pages_move;0;0
"Patch series ""unclutter thp migration""
Motivation";0;1
"THP migration is hacked into the generic migration with rather
surprising semantic";0;0
" The migration allocation callback is supposed to
check whether the THP can be migrated at once and if that is not the
case then it allocates a simple page to migrate";1;1
" unmap_and_move then
fixes that up by splitting the THP into small pages while moving the
head page to the newly allocated order-0 page";0;1
" Remaining pages are
moved to the LRU list by split_huge_page";1;0
" The same happens if the THP
allocation fails";1;0
 This is really ugly and error prone [2];1;1
"I also believe that split_huge_page to the LRU lists is inherently wrong
because all tail pages are not migrated";0;1
" Some callers will just work
around that by retrying (e.g";0;0
 memory hotplug);1;1
" There are other pfn
walkers which are simply broken though";0;0
 e.g;0;0
"madvise_inject_error will
migrate head and then advances next pfn by the huge page size";1;1
"do_move_page_to_node_array, queue_pages_range (migrate_pages, mbind),
will simply split the THP before migration if the THP migration is not
supported then falls back to single page migration but it doesn't handle
tail pages if the THP migration path is not able to allocate a fresh THP
so we end up with ENOMEM and fail the whole migration which is a
questionable behavior";0;1
" Page compaction doesn't try to migrate large
pages so it should be immune";0;1
"The first patch reworks do_pages_move which relies on a very ugly
calling semantic when the return status is pushed to the migration path
via private pointer";1;1
" It uses pre allocated fixed size batching to
achieve that";1;1
" We simply cannot do the same if a THP is to be split
during the migration path which is done in the patch 3";1;0
" Patch 2 is
follow up cleanup which removes the mentioned return status calling
convention ugliness";1;1
On a side note;0;0
"There are some semantic issues I have encountered on the way when
working on patch 1 but I am not addressing them here";1;0
 E.g;0;0
"trying to
move THP tail pages will result in either success or EBUSY (the later
one more likely once we isolate head from the LRU list)";1;1
" Hugetlb
reports EACCESS on tail pages";1;0
" Some errors are reported via status
parameter but migration failures are not even though the original
`reason' argument suggests there was an intention to do so";0;1
" From a
quick look into git history this never worked";1;0
" I have tried to keep the
semantic unchanged";1;0
"Then there is a relatively minor thing that the page isolation might
fail because of pages not being on the LRU - e.g";0;1
"because they are
sitting on the per-cpu LRU caches";0;1
 Easily fixable;1;1
This patch (of 3);1;0
"do_pages_move is supposed to move user defined memory (an array of
addresses) to the user defined numa nodes (an array of nodes one for
each address)";1;0
" The user provided status array then contains resulting
numa node for each address or an error";0;0
" The semantic of this function
is little bit confusing because only some errors are reported back";1;1
Notably migrate_pages error is only reported via the return value;0;1
" This
patch doesn't try to address these semantic nuances but rather change
the underlying implementation";0;1
"Currently we are processing user input (which can be really large) in
batches which are stored to a temporarily allocated page";0;1
" Each address
is resolved to its struct page and stored to page_to_node structure
along with the requested target numa node";0;0
" The array of these
structures is then conveyed down the page migration path via private
argument";0;1
" new_page_node then finds the corresponding structure and
allocates the proper target page";0;0
"What is the problem with the current implementation and why to change
it? Apart from being quite ugly it also doesn't cope with unexpected
pages showing up on the migration list inside migrate_pages path";0;1
" That
doesn't happen currently but the follow up patch would like to make the
thp migration code more clear and that would need to split a THP into
the list for some cases";1;1
"How does the new implementation work? Well, instead of batching into a
fixed size array we simply batch all pages that should be migrated to
the same node and isolate all of them into a linked list which doesn't
require any additional storage";1;1
" This should work reasonably well
because page migration usually migrates larger ranges of memory to a
specific node";0;1
" So the common case should work equally well as the
current implementation";1;1
" Even if somebody constructs an input where the
target numa nodes would be interleaved we shouldn't see a large
performance impact because page migration alone doesn't really benefit
from batching";0;1
" mmap_sem batching for the lookup is quite questionable
and isolate_lru_page which would benefit from batching is not using it
even in the current implementation.";0;1
mm/migrate: properly preserve write attribute in special migrate entry;0;0
"Use of pte_write(pte) is only valid for present pte, the common code
which set the migration entry can be reach for both valid present pte
and special swap entry (for device memory)";1;0
" Fix the code to use the
mpfn value which properly handle both cases";1;1
"On x86 this did not have any bad side effect because pte write bit is
below PAGE_BIT_GLOBAL and thus special swap entry have it set to 0 which
in turn means we were always creating read only special migration entry";0;0
"So once migration did finish we always write protected the CPU page
table entry (moreover this is only an issue when migrating from device
memory to system memory)";0;1
" End effect is that CPU write access would
fault again and restore write permission";0;1
"This behaviour isn't too bad; it just burns CPU cycles by forcing CPU to
take a second fault on write access";0;1
"ie, double faulting the same
address";1;1
" There is no corruption or incorrect states (it behaves as a
COWed page from a fork with a mapcount of 1).";0;1
sched/numa: avoid trapping faults and attempting migration of file-backed dirty pages;1;1
"change_pte_range is called from task work context to mark PTEs for
receiving NUMA faulting hints";0;0
" If the marked pages are dirty then
migration may fail";0;0
" Some filesystems cannot migrate dirty pages without
blocking so are skipped in MIGRATE_ASYNC mode which just wastes CPU";1;1
"Even when they can, it can be a waste of cycles when the pages are
shared forcing higher scan rates";0;1
" This patch avoids marking shared
dirty pages for hinting faults but also will skip a migration if the
page was dirtied after the scanner updated a clean page";1;0
"This is most noticeable running the NASA Parallel Benchmark when backed
by btrfs, the default root filesystem for some distributions, but also
noticeable when using XFS";1;0
"The following are results from a 4-socket machine running a 4.16-rc4
kernel with some scheduler patches that are pending for the next merge
window";1;1
"                        4.16.0-rc4             4.16.0-rc4
                 schedtip-20180309          nodirty-v1
  Time cg.D      459.07 (   0.00%)      444.21 (   3.24%)
  Time ep.D       76.96 (   0.00%)       77.69 (  -0.95%)
  Time is.D       25.55 (   0.00%)       27.85 (  -9.00%)
  Time lu.D      601.58 (   0.00%)      596.87 (   0.78%)
  Time mg.D      107.73 (   0.00%)      108.22 (  -0.45%)
is.D regresses slightly in terms of absolute time but note that that
particular load varies quite a bit from run to run";0;0
" The more relevant
observation is the total system CPU usage";0;0
"            4.16.0-rc4  4.16.0-rc4
          schedtip-20180309 nodirty-v1
  User        71471.91    70627.04
  System      11078.96     8256.13
  Elapsed       661.66      632.74
That is a substantial drop in system CPU usage and overall the workload
completes faster";1;1
" The NUMA balancing statistics are also interesting
  NUMA base PTE updates        111407972   139848884
  NUMA huge PMD updates           206506      264869
  NUMA page range updates      217139044   275461812
  NUMA hint faults               4300924     3719784
  NUMA hint local faults         3012539     3416618
  NUMA hint local percent             70          91
  NUMA pages migrated            1517487     1358420
While more PTEs are scanned due to changes in what faults are gathered,
it's clear that a far higher percentage of faults are local as the bulk
of the remote hits were dirty pages that, in this case with btrfs, had
no chance of migrating";0;1
"The following is a comparison when using XFS as that is a more realistic
filesystem choice for a data partition
                        4.16.0-rc4             4.16.0-rc4
                 schedtip-20180309          nodirty-v1r47
  Time cg.D      485.28 (   0.00%)      442.62 (   8.79%)
  Time ep.D       77.68 (   0.00%)       77.54 (   0.18%)
  Time is.D       26.44 (   0.00%)       24.79 (   6.24%)
  Time lu.D      597.46 (   0.00%)      597.11 (   0.06%)
  Time mg.D      142.65 (   0.00%)      105.83 (  25.81%)
That is a reasonable gain on two relatively long-lived workloads";0;0
" While
not presented, there is also a substantial drop in system CPu usage and
the NUMA balancing stats show similar improvements in locality as btrfs
did.";1;1
mm: add kernel_move_pages() helper, move compat syscall to mm/migrate.c;1;0
"Move compat_sys_move_pages() to mm/migrate.c and make it call a newly
introduced helper -- kernel_move_pages() -- instead of the syscall";1;1
This patch is part of a series which removes in-kernel calls to syscalls;1;1
On this basis, the syscall entry path can be streamlined;0;1
For details, see;0;0
mm, hugetlb: do not rely on overcommit limit during migration;0;0
hugepage migration relies on __alloc_buddy_huge_page to get a new page;0;0
This has 2 main disadvantages;0;0
"1) it doesn't allow to migrate any huge page if the pool is used
   completely which is not an exceptional case as the pool is static and
   unused memory is just wasted";1;1
"2) it leads to a weird semantic when migration between two numa nodes
   might increase the pool size of the destination NUMA node while the
   page is in use";0;1
" The issue is caused by per NUMA node surplus pages
   tracking (see free_huge_page)";0;1
"Address both issues by changing the way how we allocate and account
pages allocated for migration";1;1
 Those should temporal by definition;1;1
" So
we mark them that way (we will abuse page flags in the 3rd page) and
update free_huge_page to free such pages to the page allocator";0;1
" Page
migration path then just transfers the temporal status from the new page
to the old one which will be freed on the last reference";0;0
" The global
surplus count will never change during this path but we still have to be
careful when migrating a per-node suprlus page";0;1
" This is now handled in
move_hugetlb_state which is called from the migration path and it copies
the hugetlb specific page state and fixes up the accounting when needed
Rename __alloc_buddy_huge_page to __alloc_surplus_huge_page to better
reflect its purpose";0;1
" The new allocation routine for the migration path
is __alloc_migrate_huge_page";1;1
"The user visible effect of this patch is that migrated pages are really
temporal and they travel between NUMA nodes as per the migration
request";0;1
"Before migration
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:1
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
After
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:1
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
with the previous implementation, both nodes would have nr_hugepages:1
until the page is freed.";0;0
"Revert ""mm, thp: Do not make pmd/pud dirty without a reason""";1;1
This reverts commit 152e93af3cfe2d29d8136cc0a02a8612507136ee;1;0
"It was a nice cleanup in theory, but as Nicolai Stange points out, we do
need to make the page dirty for the copy-on-write case even when we
didn't end up making it writable, since the dirty bit is what we use to
check that we've gone through a COW cycle.";0;1
mm, thp: Do not make pmd/pud dirty without a reason;1;1
"Currently we make page table entries dirty all the time regardless of
access type and don't even consider if the mapping is write-protected";1;0
"The reasoning is that we don't really need dirty tracking on THP and
making the entry dirty upfront may save some time on first write to the
page";0;1
"Unfortunately, such approach may result in false-positive
can_follow_write_pmd() for huge zero page or read-only shmem file";0;0
"Let's only make page dirty only if we about to write to the page anyway
(as we do for small pages)";1;1
"I've restructured the code to make entry dirty inside
maybe_p[mu]d_mkwrite()";0;1
"It also takes into account if the vma is
write-protected.";1;0
mm/mmu_notifier: avoid call to invalidate_range() in range_end();1;1
"This is an optimization patch that only affect mmu_notifier users which
rely on the invalidate_range() callback";0;0
" This patch avoids calling that
callback twice in a row from inside __mmu_notifier_invalidate_range_end
Existing pattern (before this patch)";1;0
New pattern (after this patch);1;0
"We call the invalidate_range callback after clearing the page table
under the page table lock and we skip the call to invalidate_range
inside the __mmu_notifier_invalidate_range_end() function";0;1
Idea from Andrea Arcangeli;0;0
License cleanup: add SPDX GPL-2.0 license identifier to files with no license;1;1
"Many source files in the tree are missing licensing information, which
makes it harder for compliance tools to determine the correct license";0;1
"By default all files without license information are under the default
license of the kernel, which is GPL version 2";0;0
"Update the files which contain no license information with the 'GPL-2.0'
SPDX license identifier";1;0
" The SPDX identifier is a legally binding
shorthand, which can be used instead of the full boiler plate text";1;0
"This patch is based on work done by Thomas Gleixner and Kate Stewart and
Philippe Ombredanne";0;1
How this work was done;1;1
"Patches were generated and checked against linux-4.14-rc6 for a subset of
the use cases";1;0
 - file had no licensing information it it;1;0
" - file was a */uapi/* one with no licensing information in it,
 - file was a */uapi/* one with existing licensing information,
Further patches will be generated in subsequent months to fix up cases
where non-standard license headers were used, and references to license
had to be inferred by heuristics based on keywords";0;1
"The analysis to determine which SPDX License Identifier to be applied to
a file was done in a spreadsheet of side by side results from of the
output of two independent scanners (ScanCode & Windriver) producing SPDX
tag:value files created by Philippe Ombredanne";0;1
" Philippe prepared the
base worksheet, and did an initial spot review of a few 1000 files";0;0
"The 4.13 kernel was the starting point of the analysis with 60,537 files
assessed";1;0
" Kate Stewart did a file by file comparison of the scanner
to be applied to the file";1;1
"She confirmed any determination that was not
immediately clear with lawyers working with the Linux Foundation";0;0
Criteria used to select files for SPDX license identifier tagging was;0;1
 - Files considered eligible had to be source code files;0;0
" - Make and config files were included as candidates if they contained >5
   lines of source
 - File already had some variant of a license header in it (even if <5
   lines)";1;1
All documentation files were explicitly excluded;1;1
"The following heuristics were used to determine which SPDX license
identifiers to apply";1;0
" - when both scanners couldn't find any license traces, file was
   considered to have no license information in it, and the top level
   COPYING file license applied";1;1
   For non */uapi/* files that summary was;1;1
"   SPDX license identifier                            # files
   GPL-2.0                                              11139
   and resulted in the first patch in this series";0;0
"   If that file was a */uapi/* path one, it was ""GPL-2.0 WITH
   Linux-syscall-note"" otherwise it was ""GPL-2.0""";1;0
 Results of that was;0;1
"   SPDX license identifier                            # files
   GPL-2.0 WITH Linux-syscall-note                        930
   and resulted in the second patch in this series";0;0
" - if a file had some form of licensing information in it, and was one
   of the */uapi/* ones, it was denoted with the Linux-syscall-note if
   any GPL family license was found in the file or had no licensing in
   it (per prior point)";1;0
 Results summary;0;1
"   SPDX license identifier                            # files
   GPL-2.0 WITH Linux-syscall-note                       270
   GPL-2.0+ WITH Linux-syscall-note                      169
   ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
   ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
   LGPL-2.1+ WITH Linux-syscall-note                      15
   GPL-1.0+ WITH Linux-syscall-note                       14
   ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
   LGPL-2.0+ WITH Linux-syscall-note                       4
   LGPL-2.1 WITH Linux-syscall-note                        3
   ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
   ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
   and that resulted in the third patch in this series";0;0
" - when the two scanners agreed on the detected license(s), that became
   the concluded license(s)";0;0
" - when there was disagreement between the two scanners (one detected a
   license but the other didn't, or they both detected different
   licenses) a manual inspection of the file occurred";0;1
" - In most cases a manual inspection of the information in the file
   resulted in a clear resolution of the license that should apply (and
   which scanner probably needed to revisit its heuristics)";0;1
" - When it was not immediately clear, the license identifier was
   confirmed with lawyers working with the Linux Foundation";0;0
" - If there was any question as to the appropriate license identifier,
   the file was flagged for further research and to be revisited later
   in time";1;0
"In total, over 70 hours of logged manual review was done on the
spreadsheet to determine the SPDX license identifiers to apply to the
source files by Kate, Philippe, Thomas and, in some cases, confirmation
by lawyers working with the Linux Foundation";0;0
"Kate also obtained a third independent scan of the 4.13 code base from
FOSSology, and compared selected files where the other two scanners
disagreed against that SPDX file, to see if there was new insights";1;1
" The
Windriver scanner is based on an older version of FOSSology in part, so
they are related";0;0
"Thomas did random spot checks in about 500 files from the spreadsheets
for the uapi headers and agreed with SPDX license identifier in the
files he inspected";0;0
"For the non-uapi files Thomas did random spot checks
in about 15000 files";0;1
"In initial set of patches against 4.14-rc6, 3 files were found to have
copy/paste license identifier errors, and have been fixed to reflect the
correct identifier";0;1
"Additionally Philippe spent 10 hours this week doing a detailed manual
inspection and review of the 12,461 patched files from the initial patch
version early this week with";0;0
" - a full scancode scan run, collecting the matched texts, detected
   license ids and scores
 - reviewing anything where there was a license detected (about 500+
   files) to ensure that the applied SPDX license was correct
 - reviewing anything where there was no detection but the patch license
   was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
   SPDX license was correct
This produced a worksheet with 20 files needing minor correction";1;0
" This
worksheet was then exported into 3 different .csv files for the
different types of files to be modified";0;0
These .csv files were then reviewed by Greg;0;0
" Thomas wrote a script to
parse the csv files and add the proper SPDX tag to the file, in the
format that the file expected";1;1
" This script was further refined by Greg
based on the output to detect more types of files automatically and to
distinguish between header and source .c files (which need different
comment types.)  Finally Greg ran the script using the .csv files to
generate the patches.";1;1
mm/migrate: fix indexing bug (off by one) and avoid out of bound access;1;1
"Index was incremented before last use and thus the second array could
dereference to an invalid address (not mentioning the fact that it did
not properly clear the entry we intended to clear).";0;1
mm/hmm: avoid bloating arch that do not make use of HMM;1;1
"This moves all new code including new page migration helper behind kernel
Kconfig option so that there is no codee bloat for arch or user that do
not want to use HMM or any of its associated features";1;1
arm allyesconfig (without all the patchset, then with and this patch);0;0
"   text	   data	    bss	    dec	    hex	filename
83721896	46511131	27582964	157815991	96814b7	../without/vmlinux
83722364	46511131	27582964	157816459	968168b	vmlinux";0;0
mm/device-public-memory: device memory cache coherent with CPU;0;0
"Platform with advance system bus (like CAPI or CCIX) allow device memory
to be accessible from CPU in a cache coherent fashion";1;1
" Add a new type of
ZONE_DEVICE to represent such memory";1;1
" The use case are the same as for
the un-addressable device memory but without all the corners cases.";1;0
mm/migrate: allow migrate_vma() to alloc new page on empty entry;1;1
"This allows callers of migrate_vma() to allocate new page for empty CPU
page table entry (pte_none or back by zero page)";0;1
" This is only for
anonymous memory and it won't allow new page to be instanced if the
userfaultfd is armed";1;1
"This is useful to device driver that want to migrate a range of virtual
address and would rather allocate new memory than having to fault later";0;1
mm/migrate: support un-addressable ZONE_DEVICE page in migration;1;0
"Allow to unmap and restore special swap entry of un-addressable
ZONE_DEVICE memory.";1;0
mm/migrate: migrate_vma() unmap page from vma while collecting pages;1;1
"Common case for migration of virtual address range is page are map only
once inside the vma in which migration is taking place";1;0
" Because we
already walk the CPU page table for that range we can directly do the
unmap there and setup special migration swap entry.";0;1
mm/migrate: new memory migration helper for use with device memory;1;0
"This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory (which
can be allocated through special allocator)";1;1
" It differs from numa
migration by working on a range of virtual address and thus by doing
migration in chunk that can be large enough to use DMA engine or special
copy offloading engine";0;1
"Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...)";1;0
" As an
example IBM platform with CAPI bus can make use of this feature to migrate
between regular memory and CAPI device memory";0;1
" New CPU architecture with
a pool of high performance memory not manage as cache but presented as
regular memory (while being faster and with lower latency than DDR) will
also be prime user of this patch";1;1
"Migration to private device memory will be useful for device that have
large pool of such like GPU, NVidia plans to use HMM for that.";0;1
mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY;1;1
"Introduce a new migration mode that allow to offload the copy to a device
DMA engine";1;1
" This changes the workflow of migration and not all
address_space migratepage callback can support this";0;1
"This is intended to be use by migrate_vma() which itself is use for thing
like HMM (see include/linux/hmm.h)";1;0
No additional per-filesystem migratepage testing is needed;0;1
" I disables
MIGRATE_SYNC_NO_COPY in all problematic migratepage() callback and i
added comment in those to explain why (part of this patch)";1;1
" The commit
message is unclear it should say that any callback that wish to support
this new mode need to be aware of the difference in the migration flow
from other mode";1;1
"Some of these callbacks do extra locking while copying (aio, zsmalloc,
balloon, ...) and for DMA to be effective you want to copy multiple
pages in one DMA operations";0;1
" But in the problematic case you can not
easily hold the extra lock accross multiple call to this callback";0;1
Usual flow is;0;0
"For each page {
 1 - lock page
 2 - call migratepage() callback
 3 - (extra locking in some migratepage() callback)
 4 - migrate page state (freeze refcount, update page cache, buffer
     head, ...)
 5 - copy page
 6 - (unlock any extra lock of migratepage() callback)
 7 - return from migratepage() callback
 8 - unlock page
The new mode MIGRATE_SYNC_NO_COPY";1;1
" 1 - lock multiple pages
For each page {
 2 - call migratepage() callback
 3 - abort in all problematic migratepage() callback
 4 - migrate page state (freeze refcount, update page cache, buffer
     head, ...)
} // finished all calls to migratepage() callback
 5 - DMA copy multiple pages
 6 - unlock all the pages
To support MIGRATE_SYNC_NO_COPY in the problematic case we would need a
new callback migratepages() (for instance) that deals with multiple
pages in one transaction";1;1
"Because the problematic cases are not important for current usage I did
not wanted to complexify this patchset even more for no good reason.";0;1
mm: migrate: move_pages() supports thp migration;1;0
This patch enables thp migration for move_pages(2).;1;0
mm: thp: enable thp migration in generic path;1;0
"Add thp migration's core code, including conversions between a PMD entry
and a swap entry, setting PMD migration entry, removing PMD migration
entry, and waiting on PMD migration entries";1;0
This patch makes it possible to support thp migration;1;1
" If you fail to
allocate a destination page as a thp, you just split the source thp as
we do now, and then enter the normal page migration";0;1
" If you succeed to
allocate destination thp, you enter thp migration";0;1
" Subsequent patches
actually enable thp migration for each caller of page migration by
allowing its get_new_page() callback to allocate thps.";1;1
Sanitize 'move_pages()' permission checks;0;0
"The 'move_paghes()' system call was introduced long long ago with the
same permission checks as for sending a signal (except using
CAP_SYS_NICE instead of CAP_SYS_KILL for the overriding capability)";1;1
"That turns out to not be a great choice - while the system call really
only moves physical page allocations around (and you need other
capabilities to do a lot of it), you can check the return value to map
out some the virtual address choices and defeat ASLR of a binary that
still shares your uid";1;1
"So change the access checks to the more common 'ptrace_may_access()'
model instead";0;0
"This tightens the access checks for the uid, and also effectively
changes the CAP_SYS_NICE check to CAP_SYS_PTRACE, but it's unlikely that
anybody really _uses_ this legacy system call any more (we hav ebetter
NUMA placement models these days), so I expect nobody to notice";1;1
Famous last words.;0;0
"Revert ""mm: numa: defer TLB flush for THP migration as long as possible""";0;0
"While deferring TLB flushes is a good practice, the reverted patch
caused pending TLB flushes to be checked while the page-table lock is
not taken";1;0
" As a result, in architectures with weak memory model (PPC),
Linux may miss a memory-barrier, miss the fact TLB flushes are pending,
and cause (in theory) a memory corruption";0;1
"Since the alternative of using smp_mb__after_unlock_lock() was
considered a bit open-coded, and the performance impact is expected to
be small, the previous patch is reverted";0;1
"This reverts b0943d61b8fa (""mm: numa: defer TLB flush for THP migration
as long as possible"").";0;0
mm/migrate.c: stabilise page count when migrating transparent hugepages;1;1
"When migrating a transparent hugepage, migrate_misplaced_transhuge_page
guards itself against a concurrent fastgup of the page by checking that
the page count is equal to 2 before and after installing the new pmd";0;1
"If the page count changes, then the pmd is reverted back to the original
entry, however there is a small window where the new (possibly writable)
pmd is installed and the underlying page could be written by userspace";0;1
Restoring the old pmd could therefore result in loss of data;0;0
"This patch fixes the problem by freezing the page count whilst updating
the page tables, which protects against a concurrent fastgup without the
need to restore the old pmd in the failure case (since the page count
can no longer change under our feet).";1;1
mm: hugetlb: soft-offline: dissolve source hugepage after successful migration;0;0
Currently hugepage migrated by soft-offline (i.e;0;0
" due to correctable
memory errors) is contained as a hugepage, which means many non-error
pages in it are unreusable, i.e";0;0
 wasted;0;0
This patch solves this issue by dissolving source hugepages into buddy;1;1
"As done in previous patch, PageHWPoison is set only on a head page of
the error hugepage";1;1
" Then in dissoliving we move the PageHWPoison flag
to the raw error page so that all healthy subpages return back to buddy.";0;1
mm/hugetlb/migration: use set_huge_pte_at instead of set_pte_at;1;0
"Patch series ""HugeTLB migration support for PPC64"", v2";0;1
This patch (of 9);1;0
The right interface to use to set a hugetlb pte entry is set_huge_pte_at;0;0
Use that instead of set_pte_at.;1;0
mm: make rmap_one boolean function;1;0
"rmap_one's return value controls whether rmap_work should contine to
scan other ptes or not so it's target for changing to boolean";1;0
" Return
true if the scan should be continued";1;1
" Otherwise, return false to stop
the scanning";0;1
This patch makes rmap_one's return value to boolean.;1;1
mm: don't assume anonymous pages have SwapBacked flag;1;1
"There are a few places the code assumes anonymous pages should have
SwapBacked flag set";0;0
" MADV_FREE pages are anonymous pages but we are
going to add them to LRU_INACTIVE_FILE list and clear SwapBacked flag
for them";1;1
 The assumption doesn't hold any more, so fix them.;1;1
mm: remove unnecessary reclaimability check from NUMA balancing target;1;1
"NUMA balancing already checks the watermarks of the target node to
decide whether it's a suitable balancing target";0;0
" Whether the node is
reclaimable or not is irrelevant when we don't intend to reclaim.";0;0
mm: prevent NR_ISOLATE_* stats from going negative;1;1
"Commit 6afcf8ef0ca0 (""mm, compaction: fix NR_ISOLATED_* stats for pfn
based migration"") moved the dec_node_page_state() call (along with the
page_is_file_cache() call) to after putback_lru_page()";0;1
"But page_is_file_cache() can change after putback_lru_page() is called,
so it should be called before putback_lru_page(), as it was before that
patch, to prevent NR_ISOLATE_* stats from going negative";1;1
"Without this fix, non-CONFIG_SMP kernels end up hanging in the
shrink_active_list() due to the negative stats";1;1
 Mem-Info;1;1
"  active_anon:32567 inactive_anon:121 isolated_anon:1
  active_file:6066 inactive_file:6639 isolated_file:4294967295
  unevictable:0 dirty:115 writeback:0 unstable:0
  slab_reclaimable:2086 slab_unreclaimable:3167
  mapped:3398 shmem:18366 pagetables:1145 bounce:0
  free:1798 free_pcp:13 free_cma:0
Fixes: 6afcf8ef0ca0 (""mm, compaction: fix NR_ISOLATED_* stats for pfn based migration"")";1;1
mm: migrate: fix remove_migration_pte() for ksm pages;1;1
"I found that calling page migration for ksm pages causes the following
bug";1;1
"    page:ffffea0004d51180 count:2 mapcount:2 mapping:ffff88013c785141 index:0x913
    flags: 0x57ffffc0040068(uptodate|lru|active|swapbacked)
    raw: 0057ffffc0040068 ffff88013c785141 0000000000000913 0000000200000001
    raw: ffffea0004d5f9e0 ffffea0004d53f60 0000000000000000 ffff88007d81b800
    page dumped because: VM_BUG_ON_PAGE(!PageLocked(page))
    page->mem_cgroup:ffff88007d81b800
    ------------[ cut here ]------------
    kernel BUG at /src/linux-dev/mm/rmap.c:1086!
    invalid opcode: 0000 [#1] SMP
    Modules linked in: ppdev parport_pc virtio_balloon i2c_piix4 pcspkr parport i2c_core acpi_cpufreq ip_tables xfs libcrc32c ata_generic pata_acpi ata_piix 8139too libata virtio_blk 8139cp crc32c_intel mii virtio_pci virtio_ring serio_raw virtio floppy dm_mirror dm_region_hash dm_log dm_mod
    CPU: 0 PID: 3162 Comm: bash Not tainted 4.11.0-rc2-mm1+ #1
    Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011
    RIP: 0010:do_page_add_anon_rmap+0x1ba/0x260
    RSP: 0018:ffffc90002473b30 EFLAGS: 00010282
    RAX: 0000000000000021 RBX: ffffea0004d51180 RCX: 0000000000000006
    RDX: 0000000000000000 RSI: 0000000000000082 RDI: ffff88007dc0dfe0
    RBP: ffffc90002473b58 R08: 00000000fffffffe R09: 00000000000001c1
    R10: 0000000000000005 R11: 00000000000001c0 R12: ffff880139ab3d80
    R13: 0000000000000000 R14: 0000700000000200 R15: 0000160000000000
    FS:  00007f5195f50740(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fd450287000 CR3: 000000007a08e000 CR4: 00000000001406f0
    The problem is in the following lines";0;1
"    new = page - pvmw.page->index +
The 'new' is calculated with 'page' which is given by the caller as a
destination page and some offset adjustment for thp";1;1
" But this doesn't
properly work for ksm pages because pvmw.page->index doesn't change for
each address but linear_page_index() changes, which means that 'new'
points to different pages for each addresses backed by the ksm page";0;1
" As
a result, we try to set totally unrelated pages as destination pages,
and that causes kernel crash";0;1
"This patch fixes the miscalculation and makes ksm page migration work
fine";1;1
"Fixes: 3fe87967c536 (""mm: convert remove_migration_pte() to use page_vma_mapped_walk()"")";1;1
sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>;1;1
"We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
will have to be picked up from other headers and a couple of .c files";0;1
"Create a trivial placeholder <linux/sched/mm.h> file that just
maps to <linux/sched.h> to make this patch obviously correct and
bisectable";1;1
The APIs that are going to be moved first are;1;0
"   mm_alloc()
   __mmdrop()
   mmdrop()
   mmdrop_async_fn()
   mmdrop_async()
   mmget_not_zero()
   mmput()
   mmput_async()
   get_task_mm()
   mm_access()
   mm_release()
Include the new header in the files that are going to need it.";1;1
mm: convert remove_migration_pte() to use page_vma_mapped_walk();1;0
"remove_migration_pte() also can easily be converted to
page_vma_mapped_walk().";1;1
mm/migration: make isolate_movable_page() return int type;1;1
"Patch series ""HWPOISON: soft offlining for non-lru movable page"", v6";1;1
"After Minchan's commit bda807d44454 (""mm: migrate: support non-lru
movable page migration""), some type of non-lru page like zsmalloc and
virtio-balloon page also support migration";1;1
Therefore, we can;1;1
"1) soft offlining no-lru movable pages, which means when memory
   corrected errors occur on a non-lru movable page, we can stop to use
   it by migrating data onto another page and disable the original
   (maybe half-broken) one";1;1
2) enable memory hotplug for non-lru movable pages, i.e;1;0
"we may offline
   blocks, which include such pages, by using non-lru page migration";1;0
This patchset is heavily dependent on non-lru movable page migration;1;1
This patch (of 4);0;0
Change the return type of isolate_movable_page() from bool to int;1;0
" It
will return 0 when isolate movable page successfully, and return -EBUSY
when it isolates failed";1;0
"There is no functional change within this patch but prepare for later
patch.";1;1
mm: Use owner_priv bit for PageSwapCache, valid when PageSwapBacked;1;1
"A page is not added to the swap cache without being swap backed,
so PageSwapBacked mappings can use PG_owner_priv_1 for PageSwapCache.";0;1
lib: radix-tree: check accounting of existing slot replacement users;1;0
"The bug in khugepaged fixed earlier in this series shows that radix tree
slot replacement is fragile; and it will become more so when not only
NULL<->!NULL transitions need to be caught but transitions from and to
exceptional entries as well";0;1
 We need checks;0;0
"Re-implement radix_tree_replace_slot() on top of the sanity-checked
__radix_tree_replace()";1;0
" This requires existing callers to also pass the
radix tree root, but it'll warn us when somebody replaces slots with
contents that need proper accounting (transitions between NULL entries,
real entries, exceptional entries) and where a replacement through the
slot pointer would corrupt the radix tree node counts.";1;0
mm, compaction: fix NR_ISOLATED_* stats for pfn based migration;1;1
"Since commit bda807d44454 (""mm: migrate: support non-lru movable page
migration"") isolate_migratepages_block) can isolate !PageLRU pages which
would acct_isolated account as NR_ISOLATED_*";0;1
" Accounting these non-lru
pages NR_ISOLATED_{ANON,FILE} doesn't make any sense and it can misguide
heuristics based on those counters such as pgdat_reclaimable_pages resp";0;1
"too_many_isolated which would lead to unexpected stalls during the
direct reclaim without any good reason";0;1
" Note that
__alloc_contig_migrate_range can isolate a lot of pages at once";0;0
"On mobile devices such as 512M ram android Phone, it may use a big zram
swap";0;0
" In some cases zram(zsmalloc) uses too many non-lru but
migratedable pages, such as";0;1
"      MemTotal: 468148 kB
      Normal free:5620kB
      Free swap:4736kB
      Total swap:409596kB
      ZRAM: 164616kB(zsmalloc non-lru pages)
      active_anon:60700kB
      inactive_anon:60744kB
      active_file:34420kB
      inactive_file:37532kB
Fix this by only accounting lru pages to NR_ISOLATED_* in
isolate_migratepages_block right after they were isolated and we still
know they were on LRU";1;1
" Drop acct_isolated because it is called after
the fact and we've lost that information";0;0
" Batching per-cpu counter
doesn't make much improvement anyway";1;1
" Also make sure that we uncharge
only LRU pages when putting them back on the LRU in
putback_movable_pages resp";1;1
 when unmap_and_move migrates the page.;0;0
mm: vm_page_prot: update with WRITE_ONCE/READ_ONCE;1;0
"vma->vm_page_prot is read lockless from the rmap_walk, it may be updated
concurrently and this prevents the risk of reading intermediate values.";1;1
mm, thp: remove __GFP_NORETRY from khugepaged and madvised allocations;1;0
"After the previous patch, we can distinguish costly allocations that
should be really lightweight, such as THP page faults, with
__GFP_NORETRY";1;1
" This means we don't need to recognize khugepaged
allocations via PF_KTHREAD anymore";1;1
" We can also change THP page faults
in areas where madvise(MADV_HUGEPAGE) was used to try as hard as
khugepaged, as the process has indicated that it benefits from THP's and
is willing to pay some initial latency costs";1;1
"We can also make the flags handling less cryptic by distinguishing
GFP_TRANSHUGE_LIGHT (no reclaim at all, default mode in page fault) from
GFP_TRANSHUGE (only direct reclaim, khugepaged default)";1;1
" Adding
__GFP_NORETRY or __GFP_KSWAPD_RECLAIM is done where needed";1;0
"The patch effectively changes the current GFP_TRANSHUGE users as
follows";1;0
"  long and it's shared by multiple users, so it's worth spending some
  effort on it";0;0
 We use GFP_TRANSHUGE, and __GFP_NORETRY is not added;1;0
"  This also restores direct reclaim to this allocation, which was
  unintentionally removed by commit e4a49efe4e7e (""mm: thp: set THP defrag
  by default to madvise and add a stall-free defrag option"")
  is not an issue";1;0
" So if khugepaged ""defrag"" is enabled (the default), do
  reclaim via GFP_TRANSHUGE without __GFP_NORETRY";1;1
" We can remove the
  PF_KTHREAD check from page alloc";1;1
"  As a side-effect, khugepaged will now no longer check if the initial
  compaction was deferred or contended";1;1
" This is OK, as khugepaged sleep
  times between collapsion attempts are long enough to prevent noticeable
  disruption, so we should allow it to spend some effort";1;1
"  __GFP_RECLAIM, so just convert to GFP_TRANSHUGE_LIGHT which is
  equivalent";1;0
  are now allocating without __GFP_NORETRY;1;0
" Other vma's keep using
  __GFP_NORETRY if direct reclaim/compaction is at all allowed (by default
  it's allowed only for madvised vma's)";0;1
" The rest is conversion to
  GFP_TRANSHUGE(_LIGHT).";1;0
mm: remove reclaim and compaction retry approximations;1;0
"If per-zone LRU accounting is available then there is no point
approximating whether reclaim and compaction should retry based on pgdat
statistics";1;1
" This is effectively a revert of ""mm, vmstat: remove zone
and node double accounting by approximating retries"" with the difference
that inactive/active stats are still available";1;0
" This preserves the
history of why the approximation was retried and why it had to be
reverted to handle OOM kills on 32-bit systems.";0;1
mm, vmstat: remove zone and node double accounting by approximating retries;1;0
"The number of LRU pages, dirty pages and writeback pages must be
accounted for on both zones and nodes because of the reclaim retry
logic, compaction retry logic and highmem calculations all depending on
per-zone stats";0;1
"Many lowmem allocations are immune from OOM kill due to a check in
__alloc_pages_may_oom for (ac->high_zoneidx < ZONE_NORMAL) since commit
03668b3ceb0c (""oom: avoid oom killer for lowmem allocations"")";0;0
" The
exception is costly high-order allocations or allocations that cannot
fail";1;1
" If the __alloc_pages_may_oom avoids OOM-kill for low-order lowmem
allocations then it would fall through to __alloc_pages_direct_compact";0;1
"This patch will blindly retry reclaim for zone-constrained allocations
in should_reclaim_retry up to MAX_RECLAIM_RETRIES";1;1
" This is not ideal
but without per-zone stats there are not many alternatives";0;0
" The impact
it that zone-constrained allocations may delay before considering the
OOM killer";0;1
"As there is no guarantee enough memory can ever be freed to satisfy
compaction, this patch avoids retrying compaction for zone-contrained
allocations";0;1
"In combination, that means that the per-node stats can be used when
deciding whether to continue reclaim using a rough approximation";0;1
" While
it is possible this will make the wrong decision on occasion, it will
not infinite loop as the number of reclaim attempts is capped by
MAX_RECLAIM_RETRIES";0;1
The final step is calculating the number of dirtyable highmem pages;0;0
" As
those calculations only care about the global count of file pages in
highmem";1;0
" This patch uses a global counter used instead of per-zone
stats as it is sufficient";1;1
"In combination, this allows the per-zone LRU and dirty state counters to
be removed.";1;1
mm: move most file-based accounting to the node;0;0
"There are now a number of accounting oddities such as mapped file pages
being accounted for on the node while the total number of file pages are
accounted on the zone";0;1
" This can be coped with to some extent but it's
confusing so this patch moves the relevant file-based accounted";0;1
" Due to
throttling logic in the page allocator for reliable OOM detection, it is
still necessary to track dirty and writeback pages on a per-zone basis.";0;1
mm: rename NR_ANON_PAGES to NR_ANON_MAPPED;1;0
NR_FILE_PAGES  is the number of        file pages;1;1
NR_FILE_MAPPED is the number of mapped file pages;1;1
NR_ANON_PAGES  is the number of mapped anon pages;0;0
"This is unhelpful naming as it's easy to confuse NR_FILE_MAPPED and
NR_ANON_PAGES for mapped pages";1;1
" This patch renames NR_ANON_PAGES so we
have
NR_FILE_PAGES  is the number of        file pages";1;1
NR_FILE_MAPPED is the number of mapped file pages;1;1
NR_ANON_MAPPED is the number of mapped anon pages.;0;0
mm, vmscan: move LRU lists to node;1;1
"This moves the LRU lists from the zone to the node and related data such
as counters, tracing, congestion tracking and writeback tracking";1;1
"Unfortunately, due to reclaim and compaction retry logic, it is
necessary to account for the number of LRU pages on both zone and node
logic";0;1
" Most reclaim logic is based on the node counters but the retry
logic uses the zone counters which do not distinguish inactive and
active sizes";1;1
" It would be possible to leave the LRU counters on a
per-zone basis but it's a heavier calculation across multiple cache
lines that is much more frequent than the retry checks";0;1
"Other than the LRU counters, this is mostly a mechanical patch but note
that it introduces a number of anomalies";1;1
" For example, the scans are
per-zone but using per-node counters";0;0
" We also mark a node as congested
when a zone is congested";1;0
" This causes weird problems that are fixed
later but is easier to review";0;1
"In the event that there is excessive overhead on 32-bit systems due to
the nodes being on LRU then there are two potential solutions
1";0;0
"Long-term isolation of highmem pages when reclaim is lowmem
   When pages are skipped, they are immediately added back onto the LRU
   list";0;0
"If lowmem reclaim persisted for long periods of time, the same
   highmem pages get continually scanned";0;1
"The idea would be that lowmem
   keeps those pages on a separate list until a reclaim for highmem pages
   arrives that splices the highmem pages back onto the LRU";0;1
"It potentially
   could be implemented similar to the UNEVICTABLE list";0;1
"   That would reduce the skip rate with the potential corner case is that
   highmem pages have to be scanned and reclaimed to free lowmem slab pages";1;0
2;1;0
"Linear scan lowmem pages if the initial LRU shrink fails
   This will break LRU ordering but may be preferable and faster during
   memory pressure than skipping LRU pages.";1;0
mm: introduce do_set_pmd();1;0
With postponed page table allocation we have chance to setup huge pages;0;1
do_set_pte() calls do_set_pmd() if following criteria met:;1;0
rmap: support file thp;1;1
"Naive approach: on mapping/unmapping the page as compound we update
->_mapcount on each 4k page";1;0
" That's not efficient, but it's not obvious
how we can optimize this";0;1
 We can look into optimization later;1;1
"PG_double_map optimization doesn't work for file pages since lifecycle
of file pages is different comparing to anon pages: file page can be
mapped again at any time.";0;1
mm: balloon: use general non-lru movable page feature;1;1
"Now, VM has a feature to migrate non-lru movable pages so balloon
doesn't need custom migration hooks in migrate.c and compaction.c";0;1
"Instead, this patch implements the page->mapping->a_ops->
{isolate|migrate|putback} functions";1;1
"With that, we could remove hooks for ballooning in general migration
functions and make balloon compaction simple.";0;0
mm: migrate: support non-lru movable page migration;1;1
"We have allowed migration for only LRU pages until now and it was enough
to make high-order pages";1;0
" But recently, embedded system(e.g., webOS,
android) uses lots of non-movable pages(e.g., zram, GPU memory) so we
have seen several reports about troubles of small high-order allocation";0;1
For fixing the problem, there were several efforts (e,g,;0;0
" enhance
compaction algorithm, SLUB fallback to 0-order page, reserved memory,
vmalloc and so on) but if there are lots of non-movable pages in system,
their solutions are void in the long run";0;0
"So, this patch is to support facility to change non-movable pages with
movable";1;1
" For the feature, this patch introduces functions related to
migration to address_space_operations as well as some page flags";0;1
"If a driver want to make own pages movable, it should define three
functions which are function pointers of struct
address_space_operations";1;1
"What VM expects on isolate_page function of driver is to return *true*
if driver isolates page successfully";0;1
" On returing true, VM marks the
page as PG_isolated so concurrent isolation in several CPUs skip the
page for isolation";0;0
" If a driver cannot isolate the page, it should
return *false*";1;1
"Once page is successfully isolated, VM uses page.lru fields so driver
shouldn't expect to preserve values in that fields";1;1
2;1;0
"int (*migratepage) (struct address_space *mapping,
After isolation, VM calls migratepage of driver with isolated page";0;0
" The
function of migratepage is to move content of the old page to new page
and set up fields of struct page newpage";1;1
" Keep in mind that you should
indicate to the VM the oldpage is no longer movable via
__ClearPageMovable() under page_lock if you migrated the oldpage
successfully and returns 0";0;1
" If driver cannot migrate the page at the
moment, driver can return -EAGAIN";0;1
" On -EAGAIN, VM will retry page
migration in a short time because VM interprets -EAGAIN as ""temporal
migration failure""";1;1
" On returning any error except -EAGAIN, VM will give
up the page migration without retrying in this time";1;1
Driver shouldn't touch page.lru field VM using in the functions;0;0
"If migration fails on isolated page, VM should return the isolated page
to the driver so VM calls driver's putback_page with migration failed
page";0;0
" In this function, driver should put the isolated page back to the
own data structure";1;1
4;0;0
"non-lru movable page flags
There are two page flags for supporting non-lru movable page";1;1
"Driver should use the below function to make page movable under
page_lock";1;1
"	void __SetPageMovable(struct page *page, struct address_space *mapping)
It needs argument of address_space for registering migration family
functions which will be called by VM";0;0
" Exactly speaking, PG_movable is
not a real flag of struct page";0;1
" Rather than, VM reuses page->mapping's
lower bits to represent it";1;0
so driver shouldn't access page->mapping directly;1;1
" Instead, driver
should use page_mapping which mask off the low two bits of page->mapping
so it can get right struct address_space";1;1
For testing of non-lru movable page, VM supports __PageMovable function;1;0
"However, it doesn't guarantee to identify non-lru movable page because
page->mapping field is unified with other variables in struct page";0;1
" As
well, if driver releases the page after isolation by VM, page->mapping
doesn't have stable value although it has PAGE_MAPPING_MOVABLE (Look at
__ClearPageMovable)";1;1
" But __PageMovable is cheap to catch whether page
is LRU or non-lru movable once the page has been isolated";1;1
" Because LRU
pages never can have PAGE_MAPPING_MOVABLE in page->mapping";0;1
" It is also
good for just peeking to test non-lru movable pages before more
expensive checking with lock_page in pfn scanning to select victim";0;1
For guaranteeing non-lru movable page, VM provides PageMovable function;1;0
"Unlike __PageMovable, PageMovable functions validates page->mapping and
mapping->a_ops->isolate_page under lock_page";0;0
" The lock_page prevents
sudden destroying of page->mapping";0;1
"Driver using __SetPageMovable should clear the flag via
__ClearMovablePage under page_lock before the releasing the page";0;1
"To prevent concurrent isolation among several CPUs, VM marks isolated
page as PG_isolated under lock_page";0;0
" So if a CPU encounters PG_isolated
non-lru movable page, it can skip it";1;1
" Driver doesn't need to manipulate
the flag because VM will set/clear it automatically";0;1
" Keep in mind that
if driver sees PG_isolated page, it means the page have been isolated by
VM so it shouldn't touch page.lru field";0;1
" PG_isolated is alias with
PG_reclaim flag so driver shouldn't use the flag for own purpose.";0;1
mm: use put_page() to free page instead of putback_lru_page();1;0
"Recently, I got many reports about perfermance degradation in embedded
system(Android mobile phone, webOS TV and so on) and easy fork fail";1;1
The problem was fragmentation caused by zram and GPU driver mainly;0;1
"With memory pressure, their pages were spread out all of pageblock and
it cannot be migrated with current compaction algorithm which supports
only LRU pages";0;1
" In the end, compaction cannot work well so reclaimer
shrinks all of working set pages";0;1
" It made system very slow and even to
fail to fork easily which requires order-[2 or 3] allocations";0;0
"Other pain point is that they cannot use CMA memory space so when OOM
kill happens, I can see many free pages in CMA area, which is not memory
efficient";0;1
" In our product which has big CMA memory, it reclaims zones
too exccessively to allocate GPU and zram page although there are lots
of free space in CMA so system becomes very slow easily";1;0
"To solve these problem, this patch tries to add facility to migrate
non-lru pages via introducing new functions and page flags to help
migration";0;1
"struct address_space_operations {
new page flags
	PG_movable
	PG_isolated
For details, please read description in ""mm: migrate: support non-lru
movable page migration""";0;0
"Originally, Gioh Kim had tried to support this feature but he moved so I
took over the work";1;0
" I took many code from his work and changed a little
bit and Konstantin Khlebnikov helped Gioh a lot so he should deserve to
have many credit, too";0;1
"And I should mention Chulmin who have tested this patchset heavily so I
can find many bugs from him";1;1
" :)
Thanks, Gioh, Konstantin and Chulmin!
This patchset consists of five parts";1;1
1;0;0
"clean up migration
  mm: use put_page to free page instead of putback_lru_page
2";1;1
"add non-lru page migration feature
  mm: migrate: support non-lru movable page migration
3";1;1
"rework KVM memory-ballooning
  mm: balloon: use general non-lru movable page feature
4";1;0
"zsmalloc refactoring for preparing page migration
  zsmalloc: keep max_object in size_class
  zsmalloc: use bit_spin_lock
  zsmalloc: use accessor
  zsmalloc: factor page chain functionality out
  zsmalloc: introduce zspage structure
  zsmalloc: separate free_zspage from putback_zspage
  zsmalloc: use freeobj for index
5";1;0
"zsmalloc page migration
  zsmalloc: page migration support
  zram: use __GFP_MOVABLE for memory allocation
This patch (of 12)";0;0
Procedure of page migration is as follows;0;0
"First of all, it should isolate a page from LRU and try to migrate the
page";1;1
 If it is successful, it releases the page for freeing;1;0
Otherwise, it should put the page back to LRU list;1;1
"For LRU pages, we have used putback_lru_page for both freeing and
putback to LRU list";1;1
" It's okay because put_page is aware of LRU list so
if it releases last refcount of the page, it removes the page from LRU
list";1;0
" However, It makes unnecessary operations (e.g., lru_cache_add,
pagevec and flags operations";1;1
" It would be not significant but no worth
to do) and harder to support new non-lru page migration because put_page
isn't aware of non-lru page's data structure";1;1
"To solve the problem, we can add new hook in put_page with PageMovable
flags check but it can increase overhead in hot path and needs new
locking scheme to stabilize the flag check with put_page";1;1
So, this patch cleans it up to divide two semantic(ie, put and putback);1;1
"If migration is successful, use put_page instead of putback_lru_page and
use putback_lru_page only on failure";1;1
" That makes code more readable and
doesn't add overhead in put_page";1;1
"Comment from Vlastimil
 ""Yeah, and compaction (perhaps also other migration users) has to drain
  the lru pvec..";0;1
" Getting rid of this stuff is worth even by itself.""";0;0
mm: Export migrate_page_move_mapping and migrate_page_copy;1;0
"Export these symbols such that UBIFS can implement
->migratepage.";0;1
mm, migrate: increment fail count on ENOMEM;1;1
"If page migration fails due to -ENOMEM, nr_failed should still be
incremented for proper statistics";0;1
"This was encountered recently when all page migration vmstats showed 0,
and inferred that migrate_pages() was never called, although in reality
the first page migration failed because compaction_alloc() failed to
find a migration target";0;1
"This patch increments nr_failed so the vmstat is properly accounted on
ENOMEM.";1;1
mm: use __SetPageSwapBacked and dont ClearPageSwapBacked;1;1
"v3.16 commit 07a427884348 (""mm: shmem: avoid atomic operation during
shmem_getpage_gfp"") rightly replaced one instance of SetPageSwapBacked
by __SetPageSwapBacked, pointing out that the newly allocated page is
not yet visible to other users (except speculative get_page_unless_zero-
ers, who may not update page flags before their further checks)";0;0
That was part of a series in which Mel was focused on tmpfs profiles;0;0
"but almost all SetPageSwapBacked uses can be so optimized, with the same
justification";0;1
Remove ClearPageSwapBacked from __read_swap_cache_async() error path;1;0
it's not an error to free a page with PG_swapbacked set;0;0
"Follow a convention of __SetPageLocked, __SetPageSwapBacked instead of
doing it differently in different places; but that's for tidiness - if
the ordering actually mattered, we should not be using the __variants";1;1
"There's probably scope for further __SetPageFlags in other places, but
SwapBacked is the one I'm interested in at the moment.";0;0
mm/hwpoison: fix wrong num_poisoned_pages accounting;1;1
"Currently, migration code increses num_poisoned_pages on *failed*
migration page as well as successfully migrated one at the trial of
memory-failure";1;1
 It will make the stat wrong;0;1
" As well, it marks the
page as PG_HWPoison even if the migration trial failed";1;1
" It would mean
we cannot recover the corrupted page using memory-failure facility";0;1
This patches fixes it.;1;1
mm: make remove_migration_ptes() beyond mm/migration.c;1;0
Make remove_migration_ptes() available to be used in split_huge_page();1;0
"New parameter 'locked' added: as with try_to_umap() we need a way to
indicate that caller holds rmap lock";1;1
"We also shouldn't try to mlock() pte-mapped huge pages: pte-mapeed THP
pages are never mlocked.";0;1
mm: introduce page reference manipulation functions;1;0
"The success of CMA allocation largely depends on the success of
migration and key factor of it is page reference count";0;0
" Until now, page
reference is manipulated by direct calling atomic functions so we cannot
follow up who and where manipulate it";0;1
" Then, it is hard to find actual
reason of CMA allocation failure";0;1
" CMA allocation should be guaranteed
to succeed so finding offending place is really important";0;1
"In this patch, call sites where page reference is manipulated are
converted to introduced wrapper function";0;1
" This is preparation step to
add tracepoint to each page reference manipulation function";0;0
" With this
facility, we can easily find reason of CMA allocation failure";0;1
" There is
no functional change in this patch";1;0
In addition, this patch also converts reference read sites;1;1
" It will
help a second step that renames page._count to something else and
prevents later attempt to direct access to it (Suggested by Andrew).";0;1
mm/thp/migration: switch from flush_tlb_range to flush_pmd_tlb_range;0;0
We remove one instace of flush_tlb_range here;1;0
" That was added by commit
f714f4f20e59 (""mm: numa: call MMU notifiers on THP migration"")";0;0
" But the
pmdp_huge_clear_flush_notify should have done the require flush for us";0;1
Hence remove the extra flush.;1;0
mm: migrate: consolidate mem_cgroup_migrate() calls;0;0
"Rather than scattering mem_cgroup_migrate() calls all over the place,
have a single call from a safe place where every migration operation
eventually ends up in - migrate_page_copy().";1;1
mm: migrate: do not touch page->mem_cgroup of live pages;0;0
"Changing a page's memcg association complicates dealing with the page,
so we want to limit this as much as possible";1;1
 Page migration e.g;0;0
" does
not have to do that";0;0
" Just like page cache replacement, it can forcibly
charge a replacement page, and then uncharge the old page when it gets
freed";1;1
" Temporarily overcharging the cgroup by a single page is not an
issue in practice, and charging is so cheap nowadays that this is much
preferrable to the headache of messing with live pages";0;1
"The only place that still changes the page->mem_cgroup binding of live
pages is when pages move along with a task to another cgroup";0;0
" But that
path isolates the page from the LRU, takes the page lock, and the move
lock (lock_page_memcg())";1;0
" That means page->mem_cgroup is always stable
in callers that have the page isolated from the LRU or locked";0;1
" Lighter
unlocked paths, like writeback accounting, can use lock_page_memcg().";1;0
mm, page_owner: track and print last migrate reason;1;1
"During migration, page_owner info is now copied with the rest of the
page, so the stacktrace leading to free page allocation during migration
is overwritten";1;0
" For debugging purposes, it might be however useful to
know that the page has been migrated since its initial allocation";0;1
" This
might happen many times during the lifetime for different reasons and
fully tracking this, especially with stacktraces would incur extra
memory costs";0;0
" As a compromise, store and print the migrate_reason of
the last migration that occurred to the page";1;0
" This is enough to
distinguish compaction, numa balancing etc";0;1
Example page_owner entry after the patch;0;0
"  Page allocated via order 0, mask 0x24200ca(GFP_HIGHUSER_MOVABLE)
  PFN 628753 type Movable Block 1228 type Movable Flags 0x1fffff80040030(dirty|lru|swapbacked)
   [<ffffffff811682c4>] __alloc_pages_nodemask+0x134/0x230
   [<ffffffff811b6325>] alloc_pages_vma+0xb5/0x250
   [<ffffffff81177491>] shmem_alloc_page+0x61/0x90
   [<ffffffff8117a438>] shmem_getpage_gfp+0x678/0x960
   [<ffffffff8117c2b9>] shmem_fallocate+0x329/0x440
   [<ffffffff811de600>] vfs_fallocate+0x140/0x230
   [<ffffffff811df434>] SyS_fallocate+0x44/0x70
   [<ffffffff8158cc2e>] entry_SYSCALL_64_fastpath+0x12/0x71
  Page has been migrated, last migrate reason: compaction";0;0
mm, page_owner: copy page owner info during migration;1;1
"The page_owner mechanism stores gfp_flags of an allocation and stack
trace that lead to it";0;1
" During page migration, the original information
is practically replaced by the allocation of free page as the migration
target";0;0
" Arguably this is less useful and might lead to all the
page_owner info for migratable pages gradually converge towards
compaction or numa balancing migrations";0;1
" It has also lead to
inaccuracies such as one fixed by commit e2cfc91120fa (""mm/page_owner";0;1
"set correct gfp_mask on page_owner"")";1;0
This patch thus introduces copying the page_owner info during migration;1;1
"However, since the fact that the page has been migrated from its
original place might be useful for debugging, the next patch will
introduce a way to track that information as well.";1;1
mm: numa: quickly fail allocations for NUMA balancing on full nodes;1;1
"Commit 4167e9b2cf10 (""mm: remove GFP_THISNODE"") removed the GFP_THISNODE
flag combination due to confusing semantics";1;0
" It noted that
alloc_misplaced_dst_page() was one such user after changes made by
commit e97ca8e5b864 (""mm: fix GFP_THISNODE callers and clarify"")";1;0
"Unfortunately when GFP_THISNODE was removed, users of
alloc_misplaced_dst_page() started waking kswapd and entering direct
reclaim because the wrong GFP flags are cleared";0;1
" The consequence is
that workloads that used to fit into memory now get reclaimed which is
addressed by this patch";1;0
"The problem can be demonstrated with ""mutilate"" that exercises memcached
which is software dedicated to memory object caching";0;1
" The configuration
uses 80% of memory and is run 3 times for varying numbers of clients";0;0
"The results on a 4-socket NUMA box are
mutilate
                          vanilla           numaswap-v1
Hmean    1      8394.71 (  0.00%)     8395.32 (  0.01%)
Hmean    4     30024.62 (  0.00%)    34513.54 ( 14.95%)
Hmean    7     32821.08 (  0.00%)    70542.96 (114.93%)
Hmean    12    55229.67 (  0.00%)    93866.34 ( 69.96%)
Hmean    21    39438.96 (  0.00%)    85749.21 (117.42%)
Hmean    30    37796.10 (  0.00%)    50231.49 ( 32.90%)
Hmean    47    18070.91 (  0.00%)    38530.13 (113.22%)
The metric is queries/second with the more the better";0;1
" The results are
way outside of the noise and the reason for the improvement is obvious
from some of the vmstats
                               vanillanumaswap-v1r1
Minor Faults                1929399272  2146148218
Major Faults                  19746529        3567
Swap Ins                      57307366        9913
Swap Outs                     50623229       17094
Allocation stalls                35909         443
DMA allocs                           0           0
DMA32 allocs                  72976349   170567396
Normal allocs               5306640898  5310651252
Movable allocs                       0           0
Direct pages scanned         404130893      799577
Kswapd pages scanned         160230174           0
Kswapd pages reclaimed        55928786           0
Direct pages reclaimed         1843936       41921
Page writes file                  2391           0
Page writes anon              50623229       17094
The vanilla kernel is swapping like crazy with large amounts of direct
reclaim and kswapd activity";0;0
" The figures are aggregate but it's known
that the bad activity is throughout the entire test";0;0
"Note that simple streaming anon/file memory consumers also see this
problem but it's not as obvious";1;1
" In those cases, kswapd is awake when
it should not be";0;1
"As there are at least two reclaim-related bugs out there, it's worth
spelling out the user-visible impact";0;1
" This patch only addresses bugs
related to excessive reclaim on NUMA hardware when the working set is
larger than a NUMA node";0;1
" There is a bug related to high kswapd CPU
usage but the reports are against laptops and other UMA hardware and is
not addressed by this patch.";0;1
thp: introduce deferred_split_huge_page();1;0
Currently we don't split huge page on partial unmap;0;0
" It's not an ideal
situation";0;1
 It can lead to memory overhead;0;1
Furtunately, we can detect partial unmap on page_remove_rmap();1;1
" But we
cannot call split_huge_page() from there due to locking context";0;1
"It's also counterproductive to do directly from munmap() codepath: in
many cases we will hit this from exit(2) and splitting the huge page
just to free it up in small pages is not what we really want";0;1
"The patch introduce deferred_split_huge_page() which put the huge page
into queue for splitting";1;1
" The splitting itself will happen when we get
memory pressure via shrinker interface";0;0
" The page will be dropped from
list on freeing through compound page destructor.";1;0
thp, mm: split_huge_page(): caller need to lock page;0;0
"We're going to use migration entries instead of compound_lock() to
stabilize page refcounts";1;0
" Setup and remove migration entries require
page to be locked";1;0
Some of split_huge_page() callers already have the page locked;1;1
" Let's
require everybody to lock the page before calling split_huge_page().";1;0
mm: rework mapcount accounting to enable 4k mapping of THPs;1;0
We're going to allow mapping of individual 4k pages of THP compound;1;0
" It
means we need to track mapcount on per small page basis";0;1
"Straight-forward approach is to use ->_mapcount in all subpages to track
how many time this subpage is mapped with PMDs or PTEs combined";0;1
" But
this is rather expensive: mapping or unmapping of a THP page with PMD
would require HPAGE_PMD_NR atomic operations instead of single we have
now";0;0
"The idea is to store separately how many times the page was mapped as
whole -- compound_mapcount";0;1
" This frees up ->_mapcount in subpages to
track PTE mapcount";0;1
"We use the same approach as with compound page destructor and compound
order to store compound_mapcount: use space in first tail page,
->mapping this time";1;0
"Any time we map/unmap whole compound page (THP or hugetlb) -- we
increment/decrement compound_mapcount";0;1
" When we map part of compound
page with PTE we operate on ->_mapcount of the subpage";0;0
page_mapcount() counts both: PTE and PMD mappings of the page;1;0
Basically, we have mapcount for a subpage spread over two counters;0;0
" It
makes tricky to detect when last mapcount for a page goes away";0;0
We introduced PageDoubleMap() for this;0;1
" When we split THP PMD for the
first time and there's other PMD mapping left we offset up ->_mapcount
in all subpages by one and set PG_double_map on the compound page";0;1
These additional references go away with last compound_mapcount;0;0
"This approach provides a way to detect when last mapcount goes away on
per small page basis without introducing new overhead for most common
cases.";1;0
rmap: add argument to charge compound page;1;0
"We're going to allow mapping of individual 4k pages of THP compound
page";1;0
" It means we cannot rely on PageTransHuge() check to decide if
map/unmap small page or THP";0;1
"The patch adds new argument to rmap functions to indicate whether we
want to operate on whole compound page or only the small page.";1;1
page-flags: define PG_locked behavior on compound pages;1;1
lock_page() must operate on the whole compound page;0;1
" It doesn't make
much sense to lock part of compound page";0;1
" Change code to use head
page's PG_locked, if tail page is passed";1;1
"This patch also gets rid of custom helper functions --
__set_page_locked() and __clear_page_locked()";1;1
" They are replaced with
helpers generated by __SETPAGEFLAG/__CLEARPAGEFLAG";0;0
" Tail pages to these
helper would trigger VM_BUG_ON()";0;0
SLUB uses PG_locked as a bit spin locked;1;1
" IIUC, tail pages should never
appear there";0;1
" VM_BUG_ON() is added to make sure that this assumption is
correct.";0;0
mm, page_alloc: rename __GFP_WAIT to __GFP_RECLAIM;1;0
"__GFP_WAIT was used to signal that the caller was in atomic context and
could not sleep";0;0
" Now it is possible to distinguish between true atomic
context and callers that are not willing to sleep";0;1
" The latter should
clear __GFP_DIRECT_RECLAIM so kswapd will still wake";1;0
" As clearing
__GFP_WAIT behaves differently, there is a risk that people will clear the
wrong flags";0;0
" This patch renames __GFP_WAIT to __GFP_RECLAIM to clearly
indicate what it does -- setting it allows all reclaim activity, clearing
them prevents it.";1;1
mm, page_alloc: distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd;0;1
"__GFP_WAIT has been used to identify atomic context in callers that hold
spinlocks or are in interrupts";0;0
" They are expected to be high priority and
have access one of two watermarks lower than ""min"" which can be referred
to as the ""atomic reserve""";1;0
" __GFP_HIGH users get access to the first
lower watermark and can be called the ""high priority reserve""";1;0
"Over time, callers had a requirement to not block when fallback options
were available";0;0
" Some have abused __GFP_WAIT leading to a situation where
an optimisitic allocation with a fallback option can access atomic
reserves";0;1
"This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
cannot sleep and have no alternative";0;1
" High priority users continue to use
__GFP_HIGH";1;0
" __GFP_DIRECT_RECLAIM identifies callers that can sleep and
are willing to enter direct reclaim";0;0
" __GFP_KSWAPD_RECLAIM to identify
callers that want to wake kswapd for background reclaim";0;1
" __GFP_WAIT is
redefined as a caller that is willing to enter direct reclaim and wake
kswapd for background reclaim";0;1
"This patch then converts a number of sites
o __GFP_ATOMIC is used by callers that are high priority and have memory
  pools for those requests";0;1
GFP_ATOMIC uses this flag;0;0
"o Callers that have a limited mempool to guarantee forward progress clear
  __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM";0;0
"bio allocations fall
  into this category where kswapd will still be woken but atomic reserves
  are not used as there is a one-entry mempool to guarantee progress";1;1
"o Callers that are checking if they are non-blocking should use the
  helper gfpflags_allow_blocking() where possible";1;1
"This is because
  checking for __GFP_WAIT as was done historically now can trigger false
  positives";1;1
"Some exceptions like dm-crypt.c exist where the code intent
  is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
  flag manipulations";1;0
"o Callers that built their own GFP flags instead of starting with GFP_KERNEL
  and friends now also need to specify __GFP_KSWAPD_RECLAIM";1;0
"The first key hazard to watch out for is callers that removed __GFP_WAIT
and was depending on access to atomic reserves for inconspicuous reasons";1;0
In some cases it may be appropriate for them to use __GFP_HIGH;1;1
"The second key hazard is callers that assembled their own combination of
GFP flags instead of starting with something like GFP_KERNEL";1;0
" They may
now wish to specify __GFP_KSWAPD_RECLAIM";0;0
" It's almost certainly harmless
if it's missed in most cases as other activity will wake kswapd.";0;1
mm: migrate dirty page without clear_page_dirty_for_io etc;0;1
"clear_page_dirty_for_io() has accumulated writeback and memcg subtleties
since v2.6.16 first introduced page migration; and the set_page_dirty()
which completed its migration of PageDirty, later had to be moderated to
__set_page_dirty_nobuffers(); then PageSwapBacked had to skip that too";0;0
"No actual problems seen with this procedure recently, but if you look into
what the clear_page_dirty_for_io(page)+set_page_dirty(newpage) is actually
achieving, it turns out to be nothing more than moving the PageDirty flag,
and its NR_FILE_DIRTY stat from one zone to another";1;1
"It would be good to avoid a pile of irrelevant decrementations and
incrementations, and improper event counting, and unnecessary descent of
the radix_tree under tree_lock (to set the PAGECACHE_TAG_DIRTY which
radix_tree_replace_slot() left in place anyway)";1;1
"Do the NR_FILE_DIRTY movement, like the other stats movements, while
interrupts still disabled in migrate_page_move_mapping(); and don't even
bother if the zone is the same";1;0
" Do the PageDirty movement there under
tree_lock too, where old page is frozen and newpage not yet visible";1;1
"bearing in mind that as soon as newpage becomes visible in radix_tree, an
un-page-locked set_page_dirty() might interfere (or perhaps that's just
not possible: anything doing so should already hold an additional
reference to the old page, preventing its migration; but play safe)";0;1
"But we do still need to transfer PageDirty in migrate_page_copy(), for
those who don't go the mapping route through migrate_page_move_mapping().";1;1
mm: page migration avoid touching newpage until no going back;1;0
"We have had trouble in the past from the way in which page migration's
newpage is initialized in dribs and drabs - see commit 8bdd63809160 (""mm";0;0
"fix direct reclaim writeback regression"") which proposed a cleanup";1;1
"We have no actual problem now, but I think the procedure would be clearer
(and alternative get_new_page pools safer to implement) if we assert that
newpage is not touched until we are sure that it's going to be used -
except for taking the trylock on it in __unmap_and_move()";1;1
"So shift the early initializations from move_to_new_page() into
migrate_page_move_mapping(), mapping and NULL-mapping paths";1;0
" Similarly
migrate_huge_page_move_mapping(), but its NULL-mapping path can just be
deleted: you cannot reach hugetlbfs_migrate_page() with a NULL mapping";0;1
Adjust stages 3 to 8 in the Documentation file accordingly.;0;0
mm: simplify page migration's anon_vma comment and flow;1;1
"__unmap_and_move() contains a long stale comment on page_get_anon_vma()
and PageSwapCache(), with an odd control flow that's hard to follow";0;1
"Mostly this reflects our confusion about the lifetime of an anon_vma, in
the early days of page migration, before we could take a reference to one";1;1
 Nowadays this seems quite straightforward: cut it all down to essentials;0;1
"I cannot see the relevance of swapcache here at all, so don't treat it any
differently: I believe the old comment reflects in part our anon_vma
confusions, and in part the original v2.6.16 page migration technique,
which used actual swap to migrate anon instead of swap-like migration
entries";1;1
" Why should a swapcache page not be migrated with the aid of
migration entry ptes like everything else?  So lose that comment now, and
enable migration entries for swapcache in the next patch.";1;1
mm: page migration remove_migration_ptes at lock+unlock level;1;1
"Clean up page migration a little more by calling remove_migration_ptes()
from the same level, on success or on failure, from __unmap_and_move() or
from unmap_and_move_huge_page()";1;1
"Don't reset page->mapping of a PageAnon old page in move_to_new_page(),
leave that to when the page is freed";0;1
" Except for here in page migration,
it has been an invariant that a PageAnon (bit set in page->mapping) page
stays PageAnon until it is freed, and I think we're safer to keep to that";0;0
"And with the above rearrangement, it's necessary because zap_pte_range()
wants to identify whether a migration entry represents a file or an anon
page, to update the appropriate rss stats without waiting on it.";1;1
mm: page migration trylock newpage at same level as oldpage;1;1
"Clean up page migration a little by moving the trylock of newpage from
move_to_new_page() into __unmap_and_move(), where the old page has been
locked";1;1
" Adjust unmap_and_move_huge_page() and balloon_page_migrate()
accordingly";0;0
"But make one kind-of-functional change on the way: whereas trylock of
newpage used to BUG() if it failed, now simply return -EAGAIN if so";1;1
"Cutting out BUG()s is good, right?  But, to be honest, this is really to
extend the usefulness of the custom put_new_page feature, allowing a pool
of new pages to be shared perhaps with racing uses";1;1
"Use an ""else"" instead of that ""skip_unmap"" label.";1;1
mm: page migration use the put_new_page whenever necessary;1;1
"I don't know of any problem from the way it's used in our current tree,
but there is one defect in page migration's custom put_new_page feature";1;1
"An unused newpage is expected to be released with the put_new_page(), but
there was one MIGRATEPAGE_SUCCESS (0) path which released it with
putback_lru_page(): which can be very wrong for a custom pool";0;1
"Fixed more easily by resetting put_new_page once it won't be needed, than
by adding a further flag to modify the rc test.";1;1
mm: correct a couple of page migration comments;0;0
"It's migrate.c not migration,c, and nowadays putback_movable_pages() not
putback_lru_pages().";0;1
mm: rename mem_cgroup_migrate to mem_cgroup_replace_page;1;1
"After v4.3's commit 0610c25daa3e (""memcg: fix dirty page migration"")
mem_cgroup_migrate() doesn't have much to offer in page migration: convert
migrate_misplaced_transhuge_page() to set_page_memcg() instead";0;1
"Then rename mem_cgroup_migrate() to mem_cgroup_replace_page(), since its
remaining callers are replace_page_cache_page() and shmem_replace_page()";1;1
both of whom passed lrucare true, so just eliminate that argument.;1;1
mm: page migration fix PageMlocked on migrated pages;1;1
"Commit e6c509f85455 (""mm: use clear_page_mlock() in page_remove_rmap()"")
in v3.7 inadvertently made mlock_migrate_page() impotent: page migration
unmaps the page from userspace before migrating, and that commit clears
PageMlocked on the final unmap, leaving mlock_migrate_page() with
nothing to do";0;0
" Not a serious bug, the next attempt at reclaiming the
page would fix it up; but a betrayal of page migration's intent - the
new page ought to emerge as PageMlocked";1;1
"I don't see how to fix it for mlock_migrate_page() itself; but easily
fixed in remove_migration_pte(), by calling mlock_vma_page() when the vma
is VM_LOCKED - under pte lock as in try_to_unmap_one()";1;1
"Delete mlock_migrate_page()?  Not quite, it does still serve a purpose for
migrate_misplaced_transhuge_page(): where we could replace it by a test,
clear_page_mlock(), mlock_vma_page() sequence; but would that be an
improvement?  mlock_migrate_page() is fairly lean, and let's make it
leaner by skipping the irq save/restore now clearly not needed.";1;1
mm, migrate: count pages failing all retries in vmstat and tracepoint;1;0
"Migration tries up to 10 times to migrate pages that return -EAGAIN until
it gives up";0;1
" If some pages fail all retries, they are counted towards the
number of failed pages that migrate_pages() returns";0;1
" They should also be
counted in the /proc/vmstat pgmigrate_fail and in the mm_migrate_pages
tracepoint.";1;1
memcg: fix dirty page migration;1;1
"The problem starts with a file backed dirty page which is charged to a
memcg";0;1
 Then page migration is used to move oldpage to newpage;0;0
Migration;0;0
" - copies the oldpage's data to newpage
 - clears oldpage.PG_dirty
 - sets newpage.PG_dirty
 - uncharges oldpage from memcg
 - charges newpage to memcg
Clearing oldpage.PG_dirty decrements the charged memcg's dirty page
count";0;0
"However, because newpage is not yet charged, setting newpage.PG_dirty
does not increment the memcg's dirty page count";0;0
" After migration
completes newpage.PG_dirty is eventually cleared, often in
account_page_cleaned()";0;1
" At this time newpage is charged to a memcg so
the memcg's dirty page count is decremented which causes underflow
because the count was not previously incremented by migration";0;1
" This
underflow causes balance_dirty_pages() to see a very large unsigned
number of dirty memcg pages which leads to aggressive throttling of
buffered writes by processes in non root memcg";0;1
This issue;0;0
 - can harm performance of non root memcg buffered writes;1;1
" - can report too small (even negative) values in
   memory.stat[(total_)dirty] counters of all memcg, including the root";0;1
"To avoid polluting migrate.c with #ifdef CONFIG_MEMCG checks, introduce
page_memcg() and set_page_memcg() helpers";1;0
Test;0;0
"    0) setup and enter limited memcg
    mkdir /sys/fs/cgroup/test
    echo 1G > /sys/fs/cgroup/test/memory.limit_in_bytes
    echo $$ > /sys/fs/cgroup/test/cgroup.procs
    1) buffered writes baseline
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
    2) buffered writes with compaction antagonist to induce migration
    yes 1 > /proc/sys/vm/compact_memory &
    rm -rf /data/tmp/foo
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    kill %
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
    3) buffered writes without antagonist, should match baseline
    rm -rf /data/tmp/foo
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
                       (speed, dirty residue)
             unpatched                       patched
    1) 841 MB/s 0 dirty pages          886 MB/s 0 dirty pages
    2) 611 MB/s -33427456 dirty pages  793 MB/s 0 dirty pages
    3) 114 MB/s -33427456 dirty pages  891 MB/s 0 dirty pages
    Notice that unpatched baseline performance (1) fell after
    migration (3): 841 -> 114 MB/s";0;1
" In the patched kernel, post
    migration performance matches baseline";0;0
"Fixes: c4843a7593a9 (""memcg: add per cgroup dirty page accounting"")";1;1
mm: migrate: hugetlb: putback destination hugepage to active list;1;0
"Since commit bcc54222309c (""mm: hugetlb: introduce page_huge_active"")
each hugetlb page maintains its active flag to avoid a race condition
betwe= en multiple calls of isolate_huge_page(), but current kernel
doesn't set the f= lag on a hugepage allocated by migration because the
proper putback routine isn= 't called";0;1
" This means that users could
still encounter the race referred to by bcc54222309c in this special
case, so this patch fixes it";1;1
"Fixes: bcc54222309c (""mm: hugetlb: introduce page_huge_active"")";1;1
mm: introduce idle page tracking;1;0
"Knowing the portion of memory that is not used by a certain application or
memory cgroup (idle memory) can be useful for partitioning the system
efficiently, e.g";0;1
 by setting memory cgroup limits appropriately;1;0
"Currently, the only means to estimate the amount of idle memory provided
by the kernel is /proc/PID/{clear_refs,smaps}: the user can clear the
access bit for all pages mapped to a particular process by writing 1 to
clear_refs, wait for some time, and then count smaps:Referenced";0;1
" However,
this method has two serious shortcomings";0;1
" - it does not count unmapped file pages
 - it affects the reclaimer logic
To overcome these drawbacks, this patch introduces two new page flags,
Idle and Young, and a new sysfs file, /sys/kernel/mm/page_idle/bitmap";1;1
"A page's Idle flag can only be set from userspace by setting bit in
/sys/kernel/mm/page_idle/bitmap at the offset corresponding to the page,
and it is cleared whenever the page is accessed either through page tables
(it is cleared in page_referenced() in this case) or using the read(2)
system call (mark_page_accessed())";1;0
"Thus by setting the Idle flag for
pages of a particular workload, which can be found e.g";0;1
" by reading
/proc/PID/pagemap, waiting for some time to let the workload access its
working set, and then reading the bitmap file, one can estimate the amount
of pages that are not used by the workload";1;1
"The Young page flag is used to avoid interference with the memory
reclaimer";1;1
" A page's Young flag is set whenever the Access bit of a page
table entry pointing to the page is cleared by writing to the bitmap file";0;1
"If page_referenced() is called on a Young page, it will add 1 to its
return value, therefore concealing the fact that the Access bit was
cleared";1;1
"Note, since there is no room for extra page flags on 32 bit, this feature
uses extended page flags when compiled on 32 bit.";0;1
mm: rename alloc_pages_exact_node() to __alloc_pages_node();1;1
"alloc_pages_exact_node() was introduced in commit 6484eb3e2a81 (""page
allocator: do not check NUMA node ID when the caller knows the node is
valid"") as an optimized variant of alloc_pages_node(), that doesn't
fallback to current node for nid == NUMA_NO_NODE";0;1
" Unfortunately the
name of the function can easily suggest that the allocation is
restricted to the given node and fails otherwise";0;1
" In truth, the node is
only preferred, unless __GFP_THISNODE is passed among the gfp flags";0;0
"The misleading name has lead to mistakes in the past, see for example
commits 5265047ac301 (""mm, thp: really limit transparent hugepage
allocation to local node"") and b360edb43f8e (""mm, mempolicy";0;1
"migrate_to_node should only migrate to node"")";0;1
"Another issue with the name is that there's a family of
alloc_pages_exact*() functions where 'exact' means exact size (instead
of page order), which leads to more confusion";0;1
"To prevent further mistakes, this patch effectively renames
alloc_pages_exact_node() to __alloc_pages_node() to better convey that
it's an optimized variant of alloc_pages_node() not intended for general
usage";0;1
 Both functions get described in comments;1;0
"It has been also considered to really provide a convenience function for
allocations restricted to a node, but the major opinion seems to be that
__GFP_THISNODE already provides that functionality and we shouldn't
duplicate the API needlessly";0;1
" The number of users would be small
anyway";0;1
"Existing callers of alloc_pages_exact_node() are simply converted to
call __alloc_pages_node(), with the exception of sba_alloc_coherent()
which open-codes the check for NUMA_NO_NODE, so it is converted to use
alloc_pages_node() instead";0;1
" This means it no longer performs some
VM_BUG_ON checks, and since the current check for nid in
alloc_pages_node() uses a 'nid < 0' comparison (which includes
NUMA_NO_NODE), it may hide wrong values which would be previously
exposed";0;1
Both differences will be rectified by the next patch;0;0
"To sum up, this patch makes no functional changes, except temporarily
hiding potentially buggy callers";1;1
" Restricting the checks in
alloc_pages_node() is left for the next patch which can in turn expose
more existing buggy callers.";0;1
mm/hwpoison: fix race between soft_offline_page and unpoison_memory;1;1
"Wanpeng Li reported a race between soft_offline_page() and
unpoison_memory(), which causes the following kernel panic";0;1
"   BUG: Bad page state in process bash  pfn:97000
   page:ffffea00025c0000 count:0 mapcount:1 mapping:          (null) index:0x7f4fdbe00
   flags: 0x1fffff80080048(uptodate|active|swapbacked)
   page dumped because: PAGE_FLAGS_CHECK_AT_FREE flag(s) set
   bad because of flags";1;0
"   flags: 0x40(active)
   Modules linked in: snd_hda_codec_hdmi i915 rpcsec_gss_krb5 nfsv4 dns_resolver bnep rfcomm nfsd bluetooth auth_rpcgss nfs_acl nfs rfkill lockd grace sunrpc i2c_algo_bit drm_kms_helper snd_hda_codec_realtek snd_hda_codec_generic drm snd_hda_intel fscache snd_hda_codec x86_pkg_temp_thermal coretemp kvm_intel snd_hda_core snd_hwdep kvm snd_pcm snd_seq_dummy snd_seq_oss crct10dif_pclmul snd_seq_midi crc32_pclmul snd_seq_midi_event ghash_clmulni_intel snd_rawmidi aesni_intel lrw gf128mul snd_seq glue_helper ablk_helper snd_seq_device cryptd fuse snd_timer dcdbas serio_raw mei_me parport_pc snd mei ppdev i2c_core video lp soundcore parport lpc_ich shpchp mfd_core ext4 mbcache jbd2 sd_mod e1000e ahci ptp libahci crc32c_intel libata pps_core
   CPU: 3 PID: 2211 Comm: bash Not tainted 4.2.0-rc5-mm1+ #45
   Hardware name: Dell Inc";0;1
"OptiPlex 7020/0F5C5X, BIOS A03 01/08/2015
   This race is explained like below";0;1
"  CPU0                    CPU1
  soft_offline_page
  __soft_offline_page
  TestSetPageHWPoison
                        unpoison_memory
                        PageHWPoison check (true)
                        TestClearPageHWPoison
                        put_page    -> release refcount held by get_hwpoison_page in unpoison_memory
                        put_page    -> release refcount held by isolate_lru_page in __soft_offline_page
  migrate_pages
The second put_page() releases refcount held by isolate_lru_page() which
will lead to unmap_and_move() releases the last refcount of page and w/
mapcount still 1 since try_to_unmap() is not called if there is only one
user map the page";0;1
" Anyway, the page refcount and mapcount will still
mess if the page is mapped by multiple users";1;1
"This race was introduced by commit 4491f71260 (""mm/memory-failure: set
PageHWPoison before migrate_pages()""), which focuses on preventing the
reuse of successfully migrated page";0;0
" Before this commit we prevent the
reuse by changing the migratetype to MIGRATE_ISOLATE during soft
offlining, which has the following problems, so simply reverting the
commit is not a best option";1;0
"  1) it doesn't eliminate the reuse completely, because
     set_migratetype_isolate() can fail to set MIGRATE_ISOLATE to the
     target page if the pageblock of the page contains one or more
     unmovable pages (i.e";0;1
 has_unmovable_pages() returns true);0;0
"  2) the original code changes migratetype to MIGRATE_ISOLATE
     forcibly, and sets it to MIGRATE_MOVABLE forcibly after soft offline,
     regardless of the original migratetype state, which could impact
     other subsystems like memory hotplug or compaction";0;1
"This patch moves PageSetHWPoison just after put_page() in
unmap_and_move(), which closes up the reported race window and minimizes
another race window b/w SetPageHWPoison and reallocation (which causes
the reuse of soft-offlined page.) The latter race window still exists
but it's acceptable, because it's rare and effectively the same as
ordinary ""containment failure"" case even if it happens, so keep the
window open is acceptable";0;1
"Fixes: 4491f71260 (""mm/memory-failure: set PageHWPoison before migrate_pages()"")";0;1
mm: fix status code which move_pages() returns for zero page;1;0
"The manpage for move_pages(2) specifies that status code for zero page is
supposed to be -EFAULT";0;0
 Currently kernel return -ENOENT in this case;0;1
follow_page() can do it for us, if we would ask for FOLL_DUMP;1;1
" The use of
FOLL_DUMP also means that the upper layer page tables pages are no longer
allocated.";1;0
mm/memory-failure: set PageHWPoison before migrate_pages();0;1
"Now page freeing code doesn't consider PageHWPoison as a bad page, so by
setting it before completing the page containment, we can prevent the
error page from being reused just after successful page migration";1;1
"I added TTU_IGNORE_HWPOISON for try_to_unmap() to make sure that the
page table entry is transformed into migration entry, not to hwpoison
entry.";1;1
mm: check __PG_HWPOISON separately from PAGE_FLAGS_CHECK_AT_*;1;1
"The race condition addressed in commit add05cecef80 (""mm: soft-offline";0;1
"don't free target page in successful page migration"") was not closed
completely, because that can happen not only for soft-offline, but also
for hard-offline";0;0
" Consider that a slab page is about to be freed into
buddy pool, and then an uncorrected memory error hits the page just
after entering __free_one_page(), then VM_BUG_ON_PAGE(page->flags &
PAGE_FLAGS_CHECK_AT_PREP) is triggered, despite the fact that it's not
necessary because the data on the affected page is not consumed";0;1
"To solve it, this patch drops __PG_HWPOISON from page flag checks at
allocation/free time";1;1
" I think it's justified because __PG_HWPOISON
flags is defined to prevent the page from being reused, and setting it
outside the page's alloc-free cycle is a designed behavior (not a bug.)
For recent months, I was annoyed about BUG_ON when soft-offlined page
remains on lru cache list for a while, which is avoided by calling
put_page() instead of putback_lru_page() in page migration's success
path";1;1
" This means that this patch reverts a major change from commit
add05cecef80 about the new refcounting rule of soft-offlined pages, so
""reuse window"" revives";1;1
 This will be closed by a subsequent patch.;0;0
mm: clarify that the function operates on hugepage pte;0;0
We have confusing functions to clear pmd, pmd_clear_* and pmd_clear;0;0
" Add
_huge_ to pmdp_clear functions so that we are clear that they operate on
hugepage pte";0;0
"We don't bother about other functions like pmdp_set_wrprotect,
pmdp_clear_flush_young, because they operate on PTE bits and hence
indicate they are operating on hugepage ptes";0;1
mm: soft-offline: don't free target page in successful page migration;0;0
"Stress testing showed that soft offline events for a process iterating
""mmap-pagefault-munmap"" loop can trigger
VM_BUG_ON(PAGE_FLAGS_CHECK_AT_PREP) in __free_one_page()";0;0
"  Soft offlining page 0x70fe1 at 0x70100008d000
  Soft offlining page 0x705fb at 0x70300008d000
  page:ffffea0001c3f840 count:0 mapcount:0 mapping:          (null) index:0x2
  flags: 0x1fffff80800000(hwpoison)
  page dumped because: VM_BUG_ON_PAGE(page->flags & ((1 << 25) - 1))
  ------------[ cut here ]------------
  kernel BUG at /src/linux-dev/mm/page_alloc.c:585!
  invalid opcode: 0000 [#1] SMP DEBUG_PAGEALLOC
  Modules linked in: cfg80211 rfkill crc32c_intel microcode ppdev parport_pc pcspkr serio_raw virtio_balloon parport i2c_piix4 virtio_blk virtio_net ata_generic pata_acpi floppy
  CPU: 3 PID: 1779 Comm: test_base_madv_ Not tainted 4.0.0-v4.0-150511-1451-00009-g82360a3730e6 #139
  RIP: free_pcppages_bulk+0x52a/0x6f0
  When soft offline successfully migrates page, the source page is supposed
to be freed";0;0
" But there is a race condition where a source page looks
isolated (i.e";0;1
" the refcount is 0 and the PageHWPoison is set) but
somewhat linked to pcplist";0;0
" Then another soft offline event calls
drain_all_pages() and tries to free such hwpoisoned page, which is
forbidden";0;1
"This odd page state seems to happen due to the race between put_page() in
putback_lru_page() and __pagevec_lru_add_fn()";1;1
" But I don't want to play
with tweaking drain code as done in commit 9ab3b598d2df ""mm: hwpoison";1;0
"drop lru_add_drain_all() in __soft_offline_page()"", or to change page
freeing code for this soft offline's purpose";1;0
"Instead, let's think about the difference between hard offline and soft
offline";1;1
" There is an interesting difference in how to isolate the in-use
page between these, that is, hard offline marks PageHWPoison of the target
page at first, and doesn't free it by keeping its refcount 1";0;1
" OTOH, soft
offline tries to free the target page then marks PageHWPoison";0;0
" This
difference might be the source of complexity and result in bugs like the
above";0;1
" So making soft offline isolate with keeping refcount can be a
solution for this problem";0;1
"We can pass to page migration code the ""reason"" which shows the caller, so
let's use this more to avoid calling putback_lru_page() when called from
soft offline, which effectively does the isolation for soft offline";1;1
" With
this change, target pages of soft offline never be reused without changing
migratetype, so this patch also removes the related code.";1;1
mm/migrate: check-before-clear PageSwapCache;1;1
"With the page flag sanitization patchset, an invalid usage of
ClearPageSwapCache() is detected in migration_page_copy()";0;1
"migrate_page_copy() is shared by both normal and hugepage (both thp and
hugetlb) code path, so let's check PageSwapCache() and clear it if it's
set to avoid misuse of the invalid clear operation.";1;1
mm: numa: remove migrate_ratelimited;1;1
"This code is dead since commit 9e645ab6d089 (""sched/numa: Continue PTE
scanning even if migrate rate limited"") so remove it.";1;0
"mm/migrate: mark unmap_and_move() ""noinline"" to avoid ICE in gcc 4.7.3";1;0
With gcc version 4.7.3 (Ubuntu/Linaro 4.7.3-12ubuntu1) ;0;0
    mm/migrate.c: In function `migrate_pages';0;1
"    mm/migrate.c:1148:1: internal compiler error: in push_minipool_fix, at config/arm/arm.c:13500
    Please submit a full bug report,
    with preprocessed source if appropriate";1;1
    See <file:///usr/share/doc/gcc-4.7/README.Bugs> for instructions;1;0
    Preprocessed source stored into /tmp/ccPoM1tr.out file, please attach this to your bugreport;1;0
"    make[1]: *** [mm/migrate.o] Error 1
    make: *** [mm/migrate.o] Error 2
Mark unmap_and_move() (which is used in a single place only) ""noinline""
to work around this compiler bug.";1;1
mm: convert p[te|md]_mknonnuma and remaining page table manipulations;1;0
"With PROT_NONE, the traditional page table manipulation functions are
sufficient.";0;0
mm: numa: do not dereference pmd outside of the lock during NUMA hinting fault;0;0
"Automatic NUMA balancing depends on being able to protect PTEs to trap a
fault and gather reference locality information";0;0
" Very broadly speaking
it would mark PTEs as not present and use another bit to distinguish
between NUMA hinting faults and other types of faults";1;1
" It was
universally loved by everybody and caused no problems whatsoever";0;0
" That
last sentence might be a lie";0;0
"This series is very heavily based on patches from Linus and Aneesh to
replace the existing PTE/PMD NUMA helper functions with normal change
protections";0;0
" I did alter and add parts of it but I consider them
relatively minor contributions";1;0
" At their suggestion, acked-bys are in
there but I've no problem converting them to Signed-off-by if requested";0;1
"AFAIK, this has received no testing on ppc64 and I'm depending on Aneesh
for that";1;0
" I tested trinity under kvm-tool and passed and ran a few
other basic tests";1;1
" At the time of writing, only the short-lived tests
have completed but testing of V2 indicated that long-term testing had no
surprises";0;0
" In most cases I'm leaving out detail as it's not that
interesting";0;1
"specjbb single JVM: There was negligible performance difference in the
	benchmark itself for short runs";0;1
"However, system activity is
	higher and interrupts are much higher over time -- possibly TLB
	flushes";0;0
Migrations are also higher;1;0
"Overall, this is more overhead
	but considering the problems faced with the old approach I think
	we just have to suck it up and find another way of reducing the
	overhead";1;1
"specjbb multi JVM: Negligible performance difference to the actual benchmark
	but like the single JVM case, the system overhead is noticeably
	higher";1;0
 Again, interrupts are a major factor;0;0
"autonumabench: This was all over the place and about all that can be
	reasonably concluded is that it's different but not necessarily
	better or worse";0;1
"autonumabench
                                     3.18.0-rc5            3.18.0-rc5
                                 mmotm-20141119         protnone-v3r3
User    NUMA01               32380.24 (  0.00%)    21642.92 ( 33.16%)
User    NUMA01_THEADLOCAL    22481.02 (  0.00%)    22283.22 (  0.88%)
User    NUMA02                3137.00 (  0.00%)     3116.54 (  0.65%)
User    NUMA02_SMT            1614.03 (  0.00%)     1543.53 (  4.37%)
System  NUMA01                 322.97 (  0.00%)     1465.89 (-353.88%)
System  NUMA01_THEADLOCAL       91.87 (  0.00%)       49.32 ( 46.32%)
System  NUMA02                  37.83 (  0.00%)       14.61 ( 61.38%)
System  NUMA02_SMT               7.36 (  0.00%)        7.45 ( -1.22%)
Elapsed NUMA01                 716.63 (  0.00%)      599.29 ( 16.37%)
Elapsed NUMA01_THEADLOCAL      553.98 (  0.00%)      539.94 (  2.53%)
Elapsed NUMA02                  83.85 (  0.00%)       83.04 (  0.97%)
Elapsed NUMA02_SMT              86.57 (  0.00%)       79.15 (  8.57%)
CPU     NUMA01                4563.00 (  0.00%)     3855.00 ( 15.52%)
CPU     NUMA01_THEADLOCAL     4074.00 (  0.00%)     4136.00 ( -1.52%)
CPU     NUMA02                3785.00 (  0.00%)     3770.00 (  0.40%)
CPU     NUMA02_SMT            1872.00 (  0.00%)     1959.00 ( -4.65%)
System CPU usage of NUMA01 is worse but it's an adverse workload on this
machine so I'm reluctant to conclude that it's a problem that matters";0;1
" On
the other workloads that are sensible on this machine, system CPU usage is
great";0;1
" Overall time to complete the benchmark is comparable
          3.18.0-rc5  3.18.0-rc5
        mmotm-20141119protnone-v3r3
User        59612.50    48586.44
System        460.22     1537.45
Elapsed      1442.20     1304.29
NUMA alloc hit                 5075182     5743353
NUMA alloc miss                      0           0
NUMA interleave hit                  0           0
NUMA alloc local               5075174     5743339
NUMA base PTE updates        637061448   443106883
NUMA huge PMD updates          1243434      864747
NUMA page range updates     1273699656   885857347
NUMA hint faults               1658116     1214277
NUMA hint local faults          959487      754113
NUMA hint local percent             57          62
NUMA pages migrated            5467056    61676398
The NUMA pages migrated look terrible but when I looked at a graph of the
activity over time I see that the massive spike in migration activity was
during NUMA01";0;1
" This correlates with high system CPU usage and could be
simply down to bad luck but any modifications that affect that workload
would be related to scan rates and migrations, not the protection
mechanism";0;1
 For all other workloads, migration activity was comparable;1;1
"Overall, headline performance figures are comparable but the overhead is
higher, mostly in interrupts";0;0
" To some extent, higher overhead from this
approach was anticipated but not to this degree";0;1
" It's going to be
necessary to reduce this again with a separate series in the future";0;1
" It's
still worth going ahead with this series though as it's likely to avoid
constant headaches with Xen and is probably easier to maintain";0;1
This patch (of 10);1;0
"A transhuge NUMA hinting fault may find the page is migrating and should
wait until migration completes";1;0
" The check is race-prone because the pmd
is deferenced outside of the page lock and while the race is tiny, it'll
be larger if the PMD is cleared while marking PMDs for hinting fault";0;1
This patch closes the race.;1;1
mm/hugetlb: take page table lock in follow_huge_pmd();0;0
"We have a race condition between move_pages() and freeing hugepages, where
move_pages() calls follow_page(FOLL_GET) for hugepages internally and
tries to get its refcount without preventing concurrent freeing";0;1
" This
race crashes the kernel, so this patch fixes it by moving FOLL_GET code
for hugepages into follow_huge_pmd() with taking the page table lock";1;1
This patch intentionally removes page==NULL check after pte_page;1;1
"This is justified because pte_page() never returns NULL for any
architectures or configurations";0;0
"This patch changes the behavior of follow_huge_pmd() for tail pages and
then tail pages can be pinned/returned";1;1
" So the caller must be changed to
properly handle the returned tail pages";1;1
"We could have a choice to add the similar locking to
follow_huge_(addr|pud) for consistency, but it's not necessary because
currently these functions don't support FOLL_GET flag, so let's leave it
for future development";1;1
Here is the reproducer;1;0
"  #include <stdio.h>
  #include <stdlib.h>
  #include <numaif.h>
  #define ADDR_INPUT      0x700000000000UL
  #define HPS             0x200000
  #define PS              0x1000
                  ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
                  ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
  #include <stdio.h>
  #include <sys/mman.h>
  #include <string.h>
  #define ADDR_INPUT      0x700000000000UL
  #define HPS             0x200000
                  p = mmap((void *)ADDR_INPUT, nr_hp * HPS, PROT_READ | PROT_WRITE,
  $ sysctl vm.nr_hugepages=40
  $ ./hugepage 10 &
  $ ./movepages 10 $(pgrep -f hugepage)
Fixes: e632a938d914 (""mm: migrate: add hugepage migration code to move_pages()"")";1;1
rmap: drop support of non-linear mappings;1;0
We don't create non-linear mappings anymore;1;1
" Let's drop code which
handles them in rmap.";1;0
vm_area_operations: kill ->migrate();1;0
"the only instance this method has ever grown was one in kernfs -
one that call ->migrate() of another vm_ops if it exists.";1;0
mm: unmapped page migration avoid unmap+remap overhead;1;1
"Page migration's __unmap_and_move(), and rmap's try_to_unmap(), were
created for use on pages almost certainly mapped into userspace";0;1
" But
nowadays compaction often applies them to unmapped page cache pages: which
may exacerbate contention on i_mmap_rwsem quite unnecessarily, since
try_to_unmap_file() makes no preliminary page_mapped() check";1;1
"Now check page_mapped() in __unmap_and_move(); and avoid repeating the
same overhead in rmap_walk_file() - don't remove_migration_ptes() when we
never inserted any";1;0
"(The PageAnon(page) comment blocks now look even sillier than before, but
clean that up on some other occasion";1;1
" And note in passing that
try_to_unmap_one() does not use a migration entry when PageSwapCache, so
remove_migration_ptes() will then not update that swap entry to newpage
pte: not a big deal, but something else to clean up later.)
Davidlohr remarked in ""mm,fs: introduce helpers around the i_mmap_mutex""
conversion to i_mmap_rwsem, that ""The biggest winner of these changes is
migration"": a part of the reason might be all of that unnecessary taking
of i_mmap_mutex in page migration; and it's rather a shame that I didn't
get around to sending this patch in before his - this one is much less
useful after Davidlohr's conversion to rwsem, but still good.";1;1
mm/balloon_compaction: redesign ballooned pages management;0;0
Sasha Levin reported KASAN splash inside isolate_migratepages_range();0;1
"Problem is in the function __is_movable_balloon_page() which tests
AS_BALLOON_MAP in page->mapping->flags";0;1
" This function has no protection
against anonymous pages";0;1
" As result it tried to check address space flags
inside struct anon_vma";1;1
Further investigation shows more problems in current implementation;0;1
  balloon_page_movable() checks page flags and page_count;0;0
" In
  __unmap_and_move() page is locked, reference counter is elevated, thus
  balloon_page_movable() always fails";0;1
" As a result execution goes to the
  normal migration path";0;1
" virtballoon_migratepage() returns
  MIGRATEPAGE_BALLOON_SUCCESS instead of MIGRATEPAGE_SUCCESS,
  move_to_new_page() thinks this is an error code and assigns
  newpage->mapping to NULL";1;1
" Newly migrated page lose connectivity with
  balloon an all ability for further migration";0;0
  isolation ballooned page;1;0
" This function releases lru_lock periodically,
  this makes migration mostly impossible for some pages";0;1
"  balloon_page_isolate could be executed in parallel with dequeue between
  picking page from list and locking page_lock";1;0
" Race is rare because they
  use trylock_page() for locking";1;1
This patch fixes all of them;1;1
"Instead of fake mapping with special flag this patch uses special state of
page->_mapcount: PAGE_BALLOON_MAPCOUNT_VALUE = -256";0;1
" Buddy allocator uses
PAGE_BUDDY_MAPCOUNT_VALUE = -128 for similar purpose";0;0
" Storing mark
directly in struct page makes everything safer and easier";1;1
PagePrivate is used to mark pages present in page list (i.e;1;0
" not
isolated, like PageLRU for normal pages)";1;0
" It replaces special rules for
reference counter and makes balloon migration similar to migration of
normal pages";1;0
" This flag is protected by page_lock together with link to
the balloon device.";0;1
mm: migrate: Close race between migration completion and mprotect;0;1
"A migration entry is marked as write if pte_write was true at the time the
entry was created";1;0
"The VMA protections are not double checked when migration
entries are being removed as mprotect marks write-migration-entries as
read";1;0
"It means that potentially we take a spurious fault to mark PTEs write
again but it's straight-forward";0;0
"However, there is a race between write
migrations being marked read and migrations finishing";0;1
"This potentially
allows a PTE to be write that should have been read";0;1
"Close this race by
double checking the VMA permissions using maybe_mkwrite when migration
completes.";1;1
mm: memcontrol: rewrite uncharge API;1;1
"The memcg uncharging code that is involved towards the end of a page's
lifetime - truncation, reclaim, swapout, migration - is impressively
complicated and fragile";0;0
"Because anonymous and file pages were always charged before they had their
page->mapping established, uncharges had to happen when the page type
could still be known from the context; as in unmap for anonymous, page
cache removal for file and shmem pages, and swap cache truncation for swap
pages";1;1
" However, these operations happen well before the page is actually
freed, and so a lot of synchronization is necessary";1;1
"- Charging, uncharging, page migration, and charge migration all need
  to take a per-page bit spinlock as they could race with uncharging";1;1
"- Swap cache truncation happens during both swap-in and swap-out, and
  possibly repeatedly before the page is actually freed";1;0
" This means
  that the memcg swapout code is called from many contexts that make
  no sense and it has to figure out the direction from page state to
  make sure memory and memory+swap are always correctly charged";1;1
"- On page migration, the old page might be unmapped but then reused,
  so memcg code has to prevent untimely uncharging in that case";0;1
"  Because this code - which should be a simple charge transfer - is so
  special-cased, it is not reusable for replace_page_cache()";1;1
"But now that charged pages always have a page->mapping, introduce
mem_cgroup_uncharge(), which is called after the final put_page(), when we
know for sure that nobody is looking at the page anymore";0;1
"For page migration, introduce mem_cgroup_migrate(), which is called after
the migration is successful and the new page is fully rmapped";1;0
" Because
the old page is no longer uncharged after migration, prevent double
charges by decoupling the page's memcg association (PCG_USED and
pc->mem_cgroup) from the page holding an actual charge";1;1
" The new bits
PCG_MEM and PCG_MEMSW represent the respective charges and are transferred
to the new page during migration";1;1
"mem_cgroup_migrate() is suitable for replace_page_cache() as well,
which gets rid of mem_cgroup_replace_page_cache()";1;1
" However, care
needs to be taken because both the source and the target page can
already be charged and on the LRU when fuse is splicing: grab the page
lock on the charge moving side to prevent changing pc->mem_cgroup of a
page under migration";0;1
" Also, the lruvecs of both pages change as we
uncharge the old and charge the new during migration, and putback may
race with us, so grab the lru lock and isolate the pages iff on LRU to
prevent races and ensure the pages are on the right lruvec afterward";1;1
"Swap accounting is massively simplified: because the page is no longer
uncharged as early as swap cache deletion, a new mem_cgroup_swapout() can
transfer the page's memory+swap charge (PCG_MEMSW) to the swap entry
before the final put_page() in page reclaim";1;0
"Finally, page_cgroup changes are now protected by whatever protection the
page itself offers: anonymous pages are charged under the page table lock,
whereas page cache insertions, swapin, and migration hold the page lock";0;1
Uncharging happens under full exclusion with no outstanding references;0;0
"Charging and uncharging also ensure that the page is off-LRU, which
serializes against charge migration";1;1
" Remove the very costly page_cgroup
lock and set pc->flags non-atomically.";1;0
mm: fix direct reclaim writeback regression;1;1
Shortly before 3.16-rc1, Dave Jones reported;0;0
"  WARNING: CPU: 3 PID: 19721 at fs/xfs/xfs_aops.c:971
           xfs_vm_writepage+0x5ce/0x630 [xfs]()
  CPU: 3 PID: 19721 Comm: trinity-c61 Not tainted 3.15.0+ #3
   970   if (WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) ==
 971                   PF_MEMALLOC))
I did not respond at the time, because a glance at the PageDirty block
in shrink_page_list() quickly shows that this is impossible: we don't do
writeback on file pages (other than tmpfs) from direct reclaim nowadays";0;1
Dave was hallucinating, but it would have been disrespectful to say so;0;0
"However, my own /var/log/messages now shows similar complaints
  WARNING: CPU: 1 PID: 28814 at fs/ext4/inode.c:1881 ext4_writepage+0xa7/0x38b()
  WARNING: CPU: 0 PID: 27347 at fs/ext4/inode.c:1764 ext4_writepage+0xa7/0x38b()
from stressing some mmotm trees during July";1;0
"Could a dirty xfs or ext4 file page somehow get marked PageSwapBacked,
so fail shrink_page_list()'s page_is_file_cache() test, and so proceed
to mapping->a_ops->writepage()?
Yes, 3.16-rc1's commit 68711a746345 (""mm, migration: add destination
page freeing callback"") has provided such a way to compaction: if
migrating a SwapBacked page fails, its newpage may be put back on the
list for later use with PageSwapBacked still set, and nothing will clear
Whether that can do anything worse than issue WARN_ON_ONCEs, and get
some statistics wrong, is unclear: easier to fix than to think through
the consequences";0;1
"Fixing it here, before the put_new_page(), addresses the bug directly,
but is probably the worst place to fix it";1;1
" Page migration is doing too
many parts of the job on too many levels: fixing it in
move_to_new_page() to complement its SetPageSwapBacked would be
preferable, except why is it (and newpage->mapping and newpage->index)
done there, rather than down in migrate_page_move_mapping(), once we are
sure of success? Not a cleanup to get into right now, especially not
with memcg cleanups coming in 3.17.";0;1
mm: let mm_find_pmd fix buggy race with THP fault;1;1
Trinity has reported;0;0
"    BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
    IP: __lock_acquire (kernel/locking/lockdep.c:3070 (discriminator 1))
    CPU: 6 PID: 16173 Comm: trinity-c364 Tainted: G        W
                            3.15.0-rc1-next-20140415-sasha-00020-gaa90d09 #398
    lock_acquire (arch/x86/include/asm/current.h:14
                  kernel/locking/lockdep.c:3602)
    _raw_spin_lock (include/linux/spinlock_api_smp.h:143
                    kernel/locking/spinlock.c:151)
    remove_migration_pte (mm/migrate.c:137)
    rmap_walk (mm/rmap.c:1628 mm/rmap.c:1699)
    remove_migration_ptes (mm/migrate.c:224)
    migrate_pages (mm/migrate.c:922 mm/migrate.c:960 mm/migrate.c:1126)
    migrate_misplaced_page (mm/migrate.c:1733)
    __handle_mm_fault (mm/memory.c:3762 mm/memory.c:3812 mm/memory.c:3925)
    handle_mm_fault (mm/memory.c:3948)
    __get_user_pages (mm/memory.c:1851)
    __mlock_vma_pages_range (mm/mlock.c:255)
    __mm_populate (mm/mlock.c:711)
    SyS_mlockall (include/linux/mm.h:1799 mm/mlock.c:817 mm/mlock.c:791)
I believe this comes about because, whereas collapsing and splitting THP
functions take anon_vma lock in write mode (which excludes concurrent
rmap walks), faulting THP functions (write protection and misplaced
NUMA) do not - and mostly they do not need to";0;0
"But they do use a pmdp_clear_flush(), set_pmd_at() sequence which, for
an instant (indeed, for a long instant, given the inter-CPU TLB flush in
there), leaves *pmd neither present not trans_huge";0;1
"Which can confuse a concurrent rmap walk, as when removing migration
ptes, seen in the dumped trace";0;0
" Although that rmap walk has a 4k page
to insert, anon_vmas containing THPs are in no way segregated from
4k-page anon_vmas, so the 4k-intent mm_find_pmd() does need to cope with
that instant when a trans_huge pmd is temporarily absent";0;1
"I don't think we need strengthen the locking at the THP end: it's easily
handled with an ACCESS_ONCE() before testing both conditions";1;0
"And since mm_find_pmd() had only one caller who wanted a THP rather than
a pmd, let's slightly repurpose it to fail when it hits a THP or
non-present pmd, and open code split_huge_page_address() again.";1;1
hugetlb: rename hugepage_migration_support() to ..._supported();1;1
"We already have a function named hugepages_supported(), and the similar
name hugepage_migration_support() is a bit unconfortable, so let's rename
it hugepage_migration_supported().";1;1
mm, migration: add destination page freeing callback;1;0
"Memory migration uses a callback defined by the caller to determine how to
allocate destination pages";1;1
" When migration fails for a source page,
however, it frees the destination page back to the system";0;0
"This patch adds a memory migration callback defined by the caller to
determine how to free destination pages";1;1
" If a caller, such as memory
compaction, builds its own freelist for migration targets, this can reuse
already freed memory instead of scanning additional memory";1;1
"If the caller provides a function to handle freeing of destination pages,
it is called when page migration fails";1;1
" If the caller passes NULL then
freeing back to the system will be handled as usual";1;1
" This patch
introduces no functional change.";1;0
mm: numa: add migrated transhuge pages to LRU the same way as base pages;1;1
"Migration of misplaced transhuge pages uses page_add_new_anon_rmap() when
putting the page back as it avoided an atomic operations and added the new
page to the correct LRU";1;1
" A side-effect is that the page gets marked
activated as part of the migration meaning that transhuge and base pages
are treated differently from an aging perspective than base page
migration";0;0
"This patch uses page_add_anon_rmap() and putback_lru_page() on completion
of a transhuge migration similar to base page migration";1;1
" It would require
fewer atomic operations to use lru_cache_add without taking an additional
reference to the page";0;1
" The downside would be that it's still different to
base page migration and unevictable pages may be added to the wrong LRU
for cleaning up later";0;1
" Testing of the usual workloads did not show any
adverse impact to the change.";1;0
mm: fix swapops.h:131 bug if remap_file_pages raced migration;1;1
"Add remove_linear_migration_ptes_from_nonlinear(), to fix an interesting
little include/linux/swapops.h:131 BUG_ON(!PageLocked) found by trinity";1;1
"indicating that remove_migration_ptes() failed to find one of the
migration entries that was temporarily inserted";0;0
"The problem comes from remap_file_pages()'s switch from vma_interval_tree
(good for inserting the migration entry) to i_mmap_nonlinear list (no good
range does not cover the whole of the vma (zap_pte() clears the range)";0;0
"remove_migration_ptes() needs a file_nonlinear method to go down the
i_mmap_nonlinear list, applying linear location to look for migration
entries in those vmas too, just in case there was this race";1;1
but it never needed vma passed in - vma comes from its own iteration;0;1
"Reported-and-tested-by: Dave Jones <davej@redhat.com>
Reported-and-tested-by: Sasha Levin <sasha.levin@oracle.com>";1;0
mm: fix GFP_THISNODE callers and clarify;1;1
"GFP_THISNODE is for callers that implement their own clever fallback to
remote nodes";0;0
" It restricts the allocation to the specified node and
does not invoke reclaim, assuming that the caller will take care of it
when the fallback fails, e.g";0;1
" through a subsequent allocation request
without GFP_THISNODE set";0;0
"However, many current GFP_THISNODE users only want the node exclusive
aspect of the flag, without actually implementing their own fallback or
triggering reclaim if necessary";0;1
" This results in things like page
migration failing prematurely even when there is easily reclaimable
memory available, unless kswapd happens to be running already or a
concurrent allocation attempt triggers the necessary reclaim";0;1
"Convert all callsites that don't implement their own fallback strategy
to __GFP_THISNODE";1;0
" This restricts the allocation a single node too, but
at the same time allows the allocator to enter the slowpath, wake
kswapd, and invoke direct reclaim if necessary, to make the allocation
happen when memory is full.";0;1
mm/migrate.c: fix setting of cpupid on page migration twice against normal page;1;1
"Commit 7851a45cd3f6 (""mm: numa: Copy cpupid on page migration"") copies
over the cpupid at page migration time";0;0
" It is unnecessary to set it
again in alloc_misplaced_dst_page().";0;1
sched/numa: fix setting of cpupid on page migration twice;1;1
"Commit 7851a45cd3f6 (""mm: numa: Copy cpupid on page migration"") copiess
over the cpupid at page migration time";0;0
" It is unnecessary to set it
again in migrate_misplaced_transhuge_page().";0;1
mm: dump page when hitting a VM_BUG_ON using VM_BUG_ON_PAGE;0;0
Most of the VM_BUG_ON assertions are performed on a page;0;0
" Usually, when
one of these assertions fails we'll get a BUG_ON with a call stack and
the registers";0;0
"I've recently noticed based on the requests to add a small piece of code
that dumps the page to various VM_BUG_ON sites that the page dump is
quite useful to people debugging issues in mm";0;1
"This patch adds a VM_BUG_ON_PAGE(cond, page) which beyond doing what
VM_BUG_ON() does, also dumps the page before executing the actual
BUG_ON.";1;1
mm/migrate: remove unused function, fail_migrate_page();1;1
fail_migrate_page() isn't used anywhere, so remove it.;1;1
mm/migrate: remove putback_lru_pages, fix comment on putback_movable_pages;1;1
"Some part of putback_lru_pages() and putback_movable_pages() is
duplicated, so it could confuse us what we should use";0;1
" We can remove
putback_lru_pages() since it is not really needed now";1;1
" This makes us
undestand and maintain the code more easily";1;1
And comment on putback_movable_pages() is stale now, so fix it.;1;1
mm/migrate: correct failure handling if !hugepage_migration_support();1;1
"We should remove the page from the list if we fail with ENOSYS, since
migrate_pages() consider error cases except -ENOMEM and -EAGAIN as
permanent failure and it assumes that the page would be removed from the
list";1;1
 Without this patch, we could overcount number of failure;1;1
"In addition, we should put back the new hugepage if
!hugepage_migration_support()";1;1
 If not, we would leak hugepage memory.;0;1
mm/migrate: add comment about permanent failure path;1;1
"Let's add a comment about where the failed page goes to, which makes
code more readable.";1;1
mm: numa: trace tasks that fail migration due to rate limiting;0;1
"A low local/remote numa hinting fault ratio is potentially explained by
failed migrations";0;1
" This patch adds a tracepoint that fires when
migration fails due to migration rate limitation.";1;1
mm: numa: limit scope of lock for NUMA migrate rate limiting;0;0
"NUMA migrate rate limiting protects a migration counter and window using
a lock but in some cases this can be a contended lock";0;1
" It is not
critical that the number of pages be perfect, lost updates are
acceptable";0;1
 Reduce the importance of this lock.;1;1
mm: numa: make NUMA-migrate related functions static;0;1
"numamigrate_update_ratelimit and numamigrate_isolate_page only have
callers in mm/migrate.c";1;1
 This patch makes them static.;1;0
mm/rmap: make rmap_walk to get the rmap_walk_control argument;1;0
"In each rmap traverse case, there is some difference so that we need
function pointers and arguments to them in order to handle these
For this purpose, struct rmap_walk_control is introduced in this patch,
and will be extended in following patch";0;1
" Introducing and extending are
separate, because it clarify changes.";1;0
mmu_notifier: call mmu_notifier_invalidate_range() from VMM;0;0
"Add calls to the new mmu_notifier_invalidate_range() function to all
places in the VMM that need it.";0;1
aio/migratepages: make aio migrate pages sane;0;0
"The arbitrary restriction on page counts offered by the core
migrate_page_move_mapping() code results in rather suspicious looking
fiddling with page reference counts in the aio_migratepage() operation";0;1
"To fix this, make migrate_page_move_mapping() take an extra_count parameter
that allows aio to tell the code about its own reference count on the page
being migrated";1;1
"While cleaning up aio_migratepage(), make it validate that the old page
being passed in is actually what aio_migratepage() expects to prevent
misbehaviour in the case of races.";1;1
mm: numa: defer TLB flush for THP migration as long as possible;0;0
THP migration can fail for a variety of reasons;0;1
" Avoid flushing the TLB
to deal with THP migration races until the copy is ready to start.";0;0
mm: numa: avoid unnecessary disruption of NUMA hinting during migration;1;1
"do_huge_pmd_numa_page() handles the case where there is parallel THP
migration";0;1
" However, by the time it is checked the NUMA hinting
information has already been disrupted";0;1
" This patch adds an earlier
check with some helpers.";1;1
mm: numa: avoid unnecessary work on the failure path;1;1
"If a PMD changes during a THP migration then migration aborts but the
failure path is doing more work than is necessary.";0;1
mm: numa: call MMU notifiers on THP migration;0;1
"MMU notifiers must be called on THP page migration or secondary MMUs
will get very confused.";0;0
mm: numa: serialise parallel get_user_page against THP migration;1;0
"Base pages are unmapped and flushed from cache and TLB during normal
page migration and replaced with a migration entry that causes any
parallel NUMA hinting fault or gup to block until migration completes";0;0
"THP does not unmap pages due to a lack of support for migration entries
at a PMD level";0;0
" This allows races with get_user_pages and
get_user_pages_fast which commit 3f926ab945b6 (""mm: Close races between
THP migration and PMD numa clearing"") made worse by introducing a
pmd_clear_flush()";0;0
"This patch forces get_user_page (fast and normal) on a pmd_numa page to
go through the slow get_user_page path where it will serialise against
THP migration and properly account for the NUMA hinting fault";0;1
" On the
migration side the page table lock is taken for each PTE update.";0;0
mm: thp: give transparent hugepage code a separate copy_page;1;0
"Right now, the migration code in migrate_page_copy() uses copy_huge_page()
for hugetlbfs and thp pages";0;0
So, yay for code reuse;1;1
 But;0;0
"  void copy_huge_page(struct page *dst, struct page *src)
and a non-hugetlbfs page has no page_hstate()";1;0
" This works 99% of the
time because page_hstate() determines the hstate from the page order
alone";0;1
" Since the page order of a THP page matches the default hugetlbfs
page order, it works";0;0
"But, if you change the default huge page size on the boot command-line
(say default_hugepagesz=1G), then we might not even *have* a 2MB hstate
so page_hstate() returns null and copy_huge_page() oopses pretty fast
since copy_huge_page() dereferences the hstate";0;1
"  void copy_huge_page(struct page *dst, struct page *src)
Mel noticed that the migration code is really the only user of these
functions";0;0
" This moves all the copy code over to migrate.c and makes
copy_huge_page() work for THP by checking for it explicitly";0;1
"I believe the bug was introduced in commit b32967ff101a (""mm: numa: Add
THP migration for the NUMA working set scanning fault case"")";0;0
mm: convert the rest to new page table lock api;1;0
Only trivial cases left;1;1
Let's convert them altogether.;1;0
mm, hugetlb: convert hugetlbfs to use split pmd lock;1;0
Hugetlb supports multiple page sizes;1;1
"We use split lock only for PMD
level, but not for PUD.";0;1
mm: Close races between THP migration and PMD numa clearing;1;0
"THP migration uses the page lock to guard against parallel allocations
but there are cases like this still open
  Task A					Task B
  do_huge_pmd_numa_page				do_huge_pmd_numa_page
  lock_page
  mpol_misplaced == -1
  unlock_page
  goto clear_pmdnuma
						lock_page
						mpol_misplaced == 2
						migrate_misplaced_transhuge
  pmd = pmd_mknonnuma
  set_pmd_at
During hours of testing, one crashed with weird errors and while I have
no direct evidence, I suspect something like the race above happened";1;0
"This patch extends the page lock to being held until the pmd_numa is
cleared to prevent migration starting in parallel while the pmd_numa is
being cleared";0;1
"It also flushes the old pmd entry and orders pagetable
insertion before rmap insertion.";0;0
mm: migration: do not lose soft dirty bit if page is in migration state;1;0
"If page migration is turned on in config and the page is migrating, we
may lose the soft dirty bit";0;1
" If fork and mprotect are called on
migrating pages (once migration is complete) pages do not obtain the
soft dirty bit in the correspond pte entries";0;0
" Fix it adding an
appropriate test on swap entries.";1;1
mm: numa: Copy cpupid on page migration;0;0
After page migration, the new page has the nidpid unset;1;1
"This makes
every fault on a recently migrated page look like a first numa fault,
leading to another page migration";1;0
"Copying over the nidpid at page migration time should prevent erroneous
migrations of recently migrated pages.";0;1
mm: numa: Change page last {nid,pid} into {cpu,pid};1;0
"Change the per page last fault tracking to use cpu,pid instead of
nid,pid";1;1
"This will allow us to try and lookup the alternate task more
easily";1;1
"Note that even though it is the cpu that is store in the page
flags that the mpol_misplaced decision is still based on the node.";0;1
sched/numa: Set preferred NUMA node based on number of private faults;0;1
"Ideally it would be possible to distinguish between NUMA hinting faults that
are private to a task and those that are shared";0;1
"If treated identically
there is a risk that shared pages bounce between nodes depending on
the order they are referenced by tasks";0;0
"Ultimately what is desirable is
that task private pages remain local to the task while shared pages are
interleaved between sharing tasks running on different nodes to give good
average performance";0;1
"This is further complicated by THP as even
applications that partition their data may not be partitioning on a huge
page boundary";0;0
"To start with, this patch assumes that multi-threaded or multi-process
applications partition their data and that in general the private accesses
are more important for cpu->memory locality in the general case";0;1
"Also,
no new infrastructure is required to treat private pages properly but
interleaving for shared pages requires additional infrastructure";0;1
"To detect private accesses the pid of the last accessing task is required
but the storage requirements are a high";0;0
"This patch borrows heavily from
Ingo Molnar's patch ""numa, mm, sched: Implement last-CPU+PID hash tracking""
to encode some bits from the last accessing task in the page flags as
well as the node information";1;1
"Collisions will occur but it is better than
just depending on the node information";1;1
"Node information is then used to
determine if a page needs to migrate";0;1
"The PID information is used to detect
private/shared accesses";1;1
"The preferred NUMA node is selected based on where
the maximum number of approximately private faults were measured";0;0
"Shared
faults are not taken into consideration for a few reasons";0;0
"First, if there are many tasks sharing the page then they'll all move
towards the same node";0;0
"The node will be compute overloaded and then
scheduled away later only to bounce back again";0;0
"Alternatively the shared
tasks would just bounce around nodes because the fault information is
effectively noise";0;0
"Either way accounting for shared faults the same as
private faults can result in lower performance overall";0;1
"The second reason is based on a hypothetical workload that has a small
number of very important, heavily accessed private pages but a large shared
array";0;0
"The shared array would dominate the number of faults and be selected
as a preferred node even though it's the wrong decision";0;1
"The third reason is that multiple threads in a process will race each
other to fault the shared page making the fault information unreliable.";0;0
mm: numa: Scan pages with elevated page_mapcount;1;0
"Currently automatic NUMA balancing is unable to distinguish between false
shared versus private pages except by ignoring pages with an elevated
page_mapcount entirely";0;1
"This avoids shared pages bouncing between the
nodes whose task is using them but that is ignored quite a lot of data";0;0
"This patch kicks away the training wheels in preparation for adding support
for identifying shared/private pages is now in place";1;1
"The ordering is so
that the impact of the shared/private detection can be easily measured";0;1
"Note
that the patch does not migrate shared, file-backed within vmas marked
VM_EXEC as these are generally shared library pages";1;1
"Migrating such pages
is not beneficial as there is an expectation they are read-shared between
caches and iTLB and iCache pressure is generally low.";0;0
mm: Close races between THP migration and PMD numa clearing;1;0
"THP migration uses the page lock to guard against parallel allocations
but there are cases like this still open
  Task A					Task B
  do_huge_pmd_numa_page				do_huge_pmd_numa_page
  lock_page
  mpol_misplaced == -1
  unlock_page
  goto clear_pmdnuma
						lock_page
						mpol_misplaced == 2
						migrate_misplaced_transhuge
  pmd = pmd_mknonnuma
  set_pmd_at
During hours of testing, one crashed with weird errors and while I have
no direct evidence, I suspect something like the race above happened";1;0
"This patch extends the page lock to being held until the pmd_numa is
cleared to prevent migration starting in parallel while the pmd_numa is
being cleared";0;1
"It also flushes the old pmd entry and orders pagetable
insertion before rmap insertion.";0;0
mm: avoid reinserting isolated balloon pages into LRU lists;1;1
"Isolated balloon pages can wrongly end up in LRU lists when
migrate_pages() finishes its round without draining all the isolated
page list";0;1
"The same issue can happen when reclaim_clean_pages_from_list() tries to
reclaim pages from an isolated page list, before migration, in the CMA
path";0;0
" Such balloon page leak opens a race window against LRU lists
shrinkers that leads us to the following kernel panic";0;1
"  BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
  IP: [<ffffffff810c2625>] shrink_page_list+0x24e/0x897
  PGD 3cda2067 PUD 3d713067 PMD 0
  Oops: 0000 [#1] SMP
  CPU: 0 PID: 340 Comm: kswapd0 Not tainted 3.12.0-rc1-22626-g4367597 #87
  Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
  RIP: shrink_page_list+0x24e/0x897
  RSP: 0000:ffff88003da499b8  EFLAGS: 00010286
  RAX: 0000000000000000 RBX: ffff88003e82bd60 RCX: 00000000000657d5
  RDX: 0000000000000000 RSI: 000000000000031f RDI: ffff88003e82bd40
  RBP: ffff88003da49ab0 R08: 0000000000000001 R09: 0000000081121a45
  R10: ffffffff81121a45 R11: ffff88003c4a9a28 R12: ffff88003e82bd40
  R13: ffff88003da0e800 R14: 0000000000000001 R15: ffff88003da49d58
  FS:  0000000000000000(0000) GS:ffff88003fc00000(0000) knlGS:0000000000000000
  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
  CR2: 00000000067d9000 CR3: 000000003ace5000 CR4: 00000000000407b0
  This patch fixes the issue, by assuring the proper tests are made at
putback_movable_pages() & reclaim_clean_pages_from_list() to avoid
isolated balloon pages being wrongly reinserted in LRU lists.";0;1
Merge git://git.kvack.org/~bcrl/aio-next;1;1
Pull aio changes from Ben LaHaise;1;1
" ""First off, sorry for this pull request being late in the merge window";1;0
  Al had raised a couple of concerns about 2 items in the series below;0;0
"  I addressed the first issue (the race introduced by Gu's use of
  mm_populate()), but he has not provided any further details on how he
  wants to rework the anon_inode.c changes (which were sent out months
  ago but have yet to be commented on)";0;1
"  The bulk of the changes have been sitting in the -next tree for a few
  months, with all the issues raised being addressed""
  aio: rcu_read_lock protection for new rcu_dereference calls
  aio: fix race in ring buffer page lookup introduced by page migration support
  aio: fix rcu sparse warnings introduced by ioctx table lookup patch
  aio: remove unnecessary debugging from aio_free_ring()
  aio: table lookup: verify ctx pointer
  staging/lustre: kiocb->ki_left is removed
  aio: fix error handling and rcu usage in ""convert the ioctx list to table lookup v3""
  aio: be defensive to ensure request batching is non-zero instead of BUG_ON()
  aio: convert the ioctx list to table lookup v3
  aio: double aio_max_nr in calculations
  aio: Kill ki_dtor
  aio: Kill ki_users
  aio: Kill unneeded kiocb members
  aio: Kill aio_rw_vect_retry()
  aio: Don't use ctx->tail unnecessarily
  aio: io_cancel() no longer returns the io_event
  aio: percpu ioctx refcount
  aio: percpu reqs_available
  aio: reqs_active -> reqs_available
  aio: fix build when migration is disabled";0;1
mm: vmscan: fix do_try_to_free_pages() livelock;1;1
"This patch is based on KOSAKI's work and I add a little more description,
please refer 
Currently, I found system can enter a state that there are lots of free
pages in a zone but only order-0 and order-1 pages which means the zone is
heavily fragmented, then high order allocation could make direct reclaim
path's long stall(ex, 60 seconds) especially in no swap and no compaciton
enviroment";1;1
" This problem happened on v3.4, but it seems issue still lives
in current tree, the reason is do_try_to_free_pages enter live lock";0;1
"kswapd will go to sleep if the zones have been fully scanned and are still
not balanced";1;0
" As kswapd thinks there's little point trying all over again
to avoid infinite loop";1;1
" Instead it changes order from high-order to
0-order because kswapd think order-0 is the most important";0;0
" Look at
73ce02e9 in detail";1;0
" If watermarks are ok, kswapd will go back to sleep
and may leave zone->all_unreclaimable =3D 0";0;0
" It assume high-order users
can still perform direct reclaim if they wish";1;0
"Direct reclaim continue to reclaim for a high order which is not a
COSTLY_ORDER without oom-killer until kswapd turn on
zone->all_unreclaimble= ";0;0
 This is because to avoid too early oom-kill;1;1
So it means direct_reclaim depends on kswapd to break this loop;1;1
"In worst case, direct-reclaim may continue to page reclaim forever when
kswapd sleeps forever until someone like watchdog detect and finally kill
the process";1;0
 As described in;1;0
"We can't turn on zone->all_unreclaimable from direct reclaim path because
direct reclaim path don't take any lock and this way is racy";0;1
" Thus this
patch removes zone->all_unreclaimable field completely and recalculates
zone reclaimable state every time";1;1
"Note: we can't take the idea that direct-reclaim see zone->pages_scanned
directly and kswapd continue to use zone->all_unreclaimable";0;1
" Because, it
is racy";0;1
" commit 929bea7c71 (vmscan: all_unreclaimable() use
zone->all_unreclaimable as a name) describes the detail.";1;0
mm: migrate: check movability of hugepage in unmap_and_move_huge_page();0;0
"Currently hugepage migration works well only for pmd-based hugepages
(mainly due to lack of testing,) so we had better not enable migration of
other levels of hugepages until we are ready for it";1;0
"Some users of hugepage migration (mbind, move_pages, and migrate_pages) do
page table walk and check pud/pmd_huge() there, so they are safe";0;1
" But the
other users (softoffline and memory hotremove) don't do this, so without
this patch they can try to migrate unexpected types of hugepages";1;1
"To prevent this, we introduce hugepage_migration_support() as an
architecture dependent check of whether hugepage are implemented on a pmd
basis or not";1;1
" And on some architecture multiple sizes of hugepages are
available, so hugepage_migration_support() also checks hugepage size.";0;0
mm: migrate: add hugepage migration code to move_pages();1;0
Extend move_pages() to handle vma with VM_HUGETLB set;1;0
" We will be able to
migrate hugepage with move_pages(2) after applying the enablement patch
which comes later in this series";0;1
"We avoid getting refcount on tail pages of hugepage, because unlike thp,
hugepage is not split and we need not care about races with splitting";0;1
And migration of larger (1GB for x86_64) hugepage are not enabled.;0;0
mm: soft-offline: use migrate_pages() instead of migrate_huge_page();1;0
"Currently migrate_huge_page() takes a pointer to a hugepage to be migrated
as an argument, instead of taking a pointer to the list of hugepages to be
migrated";1;1
" This behavior was introduced in commit 189ebff28 (""hugetlb";1;0
"simplify migrate_huge_page()""), and was OK because until now hugepage
migration is enabled only for soft-offlining which migrates only one
hugepage in a single call";1;1
"But the situation will change in the later patches in this series which
enable other users of page migration to support hugepage migration";0;0
" They
can kick migration for both of normal pages and hugepages in a single
call, so we need to go back to original implementation which uses linked
lists to collect the hugepages to be migrated";0;1
"With this patch, soft_offline_huge_page() switches to use migrate_pages(),
and migrate_huge_page() is not used any more";1;1
 So let's remove it.;1;0
mm: migrate: make core migration code aware of hugepage;1;0
"Currently hugepage migration is available only for soft offlining, but
it's also useful for some other users of page migration (clearly because
users of hugepage can enjoy the benefit of mempolicy and memory hotplug.)
So this patchset tries to extend such users to support hugepage migration";0;1
"The target of this patchset is to enable hugepage migration for NUMA
related system calls (migrate_pages(2), move_pages(2), and mbind(2)), and
memory hotplug";0;0
"This patchset does not add hugepage migration for memory compaction,
because users of memory compaction mainly expect to construct thp by
arranging raw pages, and there's little or no need to compact hugepages";0;1
"CMA, another user of page migration, can have benefit from hugepage
migration, but is not enabled to support it for now (just because of lack
of testing and expertise in CMA.)
Hugepage migration of non pmd-based hugepage (for example 1GB hugepage in
x86_64, or hugepages in architectures like ia64) is not enabled for now
(again, because of lack of testing.)
As for how these are achived, I extended the API (migrate_pages()) to
handle hugepage (with patch 1 and 2) and adjusted code of each caller to
check and collect movable hugepages (with patch 3-7)";1;1
" Remaining 2 patches
are kind of miscellaneous ones to avoid unexpected behavior";1;1
" Patch 8 is
about making sure that we only migrate pmd-based hugepages";1;1
" And patch 9
is about choosing appropriate zone for hugepage allocation";1;1
"My test is mainly functional one, simply kicking hugepage migration via
each entry point and confirm that migration is done correctly";1;0
" Test code
is available here";0;0
"  git://github.com/Naoya-Horiguchi/test_hugepage_migration_extension.git
And I always run libhugetlbfs test when changing hugetlbfs's code";0;1
" With
this patchset, no regression was found in the test";1;1
This patch (of 9);1;0
"Before enabling each user of page migration to support hugepage,
this patch enables the list of pages for migration to link not only
LRU pages, but also hugepages";1;1
"As a result, putback_movable_pages()
and migrate_pages() can handle both of LRU pages and hugepages.";0;1
fs/aio: Add support to aio ring pages migration;1;0
"As the aio job will pin the ring pages, that will lead to mem migrated
failed";0;1
"In order to fix this problem we use an anon inode to manage the aio ring
pages, and  setup the migratepage callback in the anon inode's address space, so
that when mem migrating the aio ring pages will be moved to other mem node safely.";0;1
mm: migration: add migrate_entry_wait_huge();1;1
"When we have a page fault for the address which is backed by a hugepage
under migration, the kernel can't wait correctly and do busy looping on
hugepage fault until the migration finishes";0;1
" As a result, users who try
to kick hugepage migration (via soft offlining, for example) occasionally
experience long delay or soft lockup";0;1
"This is because pte_offset_map_lock() can't get a correct migration entry
or a correct page table lock for hugepage";0;1
" This patch introduces
migration_entry_wait_huge() to solve this.";1;0
mm compaction: fix of improper cache flush in migration code;1;1
Page 'new' during MIGRATION can't be flushed with flush_cache_page();0;1
"Using flush_cache_page(vma, addr, pfn) is justified only if the page is
already placed in process page table, and that is done right after
flush_cache_page()";0;1
" But without it the arch function has no knowledge
of process PTE and does nothing";1;0
"Besides that, flush_cache_page() flushes an application cache page, but
the kernel has a different page virtual address and dirtied it";0;1
Replace it with flush_dcache_page(new) which is the proper usage;1;0
The old page is flushed in try_to_unmap_one() before migration;0;1
"This bug takes place in Sead3 board with M14Kc MIPS CPU without cache
aliasing (but Harvard arch - separate I and D cache) in tight memory
environment (128MB) each 1-3days on SOAK test";0;0
" It fails in cc1 during
kernel build (SIGILL, SIGBUS, SIGSEG) if CONFIG_COMPACTION is switched";1;0
mm: rewrite the comment over migrate_pages() more comprehensibly;1;1
"The comment over migrate_pages() looks quite weird, and makes it hard to
grasp what it is trying to say";0;1
 Rewrite it more comprehensibly.;1;1
mm/migrate: fix comment typo syncronous->synchronous;1;1
;0;0
mm: remove offlining arg to migrate_pages;1;1
"No functional change, but the only purpose of the offlining argument to
migrate_pages() etc, was to ensure that __unmap_and_move() could migrate a
KSM page for memory hotremove (which took ksm_thread_mutex) but not for
other callers";1;0
 Now all cases are safe, remove the arg.;1;1
ksm: enable KSM page migration;1;0
"Migration of KSM pages is now safe: remove the PageKsm restrictions from
mempolicy.c and migrate.c";1;1
"But keep PageKsm out of __unmap_and_move()'s anon_vma contortions, which
are irrelevant to KSM: it looks as if that code was preventing hotremove
migration of KSM pages, unless they happened to be in swapcache";1;0
"There is some question as to whether enforcing a NUMA mempolicy migration
ought to migrate KSM pages, mapped into entirely unrelated processes; but
moving page_mapcount > 1 is only permitted with MPOL_MF_MOVE_ALL anyway,
and it seems reasonable to assume that you wouldn't set MADV_MERGEABLE on
any area where this is a worry.";1;1
ksm: make KSM page migration possible;1;0
"KSM page migration is already supported in the case of memory hotremove,
which takes the ksm_thread_mutex across all its migrations to keep life
simple";1;0
"But the new KSM NUMA merge_across_nodes knob introduces a problem, when
it's set to non-default 0: if a KSM page is migrated to a different NUMA
node, how do we migrate its stable node to the right tree?  And what if
that collides with an existing stable node?
So far there's no provision for that, and this patch does not attempt to
deal with it either";1;1
" But how will I test a solution, when I don't know
how to hotremove memory?  The best answer is to enable KSM page migration
in all cases now, and test more common cases";1;1
" With THP and compaction
added since KSM came in, page migration is now mainstream, and it's a
shame that a KSM page can frustrate freeing a page block";0;0
"Without worrying about merge_across_nodes 0 for now, this patch gets KSM
page migration working reliably for default merge_across_nodes 1 (but
leave the patch enabling it until near the end of the series)";1;1
"It's much simpler than I'd originally imagined, and does not require an
additional tier of locking: page migration relies on the page lock, KSM
page reclaim relies on the page lock, the page lock is enough for KSM page
migration too";0;1
"Almost all the care has to be in get_ksm_page(): that's the function which
worries about when a stable node is stale and should be freed, now it also
has to worry about the KSM page being migrated";1;1
"The only new overhead is an additional put/get/lock/unlock_page when
stable_tree_search() arrives at a matching node: to make sure migration
respects the raised page count, and so does not migrate the page while
we're busy with it here";1;0
" That's probably avoidable, either by changing
internal interfaces from using kpage to stable_node, or by moving the
ksm_migrate_page() callsite into a page_freeze_refs() section (even if not
swapcache); but this works well, I've no urge to pull it apart now";1;1
"(Descents of the stable tree may pass through nodes whose KSM pages are
under migration: being unlocked, the raised page count does not prevent
that, nor need it: it's safe to memcmp against either old or new page.)
You might worry about mremap, and whether page migration's rmap_walk to
remove migration entries will find all the KSM locations where it inserted
earlier: that should already be handled, by the satisfyingly heavy hammer
of move_vma()'s call to ksm_madvise(,,,MADV_UNMERGEABLE,).";1;1
mm: rename page struct field helpers;1;0
"The function names page_xchg_last_nid(), page_last_nid() and
reset_page_last_nid() were judged to be inconsistent so rename them to a
struct_field_op style pattern";1;1
" As it looked jarring to have
reset_page_mapcount() and page_nid_reset_last() beside each other in
memmap_init_zone(), this patch also renames reset_page_mapcount() to
page_mapcount_reset()";1;1
" There are others like init_page_count() but as
it is used throughout the arch code a rename would likely cause more
conflicts than it is worth.";1;1
mm: numa: cleanup flow of transhuge page migration;0;1
"When correcting commit 04fa5d6a6547 (""mm: migrate: check page_count of
THP before migrating"") Hugh Dickins noted that the control flow for
transhuge migration was difficult to follow";0;1
" Unconditionally calling
put_page() in numamigrate_isolate_page() made the failure paths of both
migrate_misplaced_transhuge_page() and migrate_misplaced_page() more
complex that they should be";1;1
" Further, he was extremely wary that an
unlock_page() should ever happen after a put_page() even if the
put_page() should never be the final put_page";1;0
"Hugh implemented the following cleanup to simplify the path by calling
putback_lru_page() inside numamigrate_isolate_page() if it failed to
isolate and always calling unlock_page() within
migrate_misplaced_transhuge_page()";1;1
"There is no functional change after this patch is applied but the code
is easier to follow and unlock_page() always happens before put_page().";1;1
mm: numa: take THP into account when migrating pages for NUMA balancing;0;0
"Wanpeng Li pointed out that numamigrate_isolate_page() assumes that only
one base page is being migrated when in fact it can also be checking
THP";0;1
"The consequences are that a migration will be attempted when a target
node is nearly full and fail later";0;0
" It's unlikely to be user-visible
but it should be fixed";0;1
" While we are there, migrate_balanced_pgdat()
should treat nr_migrate_pages as an unsigned long as it is treated as a
watermark.";0;0
mm/hugetlb: set PTE as huge in hugetlb_change_protection and remove_migration_pte;1;0
"When setting a huge PTE, besides calling pte_mkhuge(), we also need to
call arch_make_huge_pte(), which we indeed do in make_huge_pte(), but we
forget to do in hugetlb_change_protection() and remove_migration_pte().";1;1
mm: migrate: check page_count of THP before migrating;1;1
"Hugh Dickins pointed out that migrate_misplaced_transhuge_page() does
not check page_count before migrating like base page migration and
khugepage";1;0
 He could not see why this was safe and he is right;0;1
"The potential impact of the bug is avoided due to the limitations of
NUMA balancing";0;0
" The page_mapcount() check ensures that only a single
address space is using this page and as THPs are typically private it
should not be possible for another address space to fault it in
parallel";0;1
" If the address space has one associated task then it's
difficult to have both a GUP pin and be referencing the page at the same
time";1;0
" If there are multiple tasks then a buggy scenario requires that
another thread be accessing the page while the direct IO is in flight";0;1
"This is dodgy behaviour as there is a possibility of corruption with or
without THP migration";0;1
" It would be
While we happen to be safe for the most part it is shoddy to depend on
such ""safety"" so this patch checks the page count similar to anonymous
pages";1;1
" Note that this does not mean that the page_mapcount() check can
go away";0;0
" If we were to remove the page_mapcount() check the the THP
would have to be unmapped from all referencing PTEs, replaced with
migration PTEs and restored properly afterwards.";1;0
mm,numa: fix update_mmu_cache_pmd call;1;1
"This build error is currently hidden by the fact that the x86
implementation of 'update_mmu_cache_pmd()' is a macro that doesn't use
its last argument, but commit b32967ff101a (""mm: numa: Add THP migration
for the NUMA working set scanning fault case"") introduced a call with
the wrong third argument";0;1
In the akpm tree, it causes this build error;1;1
  mm/migrate.c: In function 'migrate_misplaced_transhuge_page_put';0;1
"  mm/migrate.c:1666:2: error: incompatible type for argument 3 of 'update_mmu_cache_pmd'
  arch/x86/include/asm/pgtable.h:792:20: note: expected 'struct pmd_t *' but argument is of type 'pmd_t'
Fix it.";1;1
mm,migrate: use N_MEMORY instead N_HIGH_MEMORY;1;0
N_HIGH_MEMORY stands for the nodes that has normal or high memory;0;0
N_MEMORY stands for the nodes that has any memory;0;0
"The code here need to handle with the nodes which have memory, we should
use N_MEMORY instead.";1;1
mm: introduce putback_movable_pages();1;0
"The PATCH ""mm: introduce compaction and migration for virtio ballooned pages""
hacks around putback_lru_pages() in order to allow ballooned pages to be
re-inserted on balloon page list as if a ballooned page was like a LRU page";1;0
"As ballooned pages are not legitimate LRU pages, this patch introduces
putback_movable_pages() to properly cope with cases where the isolated
pageset contains ballooned pages and LRU pages, thus fixing the mentioned
inelegant hack around putback_lru_pages().";0;1
mm: introduce compaction and migration for ballooned pages;1;0
"Memory fragmentation introduced by ballooning might reduce significantly
the number of 2MB contiguous memory blocks that can be used within a guest,
thus imposing performance penalties associated with the reduced number of
transparent huge pages that could be used by the guest workload";0;1
"This patch introduces the helper functions as well as the necessary changes
to teach compaction and migration bits how to cope with pages which are
part of a guest memory balloon, in order to make them movable by memory
compaction procedures.";0;1
mm: adjust address_space_operations.migratepage() return code;0;1
"Memory fragmentation introduced by ballooning might reduce significantly
the number of 2MB contiguous memory blocks that can be used within a
guest, thus imposing performance penalties associated with the reduced
number of transparent huge pages that could be used by the guest workload";0;1
This patch-set follows the main idea discussed at 2012 LSFMMS session;1;0
"""Ballooning for transparent huge pages"" -- to introduce the required changes to the virtio_balloon driver, as well as
the changes to the core compaction & migration bits, in order to make
those subsystems aware of ballooned pages and allow memory balloon pages
become movable within a guest, thus avoiding the aforementioned
fragmentation issue
Following are numbers that prove this patch benefits on allowing
compaction to be more effective at memory ballooned guests";0;1
"Results for STRESS-HIGHALLOC benchmark, from Mel Gorman's mmtests suite,
running on a 4gB RAM KVM guest which was ballooning 512mB RAM in 64mB
chunks, at every minute (inflating/deflating), while test was running";0;1
"===BEGIN stress-highalloc
STRESS-HIGHALLOC
                 highalloc-3.7     highalloc-3.7
                     rc4-clean         rc4-patch
Pass 1          55.00 ( 0.00%)    62.00 ( 7.00%)
Pass 2          54.00 ( 0.00%)    62.00 ( 8.00%)
while Rested    75.00 ( 0.00%)    80.00 ( 5.00%)
MMTests Statistics: duration
           rc4-clean   rc4-patch
User         1207.59     1207.46
System       1300.55     1299.61
Elapsed      2273.72     2157.06
MMTests Statistics: vmstat
                          rc4-clean   rc4-patch
Page Ins                    3581516     2374368
Page Outs                  11148692    10410332
Swap Ins                         80          47
Swap Outs                      3641         476
Direct pages scanned          37978       33826
Kswapd pages scanned        1828245     1342869
Kswapd pages reclaimed      1710236     1304099
Direct pages reclaimed        32207       31005
Kswapd efficiency               93%         97%
Kswapd velocity             804.077     622.546
Direct efficiency               84%         91%
Direct velocity              16.703      15.682
Percentage direct scans          2%          2%
Page writes by reclaim        79252        9704
Page writes file              75611        9228
Page writes anon               3641         476
Page reclaim immediate        16764       11014
Page rescued immediate            0           0
Slabs scanned               2171904     2152448
Direct inode steals             385        2261
Kswapd inode steals          659137      609670
Kswapd skipped wait               1          69
THP fault alloc                 546         631
THP collapse alloc              361         339
THP splits                      259         263
THP fault fallback               98          50
THP collapse fail                20          17
Compaction stalls               747         499
Compaction success              244         145
Compaction failures             503         354
Compaction pages moved       370888      474837
Compaction move failure       77378       65259
===END stress-highalloc
This patch";0;1
"Introduce MIGRATEPAGE_SUCCESS as the default return code for
address_space_operations.migratepage() method and documents the expected
return code for the same method in failure cases.";1;1
mm: introduce mm_find_pmd();1;0
"Several place need to find the pmd by(mm_struct, address), so introduce a
function to simplify it.";1;1
mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable;1;0
"rmap_walk_anon() and try_to_unmap_anon() appears to be too
careful about locking the anon vma: while it needs protection
against anon vma list modifications, it does not need exclusive
access to the list itself";1;1
"Transforming this exclusive lock to a read-locked rwsem removes
a global lock from the hot path of page-migration intense
threaded workloads which can cause pathological performance like
this";0;0
"    96.43%        process 0  [kernel.kallsyms]  [k] perf_trace_sched_switch
                  --- perf_trace_sched_switch
                      __schedule
                      schedule
                      schedule_preempt_disabled
                      __mutex_lock_common.isra.6
                      __mutex_lock_slowpath
                      mutex_lock
                     |--50.61%-- rmap_walk
                     |          move_to_new_page
                     |          migrate_pages
                     |          migrate_misplaced_page
                     |          __do_numa_page.isra.69
                     |          handle_pte_fault
                     |          handle_mm_fault
                     |          __do_page_fault
                     |          do_page_fault
                     |          page_fault
                     |          __memset_sse2
                     |           --100.00%-- worker_thread
                     |                      --100.00%-- start_thread
                      --49.39%-- page_lock_anon_vma
                                try_to_unmap_anon
                                try_to_unmap
                                migrate_pages
                                migrate_misplaced_page
                                __do_numa_page.isra.69
                                handle_pte_fault
                                handle_mm_fault
                                __do_page_fault
                                do_page_fault
                                page_fault
                                __memset_sse2
                                 --100.00%-- worker_thread
                                           start_thread
With this change applied the profile is now nicely flat
and there's no anon-vma related scheduling/blocking";1;1
"Rename anon_vma_[un]lock() => anon_vma_[un]lock_write(),
to make it clearer that it's an exclusive write-lock in
that case - suggested by Rik van Riel.";0;1
mm: migrate: Account a transhuge page properly when rate limiting;1;1
"If there is excessive migration due to NUMA balancing it gets rate
limited";0;1
"It does this by counting the number of pages it has migrated
recently but counts a transhuge page as 1 page";0;1
Account for it properly.;1;1
mm: numa: Account for failed allocations and isolations as migration failures;1;1
Subject says it all;1;0
"Allocation failures and a failure to isolate should
be accounted as a migration failure";0;1
"This is partially another
difference between base page and transhuge page migration";0;1
"A base page
migration makes multiple attempts for these conditions before it would
be accounted for as a failure.";0;0
mm: numa: Add THP migration for the NUMA working set scanning fault case build fix;1;1
"Commit ""Add THP migration for the NUMA working set scanning fault case""
breaks the build because HPAGE_PMD_SHIFT and HPAGE_PMD_MASK defined to
explode without CONFIG_TRANSPARENT_HUGEPAGE";1;0
mm/migrate.c: In function 'migrate_misplaced_transhuge_page_put';0;1
"mm/migrate.c:1549: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1564: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1566: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1573: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1606: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1648: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
CONFIG_NUMA_BALANCING allows compilation without enabling transparent
hugepages, so define the dummy function for such a configuration and only
define migrate_misplaced_transhuge_page_put() when transparent hugepages
are enabled.";1;1
mm: numa: Add THP migration for the NUMA working set scanning fault case.;1;0
"Note: This is very heavily based on a patch from Peter Zijlstra with
	fixes from Ingo Molnar, Hugh Dickins and Johannes Weiner";0;1
" That patch
	put a lot of migration logic into mm/huge_memory.c where it does
	not belong";1;0
"This version puts tries to share some of the migration
	logic with migrate_misplaced_page";0;0
" However, it should be noted
	that now migrate.c is doing more with the pagetable manipulation
	than is preferred";0;1
"The end result is barely recognisable so as
	before, the signed-offs had to be removed but will be re-added if
	the original authors are ok with it";1;0
Add THP migration for the NUMA working set scanning fault case;1;0
It uses the page lock to serialize;0;0
"No migration pte dance is
necessary because the pte is already unmapped when we decide
to migrate.";0;1
mm: numa: migrate: Set last_nid on newly allocated page;0;0
Pass last_nid from misplaced page to newly allocated migration target page.;1;0
mm: numa: Rate limit setting of pte_numa if node is saturated;1;0
"If there are a large number of NUMA hinting faults and all of them
are resulting in migrations it may indicate that memory is just
bouncing uselessly around";0;1
"NUMA balancing cost is likely exceeding
any benefit from locality";0;1
"Rate limit the PTE updates if the node
is migration rate-limited";1;0
"As noted in the comments, this distorts
the NUMA faulting statistics.";1;1
mm: numa: Rate limit the amount of memory that is migrated between nodes;0;0
NOTE: This is very heavily based on similar logic in autonuma;0;0
"It should
	be signed off by Andrea but because there was no standalone
	patch and it's sufficiently different from what he did that
	the signed-off is omitted";1;1
Will be added back if requested;1;0
"If a large number of pages are misplaced then the memory bus can be
saturated just migrating pages between nodes";0;1
"This patch rate-limits
the amount of memory that can be migrating between nodes.";1;1
mm: numa: Add pte updates, hinting and migration stats;1;1
"It is tricky to quantify the basic cost of automatic NUMA placement in a
meaningful manner";0;0
"This patch adds some vmstats that can be used as part
of a basic costing model";1;1
"u    = basic unit = sizeof(void *)
Ca   = cost of struct page access = sizeof(struct page) / u
Cpte = Cost PTE access = Ca
Cupdate = Cost PTE update = (2 * Cpte) + (2 * Wlock)
	where Cpte is incurred twice for a read and a write and Wlock
	is a constant representing the cost of taking or releasing a
	lock
Cnumahint = Cost of a minor page fault = some high constant e.g";1;1
"1000
Cpagerw = Cost to read or write a full page = Ca + PAGE_SIZE/u
Ci = Cost of page isolation = Ca + Wi
	where Wi is a constant that should reflect the approximate cost
	of the locking operation
Cpagecopy = Cpagerw + (Cpagerw * Wnuma) + Ci + (Ci * Wnuma)
	where Wnuma is the approximate NUMA factor";1;1
1 is local;0;0
"1.2
	would imply that remote accesses are 20% more expensive
Balancing cost = Cpte * numa_pte_updates +
		Cnumahint * numa_hint_faults +
		Ci * numa_pages_migrated +
		Cpagecopy * numa_pages_migrated
Note that numa_pages_migrated is used as a measure of how many pages
were isolated even though it would miss pages that failed to migrate";0;1
"A
vmstat counter could have been added for it but the isolation cost is
pretty marginal in comparison to the overall cost so it seemed overkill";0;0
"The ideal way to measure automatic placement benefit would be to count
the number of remote accesses versus local accesses and do something like
	benefit = (remote_accesses_before - remove_access_after) * Wnuma
but the information is not readily available";1;0
"As a workload converges, the
expection would be that the number of remote numa hints would reduce to 0";0;1
"	convergence = numa_hint_faults_local / numa_hint_faults
		where this is measured for the last N number of
		numa hints recorded";0;1
"When the workload is fully
		converged the value is 1";0;0
"This can measure if the placement policy is converging and how fast it is
doing it.";1;1
mm: migrate: Drop the misplaced pages reference count if the target node is full;1;0
"If we have to avoid migrating to a node that is nearly full, put page
and return zero.";1;1
mm: migrate: Introduce migrate_misplaced_page();1;0
"Note: This was originally based on Peter's patch ""mm/migrate: Introduce
	migrate_misplaced_page()"" but borrows extremely heavily from Andrea's
	""autonuma: memory follows CPU algorithm and task/mm_autonuma stats
	collection""";1;1
"The end result is barely recognisable so signed-offs
	had to be dropped";0;1
"If original authors are ok with it, I'll
	re-add the signed-off-bys";1;1
"Add migrate_misplaced_page() which deals with migrating pages from
faults";1;0
"Based-on-work-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Based-on-work-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
Based-on-work-by: Andrea Arcangeli <aarcange@redhat.com>";0;0
mm: migrate: Add a tracepoint for migrate_pages;1;0
"The pgmigrate_success and pgmigrate_fail vmstat counters tells the user
about migration activity but not the type or the reason";0;1
"This patch adds
a tracepoint to identify the type of page migration and why the page is
being migrated.";1;1
mm: compaction: Move migration fail/success stats to migrate.c;0;1
"The compact_pages_moved and compact_pagemigrate_failed events are
convenient for determining if compaction is active and to what
degree migration is succeeding but it's at the wrong level";1;1
"Other
users of migration may also want to know if migration is working
properly and this will be particularly true for any automated
NUMA migration";0;1
"This patch moves the counters down to migration
with the new events called pgmigrate_success and pgmigrate_fail";1;1
"The compact_blocks_moved counter is removed because while it was
useful for debugging initially, it's worthless now as no meaningful
conclusions can be drawn from its value.";0;1
mm: memcg: fix compaction/migration failing due to memcg limits;1;1
"Compaction (and page migration in general) can currently be hindered
through pages being owned by memory cgroups that are at their limits and
unreclaimable";0;0
"The reason is that the replacement page is being charged against the limit
while the page being replaced is also still charged";0;0
" But this seems
unnecessary, given that only one of the two pages will still be in use
after migration finishes";1;1
"This patch changes the memcg migration sequence so that the replacement
page is not charged";1;1
" Whatever page is still in use after successful or
failed migration gets to keep the charge of the page that was going to be
replaced";0;0
"The replacement page will still show up temporarily in the rss/cache
statistics, this can be fixed in a later patch as it's less urgent.";1;1
hugetlb/cgroup: migrate hugetlb cgroup info from oldpage to new page during migration;1;0
"With HugeTLB pages, hugetlb cgroup is uncharged in compound page
destructor";0;0
" Since we are holding a hugepage reference, we can be sure
that old page won't get uncharged till the last put_page().";0;1
hugetlb: simplify migrate_huge_page();1;1
"Since we migrate only one hugepage, don't use linked list for passing the
page around";1;0
 Directly pass the page that need to be migrated as argument;1;1
This also removes the usage of page->lru in the migrate path.;1;1
mm: fix warning in __set_page_dirty_nobuffers;1;1
"New tmpfs use of !PageUptodate pages for fallocate() is triggering the
WARNING: at mm/page-writeback.c:1990 when __set_page_dirty_nobuffers()
is called from migrate_page_copy() for compaction";0;0
"It is anomalous that migration should use __set_page_dirty_nobuffers()
on an address_space that does not participate in dirty and writeback
accounting; and this has also been observed to insert surprising dirty
tags into a tmpfs radix_tree, despite tmpfs not using tags at all";1;0
"We should probably give migrate_page_copy() a better way to preserve the
tag and migrate accounting info, when mapping_cap_account_dirty()";1;1
" But
that needs some more work: so in the interim, avoid the warning by using
a simple SetPageDirty on PageSwapBacked pages";1;1
Reported-and-tested-by: Dave Jones <davej@redhat.com>;1;0
userns: Convert the move_pages, and migrate_pages permission checks to use uid_eq;1;0
;0;0
mm: fix NULL ptr dereference in move_pages;1;1
"Commit 3268c63 (""mm: fix move/migrate_pages() race on task struct"") has
added an odd construct where 'mm' is checked for being NULL, and if it is,
it would get dereferenced anyways by mput()ing it.";1;1
mm: fix move/migrate_pages() race on task struct;1;1
Migration functions perform the rcu_read_unlock too early;1;1
" As a result
the task pointed to may change from under us";0;1
" This can result in an oops,
as reported by Dave Hansen in 
The following patch extend the period of the rcu_read_lock until after the
permissions checks are done";1;0
" We also take a refcount so that the task
reference is stable when calling security check functions and performing
cpuset node validation (which takes a mutex)";1;0
"The refcount is dropped before actual page migration occurs so there is no
change to the refcounts held during page migration";1;1
"Also move the determination of the mm of the task struct to immediately
before the do_migrate*() calls so that it is clear that we switch from
handling the task during permission checks to the mm for the actual
migration";0;1
" Since the determination is only done once and we then no
longer use the task_struct we can be sure that we operate on a specific
address space that will not change from under us.";1;1
memcg: fix GPF when cgroup removal races with last exit;1;1
"When moving tasks from old memcg (with move_charge_at_immigrate on new
memcg), followed by removal of old memcg, hit General Protection Fault in
mem_cgroup_lru_del_list() (called from release_pages called from
free_pages_and_swap_cache from tlb_flush_mmu from tlb_finish_mmu from
exit_mmap from mmput from exit_mm from do_exit)";0;0
"Somewhat reproducible, takes a few hours: the old struct mem_cgroup has
been freed and poisoned by SLAB_DEBUG, but mem_cgroup_lru_del_list() is
still trying to update its stats, and take page off lru before freeing";1;1
"A task, or a charge, or a page on lru: each secures a memcg against
removal";0;1
" In this case, the last task has been moved out of the old memcg,
and it is exiting: anonymous pages are uncharged one by one from the
memcg, as they are zapped from its pagetables, so the charge gets down to
0; but the pages themselves are queued in an mmu_gather for freeing";0;0
"Most of those pages will be on lru (and force_empty is careful to
lru_add_drain_all, to add pages from pagevec to lru first), but not
necessarily all: perhaps some have been isolated for page reclaim, perhaps
some isolated for other reasons";1;1
" So, force_empty may find no task, no
charge and no page on lru, and let the removal proceed";1;1
"There would still be no problem if these pages were immediately freed; but
typically (and the put_page_testzero protocol demands it) they have to be
added back to lru before they are found freeable, then removed from lru
and freed";1;1
" We don't see the issue when adding, because the
mem_cgroup_iter() loops keep their own reference to the memcg being
scanned; but when it comes to mem_cgroup_lru_del_list()";1;0
"I believe this was not an issue in v3.2: there, PageCgroupAcctLRU and
PageCgroupUsed flags were used (like a trick with mirrors) to deflect view
of pc->mem_cgroup to the stable root_mem_cgroup when neither set";1;1
"38c5d72f3ebe (""memcg: simplify LRU handling by new rule"") mercifully
removed those convolutions, but left this General Protection Fault";1;1
"But it's surprisingly easy to restore the old behaviour: just check
PageCgroupUsed in mem_cgroup_lru_add_list() (which decides on which lruvec
to add), and reset pc to root_mem_cgroup if page is uncharged";1;0
" A risky
change?  just going back to how it worked before; testing, and an audit of
uses of pc->mem_cgroup, show no problem";0;1
"And there's a nice bonus: with mem_cgroup_lru_add_list() itself making
sure that an uncharged page goes to root lru, mem_cgroup_reset_owner() no
longer has any purpose, and we can safely revert 4e5f01c2b9b9 (""memcg";1;1
"clear pc->mem_cgroup if necessary"")";1;1
"Calling update_page_reclaim_stat() after add_page_to_lru_list() in swap.c
is not strictly necessary: the lru_lock there, with RCU before memcg
structures are freed, makes mem_cgroup_get_reclaim_stat_from_page safe
without that; but it seems cleaner to rely on one dependency less.";0;1
mm: postpone migrated page mapping reset;1;1
Postpone resetting page->mapping until the final remove_migration_ptes();1;0
"Otherwise the expression PageAnon(migration_entry_to_page(entry)) does not
work.";0;0
mm: compaction: introduce sync-light migration for use by compaction;1;0
"This patch adds a lightweight sync migrate operation MIGRATE_SYNC_LIGHT
mode that avoids writing back pages to backing storage";1;1
" Async compaction
maps to MIGRATE_ASYNC while sync compaction maps to MIGRATE_SYNC_LIGHT";0;1
"For other migrate_pages users such as memory hotplug, MIGRATE_SYNC is
used";0;1
"This avoids sync compaction stalling for an excessive length of time,
particularly when copying files to a USB stick where there might be a
large number of dirty pages backed by a filesystem that does not support
->writepages.";0;1
mm: compaction: determine if dirty pages can be migrated without blocking within ->migratepage;0;1
"Asynchronous compaction is used when allocating transparent hugepages to
avoid blocking for long periods of time";0;1
" Due to reports of stalling,
there was a debate on disabling synchronous compaction but this severely
impacted allocation success rates";0;0
" Part of the reason was that many dirty
	if (PageDirty(page) && !sync &&
		mapping->a_ops->migratepage != migrate_page)
This skips over all mapping aops using buffer_migrate_page() even though
it is possible to migrate some of these pages without blocking";0;1
" This
patch updates the ->migratepage callback with a ""sync"" parameter";1;1
" It is
the responsibility of the callback to fail gracefully if migration would
block.";1;0
memcg: clear pc->mem_cgroup if necessary.;1;1
"This is a preparation before removing a flag PCG_ACCT_LRU in page_cgroup
and reducing atomic ops/complexity in memcg LRU handling";1;1
"In some cases, pages are added to lru before charge to memcg and pages
are not classfied to memory cgroup at lru addtion";0;0
" Now, the lru where
the page should be added is determined a bit in page_cgroup->flags and
pc->mem_cgroup";1;0
 I'd like to remove the check of flag;1;0
"To handle the case pc->mem_cgroup may contain stale pointers if pages
are added to LRU before classification";1;0
" This patch resets
pc->mem_cgroup to root_mem_cgroup before lru additions.";1;1
mm/migrate.c: remove the unused macro lru_to_page;1;1
lru_to_page is not used in mm/migrate.c.;0;1
mm/migrate.c: cleanup comment for migration_entry_wait();1;1
migration_entry_wait() can also be called from hugetlb_fault() now;0;0
Remove the incorrect comment.;1;1
mm: migrate: one less atomic operation;0;1
"migrate_page_move_mapping() drops a reference from the old page after
unfreezing its counter";0;0
" Both operations can be merged into a single
atomic operation by directly unfreezing to one less reference";0;1
The same applies to migrate_huge_page_move_mapping().;0;1
mm/migrate.c: pair unlock_page() and lock_page() when migrating huge pages;1;1
Avoid unlocking and unlocked page if we failed to lock it.;1;1
mm: migration: clean up unmap_and_move();1;1
unmap_and_move() is one a big messy function;0;0
 Clean it up.;1;0
mm: Map most files to use export.h instead of module.h;1;0
"The files changed within are only using the EXPORT_SYMBOL
macro variants";0;0
" They are not using core modular infrastructure
and hence don't need module.h but only the export.h header.";0;1
mm: fix race between mremap and removing migration entry;1;1
"I don't usually pay much attention to the stale ""? "" addresses in
stack backtraces, but this lucky report from Pawel Sikora hints that
mremap's move_ptes() has inadequate locking against page migration";0;0
 3.0 BUG_ON(!PageLocked(p)) in migration_entry_to_page();0;0
" kernel BUG at include/linux/swapops.h:105!
 RIP: 0010:[<ffffffff81127b76>]  [<ffffffff81127b76>]
                       migration_entry_wait+0x156/0x160
  [<ffffffff811016a1>] handle_pte_fault+0xae1/0xaf0
  [<ffffffff810feee2>] ? __pte_alloc+0x42/0x120
  [<ffffffff8112c26b>] ? do_huge_pmd_anonymous_page+0xab/0x310
  [<ffffffff81102a31>] handle_mm_fault+0x181/0x310
  [<ffffffff81106097>] ? vma_adjust+0x537/0x570
  [<ffffffff81424bed>] do_page_fault+0x11d/0x4e0
  [<ffffffff81109a05>] ? do_mremap+0x2d5/0x570
  [<ffffffff81421d5f>] page_fault+0x1f/0x30
mremap's down_write of mmap_sem, together with i_mmap_mutex or lock,
and pagetable locks, were good enough before page migration (with its
requirement that every migration entry be found) came in, and enough
while migration always held mmap_sem; but not enough nowadays, when
there's memory hotremove and compaction";0;1
"The danger is that move_ptes() lets a migration entry dodge around
behind remove_migration_pte()'s back, so it's in the old location when
looking at the new, then in the new location when looking at the old";1;0
"Either mremap's move_ptes() must additionally take anon_vma lock(), or
migration's remove_migration_pte() must stop peeking for is_swap_entry()
before it takes pagetable lock";1;0
"Consensus chooses the latter: we prefer to add overhead to migration
than to mremapping, which gets used by JVMs and by exec stack setup";0;0
Reported-and-tested-by: Paweł Sikora <pluto@agmk.net>;1;0
migrate: don't account swapcache as shmem;1;1
"swapcache will reach the below code path in migrate_page_move_mapping,
and swapcache is accounted as NR_FILE_PAGES but it's not accounted as
NR_SHMEM";0;1
"Hugh pointed out we must use PageSwapCache instead of comparing
mapping to &swapper_space, to avoid build failure with CONFIG_SWAP=n.";1;1
mm: use refcounts for page_lock_anon_vma();1;0
Convert page_lock_anon_vma() over to use refcounts;1;0
" This is done to
prepare for the conversion of anon_vma from spinlock to mutex";1;0
"Sadly this inceases the cost of page_lock_anon_vma() from one to two
atomics, a follow up patch addresses this, lets keep that simple for now.";1;1
Fix common misspellings;1;1
Fixes generated by 'codespell' and manually reviewed.;0;1
memcg: fix ugly initialization of return value is in caller;1;1
Remove initialization of vaiable in caller of memory cgroup function;1;0
"Actually, it's return value of memcg function but it's initialized in
caller";0;1
"Some memory cgroup uses following style to bring the result of start
function to the end function for avoiding races";0;1
"   mem_cgroup_start_A(&(*ptr))
   mem_cgroup_end_A(*ptr)
In some calls, *ptr should be initialized to NULL be caller";0;0
" But it's
ugly";0;1
 This patch fixes that *ptr is initialized by _start function.;0;1
mm: compaction: Use async migration for __GFP_NO_KSWAPD and enforce no writeback;1;0
"__GFP_NO_KSWAPD allocations are usually very expensive and not mandatory
to succeed as they have graceful fallback";0;0
" Waiting for I/O in those,
tends to be overkill in terms of latencies, so we can reduce their latency
by disabling sync migrate";1;1
"Unfortunately, even with async migration it's still possible for the
process to be blocked waiting for a request slot (e.g";0;1
" get_request_wait
in the block layer) when ->writepage is called";0;0
" To prevent
__GFP_NO_KSWAPD blocking, this patch prevents ->writepage being called on
dirty page cache for asynchronous migration";0;1
Addresses ;0;0
mm: rename drop_anon_vma() to put_anon_vma();1;0
The normal code pattern used in the kernel is: get/put.;0;0
mm: add replace_page_cache_page() function;1;0
This function basically does;0;0
"Except it does this atomically, so there's no possibility for the ""add"" to
fail because of a race";1;1
"If memory cgroups are enabled, then the memory cgroup charge is also moved
from the old page to the new";1;0
"This function is currently used by fuse to move pages into the page cache
on read, instead of copying the page contents.";0;0
mm: grab rcu read lock in move_pages();1;0
"The move_pages() usage of find_task_by_vpid() requires rcu_read_lock() to
prevent free_pid() from reclaiming the pid";0;1
"Without this patch, RCU warnings are printed in v2.6.38-rc4 move_pages()
with";1;1
"  CONFIG_LOCKUP_DETECTOR=y
  CONFIG_PREEMPT=y
  CONFIG_LOCKDEP=y
  CONFIG_PROVE_LOCKING=y
  CONFIG_PROVE_RCU=y
Previously, migrate_pages() went through a similar transformation
replacing usage of tasklist_lock with rcu read lock";1;1
"  commit 55cfaa3cbdd29c4919ecb5fb8965c310f357e48c
  Author: Zeng Zhaoming <zengzm.kernel@gmail.com>
  Date:   Thu Dec 2 14:31:13 2010 -0800
      mm/mempolicy.c: add rcu read lock to protect pid structure
  commit 1e50df39f6e2c3a4a3394df62baa8a213df16c54
  Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
  Date:   Thu Jan 13 15:46:14 2011 -0800
      mempolicy: remove tasklist_lock from migrate_pages";1;0
mm/migration: fix page corruption during hugepage migration;1;1
"If migrate_huge_page by memory-failure fails , it calls put_page in itself
to decrease page reference and caller of migrate_huge_page also calls
putback_lru_pages";0;1
" It can do double free of page so it can make page
corruption on page holder";1;1
"In addtion, clean of pages on caller is consistent behavior with
migrate_pages by cf608ac19c (""mm: compaction: fix COMPACTPAGEFAILED
counting"").";0;1
mm: when migrate_pages returns 0, all pages must have been released;0;0
"In some cases migrate_pages could return zero while still leaving a few
pages in the pagelist (and some caller wouldn't notice it has to call
putback_lru_pages after commit cf608ac19c9 (""mm: compaction: fix
COMPACTPAGEFAILED counting""))";0;1
"Add one missing putback_lru_pages not added by commit cf608ac19c95 (""mm";1;0
"compaction: fix COMPACTPAGEFAILED counting"").";1;1
mm: migration: clarify migrate_pages() comment;1;1
"Callers of migrate_pages should putback_lru_pages to return pages
isolated to LRU or free list";0;1
 Now comment is rather confusing;0;1
" It says
caller always have to call it";1;1
"It is more clear to point out that the caller has to call it if
migrate_pages's return value isn't zero.";0;1
memcg: fix memory migration of shmem swapcache;1;1
"In the current implementation mem_cgroup_end_migration() decides whether
the page migration has succeeded or not by checking ""oldpage->mapping""";1;0
"But if we are tring to migrate a shmem swapcache, the page->mapping of it
is NULL from the begining, so the check would be invalid";0;1
" As a result,
mem_cgroup_end_migration() assumes the migration has succeeded even if
it's not, so ""newpage"" would be freed while it's not uncharged";0;1
"This patch fixes it by passing mem_cgroup_end_migration() the result of
the page migration.";1;1
mm: fix hugepage migration;1;1
"2.6.37 added an unmap_and_move_huge_page() for memory failure recovery,
but its anon_vma handling was still based around the 2.6.35 conventions";1;0
"Update it to use page_lock_anon_vma, get_anon_vma, page_unlock_anon_vma,
drop_anon_vma in the same way as we're now changing unmap_and_move()";1;1
"I don't particularly like to propose this for stable when I've not seen
its problems in practice nor tested the solution: but it's clearly out of
synch at present.";1;1
mm: fix migration hangs on anon_vma lock;1;1
"Increased usage of page migration in mmotm reveals that the anon_vma
locking in unmap_and_move() has been deficient since 2.6.36 (or even
earlier)";0;1
" Review at the time of f18194275c39835cb84563500995e0d503a32d9a
(""mm: fix hang on anon_vma->root->lock"") missed the issue here: the
anon_vma to which we get a reference may already have been freed back to
its slab (it is in use when we check page_mapped, but that can change),
and so its anon_vma->root may be switched at any moment by reuse in
anon_vma_prepare";1;1
"Perhaps we could fix that with a get_anon_vma_unless_zero(), but let's
not: just rely on page_lock_anon_vma() to do all the hard thinking for us,
then we don't need any rcu read locking over here";1;1
"In removing the rcu_unlock label: since PageAnon is a bit in
page->mapping, it's impossible for a !page->mapping page to be anon; but
insert VM_BUG_ON in case the implementation ever changes.";1;1
mm: migration: use rcu_dereference_protected when dereferencing the radix tree slot during file page migration;1;0
"migrate_pages() -> unmap_and_move() only calls rcu_read_lock() for
anonymous pages, as introduced by git commit
989f89c57e6361e7d16fbd9572b5da7d313b073d (""fix rcu_read_lock() in page
migraton"")";1;1
" The point of the RCU protection there is part of getting a
stable reference to anon_vma and is only held for anon pages as file pages
are locked which is sufficient protection against freeing";1;1
"However, while a file page's mapping is being migrated, the radix tree is
double checked to ensure it is the expected page";1;1
" This uses
radix_tree_deref_slot() -> rcu_dereference() without the RCU lock held
triggering the following warning";1;1
 stack backtrace;1;0
"This patch introduces radix_tree_deref_slot_protected() which calls
rcu_dereference_protected()";1;1
" Users of it must pass in the
mapping->tree_lock that is protecting this dereference";1;0
" Holding the tree
lock protects against parallel updaters of the radix tree meaning that
rcu_dereference_protected is allowable.";1;0
thp: pmd_trans_huge migrate bugcheck;1;0
"No pmd_trans_huge should ever materialize in migration ptes areas, because
we split the hugepage before migration ptes are instantiated.";1;0
mm: migration: cleanup migrate_pages API by matching types for offlining and sync;0;1
"With the introduction of the boolean sync parameter, the API looks a
little inconsistent as offlining is still an int";1;1
" Convert offlining to a
bool for the sake of being tidy.";0;0
mm: migration: allow migration to operate asynchronously and avoid synchronous compaction in the faster path;1;0
Migration synchronously waits for writeback if the initial passes fails;1;0
"Callers of memory compaction do not necessarily want this behaviour if the
caller is latency sensitive or expects that synchronous migration is not
going to have a significantly better success rate";1;1
"This patch adds a sync parameter to migrate_pages() allowing the caller to
indicate if wait_on_page_writeback() is allowed within migration or not";1;1
"For reclaim/compaction, try_to_compact_pages() is first called
asynchronously, direct reclaim runs and then try_to_compact_pages() is
called synchronously as there is a greater expectation that it'll succeed.";0;0
mm: vmscan: reclaim order-0 and use compaction instead of lumpy reclaim;0;0
Lumpy reclaim is disruptive;0;0
" It reclaims a large number of pages and
ignores the age of the pages it reclaims";0;1
" This can incur significant
stalls and potentially increase the number of major faults";0;1
"Compaction has reached the point where it is considered reasonably stable
(meaning it has passed a lot of testing) and is a potential candidate for
displacing lumpy reclaim";1;1
" This patch introduces an alternative to lumpy
reclaim whe compaction is available called reclaim/compaction";0;0
" The basic
operation is very simple - instead of selecting a contiguous range of
pages to reclaim, a number of order-0 pages are reclaimed and then
compaction is later by either kswapd (compact_zone_order()) or direct
compaction (__alloc_pages_direct_compact()).";0;0
mm/migrate.c: fix compilation error;1;1
GCC complained about update_mmu_cache() not being defined in migrate.c;0;1
Including <asm/tlbflush.h> seems to solve the problem.;0;1
mm: fix error reporting in move_pages() syscall;1;1
"The vma returned by find_vma does not necessarily include the target
address";1;1
" If this happens the code tries to follow a page outside of any
vma and returns ENOENT instead of EFAULT.";0;0
mm: compaction: fix COMPACTPAGEFAILED counting;1;1
Presently update_nr_listpages() doesn't have a role;0;1
" That's because lists
passed is always empty just after calling migrate_pages";1;1
" The
migrate_pages cleans up page list which have failed to migrate before
returning by aaa994b3";0;0
" [PATCH] page migration: handle freeing of pages in migrate_pages()
 Do not leave pages on the lists passed to migrate_pages()";1;1
" Seems that we will
 not need any postprocessing of pages";0;1
" This will simplify the handling of
 pages by the callers of migrate_pages()";1;1
At that time, we thought we don't need any postprocessing of pages;0;1
" But
the situation is changed";0;1
" The compaction need to know the number of
failed to migrate for COMPACTPAGEFAILED stat
This patch makes new rule for caller of migrate_pages to call
putback_lru_pages";1;1
" So caller need to clean up the lists so it has a
chance to postprocess the pages";1;1
 [suggested by Christoph Lameter];0;0
writeback: remove nonblocking/encountered_congestion references;1;1
"This removes more dead code that was somehow missed by commit 0d99519efef
(writeback: remove unused nonblocking and congestion checks)";1;1
" There are
no behavior change except for the removal of two entries from one of the
ext4 tracing interface";1;1
"The nonblocking checks in ->writepages are no longer used because the
flusher now prefer to block on get_request_wait() than to skip inodes on
IO congestion";0;0
 The latter will lead to more seeky IO;0;1
"The nonblocking checks in ->writepage are no longer used because it's
redundant with the WB_SYNC_NONE check";0;1
"We no long set ->nonblocking in VM page out and page migration, because
a) it's effectively redundant with WB_SYNC_NONE in current code
b) it's old semantic of ""Don't get stuck on request queues"" is mis-behavior";0;1
"   that would skip some dirty inodes on congestion and page out others, which
   is unfair in terms of LRU age";0;1
Inspired by Christoph Hellwig;0;0
Thanks!;0;0
Fix migration.c compilation on s390;1;1
31bit s390 doesn't have huge pages and failed with;0;0
> mm/migrate.c: In function 'remove_migration_pte';0;1
"> mm/migrate.c:143:3: error: implicit declaration of function 'pte_mkhuge'
> mm/migrate.c:143:7: error: incompatible types when assigning to type 'pte_t' from type 'int'
Put that code into a ifdef";0;0
Reported by Heiko Carstens;0;0
hugetlb: hugepage migration core;1;0
This patch extends page migration code to support hugepage migration;1;1
"One of the potential users of this feature is soft offlining which
is triggered by memory corrected errors (added by the next patch.)
Todo";0;0
"- there are other users of page migration such as memory policy,
  memory hotplug and memocy compaction";1;1
  They are not ready for hugepage support for now;0;0
ChangeLog since v4;0;1
"- define migrate_huge_pages()
- remove changes on isolation/putback_lru_page()
ChangeLog since v2";1;1
"- refactor isolate/putback_lru_page() to handle hugepage
- add comment about race on unmap_and_move_huge_page()
ChangeLog since v1";1;1
"- divide migration code path for hugepage
- define routine checking migration swap entry for hugetlb";1;1
mm: extend KSM refcounts to the anon_vma root;1;0
"KSM reference counts can cause an anon_vma to exist after the processe it
belongs to have already exited";0;1
" Because the anon_vma lock now lives in
the root anon_vma, we need to ensure that the root anon_vma stays around
until after all the ""child"" anon_vmas have been freed";1;0
"The obvious way to do this is to have a ""child"" anon_vma take a reference
to the root in anon_vma_fork";1;0
" When the anon_vma is freed at munmap or
process exit, we drop the refcount in anon_vma_unlink and possibly free
the root anon_vma";0;1
"The KSM anon_vma reference count function also needs to be modified to
deal with the possibility of freeing 2 levels of anon_vma";1;1
" The easiest
way to do this is to break out the KSM magic and make it generic";1;1
When compiling without CONFIG_KSM, this code is compiled out.;1;0
mm: always lock the root (oldest) anon_vma;1;0
"Always (and only) lock the root (oldest) anon_vma whenever we do something
in an anon_vma";1;0
" The recently introduced anon_vma scalability is due to
the rmap code scanning only the VMAs that need to be scanned";0;1
" Many common
operations still took the anon_vma lock on the root anon_vma, so always
taking that lock is not expected to introduce any scalability issues";1;1
"However, always taking the same lock does mean we only need to take one
lock, which means rmap_walk on pages from any anon_vma in the vma is
excluded from occurring during an munmap, expand_stack or other operation
that needs to exclude rmap_walk and similar functions";0;1
Also add the proper locking to vma_adjust.;1;1
mm: change direct call of spin_lock(anon_vma->lock) to inline function;0;0
"Subsitute a direct call of spin_lock(anon_vma->lock) with an inline
function doing exactly the same";0;1
"This makes it easier to do the substitution to the root anon_vma lock in a
following patch";1;1
"We will deal with the handful of special locks (nested, dec_and_lock, etc)
separately.";0;1
memcg: fix mis-accounting of file mapped racy with migration;1;1
"FILE_MAPPED per memcg of migrated file cache is not properly updated,
because our hook in page_add_file_rmap() can't know to which memcg
FILE_MAPPED should be counted";1;1
"Basically, this patch is for fixing the bug but includes some big changes
to fix up other messes";1;1
Now, at migrating mapped file, events happen in following sequence;1;0
 1;0;0
allocate a new page;0;0
 2;1;0
get memcg of an old page;1;0
 3;1;0
charge ageinst a new page before migration;0;0
"But at this point,
    no changes to new page's page_cgroup, no commit for the charge";1;1
"    (IOW, PCG_USED bit is not set.)
 4";0;0
page migration replaces radix-tree, old-page and new-page;1;0
 5;1;0
page migration remaps the new page if the old page was mapped;1;1
 6;1;1
Here, the new page is unlocked;1;0
 7;0;0
"memcg commits the charge for newpage, Mark the new page's page_cgroup
    as PCG_USED";1;0
"Because ""commit"" happens after page-remap, we can count FILE_MAPPED
at ""5"", because we should avoid to trust page_cgroup->mem_cgroup";0;1
if PCG_USED bit is unset;1;0
"(Note: memcg's LRU removal code does that but LRU-isolation logic is used
 for helping it";1;0
"When we overwrite page_cgroup->mem_cgroup, page_cgroup is
 not on LRU or page_cgroup->mem_cgroup is NULL.)
We can lose file_mapped accounting information at 5 because FILE_MAPPED
is updated only when mapcount changes 0->1";1;1
So we should catch it;1;1
"BTW, historically, above implemntation comes from migration-failure
of anonymous page";1;1
"Because we charge both of old page and new page
with mapcount=0, we can't catch
  - the page is really freed before remap";0;1
"  - migration fails but it's freed before remap
or .....corner cases";0;0
New migration sequence with memcg is;1;0
 1;0;0
allocate a new page;0;0
 2;1;0
mark PageCgroupMigration to the old page;1;0
 3;1;0
charge against a new page onto the old page's memcg;0;0
"(here, new page's pc
    is marked as PageCgroupUsed.)
 4";1;1
page migration replaces radix-tree, page table, etc..;1;0
 5;1;0
"At remapping, new page's page_cgroup is now makrked as ""USED""
    We can catch 0->1 event and FILE_MAPPED will be properly updated";1;0
"    And we can catch SWAPOUT event after unlock this and freeing this
    page by unmap() can be caught";1;1
 7;0;0
Clear PageCgroupMigration of the old page;0;0
So, FILE_MAPPED will be correctly updated;1;1
"Then, for what MIGRATION flag is ?
  Without it, at migration failure, we may have to charge old page again
  because it may be fully unmapped";0;1
"""charge"" means that we have to dive into
  memory reclaim or something complated";0;1
"So, it's better to avoid
  charge it again";1;1
"Before this patch, __commit_charge() was working for
  both of the old/new page and fixed up all";0;0
"But this technique has some
  racy condtion around FILE_MAPPED and SWAPOUT etc..";1;1
"  Now, the kernel use MIGRATION flag and don't uncharge old page until
  the end of migration";1;0
I hope this change will make memcg's page migration much simpler;1;1
" This
page migration has caused several troubles";0;1
" Worth to add a flag for
simplification.";1;0
mm: compaction: memory compaction core;0;0
"This patch is the core of a mechanism which compacts memory in a zone by
relocating movable pages towards the end of the zone";0;0
A single compaction run involves a migration scanner and a free scanner;0;1
Both scanners operate on pageblock-sized areas in the zone;1;0
" The migration
scanner starts at the bottom of the zone and searches for all movable
pages within each area, isolating them onto a private list called
migratelist";0;1
" The free scanner starts at the top of the zone and searches
for suitable areas and consumes the free pages within making them
available for the migration scanner";0;1
" The pages isolated for migration are
then migrated to the newly isolated free pages.";0;0
mm: migration: allow the migration of PageSwapCache pages;1;0
"PageAnon pages that are unmapped may or may not have an anon_vma so are
not currently migrated";0;0
" However, a swap cache page can be migrated and
fits this description";0;1
" This patch identifies page swap caches and allows
them to be migrated but ensures that no attempt to made to remap the pages
would would potentially try to access an already freed anon_vma.";0;1
mm: migration: do not try to migrate unmapped anonymous pages;1;1
"rmap_walk_anon() was triggering errors in memory compaction that look like
use-after-free errors";1;0
" The problem is that between the page being
isolated from the LRU and rcu_read_lock() being taken, the mapcount of the
page dropped to 0 and the anon_vma gets freed";0;0
" This can happen during
memory compaction if pages being migrated belong to a process that exits
before migration completes";0;0
" Hence, the use-after-free race looks like
 1";1;0
"Page isolated for migration
 2";1;0
"Process exits
 3";1;1
"page_mapcount(page) drops to zero so anon_vma was no longer reliable
 4";1;0
"unmap_and_move() takes the rcu_lock but the anon_vma is already garbage
 4";1;0
"call try_to_unmap, looks up tha anon_vma and ""locks"" it but the lock
    is garbage";1;1
This patch checks the mapcount after the rcu lock is taken;1;1
" If the
mapcount is zero, the anon_vma is assumed to be freed and no further
action is taken.";1;1
mm: migration: share the anon_vma ref counts between KSM and page migration;1;0
"For clarity of review, KSM and page migration have separate refcounts on
the anon_vma";1;0
 While clear, this is a waste of memory;0;1
" This patch gets
KSM and page migration to share their toys in a spirit of harmony.";1;0
mm: migration: take a reference to the anon_vma before migrating;1;0
"This patchset is a memory compaction mechanism that reduces external
fragmentation memory by moving GFP_MOVABLE pages to a fewer number of
pageblocks";0;1
" The term ""compaction"" was chosen as there are is a number of
mechanisms that are not mutually exclusive that can be used to defragment
memory";0;1
" For example, lumpy reclaim is a form of defragmentation as was
slub ""defragmentation"" (really a form of targeted reclaim)";0;1
" Hence, this
is called ""compaction"" to distinguish it from other forms of
defragmentation";0;1
"In this implementation, a full compaction run involves two scanners
operating within a zone - a migration and a free scanner";1;1
" The migration
scanner starts at the beginning of a zone and finds all movable pages
within one pageblock_nr_pages-sized area and isolates them on a
migratepages list";0;0
" The free scanner begins at the end of the zone and
searches on a per-area basis for enough free pages to migrate all the
pages on the migratepages list";0;0
" As each area is respectively migrated or
exhausted of free pages, the scanners are advanced one area";0;0
" A compaction
run completes within a zone when the two scanners meet";0;1
"This method is a bit primitive but is easy to understand and greater
sophistication would require maintenance of counters on a per-pageblock
basis";0;1
" This would have a big impact on allocator fast-paths to improve
compaction which is a poor trade-off";0;1
"It also does not try relocate virtually contiguous pages to be physically
contiguous";1;1
" However, assuming transparent hugepages were in use, a
hypothetical khugepaged might reuse compaction code to isolate free pages,
split them and relocate userspace pages for promotion";0;0
Memory compaction can be triggered in one of three ways;0;0
" It may be
triggered explicitly by writing any value to /proc/sys/vm/compact_memory
and compacting all of memory";1;0
" It can be triggered on a per-node basis by
writing any value to /sys/devices/system/node/nodeN/compact where N is the
node ID to be compacted";0;0
" When a process fails to allocate a high-order
page, it may compact memory in an attempt to satisfy the allocation
instead of entering direct reclaim";1;0
" Explicit compaction does not finish
until the two scanners meet and direct compaction ends if a suitable page
becomes available that would meet watermarks";0;0
The series is in 14 patches;0;0
" The first three are not ""core"" to the series
but are important pre-requisites";0;0
Patch 1 reference counts anon_vma for rmap_walk_anon();1;0
"Without this
	patch, it's possible to use anon_vma after free if the caller is
	not holding a VMA or mmap_sem for the pages in question";1;1
"While
	there should be no existing user that causes this problem,
	it's a requirement for memory compaction to be stable";0;1
"The patch
	is at the start of the series for bisection reasons";1;1
Patch 2 merges the KSM and migrate counts;1;1
"It could be merged with patch 1
	but would be slightly harder to review";0;0
"Patch 3 skips over unmapped anon pages during migration as there are no
	guarantees about the anon_vma existing";1;0
"There is a window between
	when a page was isolated and migration started during which anon_vma
	could disappear";1;1
"Patch 4 notes that PageSwapCache pages can still be migrated even if they
	are unmapped";1;1
"Patch 5 allows CONFIG_MIGRATION to be set without CONFIG_NUMA
Patch 6 exports a ""unusable free space index"" via debugfs";1;1
"It's
	a measure of external fragmentation that takes the size of the
	allocation request into account";1;0
"It can also be calculated from
	userspace so can be dropped if requested
Patch 7 exports a ""fragmentation index"" which only has meaning when an
	allocation request fails";1;1
"It determines if an allocation failure
	would be due to a lack of memory or external fragmentation";0;1
"Patch 8 moves the definition for LRU isolation modes for use by compaction
Patch 9 is the compaction mechanism although it's unreachable at this point
Patch 10 adds a means of compacting all of memory with a proc trgger
Patch 11 adds a means of compacting a specific node with a sysfs trigger
Patch 12 adds ""direct compaction"" before ""direct reclaim"" if it is
	determined there is a good chance of success";1;1
"Patch 13 adds a sysctl that allows tuning of the threshold at which the
	kernel will compact or direct reclaim
Patch 14 temporarily disables compaction if an allocation failure occurs
	after compaction";1;1
Testing of compaction was in three stages;0;0
" For the test, debugging,
preempt, the sleep watchdog and lockdep were all enabled but nothing nasty
popped out";0;1
" min_free_kbytes was tuned as recommended by hugeadm to help
fragmentation avoidance and high-order allocations";0;0
" It was tested on X86,
X86-64 and PPC64";1;0
"Ths first test represents one of the easiest cases that can be faced for
lumpy reclaim or memory compaction";0;0
1;0;0
"Machine freshly booted and configured for hugepage usage with
	a) hugeadm --create-global-mounts
	b) hugeadm --pool-pages-max DEFAULT:8G
	c) hugeadm --set-recommended-min_free_kbytes
	d) hugeadm --set-recommended-shmmax
	The min_free_kbytes here is important";0;0
"Anti-fragmentation works best
	when pageblocks don't mix";0;1
"hugeadm knows how to calculate a value that
	will significantly reduce the worst of external-fragmentation-related
	events as reported by the mm_page_alloc_extfrag tracepoint";1;1
2;1;0
"Load up memory
	a) Start updatedb
	b) Create in parallel a X files of pagesize*128 in size";1;0
"Wait
	   until files are created";1;0
"By parallel, I mean that 4096 instances
	   of dd were launched, one after the other using &";0;0
"The crude
	   objective being to mix filesystem metadata allocations with
	   the buffer cache";0;0
"	c) Delete every second file so that pageblocks are likely to
	   have holes
	d) kill updatedb if it's still running
	At this point, the system is quiet, memory is full but it's full with
	clean filesystem metadata and clean buffer cache that is unmapped";0;1
"	This is readily migrated or discarded so you'd expect lumpy reclaim
	to have no significant advantage over compaction but this is at
	the POC stage";0;1
3;1;0
In increments, attempt to allocate 5% of memory as hugepages;0;0
"	   Measure how long it took, how successful it was, how many
	   direct reclaims took place and how how many compactions";1;1
"Note
	   the compaction figures might not fully add up as compactions
	   can take place for orders other than the hugepage size
X86				vanilla		compaction
Final page count                    913                916 (attempted 1002)
pages reclaimed                   68296               9791
X86-64				vanilla		compaction
Final page count:                   901                902 (attempted 1002)
Total pages reclaimed:           112599              53234
PPC64				vanilla		compaction
Final page count:                    93                 94 (attempted 110)
Total pages reclaimed:           103216              61838
There was not a dramatic improvement in success rates but it wouldn't be
expected in this case either";0;1
" What was important is that fewer pages were
reclaimed in all cases reducing the amount of IO required to satisfy a
huge page allocation";0;0
"The second tests were all performance related - kernbench, netperf, iozone
and sysbench";1;1
 None showed anything too remarkable;1;0
The last test was a high-order allocation stress test;0;0
" Many kernel
compiles are started to fill memory with a pressured mix of unmovable and
movable allocations";0;0
" During this, an attempt is made to allocate 90% of
memory as huge pages - one at a time with small delays between attempts to
avoid flooding the IO queue";0;0
"                                             vanilla   compaction
Percentage of request allocated X86               98           99
Percentage of request allocated X86-64            95           98
Percentage of request allocated PPC64             55           70
This patch";0;1
"rmap_walk_anon() does not use page_lock_anon_vma() for looking up and
locking an anon_vma and it does not appear to have sufficient locking to
ensure the anon_vma does not disappear from under it";1;1
"This patch copies an approach used by KSM to take a reference on the
anon_vma while pages are being migrated";1;0
" This should prevent rmap_walk()
running into nasty surprises later because anon_vma has been freed.";1;1
mm: remove return value of putback_lru_pages();1;1
putback_lru_page() never can fail;1;1
" So it doesn't matter count of ""the
number of pages put back""";1;1
In addition, users of this functions don't use return value;0;1
Let's remove unnecessary code.;1;1
include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h;1;1
"percpu.h is included by sched.h and module.h and thus ends up being
included when building most .c files";0;1
" percpu.h includes slab.h which
in turn includes gfp.h making everything defined by the two files
universally available and complicating inclusion dependencies";0;1
percpu.h -> slab.h dependency is about to be removed;0;1
" Prepare for
this change by updating users of gfp and slab facilities include those
headers directly instead of assuming availability";1;1
" As this conversion
needs to touch large number of source files, the following script is
used as the basis of conversion";1;1
The script does the followings;1;0
  only the necessary includes are there;1;1
 ie;0;0
"if only gfp is used,
  gfp.h, if slab is used, slab.h";1;0
"  blocks and try to put the new include such that its order conforms
  to its surrounding";1;1
" It's put in the include block which contains
  core kernel includes, in the same order that the rest are ordered -
  alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
  doesn't seem to be any matching order";1;0
"  because the file doesn't have fitting include block), it prints out
  an error message indicating which .h file needs to be added to the
  file";1;1
The conversion was done in the following steps;1;0
1;0;0
"The initial automatic conversion of all .c files updated slightly
   over 4000 files, deleting around 700 includes and adding ~480 gfp.h
   and ~3000 slab.h inclusions";1;0
" The script emitted errors for ~400
   files";1;0
2;1;0
Each error was manually checked;1;0
" Some didn't need the inclusion,
   some needed manual addition while adding it to implementation .h or
   embedding .c file was more appropriate for others";1;1
" This step added
   inclusions to around 150 files";1;0
3;1;0
"The script was run again and the output was compared to the edits
   from #2 to make sure no file was left behind";1;1
4;0;0
Several build tests were done and a couple of problems were fixed;1;1
   e.g;0;0
"lib/decompress_*.c used malloc/free() wrappers around slab
   APIs requiring slab.h to be added manually";1;0
5;1;0
"The script was run on all .h files but without automatically
   editing them as sprinkling gfp.h and slab.h inclusions around .h
   files could easily lead to inclusion dependency hell";1;1
" Most gfp.h
   inclusion directives were ignored as stuff from gfp.h was usually
   wildly available and often used in preprocessor macros";1;1
" Each
   slab.h inclusion directive was examined and added manually as
   necessary";1;0
6;1;1
percpu.h was updated not to include slab.h;1;0
7;0;0
"Build test were done on the following configurations and failures
   were fixed";1;1
" CONFIG_GCOV_KERNEL was turned off for all tests (as my
   distributed build env didn't work with gcov compiles) and a few
   more options had to be turned off depending on archs to make things
   build (like ipr on powerpc/64 which failed due to missing writeq)";1;1
8;0;0
"percpu.h modifications were reverted so that it could be applied as
   a separate patch and serve as bisection point";1;1
"Given the fact that I had only a couple of failures from tests on step
6, I'm fairly confident about the coverage of this conversion patch";1;1
"If there is a breakage, it's likely to be something in one of the arch
headers which should be easily discoverable easily on most builds of
the specific arch.";1;0
mm/migrate.c: kill anon local variable from migrate_page_copy;0;0
"commit 01b1ae63c2 (""memcg: simple migration handling"") removed
mem_cgroup_uncharge_cache_page() call from migrate_page_copy";1;1
" Local
variable `anon' is now unused.";0;1
mm: Make copy_from_user() in migrate.c statically predictable;1;1
x86-32 has had a static test for copy_on_user() overflow for a while;0;0
"This test currently fails in mm/migrate.c resulting in an
allyesconfig/allmodconfig build failure on x86-32";0;1
"In function ‘copy_from_user’,
    inlined from ‘do_pages_stat’ at
    /home/hpa/kernel/git/mm/migrate.c:1012";0;1
/home/hpa/kernel/git/arch/x86/include/asm/uaccess_32.h:212: error;1;1
"    call to ‘copy_from_user_overflow’ declared
Make the logic more explicit and therefore easier for gcc to
understand";0;1
"v2: rewrite the loop entirely using a more normal structure for a
    chunked-data loop (Linus Torvalds)";1;0
MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself;1;1
"On VIVT ARM, when we have multiple shared mappings of the same file
in the same MM, we need to ensure that we have coherency across all
copies";0;1
" We do this via make_coherent() by making the pages
uncacheable";1;1
"This used to work fine, until we allowed highmem with highpte - we
now have a page table which is mapped as required, and is not available
for modification via update_mmu_cache()";0;1
"Ralf Beache suggested getting rid of the PTE value passed to
update_mmu_cache()";0;0
"  On MIPS update_mmu_cache() calls __update_tlb() which walks pagetables
  to construct a pointer to the pte again";0;1
" Passing a pte_t * is much
  more elegant";0;1
" Maybe we might even replace the pte argument with the
  pte_t?
Ben Herrenschmidt would also like the pte pointer for PowerPC";1;1
  Passing the ptep in there is exactly what I want;1;0
" I want that
  -instead- of the PTE value, because I have issue on some ppc cases,
  for I$/D$ coherency, where set_pte_at() may decide to mask out the
  _PAGE_EXEC";1;0
"So, pass in the mapped page table pointer into update_mmu_cache(), and
remove the PTE value, updating all implementations and call sites to
suit";1;1
Includes a fix from Stephen Rothwell;1;1
  sparc: fix fallout from update_mmu_cache API change;1;1
Fix potential crash with sys_move_pages;1;0
"We incorrectly depended on the 'node_state/node_isset()' functions
testing the node range, rather than checking it explicitly";0;0
" That's not
reliable, even if it might often happen to work";0;1
" So do the proper
explicit test.";0;1
mm: remove unevictable_migrate_page function;1;0
"unevictable_migrate_page() in mm/internal.h is a relic of the since
removed UNEVICTABLE_LRU Kconfig option";1;1
" This patch removes the function
and open codes the test in migrate_page_copy().";1;1
ksm: memory hotremove migration only;1;1
"The previous patch enables page migration of ksm pages, but that soon gets
into trouble: not surprising, since we're using the ksm page lock to lock
operations on its stable_node, but page migration switches the page whose
lock is to be used for that";0;0
" Another layer of locking would fix it, but
do we need that yet?
Do we actually need page migration of ksm pages?  Yes, memory hotremove
needs to offline sections of memory: and since we stopped allocating ksm
pages with GFP_HIGHUSER, they will tend to be GFP_HIGHUSER_MOVABLE
candidates for migration";1;1
"But KSM is currently unconscious of NUMA issues, happily merging pages
from different NUMA nodes: at present the rule must be, not to use
MADV_MERGEABLE where you care about NUMA";1;1
" So no, NUMA page migration of
ksm pages does not make sense yet";1;0
So, to complete support for ksm swapping we need to make hotremove safe;1;1
"ksm_memory_callback() take ksm_thread_mutex when MEM_GOING_OFFLINE and
release it when MEM_OFFLINE or MEM_CANCEL_OFFLINE";1;1
" But if mapped pages
are freed before migration reaches them, stable_nodes may be left still
pointing to struct pages which have been removed from the system: the
stable_node needs to identify a page by pfn rather than page pointer, then
it can safely prune them when MEM_OFFLINE";0;1
And make NUMA migration skip PageKsm pages where it skips PageReserved;1;1
"But it's only when we reach unmap_and_move() that the page lock is taken
and we can be sure that raised pagecount has prevented a PageAnon from
being upgraded: so add offlining arg to migrate_pages(), to migrate ksm
page when offlining (has sufficient locking) but reject it otherwise.";0;1
ksm: rmap_walk to remove_migation_ptes;1;0
"A side-effect of making ksm pages swappable is that they have to be placed
on the LRUs: which then exposes them to isolate_lru_page() and hence to
page migration";0;1
"Add rmap_walk() for remove_migration_ptes() to use: rmap_walk_anon() and
rmap_walk_file() in rmap.c, but rmap_walk_ksm() in ksm.c";1;0
" Perhaps some
consolidation with existing code is possible, but don't attempt that yet
(try_to_unmap needs to handle nonlinears, but migration pte removal does
not)";1;1
"rmap_walk() is sadly less general than it appears: rmap_walk_anon(), like
remove_anon_migration_ptes() which it replaces, avoids calling
page_lock_anon_vma(), because that includes a page_mapped() test which
fails when all migration ptes are in place";1;1
" That was valid when NUMA page
migration was introduced (holding mmap_sem provided the missing guarantee
that anon_vma's slab had not already been destroyed), but I believe not
valid in the memory hotremove case added since";0;0
"For now do the same as before, and consider the best way to fix that
unlikely race later on";1;1
" When fixed, we can probably use rmap_walk() on
hwpoisoned ksm pages too: for now, they remain among hwpoison's various
exceptions (its PageKsm test comes before the page is locked, but its
page_lock_anon_vma fails safely if an anon gets upgraded).";1;1
mm: define PAGE_MAPPING_FLAGS;1;0
"At present we define PageAnon(page) by the low PAGE_MAPPING_ANON bit set
in page->mapping, with the higher bits a pointer to the anon_vma; and have
defined PageKsm(page) as that with NULL anon_vma";1;0
"But KSM swapping will need to store a pointer there: so in preparation for
that, now define PAGE_MAPPING_FLAGS as the low two bits, including
PAGE_MAPPING_KSM (always set along with PAGE_MAPPING_ANON, until some
other use for the bit emerges)";1;1
"Declare page_rmapping(page) to return the pointer part of page->mapping,
and page_anon_vma(page) to return the anon_vma pointer when that's what it
is";0;1
" Use these in a few appropriate places: notably, unuse_vma() has been
testing page->mapping, but is better to be testing page_anon_vma() (cases
may be added in which flag bits are set without any pointer).";1;1
mm: move inc_zone_page_state(NR_ISOLATED) to just isolated place;1;1
"Christoph pointed out inc_zone_page_state(NR_ISOLATED) should be placed
in right after isolate_page()";0;1
This patch does it.;1;0
mm: Adjust do_pages_stat() so gcc can see copy_from_user() is safe;0;1
"Slightly adjust the logic for determining the size of the
copy_form_user() in do_pages_stat(); with this change, gcc can see
that the copying is safe";0;1
Without this, we get a build error for i386 allyesconfig;1;1
/home/hpa/kernel/linux-2.6-tip.urgent/arch/x86/include/asm/uaccess_32.h:213;1;0
"error: call to ‘copy_from_user_overflow’ declared with attribute
error: copy_from_user() buffer size is not provably correct
Unlike an earlier patch from Arjan, this doesn't introduce new
variables; merely reshuffles the compare so that gcc can see that an
overflow cannot happen.";1;1
memcg: fix wrong pointer initialization at page migration when memcg is disabled.;1;1
"Lee Schermerhorn reported that he saw bad pointer dereference in
mem_cgroup_end_migration() when he disabled memcg by boot option";0;1
"memcg's page migration logic works as
	do page migration
Now, ptr is not initialized in prepare_migration when memcg is disabled
by boot option";0;1
This causes panic in end_migration;1;1
This patch fixes it.;1;1
mm: return boolean from page_has_private();0;1
"Make page_has_private() return a true boolean value and remove the double
negations from the two callsites using it for arithmetic.";1;1
mm: return boolean from page_is_file_cache();1;0
"page_is_file_cache() has been used for both boolean checks and LRU
arithmetic, which was always a bit weird";0;1
"Now that page_lru_base_type() exists for LRU arithmetic, make
page_is_file_cache() a real predicate function and adjust the
boolean-using callsites to drop those pesky double negations.";1;1
mm: vmstat: add isolate pages;1;1
"If the system is running a heavy load of processes then concurrent reclaim
can isolate a large number of pages from the LRU";0;1
"/proc/vmstat and the
output generated for an OOM do not show how many pages were isolated";1;0
This has been observed during process fork bomb testing (mstctl11 in LTP);0;0
This patch shows the information about isolated pages;1;1
Reproduced via;0;1
"% ./hackbench 140 process 1000
   => OOM occur
active_anon:146 inactive_anon:0 isolated_anon:49245
 active_file:79 inactive_file:18 isolated_file:113
 unevictable:0 dirty:0 writeback:0 unstable:0 buffer:39
 free:370 slab_reclaimable:309 slab_unreclaimable:5492
 mapped:53 shmem:15 pagetables:28140 bounce:0";1;1
mm: oom analysis: add shmem vmstat;1;1
Recently we encountered OOM problems due to memory use of the GEM cache;0;1
"Generally a large amuont of Shmem/Tmpfs pages tend to create a memory
shortage problem";0;0
"We often use the following calculation to determine the amount of shmem
pages";0;1
"shmem = NR_ACTIVE_ANON + NR_INACTIVE_ANON - NR_ANON_PAGES
however the expression does not consider isolated and mlocked pages";1;0
This patch adds explicit accounting for pages used by shmem and tmpfs.;1;1
memory hotplug: migrate swap cache page;1;0
In test, some pages in swap-cache can't be migrated, as they aren't rmap;0;1
"unmap_and_move() ignores swap-cache page which is just read in and hasn't
rmap (see the comments in the code), but swap_aops provides .migratepage";1;1
Better to migrate such pages instead of ignore them.;1;1
HWPOISON: Use bitmask/action code for try_to_unmap behaviour;0;0
"try_to_unmap currently has multiple modi (migration, munlock, normal unmap)
which are selected by magic flag variables";0;0
"The logic is not very straight
forward, because each of these flag change multiple behaviours (e.g";0;1
"migration turns off aging, not only sets up migration ptes etc.)
Also the different flags interact in magic ways";1;1
"A later patch in this series adds another mode to try_to_unmap, so
this becomes quickly unmanageable";1;1
and some additional flags as modifiers (ignore mlock, ignore aging);0;0
"This makes the logic more straight forward and allows easier extension
to new behaviours";0;1
"Change all the caller to declare what they want to
This patch is supposed to be a nop in behaviour";1;1
"If anyone can prove
it is not that would be a bug.";0;1
migration: only migrate_prep() once per move_pages();1;0
migrate_prep() is fairly expensive (72us on 16-core barcelona 1.9GHz);0;0
"Commit 3140a2273009c01c27d316f35ab76a37e105fdd8 improved move_pages()
throughput by breaking it into chunks, but it also made migrate_prep() be
called once per chunk (every 128pages or so) instead of once per
move_pages()";1;0
"This patch reverts to calling migrate_prep() only once per chunk as we did
before 2.6.29";1;0
" It is also a followup to commit
0aedadf91a70a11c4a3e7c7d99b21e5528af8d5d (""mm: move migrate_prep out from
under mmap_sem"")";0;0
"This improves migration throughput on the above machine from 600MB/s to
750MB/s.";0;0
page allocator: do not check NUMA node ID when the caller knows the node is valid;0;1
"Callers of alloc_pages_node() can optionally specify -1 as a node to mean
""allocate from the current node""";0;0
" However, a number of the callers in
fast paths know for a fact their node is valid";0;1
" To avoid a comparison and
branch, this patch adds alloc_pages_exact_node() that only checks the nid
with VM_BUG_ON()";1;1
" Callers that know their node is valid are then
converted.";0;0
FS-Cache: Recruit a page flags for cache management;1;0
Recruit a page flag to aid in cache management;1;1
" The following extra flag is
defined";1;1
" (1) PG_fscache (PG_private_2)
     The marked page is backed by a local cache and is pinning resources in the
     cache driver";0;1
"If PG_fscache is set, then things that checked for PG_private will now also
check for that";1;1
 This includes things like truncation and page invalidation;1;0
"The function page_has_private() had been added to make the checks for both
PG_private and PG_private_2 at the same time.";0;1
"migration: migrate_vmas should check ""vma""";1;1
"migrate_vmas() should check ""vma"" not ""vma->vm_next"" for for-loop condition.";0;1
[CVE-2009-0029] System call wrappers part 28;1;0
;0;0
memcg: simple migration handling;1;1
"Now, management of ""charge"" under page migration is done under following
manner";0;0
"(Assume migrate page contents from oldpage to newpage)
 before
  - ""newpage"" is charged before migration";0;1
 at success;1;0
"  - ""oldpage"" is uncharged at somewhere(unmap, radix-tree-replace)
 at failure
  - ""newpage"" is uncharged";1;1
But (*1) is not reliable....because of GFP_ATOMIC;0;0
This patch tries to change behavior as following by charge/commit/cancel ops;1;0
" before
  - charge PAGE_SIZE (no target page)
 success
  - commit charge against ""newpage""";0;0
" failure
  - commit charge against ""oldpage""";1;0
"    (PCG_USED bit works effectively to avoid double-counting)
  - if ""oldpage"" is obsolete, cancel charge of PAGE_SIZE.";1;1
memcg: introduce charge-commit-cancel style of functions;1;0
There is a small race in do_swap_page();1;0
" When the page swapped-in is
charged, the mapcount can be greater than 0";0;1
" But, at the same time some
process (shares it ) call unmap and make mapcount 1->0 and the page is
uncharged";0;0
"      CPUA 			CPUB
       mapcount == 1";0;0
                                (2) mapcount 1 => 0;0;0
			        (3) uncharge();0;0
"(success)
   (4) set page's rmap()
       mapcount 0=>1
Then, this swap page's account is leaked";1;0
For fixing this, I added a new interface;1;0
"  - charge
   account to res_counter by PAGE_SIZE and try to free pages if necessary";1;0
"  - commit
   register page_cgroup and add to LRU if necessary";1;0
"  - cancel
   uncharge PAGE_SIZE because of do_swap_page failure";1;1
"     CPUA
  (1) charge (always)
  (2) set page's rmap (mapcount > 0)
  (3) commit charge was necessary or not after set_pte()";0;0
This protocol uses PCG_USED bit on page_cgroup for avoiding over accounting;0;0
Usual mem_cgroup_charge_common() does charge -> commit at a time;1;0
And this patch also adds following function to clarify all charges;1;1
"  - mem_cgroup_newpage_charge() ....replacement for mem_cgroup_charge()
	called against newly allocated anon pages";0;0
"  - mem_cgroup_charge_migrate_fixup()
        called only from remove_migration_ptes()";1;0
"	we'll have to rewrite this later.(this patch just keeps old behavior)
	This function will be removed by additional patch to make migration
	clearer";1;1
"Good for clarifying ""what we do""
Then, we have 4 following charge points";1;0
"  - newpage
  - swap-in
  - add-to-cache";1;0
  - migration.;0;0
mm: add Set,ClearPageSwapCache stubs;1;0
"If we add NOOP stubs for SetPageSwapCache() and ClearPageSwapCache(), then
we can remove the #ifdef CONFIG_SWAPs from mm/migrate.c.";1;1
mm: move_pages: no need to set pp->page to ZERO_PAGE(0) by default;0;0
"pp->page is never used when not set to the right page, so there is no need
to set it to ZERO_PAGE(0) by default.";0;0
mm: rework do_pages_move() to work on page_sized chunks;1;0
"Rework do_pages_move() to work by page-sized chunks of struct page_to_node
that are passed to do_move_page_to_node_array()";0;1
" We now only have to
allocate a single page instead a possibly very large vmalloc area to store
all page_to_node entries";0;1
"As a result, new_page_node() will now have a very small lookup, hidding
much of the overall sys_move_pages() overhead.";0;0
mm: Don't touch uninitialized variable in do_pages_stat_array();0;1
"Commit 80bba1290ab5122c60cdb73332b26d288dc8aedd removed one necessary
variable initialization";1;1
 As a result following warning happened;1;0
"    CC      mm/migrate.o
  mm/migrate.c: In function 'sys_move_pages'";0;0
"  mm/migrate.c:1001: warning: 'err' may be used uninitialized in this function
More unfortunately, if find_vma() failed, kernel read uninitialized
memory.";1;1
mm: no get_user/put_user while holding mmap_sem in do_pages_stat?;1;0
"Since commit 2f007e74bb85b9fc4eab28524052161703300f1a, do_pages_stat()
gets the page address from user-space and puts the corresponding status
back while holding the mmap_sem for read";0;0
" There is no need to hold
mmap_sem there while some page-faults may occur";0;1
"This patch adds a temporary address and status buffer so as to only
hold mmap_sem while working on these kernel buffers";1;1
" This is
implemented by extracting do_pages_stat_array() out of do_pages_stat().";0;1
migration: fix writepage error;1;1
"Page migration's writeout() has got understandably confused by the nasty
AOP_WRITEPAGE_ACTIVATE case: as in normal success, a writepage() error has
unlocked the page, so writeout() then needs to relock it.";1;0
CRED: Use RCU to access another task's creds and to release a task's own creds;1;0
Use RCU to access another task's creds and to release a task's own creds;1;0
"This means that it will be possible for the credentials of a task to be
replaced without another task (a) requiring a full lock to read them, and (b)
seeing deallocated memory.";1;1
CRED: Separate task security context from task_struct;1;0
Separate the task security context from task_struct;1;0
" At this point, the
security data is temporarily embedded in the task_struct with two pointers
pointing to it";1;1
"Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
entry.S via asm-offsets";1;0
With comment fixes ;1;1
CRED: Wrap task credential accesses in the core kernel;1;0
"Wrap access to task credentials so that they can be separated more easily from
the task_struct during the introduction of COW creds";1;1
Change most current->(|e|s|fs)[ug]id to current_(|e|s|fs)[ug]id();1;0
Change some task->e?[ug]id to task_e?[ug]id();0;0
" In some places it makes more
sense to use RCU directly rather than a convenient wrapper; these will be
addressed by later patches.";1;1
mm: move migrate_prep out from under mmap_sem;1;0
"Move the migrate_prep outside the mmap_sem for the following system calls
1";0;0
"sys_move_pages
2";1;0
"sys_migrate_pages
3";1;0
"sys_mbind()
It really does not matter when we flush the lru";0;1
" The system is free to
add pages onto the lru even during migration which will make the page
migration either skip the page (mbind, migrate_pages) or return a busy
state (move_pages)";1;0
Fixes this lockdep warning (and potential deadlock);0;1
"Some VM place has
      mmap_sem -> kevent_wq via lru_add_drain_all()
net/core/dev.c::dev_ioctl()  has
     rtnl_lock  ->  mmap_sem        (*) the ioctl has copy_from_user() and it can do page fault";0;0
"linkwatch_event has
     kevent_wq -> rtnl_lock";0;0
memcg: make page->mapping NULL before uncharge;0;0
"This patch tries to make page->mapping to be NULL before
mem_cgroup_uncharge_cache_page() is called";0;1
"""page->mapping == NULL"" is a good check for ""whether the page is still
radix-tree or not""";0;0
 This patch also adds BUG_ON() to;1;1
mm: extract do_pages_move() out of sys_move_pages();1;0
"To prepare the chunking, move the sys_move_pages() code that is used when
nodes!=NULL into do_pages_move()";0;0
" And rename do_move_pages() into
do_move_page_to_node_array().";1;0
mm: don't vmalloc a huge page_to_node array for do_pages_stat();1;0
do_pages_stat() does not need any page_to_node entry for real;0;1
" Just pass
the pointers to the user-space page address array and to the user-space
status array, and have do_pages_stat() traverse the former and fill the
latter directly.";1;1
mm: stop returning -ENOENT from sys_move_pages() if nothing got migrated;0;0
A patchset reworking sys_move_pages();1;0
" It removes the possibly large
vmalloc by using multiple chunks when migrating large buffers";1;1
" It also
dramatically increases the throughput for large buffers since the lookup
in new_page_node() is now limited to a single chunk, causing the quadratic
complexity to have a much slower impact";0;1
" There is no need to use any
radix-tree-like structure to improve this lookup";1;1
"sys_move_pages() duration on a 4-quadcore-opteron 2347HE (1.9Gz),
migrating between nodes #2 and #3";1;0
"	length		move_pages (us)		move_pages+patch (us)
	4kB		126			98
	40kB		198			168
	400kB		963			937
	4MB		12503			11930
	40MB		246867			11848
Patches #1 and #4 are the important ones";1;1
"1) stop returning -ENOENT from sys_move_pages() if nothing got migrated
2) don't vmalloc a huge page_to_node array for do_pages_stat()
3) extract do_pages_move() out of sys_move_pages()
4) rework do_pages_move() to work on page_sized chunks
5) move_pages: no need to set pp->page to ZERO_PAGE(0) by default
This patch";0;0
"There is no point in returning -ENOENT from sys_move_pages() if all pages
were already on the right node, while we return 0 if only 1 page was not";0;1
"Most application don't know where their pages are allocated, so it's not
an error to try to migrate them anyway";0;0
"Just return 0 and let the status array in user-space be checked if the
application needs details";1;0
It will make the upcoming chunked-move_pages() support much easier.;1;1
mlock: mlocked pages are unevictable;1;0
"Make sure that mlocked pages also live on the unevictable LRU, so kswapd
will not scan them over and over again";1;0
This is achieved through various strategies;1;1
"1) add yet another page flag--PG_mlocked--to indicate that
   the page is locked for efficient testing in vmscan and,
   optionally, fault path";1;1
" This allows early culling of
   unevictable pages, preventing them from getting to
   page_referenced()/try_to_unmap()";0;0
" Also allows separate
   accounting of mlock'd pages, as Nick's original patch
   did";1;0
"   Note:  Nick's original mlock patch used a PG_mlocked
   flag";1;0
" I had removed this in favor of the PG_unevictable
   flag + an mlock_count [new page struct member]";1;1
" I
   restored the PG_mlocked flag to eliminate the new
   count field";1;0
"2) add the mlock/unevictable infrastructure to mm/mlock.c,
   with internal APIs in mm/internal.h";1;1
" This is a rework
   of Nick's original patch to these files, taking into
   account that mlocked pages are now kept on unevictable
   LRU list";1;1
"3) update vmscan.c:page_evictable() to check PageMlocked()
   and, if vma passed in, the vm_flags";1;1
" Note that the vma
   and then only if the ""cull unevictable pages in fault
   path"" patch is included";0;0
"4) add try_to_unlock() to rmap.c to walk a page's rmap and
   ClearPageMlocked() if no other vmas have it mlocked";1;1
   Reuses as much of try_to_unmap() as possible;0;0
" This
   effectively replaces the use of one of the lru list links
   as an mlock count";1;0
" If this mechanism let's pages in mlocked
   vmas leak through w/o PG_mlocked set [I don't know that it
   does], we should catch them later in try_to_unmap()";1;1
" One
   hopes this will be rare, as it will be relatively expensive";1;0
Original mm/internal.h, mm/rmap.c and mm/mlock.c changes:;1;1
Unevictable LRU Infrastructure;1;1
"When the system contains lots of mlocked or otherwise unevictable pages,
the pageout code (kswapd) can spend lots of time scanning over these
pages";1;1
" Worse still, the presence of lots of unevictable pages can confuse
kswapd into thinking that more aggressive pageout modes are required,
resulting in all kinds of bad behaviour";0;0
"Infrastructure to manage pages excluded from reclaim--i.e., hidden from
vmscan";0;0
 Based on a patch by Larry Woodman of Red Hat;0;0
" Reworked to
maintain ""unevictable"" pages on a separate per-zone LRU list, to ""hide""
them from vmscan";1;0
"Kosaki Motohiro added the support for the memory controller unevictable
lru list";1;0
Pages on the unevictable list have both PG_unevictable and PG_lru set;0;0
"Thus, PG_unevictable is analogous to and mutually exclusive with
PG_active--it specifies which LRU list the page is on";1;0
"The unevictable infrastructure is enabled by a new mm Kconfig option
[CONFIG_]UNEVICTABLE_LRU";1;1
"A new function 'page_evictable(page, vma)' in vmscan.c tests whether or
not a page may be evictable";0;1
" Subsequent patches will add the various
!evictable tests";1;1
" We'll want to keep these tests light-weight for use in
shrink_active_list() and, possibly, the fault path";1;1
"To avoid races between tasks putting pages [back] onto an LRU list and
tasks that might be moving the page from non-evictable to evictable state,
the new function 'putback_lru_page()' -- inverse to 'isolate_lru_page()'
-- tests the ""evictability"" of a page after placing it on the LRU, before
dropping the reference";1;1
" If the page has become unevictable,
putback_lru_page() will redo the 'putback', thus moving the page to the
unevictable list";1;1
" This way, we avoid ""stranding"" evictable pages on the
unevictable list.";1;1
define page_file_cache() function;0;0
Define page_file_cache() function to answer the question;0;1
"	is page backed by a file?
Originally part of Rik van Riel's split-lru patch";1;1
" Extracted to make
available for other, independent reclaim patches";0;0
"Moved inline function to linux/mm_inline.h where it will be needed by
subsequent ""split LRU"" and ""noreclaim"" patches";1;1
"Unfortunately this needs to use a page flag, since the PG_swapbacked state
needs to be preserved all the way to the point where the page is last
removed from the LRU";1;1
" Trying to derive the status from other info in the
page resulted in wrong VM statistics in earlier split VM patchsets";0;0
"The total number of page flags in use on a 32 bit machine after this patch
is 19.";0;1
swap: use an array for the LRU pagevecs;1;1
Turn the pagevecs into an array just like the LRUs;1;0
" This significantly
cleans up the source code and reduces the size of the kernel by about 13kB
after all the LRU lists have been created further down in the split VM
patch series.";1;0
vmscan: move isolate_lru_page() to vmscan.c;1;1
"On large memory systems, the VM can spend way too much time scanning
through pages that it cannot (or should not) evict from memory";0;1
" Not only
does it use up CPU time, but it also provokes lock contention and can
leave large systems under memory presure in a catatonic state";0;1
This patch series improves VM scalability by;0;0
"1) putting filesystem backed, swap backed and unevictable pages
   onto their own LRUs, so the system only scans the pages that it
   can/should evict from memory
2) switching to two handed clock replacement for the anonymous LRUs,
   so the number of pages that need to be scanned when the system
   starts swapping is bound to a reasonable number
3) keeping unevictable pages off the LRU completely, so the
   VM does not waste CPU time scanning them";1;1
"ramfs, ramdisk,
   SHM_LOCKED shared memory segments and mlock()ed VMA pages
   are keept on the unevictable list";1;0
This patch;0;0
isolate_lru_page logically belongs to be in vmscan.c than migrate.c;0;1
"It is tough, because we don't need that function without memory migration
so there is a valid argument to have it in migrate.c";1;1
" However a
subsequent patch needs to make use of it in the core mm, so we can happily
move it to vmscan.c";1;1
"Also, make the function a little more generic by not requiring that it
adds an isolated page to a given list";1;1
 Callers can do that;0;1
"	Note that we now have '__isolate_lru_page()', that does
	something quite different, visible outside of vmscan.c
	for use with memory controller";1;0
" Methinks we need to
	rationalize these names/purposes";0;1
--lts;0;0
mm: rename page trylock;1;0
"Converting page lock to new locking bitops requires a change of page flag
operation naming, so we might as well convert it to something nicer
(!TestSetPageLocked_Lock => trylock_page, SetPageLocked => set_page_locked)";1;1
This also facilitates lockdeping of page lock.;1;1
mm: spinlock tree_lock;1;1
mapping->tree_lock has no read lockers;1;1
" convert the lock from an rwlock
to a spinlock.";0;0
mm: speculative page references;1;1
"If we can be sure that elevating the page_count on a pagecache page will
pin it, we can speculatively run this operation, and subsequently check to
see if we hit the right page rather than relying on holding a lock or
otherwise pinning a reference to the page";0;1
"This can be done if get_page/put_page behaves consistently throughout the
whole tree (ie";1;1
" if we ""get"" the page after it has been used for something
else, we must be able to free it with a put_page)";0;1
"Actually, there is a period where the count behaves differently: when the
page is free or if it is a constituent page of a compound page";0;1
" We need
an atomic_inc_not_zero operation to ensure we don't try to grab the page
in either case";1;1
This patch introduces the core locking protocol to the pagecache (ie;1;1
"adds page_cache_get_speculative, and tweaks some update-side code to make
it work)";1;1
"Thanks to Hugh for pointing out an improvement to the algorithm setting
page_count to zero when we have control of all references, in order to
hold off speculative getters.";1;1
memcg: remove refcnt from page_cgroup;1;0
"memcg: performance improvements
Patch Description
 1/5 ..";1;0
"remove refcnt fron page_cgroup patch (shmem handling is fixed)
 2/5 ..";1;0
"swapcache handling patch
 3/5 ..";1;0
"add helper function for shmem's memory reclaim patch
 4/5 ..";1;0
"optimize by likely/unlikely ppatch
 5/5 ..";0;0
"remove redundunt check patch (shmem handling is fixed.)
Unix bench result";1;1
"== 2.6.26-rc2-mm1 + memory resource controller
Execl Throughput                           2915.4 lps   (29.6 secs, 3 samples)
C Compiler Throughput                      1019.3 lpm   (60.0 secs, 3 samples)
Shell Scripts (1 concurrent)               5796.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (8 concurrent)               1097.7 lpm   (60.0 secs, 3 samples)
Shell Scripts (16 concurrent)               565.3 lpm   (60.0 secs, 3 samples)
File Read 1024 bufsize 2000 maxblocks    1022128.0 KBps  (30.0 secs, 3 samples)
File Write 1024 bufsize 2000 maxblocks   544057.0 KBps  (30.0 secs, 3 samples)
File Copy 1024 bufsize 2000 maxblocks    346481.0 KBps  (30.0 secs, 3 samples)
File Read 256 bufsize 500 maxblocks      319325.0 KBps  (30.0 secs, 3 samples)
File Write 256 bufsize 500 maxblocks     148788.0 KBps  (30.0 secs, 3 samples)
File Copy 256 bufsize 500 maxblocks       99051.0 KBps  (30.0 secs, 3 samples)
File Read 4096 bufsize 8000 maxblocks    2058917.0 KBps  (30.0 secs, 3 samples)
File Write 4096 bufsize 8000 maxblocks   1606109.0 KBps  (30.0 secs, 3 samples)
File Copy 4096 bufsize 8000 maxblocks    854789.0 KBps  (30.0 secs, 3 samples)
Dc: sqrt(2) to 99 decimal places         126145.2 lpm   (30.0 secs, 3 samples)
                     INDEX VALUES
TEST                                        BASELINE     RESULT      INDEX
Execl Throughput                                43.0     2915.4      678.0
File Copy 1024 bufsize 2000 maxblocks         3960.0   346481.0      875.0
File Copy 256 bufsize 500 maxblocks           1655.0    99051.0      598.5
File Copy 4096 bufsize 8000 maxblocks         5800.0   854789.0     1473.8
Shell Scripts (8 concurrent)                     6.0     1097.7     1829.5
     FINAL SCORE                                                     991.3
== 2.6.26-rc2-mm1 + this set ==
Execl Throughput                           3012.9 lps   (29.9 secs, 3 samples)
C Compiler Throughput                       981.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (1 concurrent)               5872.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (8 concurrent)               1120.3 lpm   (60.0 secs, 3 samples)
Shell Scripts (16 concurrent)               578.0 lpm   (60.0 secs, 3 samples)
File Read 1024 bufsize 2000 maxblocks    1003993.0 KBps  (30.0 secs, 3 samples)
File Write 1024 bufsize 2000 maxblocks   550452.0 KBps  (30.0 secs, 3 samples)
File Copy 1024 bufsize 2000 maxblocks    347159.0 KBps  (30.0 secs, 3 samples)
File Read 256 bufsize 500 maxblocks      314644.0 KBps  (30.0 secs, 3 samples)
File Write 256 bufsize 500 maxblocks     151852.0 KBps  (30.0 secs, 3 samples)
File Copy 256 bufsize 500 maxblocks      101000.0 KBps  (30.0 secs, 3 samples)
File Read 4096 bufsize 8000 maxblocks    2033256.0 KBps  (30.0 secs, 3 samples)
File Write 4096 bufsize 8000 maxblocks   1611814.0 KBps  (30.0 secs, 3 samples)
File Copy 4096 bufsize 8000 maxblocks    847979.0 KBps  (30.0 secs, 3 samples)
Dc: sqrt(2) to 99 decimal places         128148.7 lpm   (30.0 secs, 3 samples)
                     INDEX VALUES
TEST                                        BASELINE     RESULT      INDEX
Execl Throughput                                43.0     3012.9      700.7
File Copy 1024 bufsize 2000 maxblocks         3960.0   347159.0      876.7
File Copy 256 bufsize 500 maxblocks           1655.0   101000.0      610.3
File Copy 4096 bufsize 8000 maxblocks         5800.0   847979.0     1462.0
Shell Scripts (8 concurrent)                     6.0     1120.3     1867.2
     FINAL SCORE                                                    1004.6
This patch";0;0
Remove refcnt from page_cgroup();1;0
"After this,
	* Anon page is newly mapped";0;0
	* File page is added to mapping->tree;1;1
	* Anon page is fully unmapped;1;0
	* File page is removed from LRU;1;1
There is no change in behavior from user's view;1;0
"This patch also removes unnecessary calls in rmap.c which was used only for
refcnt mangement.";1;1
memcg: better migration handling;1;1
"This patch changes page migration under memory controller to use a
Before";1;1
 - page_cgroup is migrated from an old page to a new page;0;0
After;0;0
 - a new page is accounted , no reuse of page_cgroup;1;1
Pros;0;0
 - We can avoid compliated lock depndencies and races in migration;1;1
Cons;0;0
 - new param to mem_cgroup_charge_common();1;0
 - mem_cgroup_getref() is added for handling ref_cnt ping-pong;1;0
"This version simplifies complicated lock dependency in page migraiton
under memory resource controller";1;1
  new refcnt sequence is following;1;1
a mapped page;0;0
  prepage_migration() ....;0;1
"+1 to NEW page
  try_to_unmap()      ....";0;0
all refs to OLD page is gone;0;0
  move_pages()        ....;1;0
+1 to NEW page if page cache;1;0
  remap..;0;0
           ....;0;0
all refs from *map* is added to NEW one;1;0
  end_migration()     ....;1;1
-1 to New page;0;0
  page's mapcount + (page_is_cache) refs are added to NEW one.;0;0
mm: make CONFIG_MIGRATION available w/o CONFIG_NUMA;1;1
"We'd like to support CONFIG_MEMORY_HOTREMOVE on s390, which depends on
CONFIG_MIGRATION";1;0
" So far, CONFIG_MIGRATION is only available with NUMA
support";1;1
"This patch makes CONFIG_MIGRATION selectable for architectures that define
ARCH_ENABLE_MEMORY_HOTREMOVE";1;1
" When MIGRATION is enabled w/o NUMA, the
kernel won't compile because migrate_vmas() does not know about
vm_ops->migrate() and vma_migratable() does not know about policy_zone";0;1
"To fix this, those two functions can be restricted to '#ifdef CONFIG_NUMA'
because they are not being used w/o NUMA";1;1
" vma_migratable() is moved over
from migrate.h to mempolicy.h.";1;1
mm/migrate.c should #include <linux/syscalls.h>;1;1
"Every file should include the headers containing the externs for its
global functions (in this case for sys_move_pages()).";1;0
Christoph has moved;1;1
"Remove all clameter@sgi.com addresses from the kernel tree since they will
become invalid on June 27th";1;1
" Change my maintainer email address for the
slab allocators to cl@linux-foundation.org (which will be the new email
address for the future).";0;0
Reinstate ZERO_PAGE optimization in 'get_user_pages()' and fix XIP;1;1
"KAMEZAWA Hiroyuki and Oleg Nesterov point out that since the commit
557ed1fa2620dc119adb86b34c614e152a629a80 (""remove ZERO_PAGE"") removed
the ZERO_PAGE from the VM mappings, any users of get_user_pages() will
generally now populate the VM with real empty pages needlessly";0;1
"We used to get the ZERO_PAGE when we did the ""handle_mm_fault()"", but
since fault handling no longer uses ZERO_PAGE for new anonymous pages,
we now need to handle that special case in follow_page() instead";0;1
"In particular, the removal of ZERO_PAGE effectively removed the core
file writing optimization where we would skip writing pages that had not
been populated at all, and increased memory pressure a lot by allocating
all those useless newly zeroed pages";0;0
"This reinstates the optimization by making the unmapped PTE case the
same as for a non-existent page table, which already did this correctly";1;1
"While at it, this also fixes the XIP case for follow_page(), where the
caller could not differentiate between the case of a page that simply
could not be used (because it had no ""struct page"" associated with it)
and a page that just wasn't mapped";1;1
"We do that by simply returning an error pointer for pages that could not
be turned into a ""struct page *""";0;1
" The error is arbitrarily picked to be
EFAULT, since that was what get_user_pages() already used for the
equivalent IO-mapped page case";0;1
[ Also removed an impossible test for pte_offset_map_lock() failing;1;1
  that's not how that function works ];0;1
mm: fix warning on memory offline;1;1
"KAMEZAWA Hiroyuki found a warning message in the buffer dirtying code that
is coming from page migration caller";0;0
"WARNING: at fs/buffer.c:720 __set_page_dirty+0x330/0x360()
What was happening is that migrate_page_copy wants to transfer the PG_dirty
bit from old page to new page, so what it would do is set_page_dirty(newpage)";0;1
"However set_page_dirty() is used to set the entire page dirty, wheras in
this case, only part of the page was dirty, and it also was not uptodate";0;1
"Marking the whole page dirty with set_page_dirty would lead to corruption or
unresolvable conditions -- a dirty && !uptodate page and dirty && !uptodate
buffers";0;1
however in the interests of keeping the change minimal...;0;0
memcg: fix VM_BUG_ON from page migration;1;1
Page migration gave me free_hot_cold_page's VM_BUG_ON page->page_cgroup;0;0
"remove_migration_pte was calling mem_cgroup_charge on the new page whenever it
found a swap pte, before it had determined it to be a migration entry";1;1
" That
left a surplus reference count on the page_cgroup, so it was still attached
when the page was later freed";1;0
Move that mem_cgroup_charge down to where we're sure it's a migration entry;0;1
"We were already under i_mmap_lock or anon_vma->lock, so its GFP_KERNEL was
already inappropriate: change that to GFP_ATOMIC";0;1
"It's essential that remove_migration_pte removes all the migration entries,
other crashes follow if not";1;0
" So proceed even when the charge fails: normally
it cannot, but after a mem_cgroup_force_empty it might - comment in the code.";0;1
bugfix for memory cgroup controller: migration under memory controller fix;1;1
While using memory control cgroup, page-migration under it works as following;0;1
 1;0;0
uncharge all refs at try to unmap;0;0
 2;1;0
"charge regs again remove_migration_ptes()
This is simple but has following problems";1;1
 The page is uncharged and charged back again if *mapped*;0;0
"    - This means that cgroup before migration can be different from one after
      migration
    - If page is not mapped but charged as page cache, charge is just ignored
      (because not mapped, it will not be uncharged before migration)
      This is memory leak";0;1
"This patch tries to keep memory cgroup at page migration by increasing
one refcnt during it";1;0
3 functions are added;0;0
" mem_cgroup_prepare_migration() --- increase refcnt of page->page_cgroup
 mem_cgroup_end_migration()     --- decrease refcnt of page->page_cgroup
 mem_cgroup_page_migration() --- copy page->page_cgroup from old page to
                                 new page";1;0
"During migration
  - old page is under PG_locked";1;0
  - new page is under PG_locked, too;1;1
  - both old page and new page is not on LRU;1;1
These 3 facts guarantee that page_cgroup() migration has no race;0;1
Tested and worked well in x86_64/fake-NUMA box.;1;0
Memory controller: make charging gfp mask aware;1;0
"Nick Piggin pointed out that swap cache and page cache addition routines
could be called from non GFP_KERNEL contexts";1;0
" This patch makes the
charging routine aware of the gfp context";1;1
" Charging might fail if the
cgroup is over it's limit, in which case a suitable error is returned";0;0
This patch was tested on a Powerpc box;1;0
" I am still looking at being able
to test the path, through which allocations happen in non GFP_KERNEL
contexts.";1;0
Memory controller: memory accounting;1;0
Add the accounting hooks;1;0
" The accounting is carried out for RSS and Page
Cache (unmapped) pages";1;0
 There is now a common limit and accounting for both;1;0
"The RSS accounting is accounted at page_add_*_rmap() and page_remove_rmap()
time";1;1
" Page cache is accounted at add_to_page_cache(),
__delete_from_page_cache()";0;0
 Swap cache is also accounted for;1;0
"Each page's page_cgroup is protected with the last bit of the
page_cgroup pointer, this makes handling of race conditions involving
simultaneous mappings of a page easier";0;1
" A reference count is kept in the
page_cgroup to deal with cases where a page might be unmapped from the RSS
of all tasks, but still lives in the page cache";1;0
"Credits go to Vaidyanathan Srinivasan for helping with reference counting work
of the page cgroup";0;0
" Almost all of the page cache accounting code has help
from Vaidyanathan Srinivasan.";0;1
page migraton: handle orphaned pages;1;1
Orphaned page might have fs-private metadata, the page is truncated;0;1
" As
the page hasn't mapping, page migration refuse to migrate the page";0;1
" It
appears the page is only freed in page reclaim and if zone watermark is
low, the page is never freed, as a result migration always fail";0;1
" I thought
we could free the metadata so such page can be freed in migration and make
migration more reliable.";1;1
maps4: move is_swap_pte;1;0
Move is_swap_pte helper function to swapops.h for use by pagemap code;1;1
Typo fixes retrun -> return;0;1
Typo fixes retrun -> return;0;1
Uninline find_task_by_xxx set of functions;0;0
"The find_task_by_something is a set of macros are used to find task by pid
depending on what kind of pid is proposed - global or virtual one";1;1
" All of
them are wrappers above the most generic one - find_task_by_pid_type_ns() -
and just substitute some args for it";1;1
"It turned out, that dereferencing the current->nsproxy->pid_ns construction
and pushing one more argument on the stack inline cause kernel text size to
grow";1;1
This patch moves all this stuff out-of-line into kernel/pid.c;1;1
" Together
with the next patch it saves a bit less than 400 bytes from the .text
section.";1;1
pid namespaces: changes to show virtual ids to user;1;1
This is the largest patch in the set;1;0
"Make all (I hope) the places where
the pid is shown to or get from user operate on the virtual pids";1;0
The idea is;0;0
" - all in-kernel data structures must store either struct pid itself
 - when seeking the task from kernel code with the stored id one
 - when showing pid's numerical value to the user the virtual one
   should be used, but however when one shows task's pid outside this
 - when getting the pid from userspace one need to consider this as
   the virtual one and use appropriate task/pid-searching functions.";1;1
flush icache before set_pte() on ia64: flush icache at set_pte;0;0
"Current ia64 kernel flushes icache by lazy_mmu_prot_update() *after*
set_pte()";0;0
 This is too late;1;1
" This patch removes lazy_mmu_prot_update and
add modfied set_pte() for flushing if necessary";1;1
"This patch flush icache of a page when
	new pte has exec bit";1;1
"	&& new pte has present bit
	&& new pte is user's page";1;0
"	&& (old *ptep is not present
            || new pte's pfn is not same to old *ptep's ptn)
	&& new pte's page has no Pg_arch_1 bit";1;1
	   Pg_arch_1 is set when a page is cache consistent;0;0
"I think this condition checks are much easier to understand than considering
""Where sync_icache_dcache() should be inserted ?""";0;1
"pte_user() for ia64 was removed by as
clean-up";1;0
So, I added it again.;1;0
flush cache before installing new page at migraton;0;0
"In migration, a new page should be cache flushed before set_pte() in some
archs which have virtually-tagged cache.";1;0
Memoryless nodes: Update memory policy and page migration;1;1
Online nodes now may have no memory;1;0
" The checks and initialization must
therefore be changed to no longer use the online functions";1;1
"This will correctly initialize the interleave on bootup to only target nodes
with memory and will make sys_move_pages return an error when a page is to be
moved to a memoryless node";1;1
" Similarly we will get an error if MPOL_BIND and
MPOL_INTERLEAVE is used on a memoryless node";0;1
These are somewhat new semantics;0;0
" So far one could specify memoryless nodes
and we would maybe do the right thing and just ignore the node (or we'd do
something strange like with MPOL_INTERLEAVE)";0;1
" If we want to allow the
specification of memoryless nodes via memory policies then we need to keep
checking for online nodes.";1;1
mm/migrate.c __user annotation;0;1
;0;0
fix rcu_read_lock() in page migraton;1;1
In migration fallback path, write_page() or lock_page() will be called;0;0
This causes sleep with holding rcu_read_lock().;0;1
memory unplug: isolate_lru_page fix;1;1
"release_pages() in mm/swap.c changes page_count() to be 0 without removing
PageLRU flag..";0;1
"This means isolate_lru_page() can see a page, PageLRU() &&
page_count(page)==0.";1;1
 This is BUG;0;1
" (get_page() will be called against
count=0 page.)";0;0
memory unplug: migration by kernel;0;0
In usual, migrate_pages(page,,) is called with holding mm->sem by system call;0;0
"(mm here is a mm_struct which maps the migration target page.)
This semaphore helps avoiding some race conditions";1;0
"But, if we want to migrate a page by some kernel codes, we have to avoid
some races";0;1
This patch adds check code for following race condition;1;1
1;0;0
A page which page->mapping==NULL can be target of migration;0;1
"Then, we have
   to check page->mapping before calling try_to_unmap()";0;0
2;1;0
"anon_vma can be freed while page is unmapped, but page->mapping remains as
   it was";1;1
We drop page->mapcount to be 0;1;0
Then we cannot trust page->mapping;0;0
"   So, use rcu_read_lock() to prevent anon_vma pointed by page->mapping from
   being freed during migration.";0;0
Add __GFP_MOVABLE for callers to flag allocations from high memory that may be migrated;1;1
It is often known at allocation time whether a page may be migrated or not;0;1
"This patch adds a flag called __GFP_MOVABLE and a new mask called
GFP_HIGH_MOVABLE";1;0
" Allocations using the __GFP_MOVABLE can be either migrated
using the page migration mechanism or reclaimed by syncing with backing
storage and discarding";1;0
"An API function very similar to alloc_zeroed_user_highpage() is added for
__GFP_MOVABLE allocations called alloc_zeroed_user_highpage_movable()";0;0
" The
flags used by alloc_zeroed_user_highpage() are not changed because it would
change the semantics of an existing API";0;0
" After this patch is applied there
are no in-kernel users of alloc_zeroed_user_highpage() so it probably should
be marked deprecated if this patch is merged";1;1
"Note that this patch includes a minor cleanup to the use of __GFP_ZERO in
shmem.c to keep all flag modifications to inode->mapping in the
shmem_dir_alloc() helper function";1;0
" This clean-up suggestion is courtesy of
Hugh Dickens";1;1
"Additional credit goes to Christoph Lameter and Linus Torvalds for shaping the
concept";0;0
" Credit to Hugh Dickens for catching issues with shmem swap vector
and ramfs allocations.";0;0
page migration: fix NR_FILE_PAGES accounting;1;1
"NR_FILE_PAGES must be accounted for depending on the zone that the page
belongs to";1;0
" If we replace the page in the radix tree then we may have to
shift the count to another zone.";1;0
[PATCH] Page migration: Fix vma flag checking;1;1
"Currently we do not check for vma flags if sys_move_pages is called to move
individual pages";0;0
" If sys_migrate_pages is called to move pages then we
check for vm_flags that indicate a non migratable vma but that still
includes VM_LOCKED and we can migrate mlocked pages";0;1
"Extract the vma_migratable check from mm/mempolicy.c, fix it and put it
into migrate.h so that is can be used from both locations";1;1
Problem was spotted by Lee Schermerhorn;0;1
[PATCH] radix-tree: RCU lockless readside;1;1
Make radix tree lookups safe to be performed without locks;1;1
" Readers are
protected against nodes being deleted by using RCU based freeing";0;0
" Readers
are protected against new node insertion by using memory barriers to ensure
the node itself will be properly written before it is visible in the radix
tree";0;1
Each radix tree node keeps a record of their height (above leaf nodes);1;0
"This height does not change after insertion -- when the radix tree is
extended, higher nodes are only inserted in the top";1;0
" So a lookup can take
the pointer to what is *now* the root node, and traverse down it even if
the tree is concurrently extended and this node becomes a subtree of a new
root";1;1
"""Direct"" pointers (tree height of 0, where root->rnode points directly to
the data item) are handled by using the low bit of the pointer to signal
whether rnode is a direct pointer or a pointer to a radix tree node";0;1
"When a reader wants to traverse the next branch, they will take a copy of
the pointer";0;0
" This pointer will be either NULL (and the branch is empty) or
non-NULL (and will point to a valid node).";0;1
[PATCH] Fix sys_move_pages when a NULL node list is passed;1;1
"sys_move_pages() uses vmalloc() to allocate an array of structures that is
fills with information passed from user mode and then passes to
do_stat_pages() (in the case the node list is NULL)";0;1
" do_stat_pages()
depends on a marker in the node field of the structure to decide how large
the array is and this marker is correctly inserted into the last element of
the array";1;1
" However, vmalloc() doesn't zero the memory it allocates and if
the user passes NULL for the node list, then the node fields are not filled
in (except for the end marker)";0;0
" If the memory the vmalloc() returned
happend to have a word with the marker value in it in just the right place,
do_pages_stat will fail to fill the status field of part of the array and
we will return (random) kernel data to user mode.";1;1
[PATCH] BLOCK: Make it possible to disable the block layer [try #6];1;0
Make it possible to disable the block layer;1;0
" Not all embedded devices require
it, some can make do with just JFFS2, NFS, ramfs, etc - none of which require
the block layer to be present";1;1
This patch does the following;1;0
" (*) Introduces CONFIG_BLOCK to disable the block layer, buffering and blockdev
     support";1;0
" (*) Adds dependencies on CONFIG_BLOCK to any configuration item that controls
     an item that uses the block layer";1;1
 This includes;1;1
     (*) Block I/O tracing;0;0
     (*) Disk partition code;0;1
     (*) All filesystems that are block based, eg: Ext3, ReiserFS, ISOFS;0;0
     (*) The SCSI layer;1;0
" As far as I can tell, even SCSI chardevs use the
     	 block layer to do scheduling";1;1
" Some drivers that use SCSI facilities -
     	 such as USB storage - end up disabled indirectly from this";0;1
"     (*) Various block-based device drivers, such as IDE and the old CDROM
     	 drivers";0;0
     (*) MTD blockdev handling and FTL;1;1
"     (*) JFFS - which uses set_bdev_super(), something it could avoid doing by
     	 taking a leaf out of JFFS2's book";0;1
" (*) Makes most of the contents of linux/blkdev.h, linux/buffer_head.h and
     linux/elevator.h contingent on CONFIG_BLOCK being set";0;0
" sector_div() is,
     however, still used in places, and so is still available";0;0
" (*) Also made contingent are the contents of linux/mpage.h, linux/genhd.h and
     parts of linux/fs.h";0;0
 (*) Makes a number of files in fs/ contingent on CONFIG_BLOCK;1;1
 (*) Makes mm/bounce.c (bounce buffering) contingent on CONFIG_BLOCK;1;1
" (*) set_page_dirty() doesn't call __set_page_dirty_buffers() if CONFIG_BLOCK
     is not enabled";0;1
" (*) fs/no-block.c is created to hold out-of-line stubs and things that are
     required when CONFIG_BLOCK is not set";0;1
     (*) Default blockdev file operations (to give error ENODEV on opening);1;1
 (*) Makes some /proc changes;1;0
     (*) /proc/devices does not list any blockdevs;1;0
     (*) /proc/diskstats and /proc/partitions are contingent on CONFIG_BLOCK;1;0
 (*) Makes some compat ioctl handling contingent on CONFIG_BLOCK;1;1
" (*) If CONFIG_BLOCK is not defined, makes sys_quotactl() return -ENODEV if
     given command other than Q_SYNC or if a special device is specified";1;0
" (*) In init/do_mounts.c, no reference is made to the blockdev routines if
     CONFIG_BLOCK is not defined";1;0
 This does not prohibit NFS roots or JFFS2;0;0
" (*) The bdflush, ioprio_set and ioprio_get syscalls can now be absent (return
     error ENOSYS by way of cond_syscall if so)";1;1
" (*) The seclvl_bd_claim() and seclvl_bd_release() security calls do nothing if
     CONFIG_BLOCK is not set, since they can't then happen.";1;1
[PATCH] BLOCK: Stop fallback_migrate_page() from using page_has_buffers() [try #6];0;0
"Stop fallback_migrate_page() from using page_has_buffers() since that might not
be available";0;1
 Use PagePrivate() instead since that's more general.;1;1
[PATCH] Define easier to handle GFP_THISNODE;1;1
In many places we will need to use the same combination of flags;0;1
" Specify
a single GFP_THISNODE definition for ease of use in gfp.h.";1;0
[PATCH] sys_move_pages: Do not fall back to other nodes;1;0
"If the user specified a node where we should move the page to then we
really do not want any other node.";0;1
[PATCH] Allow migration of mlocked pages;1;0
Hugh clarified the role of VM_LOCKED;1;0
" So we can now implement page
migration for mlocked pages";0;1
Allow the migration of mlocked pages;1;0
" This means that try_to_unmap must
unmap mlocked pages in the migration case.";0;1
[PATCH] page migration: Support a vma migration function;1;0
"Hooks for calling vma specific migration functions
With this patch a vma may define a vma->vm_ops->migrate function";1;1
" That
function may perform page migration on its own (some vmas may not contain page
structs and therefore cannot be handled by regular page migration";0;1
" Pages in a
vma may require special preparatory treatment before migration is possible
etc) ";0;0
 Only mmap_sem is held when the migration function is called;0;0
" The
migrate() function gets passed two sets of nodemasks describing the source and
the target of the migration";0;1
" The flags parameter either contains
MPOL_MF_MOVE	which means that only pages used exclusively by
		the specified mm should be moved
MPOL_MF_MOVE_ALL which means that pages shared with other processes
		should also be moved";0;1
The migration function returns 0 on success or an error condition;0;0
" An error
condition will prevent regular page migration from occurring";1;1
"On its own this patch cannot be included since there are no users for this
functionality";1;1
" But it seems that the uncached allocator will need this
functionality at some point.";0;0
[PATCH] SELinux: add security_task_movememory calls to mm code;1;0
"This patch inserts security_task_movememory hook calls into memory management
code to enable security modules to mediate this operation between tasks";1;0
"Since the last posting, the hook has been renamed following feedback from
Christoph Lameter.";0;0
[PATCH] page migration: sys_move_pages(): support moving of individual pages;1;1
move_pages() is used to move individual pages of a process;0;0
"The function can
be used to determine the location of pages and to move them onto the desired
node";0;1
move_pages() returns status information for each page;0;0
"long move_pages(pid, number_of_pages_to_move,
		addresses_of_pages[],
		nodes[] or NULL,
		status[],
The addresses of pages is an array of void * pointing to the
pages to be moved";0;1
"The nodes array contains the node numbers that the pages should be moved
to";1;0
"If a NULL is passed instead of an array then no pages are moved but
the status array is updated";1;1
"The status request may be used to determine
the page state before issuing another move_pages() to move pages";0;0
"The status array will contain the state of all individual page migration
attempts when the function terminates";0;0
"The status array is only valid if
move_pages() completed successfullly";1;0
Possible page states in status[];0;1
0..MAX_NUMNODES	The page is now on the indicated node;1;1
"-ENOENT		Page is not present
-EACCES		Page is mapped by multiple processes and can only
		be moved if MPOL_MF_MOVE_ALL is specified";1;0
"-EPERM		The page has been mlocked by a process/driver and
		cannot be moved";0;0
-EBUSY		Page is busy and cannot be moved;0;1
Try again later;0;0
-EFAULT		Invalid address (no VMA or zero page);1;0
-ENOMEM		Unable to allocate memory on target node;0;1
-EIO		Unable to write back page;0;0
"The page must be written
		back in order to move it since the page is dirty and the
		filesystem does not provide a migration function that
		would allow the moving of dirty pages";1;1
-EINVAL		A dirty page cannot be moved;0;1
"The filesystem does not provide
		a migration function and has no ability to write back pages";0;0
The flags parameter indicates what types of pages to move;1;1
MPOL_MF_MOVE	Move pages that are only mapped by the process;0;0
MPOL_MF_MOVE_ALL Also move pages that are mapped by multiple processes;1;0
		Requires sufficient capabilities;0;1
"Possible return codes from move_pages()
-ENOENT		No pages found that would require moving";0;0
"All pages
		are either already on the target node, not present, had an
		invalid address or could not be moved because they were
		mapped by multiple processes";0;1
"-EINVAL		Flags other than MPOL_MF_MOVE(_ALL) specified or an attempt
		to migrate pages in a kernel thread";1;0
-EPERM		MPOL_MF_MOVE_ALL specified without sufficient priviledges;0;1
		or an attempt to move a process belonging to another user;1;0
-EACCES		One of the target nodes is not allowed by the current cpuset;1;0
-ENODEV		One of the target nodes is not online;0;0
-ESRCH		Process does not exist;1;0
-E2BIG		Too many pages to move;1;0
-ENOMEM		Not enough memory to allocate control array;0;1
-EFAULT		Parameters could not be accessed;0;1
"A test program for move_pages() may be found with the patches
on ftp.kernel.org:/pub/linux/kernel/people/christoph/pmig/patches-2.6.17-rc4-mm3
From: Christoph Lameter <clameter@sgi.com>
  Detailed results for sys_move_pages()
  Pass a pointer to an integer to get_new_page() that may be used to
  indicate where the completion status of a migration operation should be
  placed";0;1
" This allows sys_move_pags() to report back exactly what happened to
  each page";0;1
  Wish there would be a better way to do this;1;1
Looks a bit hacky.;1;0
[PATCH] page migration: use allocator function for migrate_pages();1;0
"Instead of passing a list of new pages, pass a function to allocate a new
page";1;0
" This allows the correct placement of MPOL_INTERLEAVE pages during page
migration";0;1
 It also further simplifies the callers of migrate pages;1;0
"migrate_pages() becomes similar to migrate_pages_to() so drop
migrate_pages_to()";0;0
 The batching of new page allocations becomes unnecessary.;1;1
[PATCH] page migration: handle freeing of pages in migrate_pages();1;1
Do not leave pages on the lists passed to migrate_pages();0;1
" Seems that we will
not need any postprocessing of pages";0;1
" This will simplify the handling of
pages by the callers of migrate_pages().";1;1
[PATCH] page migration: simplify migrate_pages();1;1
Currently migrate_pages() is mess with lots of goto;0;0
" Extract two functions
from migrate_pages() and get rid of the gotos";0;0
"Plus we can just unconditionally set the locked bit on the new page since we
are the only one holding a reference";1;1
" Locking is to stop others from
accessing the page once we establish references to the new page";1;0
"Remove the list_del from move_to_lru in order to have finer control over list
processing.";1;0
[PATCH] More page migration: use migration entries for file pages;1;1
"This implements the use of migration entries to preserve ptes of file backed
pages during migration";1;0
" Processes can therefore be migrated back and forth
without loosing their connection to pagecache pages";0;1
Note that we implement the migration entries only for linear mappings;0;0
Nonlinear mappings still require the unmapping of the ptes for migration;1;0
And another writepage() ugliness shows up;1;1
" writepage() can drop the page
lock";0;0
" Therefore we have to remove migration ptes before calling writepages()
in order to avoid having migration entries point to unlocked pages.";0;0
[PATCH] More page migration: do not inc/dec rss counters;1;1
"If we install a migration entry then the rss not really decreases since the
page is just moved somewhere else";1;1
" We can save ourselves the work of
decrementing and later incrementing which will just eventually cause cacheline
bouncing.";0;1
[PATCH] Swapless page migration: modify core logic;1;0
"Use the migration entries for page migration
This modifies the migration code to use the new migration entries";1;0
" It now
becomes possible to migrate anonymous pages without having to add a swap
entry";1;0
"We add a couple of new functions to replace migration entries with the proper
ptes";0;0
We cannot take the tree_lock for migrating anonymous pages anymore;0;1
" However,
we know that we hold the only remaining reference to the page when the page
count reaches 1.";0;1
[PATCH] Swapless page migration: rip out swap based logic;1;0
Rip the page migration logic out;0;0
Remove all code that has to do with swapping during page migration;1;0
This also guts the ability to migrate pages to swap;1;1
" No one used that so lets
let it go for good";1;0
Page migration should be a bit broken after this patch.;1;0
[PATCH] Swapless page migration: add R/W migration entries;1;1
"Implement read/write migration ptes
We take the upper two swapfiles for the two types of migration ptes and define
a series of macros in swapops.h";0;0
The VM is modified to handle the migration entries;0;0
" migration entries can
only be encountered when the page they are pointing to is locked";1;0
" This limits
the number of places one has to fix";0;1
" We also check in copy_pte_range and in
mprotect_pte_range() for migration ptes";1;0
"We check for migration ptes in do_swap_cache and call a function that will
then wait on the page lock";0;0
" This allows us to effectively stop all accesses
to apge";0;1
"Migration entries are created by try_to_unmap if called for migration and
removed by local functions in migrate.c
From: Hugh Dickins <hugh@veritas.com>
  Several times while testing swapless page migration (I've no NUMA, just
  hacking it up to migrate recklessly while running load), I've hit the
  BUG_ON(!PageLocked(p)) in migration_entry_to_page";0;1
"  This comes from an orphaned migration entry, unrelated to the current
  correctly locked migration, but hit by remove_anon_migration_ptes as it
  checks an address in each vma of the anon_vma list";1;0
  Such an orphan may be left behind if an earlier migration raced with fork;1;0
"  copy_one_pte can duplicate a migration entry from parent to child, after
  remove_anon_migration_ptes has checked the child vma, but before it has
  removed it from the parent vma";1;0
" (If the process were later to fault on this
  orphaned entry, it would hit the same BUG from migration_entry_wait.)
  This could be fixed by locking anon_vma in copy_one_pte, but we'd rather
  not";0;1
" There's no such problem with file pages, because vma_prio_tree_add
  adds child vma after parent vma, and the page table locking at each end is
  enough to serialize";1;1
" Follow that example with anon_vma: add new vmas to the
  tail instead of the head";1;1
"  (There's no corresponding problem when inserting migration entries,
  because a missed pte will leave the page count and mapcount high, which is
  allowed for";1;1
" And there's no corresponding problem when migrating via swap,
  because a leftover swap entry will be correctly faulted";1;1
" But the swapless
  method has no refcounting of its entries.)
From: Ingo Molnar <mingo@elte.hu>
  pte_unmap_unlock() takes the pte pointer as an argument";1;0
"From: Hugh Dickins <hugh@veritas.com>
  Several times while testing swapless page migration, gcc has tried to exec
  a pointer instead of a string: smells like COW mappings are not being
  properly write-protected on fork";1;1
"  The protection in copy_one_pte looks very convincing, until at last you
  realize that the second arg to make_migration_entry is a boolean ""write"",
  and SWP_MIGRATION_READ is 30";0;0
"  Anyway, it's better done like in change_pte_range, using
  is_write_migration_entry and make_migration_entry_read";1;1
"From: Hugh Dickins <hugh@veritas.com>
  Remove unnecessary obfuscation from sys_swapon's range check on swap type,
  which blew up causing memory corruption once swapless migration made
  MAX_SWAPFILES no longer 2 ^ MAX_SWAPFILES_SHIFT.";1;1
[PATCH] page migration cleanup: move fallback handling into special function;1;1
"Move the fallback code into a new fallback function and make the function
behave like any other migration function";1;0
" This requires retaking the lock if
pageout() drops it.";1;1
"[PATCH] page migration cleanup: pass ""mapping"" to migration functions";1;1
Change handling of address spaces;1;1
"Pass a pointer to the address space in which the page is migrated to all
migration function";1;0
" This avoids repeatedly having to retrieve the address
space pointer from the page and checking it for validity";0;1
" The old page
mapping will change once migration has gone to a certain step, so it is less
confusing to have the pointer always available";0;0
"Move the setting of the mapping and index for the new page into
migrate_pages().";1;0
[PATCH] page migration cleanup: extract try_to_unmap from migration functions;1;1
"Extract try_to_unmap and rename remove_references -> move_mapping
try_to_unmap() may significantly change the page state by for example setting
the dirty bit";1;0
" It is therefore best to unmap in migrate_pages() before
calling any migration functions";0;0
"migrate_page_remove_references() will then only move the new page in place of
the old page in the mapping";1;0
" Rename the function to
migrate_page_move_mapping()";1;0
This allows us to get rid of the special unmapping for the fallback path.;0;1
[PATCH] page migration cleanup: drop nr_refs in remove_references();1;1
"Drop nr_refs parameter from migrate_page_remove_references()
The nr_refs parameter is not really useful since the number of remaining
references is always
1 for anonymous pages without a mapping
2 for pages with a mapping
3 for pages with a mapping and PagePrivate set";0;1
"Remove the early check for the number of references since we are checking
page_mapcount() earlier";1;1
" Ultimately only the refcount matters after the
tree_lock has been obtained.";0;0
[PATCH] page migration cleanup: remove useless definitions;1;1
"Remove the export for migrate_page_remove_references() and migrate_page_copy()
that are unlikely to be used directly by filesystems implementing migration";1;1
"The export was useful when buffer_migrate_page() lived in fs/buffer.c but it
has now been moved to migrate.c in the migration reorg.";0;0
[PATCH] page migration cleanup: group functions;1;1
Reorder functions in migrate.c;0;0
" Group all migration functions for struct
address_space_operations together.";0;0
[PATCH] page migration: Fix fallback behavior for dirty pages;1;1
"Currently we check PageDirty() in order to make the decision to swap out
the page";0;0
" However, the dirty information may be only be contained in the
ptes pointing to the page";0;0
" We need to first unmap the ptes before checking
for PageDirty()";0;0
" If unmap is successful then the page count of the page
will also be decreased so that pageout() works properly";1;1
This is a fix necessary for 2.6.17;1;1
" Without this fix we may migrate dirty
pages for filesystems without migration functions";0;1
" Filesystems may keep
pointers to dirty pages";0;0
" Migration of dirty pages can result in the
filesystem keeping pointers to freed pages";0;1
"Unmapping is currently not be separated out from removing all the
references to a page and moving the mapping";1;1
" Therefore try_to_unmap will
be called again in migrate_page() if the writeout is successful";0;0
" However,
it wont do anything since the ptes are already removed";1;0
"The coming updates to the page migration code will restructure the code
so that this is no longer necessary.";1;0
[PATCH] mm/migrate.c: don't export a static function;0;0
EXPORT_SYMBOL'ing of a static function is not a good idea.;0;1
[PATCH] Some page migration fixups;1;0
"- Remove sparse comment
- Remove duplicated include
- Return the correct error condition in migrate_page_remove_references().";1;1
[PATCH] page migration reorg;1;0
"Centralize the page migration functions in anticipation of additional
tinkering";0;0
" Creates a new file mm/migrate.c
1";1;1
"Extract buffer_migrate_page() from fs/buffer.c
2";1;0
"Extract central migration code from vmscan.c
3";0;0
"Extract some components from mempolicy.c
4";0;0
"Export pageout() and remove_from_swap() from vmscan.c
5";1;0
"Make it possible to configure NUMA systems without page migration
   and non-NUMA systems with page migration";0;1
I had to so some #ifdeffing in mempolicy.c that may need a cleanup.;0;1
