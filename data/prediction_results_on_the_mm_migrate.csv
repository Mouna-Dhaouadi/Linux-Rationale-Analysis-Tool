commit ID;text;author name;commit date;predicted_decision;predicted_rationale
C_kwDOACN7MtoAKDgwMDEwNzBjZmJlYzVjZDRlYTAwYjhiNDhlYTUxZGY5MTEyMmYyNjU;mm: migrate: annotate data-race in migrate_folio_unmap();Jeongjun Park;2024-09-24;1;1
C_kwDOACN7MtoAKDgwMDEwNzBjZmJlYzVjZDRlYTAwYjhiNDhlYTUxZGY5MTEyMmYyNjU;"I found a report from syzbot [1]
This report shows that the value can be changed, but in reality, the
value of __folio_set_movable() cannot be changed because it holds the
folio refcount";Jeongjun Park;2024-09-24;1;1
C_kwDOACN7MtoAKDgwMDEwNzBjZmJlYzVjZDRlYTAwYjhiNDhlYTUxZGY5MTEyMmYyNjU;"Therefore, it is appropriate to add an annotate to make KCSAN
ignore that data-race.";Jeongjun Park;2024-09-24;1;1
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;mm/codetag: add pgalloc_tag_copy();Yu Zhao;2024-09-06;1;1
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;"Add pgalloc_tag_copy() to transfer the codetag from the old folio to the
new one during migration";Yu Zhao;2024-09-06;1;0
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;" This makes original allocation sites persist
cross migration rather than lump into the get_new_folio callbacks passed
into migrate_pages(), e.g., compaction_alloc()";Yu Zhao;2024-09-06;1;1
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;"  # grep compaction_alloc /proc/allocinfo
Before this patch";Yu Zhao;2024-09-06;0;0
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;"  132968448  32463  mm/compaction.c:1880 func:compaction_alloc
After this patch";Yu Zhao;2024-09-06;0;0
C_kwDOACN7MtoAKGUwYTk1NWJmN2Y2MWNiMDM0ZDIyODczNmQ4MWMxYWIzYTQ3YTNkY2E;          0      0  mm/compaction.c:1880 func:compaction_alloc;Yu Zhao;2024-09-06;0;0
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;mm: migrate: remove unused includes;Kefeng Wang;2024-09-05;1;1
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;"random.h is not needed since commit 6c542ab75714 (""mm/demotion: build
demotion targets based on explicit memory tiers""), all functions moved
into memory-tiers";Kefeng Wang;2024-09-05;0;1
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;"nsproxy.h is not needed since commit 228ebcbe634a (""Uninline
find_task_by_xxx set of functions""), no nsproxy, we only call
find_task_by_vpid() now";Kefeng Wang;2024-09-05;0;0
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;"hugetlb_cgroup.h is not needed since commit ab5ac90aecf5 (""mm, hugetlb: do
not rely on overcommit limit during migration""), move_hugetlb_state() is
called and it belongs to hugetlb.h, which is already included";Kefeng Wang;2024-09-05;0;0
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;"balloon_compaction.h, we have more general movable_operations for non-lru
movable page migration, so it could be dropped";Kefeng Wang;2024-09-05;0;1
C_kwDOACN7MtoAKGNmYzgxOTM4OThjYTQ1MTFlNmMzY2MyMDEwMWVmMzU1NGVkYThlZmI;"memremap.h, userfaultfd_k.h and oom.h are introduced for zone device page
migration, but all functions are moved into migrate_device.c, so no needed
anymore too.";Kefeng Wang;2024-09-05;1;1
C_kwDOACN7MtoAKDQ2ZGNjN2M5MmU2Mzg3OWE0OWNmYmQ5OTk0OTg1OGRmMDMzNWExMjI;mm: migrate: simplify find_mm_struct();Nanyong Sun;2024-09-05;1;1
C_kwDOACN7MtoAKDQ2ZGNjN2M5MmU2Mzg3OWE0OWNmYmQ5OTk0OTg1OGRmMDMzNWExMjI;"Use find_get_task_by_vpid() to replace the task_struct find logic in
find_mm_struct(), note that this patch move the ptrace_may_access() call
out from rcu_read_lock() scope, this is ok because it actually does not
need it, find_get_task_by_vpid() already get the pid and task safely,
ptrace_may_access() can use the task safely, like what
sched_core_share_pid() similarly do.";Nanyong Sun;2024-09-05;1;1
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;mm: introduce a pageflag for partially mapped folios;Usama Arif;2024-08-30;1;1
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;"Currently folio->_deferred_list is used to keep track of partially_mapped
folios that are going to be split under memory pressure";Usama Arif;2024-08-30;0;0
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;" In the next
patch, all THPs that are faulted in and collapsed by khugepaged are also
going to be tracked using _deferred_list";Usama Arif;2024-08-30;1;0
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;"This patch introduces a pageflag to be able to distinguish between
partially mapped folios and others in the deferred_list at split time in
deferred_split_scan";Usama Arif;2024-08-30;1;1
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;" Its needed as __folio_remove_rmap decrements
_mapcount, _large_mapcount and _entire_mapcount, hence it won't be
possible to distinguish between partially mapped folios and others in
deferred_split_scan";Usama Arif;2024-08-30;1;1
C_kwDOACN7MtoAKDg0MjJhY2RjOTdlZDU4Mzk2OTJiNDVmODAwZGJmYjc4YWJlNjVhOTQ;"Eventhough it introduces an extra flag to track if the folio is partially
mapped, there is no functional change intended with this patch and the
flag is not useful in this patch itself, it will become useful in the next
patch when _deferred_list has non partially mapped folios.";Usama Arif;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;mm: remap unused subpages to shared zeropage when splitting isolated thp;Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"Patch series ""mm: split underused THPs"", v5";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;The current upstream default policy for THP is always;Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" However, Meta uses
madvise in production as the current THP=always policy vastly
overprovisions THPs in sparsely accessed memory areas, resulting in
excessive memory pressure and premature OOM killing";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" Using madvise +
relying on khugepaged has certain drawbacks over THP=always";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" Using
madvise hints mean THPs aren't ""transparent"" and require userspace
changes";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" Waiting for khugepaged to scan memory and collapse pages into
THP can be slow and unpredictable in terms of performance (i.e";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" you dont
know when the collapse will happen), while production environments require
predictable performance";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" If there is enough memory available, its better
for both performance and predictability to have a THP from fault time,
i.e";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" THP=always rather than wait for khugepaged to collapse it, and deal
with sparsely populated THPs when the system is running out of memory";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"This patch series is an attempt to mitigate the issue of running out of
memory when THP is always enabled";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" During runtime whenever a THP is being
faulted in or collapsed by khugepaged, the THP is added to a list";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"
Whenever memory reclaim happens, the kernel runs the deferred_split
shrinker which goes through the list and checks if the THP was underused,
i.e";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc; how many of the base 4K pages of the entire THP were zero-filled;Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"
If this number goes above a certain threshold, the shrinker will attempt
to split that THP";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" Then at remap time, the pages that were zero-filled
are mapped to the shared zeropage, hence saving memory";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" This method
avoids the downside of wasting memory in areas where THP is sparsely
filled when THP is always enabled, while still providing the upside THPs
like reduced TLB misses without having to use madvise";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"Meta production workloads that were CPU bound (>99% CPU utilzation) were
tested with THP shrinker";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc; The results after 2 hours are as follows;Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"                            | THP=madvise |  THP=always   | THP=always
                            |             |               | + shrinker series
                            |             |               | + max_ptes_none=409
Performance improvement     |      -      |    +1.8%      |     +1.7%
(over THP=madvise)          |             |               |
Memory usage                |    54.6G    | 58.8G (+7.7%) |   55.9G (+2.4%)
max_ptes_none=409 means that any THP that has more than 409 out of 512
(80%) zero filled filled pages will be split";Yu Zhao;2024-08-30;1;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"To test out the patches, the below commands without the shrinker will
invoke OOM killer immediately and kill stress, but will not fail with the
shrinker";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"echo 450 > /sys/kernel/mm/transparent_hugepage/khugepaged/max_ptes_none
mkdir /sys/fs/cgroup/test
echo $$ > /sys/fs/cgroup/test/cgroup.procs
echo 20M > /sys/fs/cgroup/test/memory.max
echo 0 > /sys/fs/cgroup/test/memory.swap.max
# allocate twice memory.max for each stress worker and touch 40/512 of
# each THP, i.e";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;vm-stride 50K;Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"# With the shrinker, max_ptes_none of 470 and below won't invoke OOM
# killer";Yu Zhao;2024-08-30;1;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"# Without the shrinker, OOM killer is invoked immediately irrespective
# of max_ptes_none value and kills stress";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"stress --vm 1 --vm-bytes 40M --vm-stride 50K
This patch (of 5)";Yu Zhao;2024-08-30;1;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"Here being unused means containing only zeros and inaccessible to
userspace";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;" When splitting an isolated thp under reclaim or migration, the
unused subpages can be mapped to the shared zeropage, hence saving memory";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"This is particularly helpful when the internal fragmentation of a thp is
high, i.e";Yu Zhao;2024-08-30;0;0
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc; it has many untouched subpages;Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKGIxZjIwMjA2MGFmZWI3ZmNiOTg0NzM5MjlkMjZmZDNkMjA5M2IwNjc;"This is also a prerequisite for THP low utilization shrinker which will be
introduced in later patches, where underutilized THPs are split, and the
zero-filled pages are freed saving memory.";Yu Zhao;2024-08-30;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;mm: count the number of anonymous THPs per size;Barry Song;2024-08-24;1;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Patch series ""mm: count the number of anonymous THPs per size"", v4";Barry Song;2024-08-24;1;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Knowing the number of transparent anon THPs in the system is crucial
for performance analysis";Barry Song;2024-08-24;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"It helps in understanding the ratio and
distribution of THPs versus small folios throughout the system";Barry Song;2024-08-24;0;0
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Additionally, partial unmapping by userspace can lead to significant waste
of THPs over time and increase memory reclamation pressure";Barry Song;2024-08-24;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"We need this
information for comprehensive system tuning";Barry Song;2024-08-24;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;This patch (of 2);Barry Song;2024-08-24;1;0
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Let's track for each anonymous THP size, how many of them are currently
allocated";Barry Song;2024-08-24;1;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;" We'll track the complete lifespan of an anon THP, starting
when it becomes an anon THP (""large anon folio"") (->mapping gets set),
until it gets freed (->mapping gets cleared)";Barry Song;2024-08-24;0;0
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Introduce a new ""nr_anon"" counter per THP size and adjust the
corresponding counter in the following cases";Barry Song;2024-08-24;1;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;   it the first time and turn it into an anon THP;Barry Song;2024-08-24;1;0
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Note that AnonPages in /proc/meminfo currently tracks the total number of
semantics";Barry Song;2024-08-24;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;" In the future, we might also want to track ""nr_anon_mapped""
for each THP size, which might be helpful when comparing it to the number
of allocated anon THPs (long-term pinning, stuck in swapcache, memory
leaks, ...)";Barry Song;2024-08-24;0;1
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;"Further note that for now, we only track anon THPs after they got their
->mapping set, for example via folio_add_new_anon_rmap()";Barry Song;2024-08-24;0;0
C_kwDOACN7MtoAKDVkNjVjOGQ3NThmMjU5NmMwMDgwMDllMzliYjI2MTRkZWVkMmM3MzA;" If we would
allocate some in the swapcache, they will only show up in the statistics
for now after they have been mapped to user space the first time, where we
call folio_add_new_anon_rmap().";Barry Song;2024-08-24;1;1
C_kwDOACN7MtoAKGYxMjY0ZTk1MzFiMGZiYWRmODhlMGI5YzgxNDNjNTQ2MzQzYjQ4MWM;mm: migrate: add isolate_folio_to_list();Kefeng Wang;2024-08-27;1;0
C_kwDOACN7MtoAKGYxMjY0ZTk1MzFiMGZiYWRmODhlMGI5YzgxNDNjNTQ2MzQzYjQ4MWM;"Add isolate_folio_to_list() helper to try to isolate HugeTLB, no-LRU
movable and LRU folios to a list, which will be reused by
do_migrate_range() from memory hotplug soon, also drop the
mf_isolate_folio() since we could directly use new helper in the
soft_offline_in_use_page().";Kefeng Wang;2024-08-27;1;1
C_kwDOACN7MtoAKDMyZjUxZWFkM2Q3NzcxY2RlYzI5Zjc1ZTA4ZDUwYTc2ZDJjNjI1M2Q;mm: remove PageSwapCache;Matthew Wilcox (Oracle);2024-08-21;1;1
C_kwDOACN7MtoAKDMyZjUxZWFkM2Q3NzcxY2RlYzI5Zjc1ZTA4ZDUwYTc2ZDJjNjI1M2Q;"This flag is now only used on folios, so we can remove all the page
accessors and reword the comments that refer to them.";Matthew Wilcox (Oracle);2024-08-21;1;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;mm,memcg: provide per-cgroup counters for NUMA balancing operations;Kaiyang Zhao;2024-08-14;0;0
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"The ability to observe the demotion and promotion decisions made by the
kernel on a per-cgroup basis is important for monitoring and tuning
containerized workloads on machines equipped with tiered memory";Kaiyang Zhao;2024-08-14;0;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"Different containers in the system may experience drastically different
memory tiering actions that cannot be distinguished from the global
counters alone";Kaiyang Zhao;2024-08-14;0;0
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"For example, a container running a workload that has a much hotter memory
accesses will likely see more promotions and fewer demotions, potentially
depriving a colocated container of top tier memory to such an extent that
its performance degrades unacceptably";Kaiyang Zhao;2024-08-14;0;0
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"For another example, some containers may exhibit longer periods between
data reuse, causing much more numa_hint_faults than numa_pages_migrated";Kaiyang Zhao;2024-08-14;0;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"
In this case, tuning hot_threshold_ms may be appropriate, but the signal
can easily be lost if only global counters are available";Kaiyang Zhao;2024-08-14;0;0
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"In the long term, we hope to introduce per-cgroup control of promotion and
demotion actions to implement memory placement policies in tiering";Kaiyang Zhao;2024-08-14;0;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;This patch set adds seven counters to memory.stat in a cgroup;Kaiyang Zhao;2024-08-14;1;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"numa_pages_migrated, numa_pte_updates, numa_hint_faults, pgdemote_kswapd,
pgdemote_khugepaged, pgdemote_direct and pgpromote_success";Kaiyang Zhao;2024-08-14;1;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;" pgdemote_*
and pgpromote_success are also available in memory.numa_stat";Kaiyang Zhao;2024-08-14;0;0
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;"count_memcg_events_mm() is added to count multiple event occurrences at
once, and get_mem_cgroup_from_folio() is added because we need to get a
reference to the memcg of a folio before it's migrated to track
numa_pages_migrated";Kaiyang Zhao;2024-08-14;0;1
C_kwDOACN7MtoAKGY3N2YwYzc1MTQ3ODk1NzcxMjVjMWIyZGYxNDU3MDMxNjE3MzYzNTk;" The accounting of PGDEMOTE_* is moved to
shrink_inactive_list() before being changed to per-cgroup.";Kaiyang Zhao;2024-08-14;1;0
C_kwDOACN7MtoAKDQyMGUwNWQwZGUxOGRmOTA3YmExYzVjMDc3YWY2OWU2YWU0NjljOWE;fs: remove calls to set and clear the folio error flag;Matthew Wilcox (Oracle);2024-08-07;1;1
C_kwDOACN7MtoAKDQyMGUwNWQwZGUxOGRmOTA3YmExYzVjMDc3YWY2OWU2YWU0NjljOWE;"Nobody checks the folio error flag any more, so we can stop setting and
clearing it";Matthew Wilcox (Oracle);2024-08-07;1;1
C_kwDOACN7MtoAKDQyMGUwNWQwZGUxOGRmOTA3YmExYzVjMDc3YWY2OWU2YWU0NjljOWE;" Also remove the documentation suggesting to not bother
setting the error bit.";Matthew Wilcox (Oracle);2024-08-07;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;mm/migrate: convert add_page_for_migration() from follow_page() to folio_walk;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"Let's use folio_walk instead, so we can avoid taking a folio reference
when we won't even be trying to migrate the folio and to get rid of
another follow_page()/FOLL_DUMP user";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;" Use FW_ZEROPAGE so we can return
""-EFAULT"" for it as documented";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"We now perform the folio_likely_mapped_shared() check under PTL, which is
what we want: relying on the mapcount and friends after dropping the PTL
does not make too much sense, as the page can get unmapped concurrently
from this process";David Hildenbrand;2024-08-02;0;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"Further, we perform the folio isolation under PTL, similar to how we
handle it for MADV_PAGEOUT";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"The possible return values for follow_page() were confusing, especially
with FOLL_DUMP set";David Hildenbrand;2024-08-02;0;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;We'll handle it like documented in the man page;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;    process;David Hildenbrand;2024-08-02;0;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;We'll keep setting -ENOENT for ZONE_DEVICE;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;" Maybe not the right thing to
do, but it likely doesn't really matter (just like for weird devmap,
whereby we fake ""not present"")";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"The other errros are left as is, and match the documentation in the man
page";David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;While at it, rename add_page_for_migration() to add_folio_for_migration();David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;"We'll lose the ""secretmem"" check, but that shouldn't really matter because
these folios cannot ever be migrated";David Hildenbrand;2024-08-02;0;1
C_kwDOACN7MtoAKDdkZmY4NzVjOTQzNmE5ZGYyZjkzY2Y1OWEzMjYzMDc2MTU2NWFmOTk;" Should vma_migratable() refuse
these VMAs?  Maybe.";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;mm/migrate: convert do_pages_stat_array() from follow_page() to folio_walk;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;"Let's use folio_walk instead, so we can avoid taking a folio reference
just to read the nid and get rid of another follow_page()/FOLL_DUMP user";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;"
Use FW_ZEROPAGE so we can return ""-EFAULT"" for it as documented";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;"The possible return values for follow_page() were confusing, especially
with FOLL_DUMP set";David Hildenbrand;2024-08-02;0;0
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY; We'll handle it like documented in the man page;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;   process;David Hildenbrand;2024-08-02;0;0
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;We'll keep setting -ENOENT for ZONE_DEVICE;David Hildenbrand;2024-08-02;1;0
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;" Maybe not the right thing to
do, but it likely doesn't really matter (just like for weird devmap,
whereby we fake ""not present"")";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;"Note that the other errors (-EACCESS, -EBUSY, -EIO, -EINVAL, -ENOMEM) so
far only applied when actually moving pages, not when only querying stats";David Hildenbrand;2024-08-02;1;1
C_kwDOACN7MtoAKDQ2ZDZhOWI0NDUwYjRmNWViZjZlNjJkMDNmNDU4MDBiNzAyMjFjNGY;"We'll effectively drop the ""secretmem"" check we had in follow_page(), but
that shouldn't really matter here, we're not accessing folio/page content
after all.";David Hildenbrand;2024-08-02;0;1
C_kwDOACN7MtoAKDZkMTkyMzAzZTgyYzdmNzExOTAyMDQxOGE0YzMwMjliZWJmOWEwZTY;mm: consider CMA pages in watermark check for NUMA balancing target node;Kaiyang Zhao;2024-08-01;1;1
C_kwDOACN7MtoAKDZkMTkyMzAzZTgyYzdmNzExOTAyMDQxOGE0YzMwMjliZWJmOWEwZTY;"Currently in migrate_balanced_pgdat(), ALLOC_CMA flag is not passed when
checking watermark on the migration target node";Kaiyang Zhao;2024-08-01;0;1
C_kwDOACN7MtoAKDZkMTkyMzAzZTgyYzdmNzExOTAyMDQxOGE0YzMwMjliZWJmOWEwZTY;" This does not match the
gfp in alloc_misplaced_dst_folio() which allows allocation from CMA";Kaiyang Zhao;2024-08-01;0;0
C_kwDOACN7MtoAKDZkMTkyMzAzZTgyYzdmNzExOTAyMDQxOGE0YzMwMjliZWJmOWEwZTY;"This causes promotion failures when there are a lot of available CMA
memory in the system";Kaiyang Zhao;2024-08-01;0;1
C_kwDOACN7MtoAKDZkMTkyMzAzZTgyYzdmNzExOTAyMDQxOGE0YzMwMjliZWJmOWEwZTY;"Therefore, we change the alloc_flags passed to zone_watermark_ok() in
migrate_balanced_pgdat().";Kaiyang Zhao;2024-08-01;0;1
C_kwDOACN7MtoAKGFjNTlhMWYwMTQ2ZjQ2YmFkN2Q1ZjhkMWIyMDc1NmVjZTQzMTIyZWM;memory tiering: count PGPROMOTE_SUCCESS when mem tiering is enabled.;Zi Yan;2024-07-24;1;1
C_kwDOACN7MtoAKGFjNTlhMWYwMTQ2ZjQ2YmFkN2Q1ZjhkMWIyMDc1NmVjZTQzMTIyZWM;"memory tiering can be enabled/disabled at runtime and
sysctl_numa_balancing_mode & NUMA_BALANCING_MEMORY_TIERING is used to
check it";Zi Yan;2024-07-24;0;1
C_kwDOACN7MtoAKGFjNTlhMWYwMTQ2ZjQ2YmFkN2Q1ZjhkMWIyMDc1NmVjZTQzMTIyZWM;" In migrate_misplaced_folio(), the check is missing when
PGPROMOTE_SUCCESS is incremented";Zi Yan;2024-07-24;0;1
C_kwDOACN7MtoAKGFjNTlhMWYwMTQ2ZjQ2YmFkN2Q1ZjhkMWIyMDc1NmVjZTQzMTIyZWM; Add the missing check.;Zi Yan;2024-07-24;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;mm/migrate: fix deadlock in migrate_pages_batch() on large folios;Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"Currently, migrate_pages_batch() can lock multiple locked folios with an
arbitrary order";Gao Xiang;2024-07-29;0;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;" Although folio_trylock() is used to avoid deadlock as
commit 2ef7dbb26990 (""migrate_pages: try migrate in batch asynchronously
firstly"") mentioned, it seems try_split_folio() is still missing";Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"It was found by compaction stress test when I explicitly enable EROFS
compressed files to use large folios, which case I cannot reproduce with
the same workload if large folio support is off (current mainline)";Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"
Typically, filesystem reads (with locked file-backed folios) could use
another bdev/meta inode to load some other I/Os (e.g";Gao Xiang;2024-07-29;1;0
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;" inode extent
metadata or caching compressed data), so the locking order will be";Gao Xiang;2024-07-29;1;0
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"  file-backed folios  (A)
     bdev/meta folios (B)
The following calltrace shows the deadlock";Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"   Thread 1 takes (B) lock and tries to take folio (A) lock
   Thread 2 takes (A) lock and tries to take folio (B) lock
[Thread 1]
INFO: task stress:1824 blocked for more than 30 seconds";Gao Xiang;2024-07-29;0;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"      Tainted: G           OE      6.10.0-rc7+ #6
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"task:stress          state:D stack:0     pid:1824  tgid:1824  ppid:1822   flags:0x0000000c
Call trace";Gao Xiang;2024-07-29;1;0
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;" __switch_to+0xec/0x138
 __schedule+0x43c/0xcb0
 schedule+0x54/0x198
 io_schedule+0x44/0x70
 folio_wait_bit_common+0x184/0x3f8
			<-- folio mapping ffff00036d69cb18 index 996  (**)
 __folio_lock+0x24/0x38
 migrate_pages_batch+0x77c/0xea0	// try_split_folio (mm/migrate.c:1486:2)
					// migrate_pages_batch (mm/migrate.c:1734:16)
		<--- LIST_HEAD(unmap_folios) has
			folio mapping 0xffff0000d184f1d8 index 1711;   (*)
 migrate_pages+0xb28/0xe90
 compact_zone+0xa08/0x10f0
 compact_node+0x9c/0x180
 sysctl_compaction_handler+0x8c/0x118
 proc_sys_call_handler+0x1a8/0x280
 proc_sys_write+0x1c/0x30
 vfs_write+0x240/0x380
 ksys_write+0x78/0x118
 __arm64_sys_write+0x24/0x38
 invoke_syscall+0x78/0x108
 el0_svc_common.constprop.0+0x48/0xf0
 do_el0_svc+0x24/0x38
 el0_svc+0x3c/0x148
 el0t_64_sync_handler+0x100/0x130
 el0t_64_sync+0x190/0x198
[Thread 2]
INFO: task stress:1825 blocked for more than 30 seconds";Gao Xiang;2024-07-29;0;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"      Tainted: G           OE      6.10.0-rc7+ #6
""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";Gao Xiang;2024-07-29;1;1
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;"task:stress          state:D stack:0     pid:1825  tgid:1825  ppid:1822   flags:0x0000000c
Call trace";Gao Xiang;2024-07-29;1;0
C_kwDOACN7MtoAKDJlNjUwNmUxYzRlZWQyNjc2YTg0MTIyMzEwNDZmMzFlMTBlMjQwZGE;" __switch_to+0xec/0x138
 __schedule+0x43c/0xcb0
 schedule+0x54/0x198
 io_schedule+0x44/0x70
 folio_wait_bit_common+0x184/0x3f8
			<-- folio = 0xfffffdffc6b503c0 (mapping == 0xffff0000d184f1d8 index == 1711) (*)
 __folio_lock+0x24/0x38
 z_erofs_runqueue+0x384/0x9c0 [erofs]
 z_erofs_readahead+0x21c/0x350 [erofs]       <-- folio mapping 0xffff00036d69cb18 range from [992, 1024] (**)
 read_pages+0x74/0x328
 page_cache_ra_order+0x26c/0x348
 ondemand_readahead+0x1c0/0x3a0
 page_cache_sync_ra+0x9c/0xc0
 filemap_get_pages+0xc4/0x708
 filemap_read+0x104/0x3a8
 generic_file_read_iter+0x4c/0x150
 vfs_read+0x27c/0x330
 ksys_pread64+0x84/0xd0
 __arm64_sys_pread64+0x28/0x40
 invoke_syscall+0x78/0x108
 el0_svc_common.constprop.0+0x48/0xf0
 do_el0_svc+0x24/0x38
 el0_svc+0x3c/0x148
 el0t_64_sync_handler+0x100/0x130
 el0t_64_sync+0x190/0x198";Gao Xiang;2024-07-29;0;0
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;mm/migrate: putback split folios when numa hint migration fails;Peter Xu;2024-07-08;1;0
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;This issue is not from any report yet, but by code observation only;Peter Xu;2024-07-08;1;0
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;"This is yet another fix besides Hugh's patch [1] but on relevant code
path, where eager split of folio can happen if the folio is already on
deferred list during a folio migration";Peter Xu;2024-07-08;1;1
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;"Here the issue is NUMA path (migrate_misplaced_folio()) may start to
encounter such folio split now even with MR_NUMA_MISPLACED hint applied";Peter Xu;2024-07-08;0;1
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;"
Then when migrate_pages() didn't migrate all the folios, it's possible the
split small folios be put onto the list instead of the original folio";Peter Xu;2024-07-08;1;0
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;"
Then putting back only the head page won't be enough";Peter Xu;2024-07-08;0;1
C_kwDOACN7MtoAKDZlNDkwMTlkYjVmN2EwOWE5YzBlOGFjNGQxMDhlNjU2YzNmOGU1ODM;Fix it by putting back all the folios on the list.;Peter Xu;2024-07-08;1;1
C_kwDOACN7MtoAKGE1ZWE1MjEyNTBhZmRmM2Q3MGM3Mjk3MDY2MGY0NGFlYmY1NmVhMTk;mm: simplify folio_migrate_mapping();Hugh Dickins;2024-07-06;1;1
C_kwDOACN7MtoAKGE1ZWE1MjEyNTBhZmRmM2Q3MGM3Mjk3MDY2MGY0NGFlYmY1NmVhMTk;"Now that folio_undo_large_rmappable() is an inline function checking
order and large_rmappable for itself (and __folio_undo_large_rmappable()
is now declared even when CONFIG_TRANASPARENT_HUGEPAGE is off) there is
no need for folio_migrate_mapping() to check large and large_rmappable
first (in the mapping case when it has had to freeze anyway).";Hugh Dickins;2024-07-06;0;1
C_kwDOACN7MtoAKGU2YzBjMDMyNDViMTRkNmMyMDU4MTRhZWU2NzEyODI1N2QwYmVhODQ;mm: provide mm_struct and address to huge_ptep_get();Christophe Leroy;2024-07-02;1;0
C_kwDOACN7MtoAKGU2YzBjMDMyNDViMTRkNmMyMDU4MTRhZWU2NzEyODI1N2QwYmVhODQ;"On powerpc 8xx huge_ptep_get() will need to know whether the given ptep is
a PTE entry or a PMD entry";Christophe Leroy;2024-07-02;1;0
C_kwDOACN7MtoAKGU2YzBjMDMyNDViMTRkNmMyMDU4MTRhZWU2NzEyODI1N2QwYmVhODQ;" This cannot be known with the PMD entry
itself because there is no easy way to know it from the content of the
entry";Christophe Leroy;2024-07-02;0;1
C_kwDOACN7MtoAKGU2YzBjMDMyNDViMTRkNmMyMDU4MTRhZWU2NzEyODI1N2QwYmVhODQ;"So huge_ptep_get() will need to know either the size of the page or get
the pmd";Christophe Leroy;2024-07-02;1;1
C_kwDOACN7MtoAKGU2YzBjMDMyNDViMTRkNmMyMDU4MTRhZWU2NzEyODI1N2QwYmVhODQ;"In order to be consistent with huge_ptep_get_and_clear(), give mm and
address to huge_ptep_get().";Christophe Leroy;2024-07-02;1;1
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;mm, virt: merge AS_UNMOVABLE and AS_INACCESSIBLE;Paolo Bonzini;2024-07-11;1;0
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"AS_UNMOVABLE is already in existing versions of Linux, while AS_INACCESSIBLE was
acked for inclusion in 6.11";Paolo Bonzini;2024-07-11;0;1
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"But really, they are the same thing: only guest_memfd uses them, at least for
now, and guest_memfd pages are unmovable because they should not be
accessed by the CPU";Paolo Bonzini;2024-07-11;0;1
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"So merge them into one; use the AS_INACCESSIBLE name which is more comprehensive";Paolo Bonzini;2024-07-11;1;1
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"At the same time, this fixes an embarrassing bug where AS_INACCESSIBLE was used
as a bit mask, despite it being just a bit index";Paolo Bonzini;2024-07-11;1;1
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"The bug was mostly benign, because AS_INACCESSIBLE's bit representation (1010)
corresponded to setting AS_UNEVICTABLE (which is already set) and AS_ENOSPC
(except no async writes can happen on the guest_memfd)";Paolo Bonzini;2024-07-11;1;0
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;" So the AS_INACCESSIBLE
flag simply had no effect";Paolo Bonzini;2024-07-11;1;0
C_kwDOACN7MtoAKDI3ZTZhMjRhNGNmM2QyNTQyMWMwZjZlYmI3YzM5ZjQ1ZmMxNGQyMGY;"Fixes: 1d23040caa8b (""KVM: guest_memfd: Use AS_INACCESSIBLE when creating guest_memfd inode"")
Fixes: c72ceafbd12c (""mm: Introduce AS_INACCESSIBLE for encrypted/confidential memory"")";Paolo Bonzini;2024-07-11;1;1
C_kwDOACN7MtoAKDNmNTk0OTM3MTM1MzRiNjFhYmNiZTJlNTc1ODJjZjdlMzFhNDJkNTE;mm: migrate: remove folio_migrate_copy();Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDNmNTk0OTM3MTM1MzRiNjFhYmNiZTJlNTc1ODJjZjdlMzFhNDJkNTE;"The folio_migrate_copy() is just a wrapper of folio_copy() and
folio_migrate_flags(), it is simple and only aio use it for now, unfold it
and remove folio_migrate_copy().";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKGYwMGIyOTViOWI2MWJiMzMyYjRmNTk1MWY0NzlhYjNhYWVhZGE1Yjg;fs: hugetlbfs: support poisoned recover from hugetlbfs_migrate_folio();Kefeng Wang;2024-06-26;1;0
C_kwDOACN7MtoAKGYwMGIyOTViOWI2MWJiMzMyYjRmNTk1MWY0NzlhYjNhYWVhZGE1Yjg;"This is similar to __migrate_folio(), use folio_mc_copy() in HugeTLB folio
migration to avoid panic when copy from poisoned folio.";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;mm: migrate: support poisoned recover from migrate folio;Kefeng Wang;2024-06-26;1;0
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;"The folio migration is widely used in kernel, memory compaction, memory
hotplug, soft offline page, numa balance, memory demote/promotion, etc,
but once access a poisoned source folio when migrating, the kerenl will
panic";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;"There is a mechanism in the kernel to recover from uncorrectable memory
errors, ARCH_HAS_COPY_MC, which is already used in other core-mm paths,
eg, CoW, khugepaged, coredump, ksm copy, see copy_mc_to_{user,kernel},
copy_mc_{user_}highpage callers";Kefeng Wang;2024-06-26;0;0
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;"In order to support poisoned folio copy recover from migrate folio, we
chose to make folio migration tolerant of memory failures and return error
for folio migration, because folio migration is no guarantee of success,
this could avoid the similar panic shown below";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;"  CPU: 1 PID: 88343 Comm: test_softofflin Kdump: loaded Not tainted 6.6.0
  pc : copy_page+0x10/0xc0
  lr : copy_highpage+0x38/0x50
  Call trace";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDA2MDkxMzk5OWQ3YTllNTBjMjgzZmRiMTUyNTNmYzI3OTc0ZGRhZGM;"   copy_page+0x10/0xc0
   folio_copy+0x78/0x90
   migrate_folio_extra+0x54/0xa0
   move_to_new_folio+0xd8/0x1f0
   migrate_folio_move+0xb8/0x300
   migrate_pages_batch+0x528/0x788
   migrate_pages_sync+0x8c/0x258
   migrate_pages+0x440/0x528
   soft_offline_in_use_page+0x2ec/0x3c0
   soft_offline_page+0x238/0x310
   soft_offline_page_store+0x6c/0xc0
   dev_attr_store+0x20/0x40
   sysfs_kf_write+0x4c/0x68
   kernfs_fop_write_iter+0x130/0x1c8
   new_sync_write+0xa4/0x138
   vfs_write+0x238/0x2d8
   ksys_write+0x74/0x110
Note, folio copy is moved in the begin of the __migrate_folio(), which
could simplify the error handling since there is no turning back if
folio_migrate_mapping() return success, the downside is the folio copied
even though folio_migrate_mapping() return fail, an optimization is to
check whether source folio does not have extra refs before we do folio
copy.";Kefeng Wang;2024-06-26;0;1
C_kwDOACN7MtoAKDUyODgxNTM5MmY4NzNmMGFmOGM2Y2RjMjc5Yzg5YmQwMTU0Y2JmNmE;mm: migrate: split folio_migrate_mapping();Kefeng Wang;2024-06-26;1;0
C_kwDOACN7MtoAKDUyODgxNTM5MmY4NzNmMGFmOGM2Y2RjMjc5Yzg5YmQwMTU0Y2JmNmE;"The folio refcount check is moved out for both !mapping and mapping folio,
also update comment from page to folio for folio_migrate_mapping()";Kefeng Wang;2024-06-26;1;1
C_kwDOACN7MtoAKDUyODgxNTM5MmY4NzNmMGFmOGM2Y2RjMjc5Yzg5YmQwMTU0Y2JmNmE;No functional change intended.;Kefeng Wang;2024-06-26;1;0
C_kwDOACN7MtoAKGJlOTU4MWVhOGMwNThkODExNTQyNTFjYjA2OTU5ODcwOTg5OTZjYWQ;mm: fix crashes from deferred split racing folio migration;Hugh Dickins;2024-07-02;1;1
C_kwDOACN7MtoAKGJlOTU4MWVhOGMwNThkODExNTQyNTFjYjA2OTU5ODcwOTg5OTZjYWQ;"Even on 6.10-rc6, I've been seeing elusive ""Bad page state""s (often on
flags when freeing, yet the flags shown are not bad: PG_locked had been
set and cleared??), and VM_BUG_ON_PAGE(page_ref_count(page) == 0)s from
deferred_split_scan()'s folio_put(), and a variety of other BUG and WARN
symptoms implying double free by deferred split and large folio migration";Hugh Dickins;2024-07-02;0;0
C_kwDOACN7MtoAKGJlOTU4MWVhOGMwNThkODExNTQyNTFjYjA2OTU5ODcwOTg5OTZjYWQ;"6.7 commit 9bcef5973e31 (""mm: memcg: fix split queue list crash when large
folio migration"") was right to fix the memcg-dependent locking broken in
85ce2c517ade (""memcontrol: only transfer the memcg data for migration""),
but missed a subtlety of deferred_split_scan(): it moves folios to its own
local list to work on them without split_queue_lock, during which time
folio->_deferred_list is not empty, but even the ""right"" lock does nothing
to secure the folio and the list it is on";Hugh Dickins;2024-07-02;0;0
C_kwDOACN7MtoAKGJlOTU4MWVhOGMwNThkODExNTQyNTFjYjA2OTU5ODcwOTg5OTZjYWQ;"Fortunately, deferred_split_scan() is careful to use folio_try_get(): so
folio_migrate_mapping() can avoid the race by folio_undo_large_rmappable()
while the old folio's reference count is temporarily frozen to 0 - adding
such a freeze in the !mapping case too (originally, folio lock and
unmapping and no swap cache left an anon folio unreachable, so no freezing
was needed there: but the deferred split queue offers a way to reach it).";Hugh Dickins;2024-07-02;1;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;mm/migrate: move NUMA hinting fault folio isolation + checks under PTL;David Hildenbrand;2024-06-20;0;0
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;"Currently we always take a folio reference even if migration will not even
be tried or isolation failed, requiring us to grab+drop an additional
reference";David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;"Further, we end up calling folio_likely_mapped_shared() while the folio
might have already been unmapped, because after we dropped the PTL, that
can easily happen";David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;" We want to stop touching mapcounts and friends from
such context, and only call folio_likely_mapped_shared() while the folio
is still mapped: mapcount information is pretty much stale and unreliable
otherwise";David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;"So let's move checks into numamigrate_isolate_folio(), rename that
function to migrate_misplaced_folio_prepare(), and call that function from
callsites where we call migrate_misplaced_folio(), but still with the PTL
held";David Hildenbrand;2024-06-20;1;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;"We can now stop taking temporary folio references, and really only take a
reference if folio isolation succeeded";David Hildenbrand;2024-06-20;1;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;" Doing the
folio_likely_mapped_shared() + folio isolation under PT lock is now
similar to how we handle MADV_PAGEOUT";David Hildenbrand;2024-06-20;1;1
C_kwDOACN7MtoAKGVlODY4MTRiMDU2MmYxODI1NWI1NWM1ZTZhMDFhMDIyODk1OTk0Y2Y;While at it, combine the folio_is_file_lru() checks.;David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;mm/migrate: make migrate_misplaced_folio() return 0 on success;David Hildenbrand;2024-06-20;1;0
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;"Patch series ""mm/migrate: move NUMA hinting fault folio isolation + checks
under PTL""";David Hildenbrand;2024-06-20;0;0
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;Let's just return 0 on success, which is less confusing;David Hildenbrand;2024-06-20;1;0
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;..;David Hildenbrand;2024-06-20;0;0
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;" especially because we got it wrong in the migrate.h stub where we
have ""return -EAGAIN; /* can't migrate now */"" instead of ""return 0;""";David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;"
Likely this wrong return value doesn't currently matter, but it certainly
adds confusion";David Hildenbrand;2024-06-20;0;1
C_kwDOACN7MtoAKDRiODhjMjNhYjhjOWJjMzg1N2Y3Yzg4NDdlMmM2YmVkOTUxODU1MzA;"We'll add migrate_misplaced_folio_prepare() next, where we want to use the
same ""return 0 on success"" approach, so let's just clean this up.";David Hildenbrand;2024-06-20;1;1
C_kwDOACN7MtoAKDkwNjYzMjg0M2QwMGE0YTQyMDcyYjE5YzQyM2IzMGE5ZTdhZGVmM2M;mm: remove MIGRATE_SYNC_NO_COPY mode;Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDkwNjYzMjg0M2QwMGE0YTQyMDcyYjE5YzQyM2IzMGE5ZTdhZGVmM2M;"Commit 2916ecc0f9d4 (""mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY"")
introduce a new MIGRATE_SYNC_NO_COPY mode to allow to offload the copy to
a device DMA engine, which is only used __migrate_device_pages() to decide
whether or not copy the old page, and the MIGRATE_SYNC_NO_COPY mode only
set in hmm, as the MIGRATE_SYNC_NO_COPY set is removed by previous
cleanup, it seems that we could remove the unnecessary
MIGRATE_SYNC_NO_COPY.";Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDk0MGQ2NjgzYzc5OTUwYjIxYjM3NjIxMjRlYWJmYTliMmY2ZmVlOTY;mm: migrate: remove migrate_folio_extra();Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDk0MGQ2NjgzYzc5OTUwYjIxYjM3NjIxMjRlYWJmYTliMmY2ZmVlOTY;"migrate_folio_extra() is only called in migrate.c now, convert it a static
function and take a new src_private argument which could be shared by
migrate_folio() and filemap_migrate_folio() to simplify code a bit.";Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDAxODc4ZjEwZjhlMDFlNmNhMTA0MGNjYzE5Yjc2ZTEwZmY3Njc4YWQ;mm: migrate: simplify __buffer_migrate_folio();Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDAxODc4ZjEwZjhlMDFlNmNhMTA0MGNjYzE5Yjc2ZTEwZmY3Njc4YWQ;"Patch series ""mm: cleanup MIGRATE_SYNC_NO_COPY mode""";Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDAxODc4ZjEwZjhlMDFlNmNhMTA0MGNjYzE5Yjc2ZTEwZmY3Njc4YWQ;"Commit 2916ecc0f9d4 (""mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY"")
introduce a new MIGRATE_SYNC_NO_COPY mode to allow to offload the copy to
a device DMA engine, which is only used __migrate_device_pages() to decide
whether or not copy the old page, and the MIGRATE_SYNC_NO_COPY mode only
used in hmm, a easy way is just to call the folio_migrate_mapping() and
folio_migrate_flags(), which help to remove the MIGRATE_SYNC_NO_COPY mode";Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKDAxODc4ZjEwZjhlMDFlNmNhMTA0MGNjYzE5Yjc2ZTEwZmY3Njc4YWQ;This patch (of 5);Kefeng Wang;2024-05-24;1;0
C_kwDOACN7MtoAKDAxODc4ZjEwZjhlMDFlNmNhMTA0MGNjYzE5Yjc2ZTEwZmY3Njc4YWQ;Use filemap_migrate_folio() helper to simplify __buffer_migrate_folio().;Kefeng Wang;2024-05-24;1;1
C_kwDOACN7MtoAKGM2NDA4MjUwNzAzNTMwMTg3Y2M2MjUwZGNkNzAyZDEyYTcxYzQ0ZjU;mm/migrate: make migrate_pages_batch() stats consistent;Zi Yan;2024-06-18;1;0
C_kwDOACN7MtoAKGM2NDA4MjUwNzAzNTMwMTg3Y2M2MjUwZGNkNzAyZDEyYTcxYzQ0ZjU;"As Ying pointed out in [1], stats->nr_thp_failed needs to be updated to
avoid stats inconsistency between MIGRATE_SYNC and MIGRATE_ASYNC when
calling migrate_pages_batch()";Zi Yan;2024-06-18;1;1
C_kwDOACN7MtoAKGM2NDA4MjUwNzAzNTMwMTg3Y2M2MjUwZGNkNzAyZDEyYTcxYzQ0ZjU;"Because if not, when migrate_pages_batch() is called via
migrate_pages(MIGRATE_ASYNC), nr_thp_failed will not be increased and when
migrate_pages_batch() is called via migrate_pages(MIGRATE_SYNC*),
nr_thp_failed will be increase in migrate_pages_sync() by
stats->nr_thp_failed += astats.nr_thp_split.";Zi Yan;2024-06-18;0;0
C_kwDOACN7MtoAKDhlMjc5Zjk3MGI1Y2IwNjI4Zjg1NmI2NzM1ZTJlNDdiNGRhOWY3NmU;mm/migrate: fix kernel BUG at mm/compaction.c:2761!;Hugh Dickins;2024-06-12;1;1
C_kwDOACN7MtoAKDhlMjc5Zjk3MGI1Y2IwNjI4Zjg1NmI2NzM1ZTJlNDdiNGRhOWY3NmU;"I hit the VM_BUG_ON(!list_empty(&cc->migratepages)) in compact_zone(); and
if DEBUG_VM were off, then pages would be lost on a local list";Hugh Dickins;2024-06-12;0;0
C_kwDOACN7MtoAKDhlMjc5Zjk3MGI1Y2IwNjI4Zjg1NmI2NzM1ZTJlNDdiNGRhOWY3NmU;"Our convention is that if migrate_pages() reports complete success (0),
then the migratepages list will be empty; but if it reports an error or
some pages remaining, then its caller must putback_movable_pages()";Hugh Dickins;2024-06-12;1;1
C_kwDOACN7MtoAKDhlMjc5Zjk3MGI1Y2IwNjI4Zjg1NmI2NzM1ZTJlNDdiNGRhOWY3NmU;"There's a new case in which migrate_pages() has been reporting complete
success, but returning with pages left on the migratepages list: when
migrate_pages_batch() successfully split a folio on the deferred list, but
then the ""Failure isn't counted"" call does not dispose of them all";Hugh Dickins;2024-06-12;0;0
C_kwDOACN7MtoAKDhlMjc5Zjk3MGI1Y2IwNjI4Zjg1NmI2NzM1ZTJlNDdiNGRhOWY3NmU;"Since that block is expecting the large folio to have been counted as 1
failure already, and since the return code is later adjusted to success
whenever the returned list is found empty, the simple way to fix this
safely is to count splitting the deferred folio as ""a failure"".";Hugh Dickins;2024-06-12;1;1
C_kwDOACN7MtoAKDZlOGNkYTRjMmM4N2IyYTQ0ODI4ZTY1MWExMDcwNTY0N2E2ZmQ1NDI;mm: convert hugetlb_page_mapping_lock_write to folio;Matthew Wilcox (Oracle);2024-04-12;1;0
C_kwDOACN7MtoAKDZlOGNkYTRjMmM4N2IyYTQ0ODI4ZTY1MWExMDcwNTY0N2E2ZmQ1NDI;"The page is only used to get the mapping, so the folio will do just as
well";Matthew Wilcox (Oracle);2024-04-12;0;1
C_kwDOACN7MtoAKDZlOGNkYTRjMmM4N2IyYTQ0ODI4ZTY1MWExMDcwNTY0N2E2ZmQ1NDI;" Both callers already have a folio available, so this saves a call
to compound_head().";Matthew Wilcox (Oracle);2024-04-12;0;1
C_kwDOACN7MtoAKDc5ODk5Y2NlMzNlMDg4N2MwNmQ0MWU3NjdhYTU0M2FhYWFlZjQ4ZTI;mm/ksm: convert chain series funcs and replace get_ksm_page;Alex Shi (tencent);2024-04-11;1;0
C_kwDOACN7MtoAKDc5ODk5Y2NlMzNlMDg4N2MwNmQ0MWU3NjdhYTU0M2FhYWFlZjQ4ZTI;"In ksm stable tree all page are single, let's convert them to use and
folios as well as stable_tree_insert/stable_tree_search funcs";Alex Shi (tencent);2024-04-11;1;1
C_kwDOACN7MtoAKDc5ODk5Y2NlMzNlMDg4N2MwNmQ0MWU3NjdhYTU0M2FhYWFlZjQ4ZTI;" And
replace get_ksm_page() by ksm_get_folio() since there is no more needs";Alex Shi (tencent);2024-04-11;1;1
C_kwDOACN7MtoAKDc5ODk5Y2NlMzNlMDg4N2MwNmQ0MWU3NjdhYTU0M2FhYWFlZjQ4ZTI;It could save a few compound_head calls.;Alex Shi (tencent);2024-04-11;0;0
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;mm/migrate: use folio_likely_mapped_shared() in add_page_for_migration();David Hildenbrand;2024-04-09;1;0
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;"We want to limit the use of page_mapcount() to the places where it is
absolutely necessary";David Hildenbrand;2024-04-09;1;1
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;" In add_page_for_migration(), we actually want to
check if the folio is mapped shared, to reject such folios";David Hildenbrand;2024-04-09;1;1
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;" So let's use
folio_likely_mapped_shared() instead";David Hildenbrand;2024-04-09;1;1
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;For small folios, fully mapped THP, and hugetlb folios, there is no change;David Hildenbrand;2024-04-09;0;0
C_kwDOACN7MtoAKDMxY2UwZDdlZjg0MTJlM2YzOGNmOTBiYjkwYjc0MzYxMzBjNjA2MTQ;"For partially mapped, shared THP, we should now do a better job at
rejecting such folios.";David Hildenbrand;2024-04-09;1;1
C_kwDOACN7MtoAKDRkYzdkMzczNzA5NTFmZTg2MjE2ZjAzYTRlMGE2OTA5ZjliOTBhOGM;remove references to page->flags in documentation;Matthew Wilcox (Oracle);2024-03-26;1;0
C_kwDOACN7MtoAKDRkYzdkMzczNzA5NTFmZTg2MjE2ZjAzYTRlMGE2OTA5ZjliOTBhOGM;"Mostly rewording, but remove entirely the copy of page_fixed_fake_head()
in the documentation; we can refer people to the actual source if
necessary.";Matthew Wilcox (Oracle);2024-03-26;1;1
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;mm: convert folio_estimated_sharers() to folio_likely_mapped_shared();David Hildenbrand;2024-02-27;1;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"Callers of folio_estimated_sharers() only care about ""mapped shared vs";David Hildenbrand;2024-02-27;0;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"
mapped exclusively"", not the exact estimate of sharers";David Hildenbrand;2024-02-27;0;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;" Let's consolidate
and unify the condition users are checking";David Hildenbrand;2024-02-27;1;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;" While at it clarify the
semantics and extend the discussion on the fuzziness";David Hildenbrand;2024-02-27;0;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"Use the ""likely mapped shared"" terminology to better express what the
(adjusted) function actually checks";David Hildenbrand;2024-02-27;1;1
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"Whether a partially-mappable folio is more likely to not be partially
mapped than partially mapped is debatable";David Hildenbrand;2024-02-27;0;1
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;" In the future, we might be
able to improve our estimate for partially-mappable folios, though";David Hildenbrand;2024-02-27;1;1
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"Note that we will now consistently detect ""mapped shared"" only if the
first subpage is actually mapped multiple times";David Hildenbrand;2024-02-27;1;1
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;" When the first subpage
is not mapped, we will consistently detect it as ""mapped exclusively""";David Hildenbrand;2024-02-27;1;0
C_kwDOACN7MtoAKGViYjM0Zjc4ZDcyYzIzMjA2MjBiYTZkNTVjYjIyYTUyOTQ5MDQ3YTE;"
This change should currently only affect the usage in
madvise_free_pte_range() and queue_folios_pte_range() for large folios: if
the first page was already unmapped, we would have skipped the folio.";David Hildenbrand;2024-02-27;0;0
C_kwDOACN7MtoAKDcyNjJmMjA4Y2E2ODEzODVkMTMzODQ0YmU4YTU4ZDliNGNhMTg1Zjc;mm/migrate: split source folio if it is on deferred split list;Zi Yan;2024-03-22;1;0
C_kwDOACN7MtoAKDcyNjJmMjA4Y2E2ODEzODVkMTMzODQ0YmU4YTU4ZDliNGNhMTg1Zjc;"If the source folio is on deferred split list, it is likely some subpages
are not used";Zi Yan;2024-03-22;0;0
C_kwDOACN7MtoAKDcyNjJmMjA4Y2E2ODEzODVkMTMzODQ0YmU4YTU4ZDliNGNhMTg1Zjc;" Split it before migration to avoid migrating unused
subpages";Zi Yan;2024-03-22;0;1
C_kwDOACN7MtoAKDcyNjJmMjA4Y2E2ODEzODVkMTMzODQ0YmU4YTU4ZDliNGNhMTg1Zjc;"Commit 616b8371539a6 (""mm: thp: enable thp migration in generic path"") did
not check if a THP is on deferred split list before migration, thus, the
destination THP is never put on deferred split list even if the source THP
might be";Zi Yan;2024-03-22;0;1
C_kwDOACN7MtoAKDcyNjJmMjA4Y2E2ODEzODVkMTMzODQ0YmU4YTU4ZDliNGNhMTg1Zjc;" The opportunity of reclaiming free pages in a partially mapped
THP during deferred list scanning is lost, but no other harmful
consequence is present[1].";Zi Yan;2024-03-22;1;0
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;mm: hugetlb: make the hugetlb migration strategy consistent;Baolin Wang;2024-03-06;1;0
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"As discussed in previous thread [1], there is an inconsistency when
handing hugetlb migration";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" When handling the migration of freed hugetlb,
it prevents fallback to other NUMA nodes in
alloc_and_dissolve_hugetlb_folio()";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" However, when dealing with in-use
hugetlb, it allows fallback to other NUMA nodes in
alloc_hugetlb_folio_nodemask(), which can break the per-node hugetlb pool
and might result in unexpected failures when node bound workloads doesn't
get what is asssumed available";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"To make hugetlb migration strategy more clear, we should list all the scenarios
of hugetlb migration and analyze whether allocation fallback is permitted";Baolin Wang;2024-03-06;1;0
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"1) Memory offline: will call dissolve_free_huge_pages() to free the
   freed hugetlb, and call do_migrate_range() to migrate the in-use
   hugetlb";Baolin Wang;2024-03-06;0;0
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" Both can break the per-node hugetlb pool, but as this is an
   explicit offlining operation, no better choice";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" So should allow the
   hugetlb allocation fallback";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;2) Memory failure: same as memory offline;Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" Should allow fallback to a
   different node might be the only option to handle it, otherwise the
   impact of poisoned memory can be amplified";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"3) Longterm pinning: will call migrate_longterm_unpinnable_pages() to
   migrate in-use and not-longterm-pinnable hugetlb, which can break the
   per-node pool";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" But we should fail to longterm pinning if can not
   allocate on current node to avoid breaking the per-node pool";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"4) Syscalls (mbind, migrate_pages, move_pages): these are explicit
   users operation to move pages to other nodes, so fallback to other
   nodes should not be prohibited";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"5) alloc_contig_range: used by CMA allocation and virtio-mem
   fake-offline to allocate given range of pages";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" Now the freed hugetlb
   migration is not allowed to fallback, to keep consistency, the in-use
   hugetlb migration should be also not allowed to fallback";Baolin Wang;2024-03-06;1;0
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;6) alloc_contig_pages: used by kfence, pgtable_debug etc;Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;" The strategy
   should be consistent with that of alloc_contig_range()";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKDQyZDBjM2ZiYjU4MTFmYmZiNjYzZDhlZGUxZDdmZmJhMDJlN2FlMTg;"Based on the analysis of the various scenarios above, introducing a new
helper to determine whether fallback is permitted according to the
migration reason..";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;mm: record the migration reason for struct migration_target_control;Baolin Wang;2024-03-06;0;0
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;"Patch series ""make the hugetlb migration strategy consistent"", v2";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;"As discussed in previous thread [1], there is an inconsistency when
handling hugetlb migration";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;" When handling the migration of freed hugetlb,
it prevents fallback to other NUMA nodes in
alloc_and_dissolve_hugetlb_folio()";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;" However, when dealing with in-use
hugetlb, it allows fallback to other NUMA nodes in
alloc_hugetlb_folio_nodemask(), which can break the per-node hugetlb pool
and might result in unexpected failures when node bound workloads doesn't
get what is asssumed available";Baolin Wang;2024-03-06;0;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;"This patchset tries to make the hugetlb migration strategy more clear
and consistent";Baolin Wang;2024-03-06;1;1
C_kwDOACN7MtoAKGU0MmRmZTRlMGE1MWI0NzZkY2M2ZjE0NjFjNTFmZGIxYjc2NTczYWE;Please find details in each patch.;Baolin Wang;2024-03-06;0;0
C_kwDOACN7MtoAKDFmMTE4M2M0YzBiYzYwOWM5OGI3MWFiOTI4MWZmNzI1MzNkODliYjA;merge mm-hotfixes-stable into mm-nonmm-stable to pick up stackdepot changes;Andrew Morton;2024-02-24;1;1
C_kwDOACN7MtoAKDFmMTE4M2M0YzBiYzYwOWM5OGI3MWFiOTI4MWZmNzI1MzNkODliYjA;;Andrew Morton;2024-02-24;0;0
C_kwDOACN7MtoAKDI3NzRmMjU2ZTdjMDIxOWUyYjBhMDg5NGFmMWM3NmJkYWJjNGY5NzQ;mm/vmscan: fix a bug calling wakeup_kswapd() with a wrong zone index;Byungchul Park;2024-02-16;1;1
C_kwDOACN7MtoAKDI3NzRmMjU2ZTdjMDIxOWUyYjBhMDg5NGFmMWM3NmJkYWJjNGY5NzQ;"With numa balancing on, when a numa system is running where a numa node
doesn't have its local memory so it has no managed zones, the following
oops has been observed";Byungchul Park;2024-02-16;0;1
C_kwDOACN7MtoAKDI3NzRmMjU2ZTdjMDIxOWUyYjBhMDg5NGFmMWM3NmJkYWJjNGY5NzQ;" It's because wakeup_kswapd() is called with a
wrong zone index, -1";Byungchul Park;2024-02-16;0;0
C_kwDOACN7MtoAKDI3NzRmMjU2ZTdjMDIxOWUyYjBhMDg5NGFmMWM3NmJkYWJjNGY5NzQ;" Fixed it by checking the index before calling
wakeup_kswapd()";Byungchul Park;2024-02-16;1;1
C_kwDOACN7MtoAKDI3NzRmMjU2ZTdjMDIxOWUyYjBhMDg5NGFmMWM3NmJkYWJjNGY5NzQ;"> BUG: unable to handle page fault for address: 00000000000033f3
> #PF: supervisor read access in kernel mode
> #PF: error_code(0x0000) - not-present page
> PGD 0 P4D 0
> Oops: 0000 [#1] PREEMPT SMP NOPTI
> CPU: 2 PID: 895 Comm: masim Not tainted 6.6.0-dirty #255
> Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS
>    rel-1.16.0-0-gd239552ce722-prebuilt.qemu.org 04/01/2014
> RIP: 0010:wakeup_kswapd (./linux/mm/vmscan.c:7812)
> Code: (omitted)
> RSP: 0000:ffffc90004257d58 EFLAGS: 00010286
> RAX: ffffffffffffffff RBX: ffff88883fff0480 RCX: 0000000000000003
> RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff88883fff0480
> RBP: ffffffffffffffff R08: ff0003ffffffffff R09: ffffffffffffffff
> R10: ffff888106c95540 R11: 0000000055555554 R12: 0000000000000003
> R13: 0000000000000000 R14: 0000000000000000 R15: ffff88883fff0940
> FS:  00007fc4b8124740(0000) GS:ffff888827c00000(0000) knlGS:0000000000000000
> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
> CR2: 00000000000033f3 CR3: 000000026cc08004 CR4: 0000000000770ee0
> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
> PKRU: 55555554";Byungchul Park;2024-02-16;0;0
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;mm/migrate: preserve exact soft-dirty state;Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;pte_mkdirty() sets both _PAGE_DIRTY and _PAGE_SOFT_DIRTY bits;Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;" The
_PAGE_SOFT_DIRTY can get set even if it wasn't set on original page before
migration";Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;" This makes non-soft-dirty pages soft-dirty just because of
migration/compaction";Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;" Clear the _PAGE_SOFT_DIRTY flag if it wasn't set on
original page";Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;"By definition of soft-dirty feature, there can be spurious soft-dirty
pages because of kernel's internal activity such as VMA merging or
migration/compaction";Paul Gofman;2024-02-06;0;1
C_kwDOACN7MtoAKDA1NTI2N2ZlYWVjYzljNmM1M2YxMjhhYTUwNzQ2ZjY0YzRlZjVjYTI;" This patch is eliminating the spurious soft-dirty
pages because of migration/compaction.";Paul Gofman;2024-02-06;1;1
C_kwDOACN7MtoAKGExNWRjNDc4NWM5OGYzNjBiZGNhNzg0ODM0NTVlMGFmZjMwMjQyY2I;mm/migrate: page_add_anon_rmap() -> folio_add_anon_rmap_pte();David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKGExNWRjNDc4NWM5OGYzNjBiZGNhNzg0ODM0NTVlMGFmZjMwMjQyY2I;Let's convert remove_migration_pte().;David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKGM0ZGZmYjBiYzIzN2Q1ZTNiNTFhZGY5NDcwNjJlNjVlZDM0YWMzYzM;mm/migrate: page_add_file_rmap() -> folio_add_file_rmap_pte();David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKGM0ZGZmYjBiYzIzN2Q1ZTNiNTFhZGY5NDcwNjJlNjVlZDM0YWMzYzM;Let's convert remove_migration_pte().;David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;mm/rmap: introduce and use hugetlb_add_file_rmap();David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;"hugetlb rmap handling differs quite a lot from ""ordinary"" rmap code";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;" For
example, hugetlb currently only supports entire mappings, and treats any
mapping as mapped using a single ""logical PTE""";David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;" Let's move it out of the
way so we can overhaul our ""ordinary"" rmap";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY; implementation/interface;David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;"Right now we're using page_dup_file_rmap() in some cases where ""ordinary""
rmap code would have used page_add_file_rmap()";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;" So let's introduce and
use hugetlb_add_file_rmap() instead";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;" We won't be adding a
""hugetlb_dup_file_rmap()"" functon for the fork() case, as it would be
doing the same: ""dup"" is just an optimization for ""add""";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;What remains is a single page_dup_file_rmap() call in fork() code;David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDQ0ODg3ZjM5OTQ1NTE5ZmE4NDA1MTMzYjFhY2QwOThmZGE5Yzk3NDY;"Add sanity checks that we end up with the right folios in the right
functions.";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;mm/rmap: rename hugepage_add* to hugetlb_add*;David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"Patch series ""mm/rmap: interface overhaul"", v2";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"This series overhauls the rmap interface, to get rid of the ""bool
compound"" / RMAP_COMPOUND parameter with the goal of making the interface
less error prone, more future proof, and more natural to extend to
""batching""";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;" Also, this converts the interface to always consume
folio+subpage, which speeds up operations on large folios";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"Further, this series adds PTE-batching variants for 4 rmap functions,
whereby only folio_add_anon_rmap_ptes() is used for batching in this
series when PTE-remapping a PMD-mapped THP";David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;" folio_remove_rmap_ptes(),
folio_try_dup_anon_rmap_ptes() and folio_dup_file_rmap_ptes() will soon
come in handy[1,2]";David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;This series performs a lot of folio conversion along the way;David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;" Most of the
added LOC in the diff are only due to documentation";David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"As we're moving to a pte/pmd interface where we clearly express the
mapping granularity we are dealing with, we first get the remainder of
hugetlb out of the way, as it is special and expected to remain special";David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"it treats everything as a ""single logical PTE"" and only currently allows
entire mappings";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"Even if we'd ever support partial mappings, I strongly assume the
interface and implementation will still differ heavily: hopefull we can
avoid working on subpages/subpage mapcounts completely and only add a
""count"" parameter for them to enable batching";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;New (extended) hugetlb interface that operates on entire folio;David Hildenbrand;2023-12-20;0;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"New ""ordinary"" interface for small folios / THP:";David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"folio_add_new_anon_rmap() will always map at the largest granularity
possible (currently, a single PMD to cover a PMD-sized THP)";David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;" Could be
extended if ever required";David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;"In the future, we might want ""_pud"" variants and eventually ""_pmds""
variants for batching";David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;I ran some simple microbenchmarks on an Intel(R) Xeon(R) Silver 4210R;David Hildenbrand;2023-12-20;1;1
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;measuring munmap(), fork(), cow, MADV_DONTNEED on each PTE ..;David Hildenbrand;2023-12-20;1;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;" and PTE
remapping PMD-mapped THPs on 1 GiB of memory";David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;For small folios, there is barely a change (< 1% improvement for me);David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKDlkNWZhZmQ1ZDg4MjQ0Njk5OTM2NmY2NzNhYjA2ZWRiYTQ1M2Y4NjI;For PTE-mapped THP:;David Hildenbrand;2023-12-20;0;0
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;mm: migrate: fix getting incorrect page mapping during page migration;Baolin Wang;2023-12-15;1;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;When running stress-ng testing, we found below kernel crash after a few hours;Baolin Wang;2023-12-15;0;0
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"Unable to handle kernel NULL pointer dereference at virtual address 0000000000000000
pc : dentry_name+0xd8/0x224
lr : pointer+0x22c/0x370
sp : ffff800025f134c0
Call trace";Baolin Wang;2023-12-15;0;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"  dentry_name+0xd8/0x224
  pointer+0x22c/0x370
  vsnprintf+0x1ec/0x730
  vscnprintf+0x2c/0x60
  vprintk_store+0x70/0x234
  vprintk_emit+0xe0/0x24c
  vprintk_default+0x3c/0x44
  vprintk_func+0x84/0x2d0
  printk+0x64/0x88
  __dump_page+0x52c/0x530
  dump_page+0x14/0x20
  set_migratetype_isolate+0x110/0x224
  start_isolate_page_range+0xc4/0x20c
  offline_pages+0x124/0x474
  memory_block_offline+0x44/0xf4
  memory_subsys_offline+0x3c/0x70
  device_offline+0xf0/0x120
After analyzing the vmcore, I found this issue is caused by page migration";Baolin Wang;2023-12-15;0;0
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"The scenario is that, one thread is doing page migration, and we will use the
target page's ->mapping field to save 'anon_vma' pointer between page unmap and
page move, and now the target page is locked and refcount is 1";Baolin Wang;2023-12-15;0;0
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"Currently, there is another stress-ng thread performing memory hotplug,
attempting to offline the target page that is being migrated";Baolin Wang;2023-12-15;0;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"It discovers that
the refcount of this target page is 1, preventing the offline operation, thus
proceeding to dump the page";Baolin Wang;2023-12-15;0;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"However, page_mapping() of the target page may
return an incorrect file mapping to crash the system in dump_mapping(), since
the target page->mapping only saves 'anon_vma' pointer without setting
PAGE_MAPPING_ANON flag";Baolin Wang;2023-12-15;1;0
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;There are seveval ways to fix this issue;Baolin Wang;2023-12-15;0;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"(1) Setting the PAGE_MAPPING_ANON flag for target page's ->mapping when saving
'anon_vma', but this can confuse PageAnon() for PFN walkers, since the target
page has not built mappings yet";Baolin Wang;2023-12-15;1;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"(2) Getting the page lock to call page_mapping() in __dump_page() to avoid crashing
the system, however, there are still some PFN walkers that call page_mapping()
without holding the page lock, such as compaction";Baolin Wang;2023-12-15;0;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"(3) Using target page->private field to save the 'anon_vma' pointer and 2 bits
page state, just as page->mapping records an anonymous page, which can remove
the page_mapping() impact for PFN walkers and also seems a simple way";Baolin Wang;2023-12-15;1;1
C_kwDOACN7MtoAKGQxYWRiMjVkZjcxMTFkZTgzYjY0NjU1YTgwYjVhMTM1YWRiZGVkNjE;"So I choose option 3 to fix this issue, and this can also fix other potential
issues for PFN walkers, such as compaction.";Baolin Wang;2023-12-15;1;1
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;mm: migrate high-order folios in swap cache correctly;Charan Teja Kalla;2023-12-14;1;0
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;"Large folios occupy N consecutive entries in the swap cache instead of
using multi-index entries like the page cache";Charan Teja Kalla;2023-12-14;1;0
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;" However, if a large folio
is re-added to the LRU list, it can be migrated";Charan Teja Kalla;2023-12-14;0;0
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;" The migration code was
not aware of the difference between the swap cache and the page cache and
assumed that a single xas_store() would be sufficient";Charan Teja Kalla;2023-12-14;0;0
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;"This leaves potentially many stale pointers to the now-migrated folio in
the swap cache, which can lead to almost arbitrary data corruption in the
future";Charan Teja Kalla;2023-12-14;0;1
C_kwDOACN7MtoAKGZjMzQ2ZDBhNzBhMTNkNTJmZTFjNGJjNDk1MTZkODNhNDJjZDdjNGM;" This can also manifest as infinite loops with the RCU read lock
held.";Charan Teja Kalla;2023-12-14;1;1
C_kwDOACN7MtoAKDYwMGYxMTFlZjUxZGMyY2JkYjMzMGIwOWQwOWYxODU2ZWZhNjQ5MTI;fs: Rename mapping private members;Matthew Wilcox (Oracle);2023-11-17;1;1
C_kwDOACN7MtoAKDYwMGYxMTFlZjUxZGMyY2JkYjMzMGIwOWQwOWYxODU2ZWZhNjQ5MTI;"It is hard to find where mapping->private_lock, mapping->private_list and
mapping->private_data are used, due to private_XXX being a relatively
common name for variables and structure members in the kernel";Matthew Wilcox (Oracle);2023-11-17;0;1
C_kwDOACN7MtoAKDYwMGYxMTFlZjUxZGMyY2JkYjMzMGIwOWQwOWYxODU2ZWZhNjQ5MTI;" To fit
with other members of struct address_space, rename them all to have an
i_ prefix";Matthew Wilcox (Oracle);2023-11-17;1;0
C_kwDOACN7MtoAKDYwMGYxMTFlZjUxZGMyY2JkYjMzMGIwOWQwOWYxODU2ZWZhNjQ5MTI; Tested with an allmodconfig build.;Matthew Wilcox (Oracle);2023-11-17;1;0
C_kwDOACN7MtoAKDAwMDNlMmE0MTQ2ODdmZmY2YTc1MjUwZDM4MWU0YWJmMzQ1ZDY2M2Y;mm: Add AS_UNMOVABLE to mark mapping as completely unmovable;Sean Christopherson;2023-10-27;1;0
C_kwDOACN7MtoAKDAwMDNlMmE0MTQ2ODdmZmY2YTc1MjUwZDM4MWU0YWJmMzQ1ZDY2M2Y;"Add an ""unmovable"" flag for mappings that cannot be migrated under any
circumstance";Sean Christopherson;2023-10-27;1;1
C_kwDOACN7MtoAKDAwMDNlMmE0MTQ2ODdmZmY2YTc1MjUwZDM4MWU0YWJmMzQ1ZDY2M2Y;" KVM will use the flag for its upcoming GUEST_MEMFD support,
which will not support compaction/migration, at least not in the
foreseeable future";Sean Christopherson;2023-10-27;0;0
C_kwDOACN7MtoAKDAwMDNlMmE0MTQ2ODdmZmY2YTc1MjUwZDM4MWU0YWJmMzQ1ZDY2M2Y;"Test AS_UNMOVABLE under folio lock as already done for the async
compaction/dirty folio case, as the mapping can be removed by truncation
while compaction is running";Sean Christopherson;2023-10-27;1;1
C_kwDOACN7MtoAKDAwMDNlMmE0MTQ2ODdmZmY2YTc1MjUwZDM4MWU0YWJmMzQ1ZDY2M2Y;" To avoid having to lock every folio with a
mapping, assume/require that unmovable mappings are also unevictable, and
have mapping_set_unmovable() also set AS_UNEVICTABLE.";Sean Christopherson;2023-10-27;0;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;mm: migrate: record the mlocked page status to remove unnecessary lru drain;Baolin Wang;2023-10-21;1;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"When doing compaction, I found the lru_add_drain() is an obvious hotspot
when migrating pages";Baolin Wang;2023-10-21;1;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;The distribution of this hotspot is as follows;Baolin Wang;2023-10-21;0;0
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"   - 18.75% compact_zone
      - 17.39% migrate_pages
         - 13.79% migrate_pages_batch
            - 11.66% migrate_folio_move
               - 7.02% lru_add_drain
                  + 7.02% lru_add_drain_cpu
               + 3.00% move_to_new_folio
                 1.23% rmap_walk
            + 1.92% migrate_folio_unmap
         + 3.20% migrate_pages_sync
      + 0.90% isolate_migratepages
The lru_add_drain() was added by commit c3096e6782b7 (""mm/migrate";Baolin Wang;2023-10-21;1;0
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"__unmap_and_move() push good newpage to LRU"") to drain the newpage to LRU
immediately, to help to build up the correct newpage->mlock_count in
remove_migration_ptes() for mlocked pages";Baolin Wang;2023-10-21;1;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;" However, if there are no
mlocked pages are migrating, then we can avoid this lru drain operation,
especailly for the heavy concurrent scenarios";Baolin Wang;2023-10-21;0;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"So we can record the source pages' mlocked status in
migrate_folio_unmap(), and only drain the lru list when the mlocked status
is set in migrate_folio_move()";Baolin Wang;2023-10-21;1;0
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"In addition, the page was already isolated from lru when migrating, so
checking the mlocked status is stable by folio_test_mlocked() in
migrate_folio_unmap()";Baolin Wang;2023-10-21;1;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;After this patch, I can see the hotpot of the lru_add_drain() is gone;Baolin Wang;2023-10-21;1;1
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"   - 9.41% migrate_pages_batch
      - 6.15% migrate_folio_move
         - 3.64% move_to_new_folio
            + 1.80% migrate_folio_extra
            + 1.70% buffer_migrate_folio
         + 1.41% rmap_walk
         + 0.62% folio_add_lru
      + 3.07% migrate_folio_unmap
Meanwhile, the compaction latency shows some improvements when running
thpscale";Baolin Wang;2023-10-21;1;0
C_kwDOACN7MtoAKGVlYmIzZGFiYmI1Y2M1OTBhZmUzMjg4MGI1ZDM3MjZkMGZiZjg4ZGI;"                            base                   patched
Amean     fault-both-1      1131.22 (   0.00%)     1112.55 *   1.65%*
Amean     fault-both-3      2489.75 (   0.00%)     2324.15 *   6.65%*
Amean     fault-both-5      3257.37 (   0.00%)     3183.18 *   2.28%*
Amean     fault-both-7      4257.99 (   0.00%)     4079.04 *   4.20%*
Amean     fault-both-12     6614.02 (   0.00%)     6075.60 *   8.14%*
Amean     fault-both-18    10607.78 (   0.00%)     8978.86 *  15.36%*
Amean     fault-both-24    14911.65 (   0.00%)    11619.55 *  22.08%*
Amean     fault-both-30    14954.67 (   0.00%)    14925.66 *   0.19%*
Amean     fault-both-32    16654.87 (   0.00%)    15580.31 *   6.45%*";Baolin Wang;2023-10-21;1;0
C_kwDOACN7MtoAKDQ5Y2FjMDNhOGYwYTU2Y2FmYTUzMjk5MTE1NjRjOTdjMTMwY2VkNDM;mm/migrate: add nr_split to trace_mm_migrate_pages stats.;Zi Yan;2023-10-17;1;0
C_kwDOACN7MtoAKDQ5Y2FjMDNhOGYwYTU2Y2FmYTUzMjk5MTE1NjRjOTdjMTMwY2VkNDM;"Add nr_split to trace_mm_migrate_pages for large folio (including THP)
split events.";Zi Yan;2023-10-17;1;0
C_kwDOACN7MtoAKGEyNTk5NDVlZmU2YWRhOTQwODdlZjY2NmU5YjM4ZjhlMzRlYTM0YmE;mm/migrate: correct nr_failed in migrate_pages_sync();Zi Yan;2023-10-17;1;1
C_kwDOACN7MtoAKGEyNTk5NDVlZmU2YWRhOTQwODdlZjY2NmU5YjM4ZjhlMzRlYTM0YmE;"nr_failed was missing the large folio splits from migrate_pages_batch()
and can cause a mismatch between migrate_pages() return value and the
number of not migrated pages, i.e., when the return value of
migrate_pages() is 0, there are still pages left in the from page list";Zi Yan;2023-10-17;0;0
C_kwDOACN7MtoAKGEyNTk5NDVlZmU2YWRhOTQwODdlZjY2NmU5YjM4ZjhlMzRlYTM0YmE;"
It will happen when a non-PMD THP large folio fails to migrate due to
-ENOMEM and is split successfully but not all the split pages are not
migrated, migrate_pages_batch() would return non-zero, but
astats.nr_thp_split = 0";Zi Yan;2023-10-17;0;0
C_kwDOACN7MtoAKGEyNTk5NDVlZmU2YWRhOTQwODdlZjY2NmU5YjM4ZjhlMzRlYTM0YmE;" nr_failed would be 0 and returned to the caller
of migrate_pages(), but the not migrated pages are left in the from page
list without being added back to LRU lists";Zi Yan;2023-10-17;0;0
C_kwDOACN7MtoAKGEyNTk5NDVlZmU2YWRhOTQwODdlZjY2NmU5YjM4ZjhlMzRlYTM0YmE;"Fix it by adding a new nr_split counter for large folio splits and adding
it to nr_failed in migrate_page_sync() after migrate_pages_batch() is
done.";Zi Yan;2023-10-17;1;1
C_kwDOACN7MtoAKDRlNjk0ZmU0ZDJmYTMwMzEzOTJiZGJlYWE4ODA2NmY2N2M4ODZhMGM;mm: migrate: use folio_xchg_last_cpupid() in folio_migrate_flags();Kefeng Wang;2023-10-18;1;0
C_kwDOACN7MtoAKDRlNjk0ZmU0ZDJmYTMwMzEzOTJiZGJlYWE4ODA2NmY2N2M4ODZhMGM;"Convert to use folio_xchg_last_cpupid() in folio_migrate_flags(), also
directly use folio_nid() instead of page_to_nid(&folio->page).";Kefeng Wang;2023-10-18;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;hugetlb: memcg: account hugetlb-backed memory in memory controller;Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"Currently, hugetlb memory usage is not acounted for in the memory
controller, which could lead to memory overprotection for cgroups with
hugetlb-backed memory";Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI; This has been observed in our production system;Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"For instance, here is one of our usecases: suppose there are two 32G
containers";Nhat Pham;2023-10-06;1;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;" The machine is booted with hugetlb_cma=6G, and each container
may or may not use up to 3 gigantic page, depending on the workload within
it";Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI; The rest is anon, cache, slab, etc;Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;" We can set the hugetlb cgroup
limit of each cgroup to 3G to enforce hugetlb fairness";Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;" But it is very
difficult to configure memory.max to keep overall consumption, including
anon, cache, slab etc";Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI; fair;Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"What we have had to resort to is to constantly poll hugetlb usage and
readjust memory.max";Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;" Similar procedure is done to other memory limits
(memory.low for e.g)";Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI; However, this is rather cumbersome and buggy;Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"
Furthermore, when there is a delay in memory limits correction, (for e.g
when hugetlb usage changes within consecutive runs of the userspace
agent), the system could be in an over/underprotected state";Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"This patch rectifies this issue by charging the memcg when the hugetlb
folio is utilized, and uncharging when the folio is freed (analogous to
the hugetlb controller)";Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;" Note that we do not charge when the folio is
allocated to the hugetlb pool, because at this point it is not owned by
any memcg";Nhat Pham;2023-10-06;0;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;Some caveats to consider;Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;    controller;Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"As stated above, hugetlb folios are only charged towards
    the memory controller when it is used";Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"Host overcommit management
    has to consider it when configuring hard limits";Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"    happen even if the hugetlb pool still has pages (but the cgroup
    limit is hit and reclaim attempt fails)";Nhat Pham;2023-10-06;0;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;    reclaim protection;Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"low, min limits tuning must take into account
    hugetlb memory";Nhat Pham;2023-10-06;1;1
C_kwDOACN7MtoAKDhjYmE5NTc2ZGY2MDFjMzg0YWJkMzM0YTUwM2MzZjZlMWUyOWVlZmI;"    be tracked by the memory controller (even if cgroup v2 is remounted
    later on).";Nhat Pham;2023-10-06;1;0
C_kwDOACN7MtoAKGVjNDdlMjUwNjI4OTAzZDQ4ZDlmZjQ5MGM2YjUzMmRmODg5YmNhYTM;mm/migrate: remove unused mm argument from do_move_pages_to_node;Gregory Price;2023-10-03;1;1
C_kwDOACN7MtoAKGVjNDdlMjUwNjI4OTAzZDQ4ZDlmZjQ5MGM2YjUzMmRmODg5YmNhYTM;This function does not actively use the mm_struct, it can be removed.;Gregory Price;2023-10-03;1;1
C_kwDOACN7MtoAKDVlZjhmMWIyYjRkOWJkMDJlNDEwNGI5MjU1MzUxZmI5Mjc5YjFiNGU;Merge mm-hotfixes-stable into mm-stable to pick up depended-upon changes.;Andrew Morton;2023-10-18;1;1
C_kwDOACN7MtoAKDVlZjhmMWIyYjRkOWJkMDJlNDEwNGI5MjU1MzUxZmI5Mjc5YjFiNGU;;Andrew Morton;2023-10-18;0;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;mm/filemap: remove hugetlb special casing in filemap.c;Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"Remove special cased hugetlb handling code within the page cache by
changing the granularity of ->index to the base page size rather than the
huge page size";Sidhartha Kumar;2023-09-26;1;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;" The motivation of this patch is to reduce complexity
within the filemap code while also increasing performance by removing
branches that are evaluated on every page cache lookup";Sidhartha Kumar;2023-09-26;1;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"To support the change in index, new wrappers for hugetlb page cache
interactions are added";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;" These wrappers perform the conversion to a linear
index which is now expected by the page cache for huge pages";Sidhartha Kumar;2023-09-26;0;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"========================= PERFORMANCE ======================================
Perf was used to check the performance differences after the patch";Sidhartha Kumar;2023-09-26;0;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"
Overall the performance is similar to mainline with a very small larger
overhead that occurs in __filemap_add_folio() and
hugetlb_add_to_page_cache()";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;" This is because of the larger overhead that
occurs in xa_load() and xa_store() as the xarray is now using more entries
to store hugetlb folios in the page cache";Sidhartha Kumar;2023-09-26;0;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"Timing
aarch64
    2MB Page Size
        6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-1 hugepages]# time fallocate -l 700GB test.txt
            real    1m49.568s
            user    0m0.000s
            sys     1m49.461s
        6.5-rc3";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root]# time fallocate -l 700GB test.txt
            real    1m47.495s
            user    0m0.000s
            sys     1m47.370s
    1GB Page Size
        6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-1 hugepages1G]# time fallocate -l 700GB test.txt
            real    1m47.024s
            user    0m0.000s
            sys     1m46.921s
        6.5-rc3";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-1 hugepages1G]# time fallocate -l 700GB test.txt
            real    1m44.551s
            user    0m0.000s
            sys     1m44.438s
    2MB Page Size
        6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-2 hugepages]# time fallocate -l 100GB test.txt
            real    0m22.383s
            user    0m0.000s
            sys     0m22.255s
        6.5-rc3";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [opc@sidhakum-ol9-2 hugepages]$ time sudo fallocate -l 100GB /dev/hugepages/test.txt
            real    0m22.735s
            user    0m0.038s
            sys     0m22.567s
    1GB Page Size
        6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-2 hugepages1GB]# time fallocate -l 100GB test.txt
            real    0m25.786s
            user    0m0.001s
            sys     0m25.589s
        6.5-rc3";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            [root@sidhakum-ol9-2 hugepages1G]# time fallocate -l 100GB test.txt
            real    0m33.454s
            user    0m0.001s
            sys     0m33.193s
aarch64";Sidhartha Kumar;2023-09-26;0;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"    workload - fallocate a 700GB file backed by huge pages
    6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;        2MB Page Size;Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            --100.00%--__arm64_sys_fallocate
                          ksys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                          |--95.04%--__pi_clear_page
                          |--3.57%--clear_huge_page
                          |          |--2.63%--rcu_all_qs
                          |           --0.91%--__cond_resched
                           --0.67%--__cond_resched
            0.17%     0.00%             0  fallocate  [kernel.vmlinux]       [k] hugetlb_add_to_page_cache
            0.14%     0.10%            11  fallocate  [kernel.vmlinux]       [k] __filemap_add_folio
    6.5-rc3
        2MB Page Size";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"                --100.00%--__arm64_sys_fallocate
                          ksys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                          |--94.91%--__pi_clear_page
                          |--4.11%--clear_huge_page
                          |          |--3.00%--rcu_all_qs
                          |           --1.10%--__cond_resched
                           --0.59%--__cond_resched
            0.08%     0.01%             1  fallocate  [kernel.kallsyms]  [k] hugetlb_add_to_page_cache
            0.05%     0.03%             3  fallocate  [kernel.kallsyms]  [k] __filemap_add_folio
    workload - fallocate a 100GB file backed by huge pages
    6.5-rc3 + this patch";Sidhartha Kumar;2023-09-26;1;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;        2MB Page Size;Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"            hugetlbfs_fallocate
            --99.57%--clear_huge_page
                --98.47%--clear_page_erms
                    --0.53%--asm_sysvec_apic_timer_interrupt
            0.04%     0.04%             1  fallocate  [kernel.kallsyms]     [k] xa_load
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] hugetlb_add_to_page_cache
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] __filemap_add_folio
            0.04%     0.00%             0  fallocate  [kernel.kallsyms]     [k] xas_store
    6.5-rc3
        2MB Page Size";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"                --99.93%--__x64_sys_fallocate
                          vfs_fallocate
                          hugetlbfs_fallocate
                           --99.38%--clear_huge_page
                                     |--98.40%--clear_page_erms
                                      --0.59%--__cond_resched
            0.03%     0.03%             1  fallocate  [kernel.kallsyms]  [k] __filemap_add_folio
========================= TESTING ======================================
This patch passes libhugetlbfs tests and LTP hugetlb tests
    Done executing testcases";Sidhartha Kumar;2023-09-26;1;1
C_kwDOACN7MtoAKGEwOGM3MTkzZTRmMThkYzg1MDhmMmQwN2QwZGUyYzViOTRjYjM5YTM;"    LTP Version:  20220527-178-g2761a81c4
page migration was also tested using Mike Kravetz's test program.[8]";Sidhartha Kumar;2023-09-26;1;0
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;mm/migrate: fix do_pages_move for compat pointers;Gregory Price;2023-10-03;1;1
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;do_pages_move does not handle compat pointers for the page list;Gregory Price;2023-10-03;1;0
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;"
correctly";Gregory Price;2023-10-03;0;0
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;" Add in_compat_syscall check and appropriate get_user fetch
when iterating the page list";Gregory Price;2023-10-03;1;0
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;"It makes the syscall in compat mode (32-bit userspace, 64-bit kernel)
work the same way as the native 32-bit syscall again, restoring the
behavior before my broken commit 5b1b561ba73c (""mm: simplify
compat_sys_move_pages"")";Gregory Price;2023-10-03;1;1
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;"More specifically, my patch moved the parsing of the 'pages' array from
the main entry point into do_pages_stat(), which left the syscall
working correctly for the 'stat' operation (nodes = NULL), while the
'move' operation (nodes != NULL) is now missing the conversion and
interprets 'pages' as an array of 64-bit pointers instead of the
intended 32-bit userspace pointers";Gregory Price;2023-10-03;1;1
C_kwDOACN7MtoAKDIyOWUyMjUzNzY2YzdjZGZlMDI0ZjFmZTI4MDAyMGNjNDcxMTA4N2M;"It is possible that nobody noticed this bug because the few
applications that actually call move_pages are unlikely to run in
compat mode because of their large memory requirements, but this
clearly fixes a user-visible regression and should have been caught by
ltp.";Gregory Price;2023-10-03;0;1
C_kwDOACN7MtoAKGZhMWRmM2Y2Mjg3ZTFlMWZkOGI1MzA5ODI4MjM4ZTJjNzI4ZTk4NWY;mm: migrate: remove isolated variable in add_page_for_migration();Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGZhMWRmM2Y2Mjg3ZTFlMWZkOGI1MzA5ODI4MjM4ZTJjNzI4ZTk4NWY;"Directly check the return of isolate_hugetlb() and folio_isolate_lru() to
remove isolated variable, also setup err = -EBUSY in advance before
isolation, and update err only when successfully queued for migration,
which could help us to unify and simplify code a bit.";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGI0MjZlZDc4ODliZTgwMzU5Y2I0ZWRlZjE0MmU1YzVmYTY5N2IwNjg;mm: migrate: remove PageHead() check for HugeTLB in add_page_for_migration();Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKGI0MjZlZDc4ODliZTgwMzU5Y2I0ZWRlZjE0MmU1YzVmYTY5N2IwNjg;"There is some different between hugeTLB and THP behave when passed the
address of a tail page, for THP, it will migrate the entire THP page, but
for HugeTLB, it will return -EACCES, or -ENOENT before commit e66f17ff7177
(""mm/hugetlb: take page table lock in follow_huge_pmd()""),
  -EACCES The page is mapped by multiple processes and can be moved
	  only if MPOL_MF_MOVE_ALL is specified";Kefeng Wang;2023-09-13;0;0
C_kwDOACN7MtoAKGI0MjZlZDc4ODliZTgwMzU5Y2I0ZWRlZjE0MmU1YzVmYTY5N2IwNjg;  -ENOENT The page is not present;Kefeng Wang;2023-09-13;0;1
C_kwDOACN7MtoAKGI0MjZlZDc4ODliZTgwMzU5Y2I0ZWRlZjE0MmU1YzVmYTY5N2IwNjg;"But when check manual[1], both of the two errnos are not suitable, it is
better to keep the same behave between hugetlb and THP when passed the
address of a tail page, so let's just remove the PageHead() check for
HugeTLB.";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGQ2NGNmY2NiYzgwNTY2M2EyYzU2OTFmNjM4Y2Y5MTk4Yjk2NzZhOWY;mm: migrate: use a folio in add_page_for_migration();Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKGQ2NGNmY2NiYzgwNTY2M2EyYzU2OTFmNjM4Y2Y5MTk4Yjk2NzZhOWY;Use a folio in add_page_for_migration() to save compound_head() calls.;Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKDdlMmE1ZTVhYjIxN2Q1ZTQxNjZjZGJkZjRhZjhjNWUzNGI2MjAwYmI;mm: migrate: use __folio_test_movable();Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKDdlMmE1ZTVhYjIxN2Q1ZTQxNjZjZGJkZjRhZjhjNWUzNGI2MjAwYmI;Use __folio_test_movable(), no need to convert from folio to page again.;Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKDczZWFiM2NhNDgxZTViZTBmMWZkODE0MDM2NWQ2MDQ0ODJmODRlZTE;mm: migrate: convert migrate_misplaced_page() to migrate_misplaced_folio();Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKDczZWFiM2NhNDgxZTViZTBmMWZkODE0MDM2NWQ2MDQ0ODJmODRlZTE;"At present, numa balance only support base page and PMD-mapped THP, but we
will expand to support to migrate large folio/pte-mapped THP in the
future, it is better to make migrate_misplaced_page() to take a folio
instead of a page, and rename it to migrate_misplaced_folio(), it is a
preparation, also this remove several compound_head() calls.";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKDJhYzllOTlmM2IyMWIyODY0MzA1ZmJmYmE0YmFlNTkxMzI3NGM0MDk;mm: migrate: convert numamigrate_isolate_page() to numamigrate_isolate_folio();Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKDJhYzllOTlmM2IyMWIyODY0MzA1ZmJmYmE0YmFlNTkxMzI3NGM0MDk;"Rename numamigrate_isolate_page() to numamigrate_isolate_folio(), then
make it takes a folio and use folio API to save compound_head() calls.";Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKDcyOGJlMjhmYWU4YzgzOGQ1MmM5MWRjZTQ4NjcxMzM3OTgxNDYzNTc;mm: migrate: remove THP mapcount check in numamigrate_isolate_page();Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKDcyOGJlMjhmYWU4YzgzOGQ1MmM5MWRjZTQ4NjcxMzM3OTgxNDYzNTc;"The check of THP mapped by multiple processes was introduced by commit
04fa5d6a6547 (""mm: migrate: check page_count of THP before migrating"") and
refactor by commit 340ef3902cf2 (""mm: numa: cleanup flow of transhuge page
migration""), which is out of date, since migrate_misplaced_page() is now
using the standard migrate_pages() for small pages and THPs, the reference
count checking is in folio_migrate_mapping(), so let's remove the special
check for THP.";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGE4YWM0YTc2N2RjZDlkODdkODIyOTA0NTkwNGQ5ZmUxNWVhNWUwZTg;mm: migrate: remove PageTransHuge check in numamigrate_isolate_page();Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGE4YWM0YTc2N2RjZDlkODdkODIyOTA0NTkwNGQ5ZmUxNWVhNWUwZTg;"Patch series ""mm: migrate: more folio conversion and unification"", v3";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKGE4YWM0YTc2N2RjZDlkODdkODIyOTA0NTkwNGQ5ZmUxNWVhNWUwZTg;"Convert more migrate functions to use a folio, it is also a preparation
for large folio migration support when balancing numa";Kefeng Wang;2023-09-13;0;1
C_kwDOACN7MtoAKGE4YWM0YTc2N2RjZDlkODdkODIyOTA0NTkwNGQ5ZmUxNWVhNWUwZTg;This patch (of 8);Kefeng Wang;2023-09-13;1;0
C_kwDOACN7MtoAKGE4YWM0YTc2N2RjZDlkODdkODIyOTA0NTkwNGQ5ZmUxNWVhNWUwZTg;"The assert VM_BUG_ON_PAGE(order && !PageTransHuge(page), page) is not very
useful,
   1) for a tail/base page, order = 0, for a head page, the order > 0 &&
      PageTransHuge() is true
   2) there is a PageCompound() check and only base page is handled in
      do_numa_page(), and do_huge_pmd_numa_page() only handle PMD-mapped
   3) even though the page is a tail page, isolate_lru_page() will post
      a warning, and fail to isolate the page
   4) if large folio/pte-mapped THP migration supported in the future,
      we could migrate the entire folio if numa fault on a tail page
so just remove the check.";Kefeng Wang;2023-09-13;1;1
C_kwDOACN7MtoAKDA5YzU1MDUwOGE0YjhmNzg0NGIxOTdjYzE2ODc3ZGQwZjdjNDJkOGY;mm/rmap: pass folio to hugepage_add_anon_rmap();David Hildenbrand;2023-09-13;1;0
C_kwDOACN7MtoAKDA5YzU1MDUwOGE0YjhmNzg0NGIxOTdjYzE2ODc3ZGQwZjdjNDJkOGY;"Let's pass a folio; we are always mapping the entire thing.";David Hildenbrand;2023-09-13;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;mm: hugetlb: add huge page size param to set_huge_pte_at();Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"Patch series ""Fix set_huge_pte_at() panic on arm64"", v2";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"This series fixes a bug in arm64's implementation of set_huge_pte_at(),
which can result in an unprivileged user causing a kernel panic";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" The
problem was triggered when running the new uffd poison mm selftest for
HUGETLB memory";Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" This test (and the uffd poison feature) was merged for
v6.5-rc7";Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"Ideally, I'd like to get this fix in for v6.6 and I've cc'ed stable
(correctly this time) to get it backported to v6.5, where the issue first
showed up";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"Description of Bug
arm64's huge pte implementation supports multiple huge page sizes, some of
which are implemented in the page table with multiple contiguous entries";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"
So set_huge_pte_at() needs to work out how big the logical pte is, so that
it can also work out how many physical ptes (or pmds) need to be written";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"
It previously did this by grabbing the folio out of the pte and querying
its size";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;However, there are cases when the pte being set is actually a swap entry;Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"
But this also used to work fine, because for huge ptes, we only ever saw
migration entries and hwpoison entries";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" And both of these types of swap
entries have a PFN embedded, so the code would grab that and everything
still worked out";Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"But over time, more calls to set_huge_pte_at() have been added that set
swap entry types that do not embed a PFN";Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" And this causes the code to go
bang";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" The triggering case is for the uffd poison test, commit
99aa77215ad0 (""selftests/mm: add uffd unit test for UFFDIO_POISON""), which
causes a PTE_MARKER_POISONED swap entry to be set, coutesey of commit
8a13897fb0da (""mm: userfaultfd: support UFFDIO_POISON for hugetlbfs"") -
added in v6.5-rc7";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" Although review shows that there are other call sites
that set PTE_MARKER_UFFD_WP (which also has no PFN), these don't trigger
on arm64 because arm64 doesn't support UFFD WP";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"If CONFIG_DEBUG_VM is enabled, we do at least get a BUG(), but otherwise,
it will dereference a bad pointer in page_folio()";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"    static inline struct folio *hugetlb_swap_entry_to_folio(swp_entry_t entry)
The simplest fix would have been to revert the dodgy cleanup commit
18f3962953e4 (""mm: hugetlb: kill set_huge_swap_pte_at()""), but since
things have moved on, this would have required an audit of all the new
set_huge_pte_at() call sites to see if they should be converted to
set_huge_swap_pte_at()";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" As per the original intent of the change, it
would also leave us open to future bugs when people invariably get it
wrong and call the wrong helper";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;So instead, I've added a huge page size parameter to set_huge_pte_at();Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"
This means that the arm64 code has the size in all cases";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" It's a bigger
change, due to needing to touch the arches that implement the function,
but it is entirely mechanical, so in my view, low risk";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"I've compile-tested all touched arches; arm64, parisc, powerpc, riscv,
s390, sparc (and additionally x86_64)";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" I've additionally booted and run
mm selftests against arm64, where I observe the uffd poison test is fixed,
and there are no other regressions";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;This patch (of 2);Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"In order to fix a bug, arm64 needs to be told the size of the huge page
for which the pte is being set in set_huge_pte_at()";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" Provide for this by
adding an `unsigned long sz` parameter to the function";Ryan Roberts;2023-09-22;0;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" This follows the
same pattern as huge_pte_clear()";Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;"This commit makes the required interface modifications to the core mm as
well as all arches that implement this function (arm64, parisc, powerpc,
riscv, s390, sparc)";Ryan Roberts;2023-09-22;0;1
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;" The actual arm64 bug will be fixed in a separate
commit";Ryan Roberts;2023-09-22;1;0
C_kwDOACN7MtoAKDkzNWQ0ZjBjNmRjOGIzNTMzZTZlMzkzNDZkZTczODlhODQ0OTAxNzg;No behavioral changes intended.;Ryan Roberts;2023-09-22;1;1
C_kwDOACN7MtoAKGQ1ZGI0ZjlkZjkzOTdkMzk4MjU2YTJlMzNhZDYzYzM5YzIxM2I5OTA;migrate: use folio_set_bh() instead of set_bh_page();Matthew Wilcox (Oracle);2023-07-13;1;0
C_kwDOACN7MtoAKGQ1ZGI0ZjlkZjkzOTdkMzk4MjU2YTJlMzNhZDYzYzM5YzIxM2I5OTA;This function was converted before folio_set_bh() existed;Matthew Wilcox (Oracle);2023-07-13;0;0
C_kwDOACN7MtoAKGQ1ZGI0ZjlkZjkzOTdkMzk4MjU2YTJlMzNhZDYzYzM5YzIxM2I5OTA;" Catch up to
the new API.";Matthew Wilcox (Oracle);2023-07-13;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;mm: merge folio_has_private()/filemap_release_folio() call pairs;David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"Patch series ""mm, netfs, fscache: Stop read optimisation when folio
removed from pagecache"", v7";David Howells;2023-06-28;0;0
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"This fixes an optimisation in fscache whereby we don't read from the cache
for a particular file until we know that there's data there that we don't
have in the pagecache";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" The problem is that I'm no longer using PG_fscache
(aka PG_private_2) to indicate that the page is cached and so I don't get
a notification when a cached page is dropped from the pagecache";David Howells;2023-06-28;1;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"The first patch merges some folio_has_private() and
filemap_release_folio() pairs and introduces a helper,
folio_needs_release(), to indicate if a release is required";David Howells;2023-06-28;1;0
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;The second patch is the actual fix;David Howells;2023-06-28;1;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" Following Willy's suggestions[1], it
adds an AS_RELEASE_ALWAYS flag to an address_space that will make
filemap_release_folio() always call ->release_folio(), even if
PG_private/PG_private_2 aren't set";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" folio_needs_release() is altered to
add a check for this";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;This patch (of 2);David Howells;2023-06-28;1;0
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;Make filemap_release_folio() check folio_has_private();David Howells;2023-06-28;1;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" Then, in most
cases, where a call to folio_has_private() is immediately followed by a
call to filemap_release_folio(), we can get rid of the test in the pair";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"There are a couple of sites in mm/vscan.c that this can't so easily be
done";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" In shrink_folio_list(), there are actually three cases (something
different is done for incompletely invalidated buffers), but
filemap_release_folio() elides two of them";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"In shrink_active_list(), we don't have have the folio lock yet, so the
check allows us to avoid locking the page unnecessarily";David Howells;2023-06-28;1;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"A wrapper function to check if a folio needs release is provided for those
places that still need to do it in the mm/ directory";David Howells;2023-06-28;1;1
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;" This will acquire
additional parts to the condition in a future patch";David Howells;2023-06-28;0;0
C_kwDOACN7MtoAKDAyMDFlYmYyNzRhMzA2YTZlYmI5NWU1ZGMyZDZhMGEyN2M3MzdjYWM;"After this, the only remaining caller of folio_has_private() outside of
mm/ is a check in fuse.";David Howells;2023-06-28;0;1
C_kwDOACN7MtoAKDkyNWM4NmExOWJhY2Y4Y2UxMGViNjY2MzI4ZmIzZmE1YWZmN2I5NTE;fs: add CONFIG_BUFFER_HEAD;Christoph Hellwig;2023-08-01;1;0
C_kwDOACN7MtoAKDkyNWM4NmExOWJhY2Y4Y2UxMGViNjY2MzI4ZmIzZmE1YWZmN2I5NTE;"Add a new config option that controls building the buffer_head code, and
select it from all file systems and stacking drivers that need it";Christoph Hellwig;2023-08-01;1;1
C_kwDOACN7MtoAKDkyNWM4NmExOWJhY2Y4Y2UxMGViNjY2MzI4ZmIzZmE1YWZmN2I5NTE;"For the block device nodes and alternative iomap based buffered I/O path
is provided when buffer_head support is not enabled, and iomap needs a
a small tweak to define the IOMAP_F_BUFFER_HEAD flag to 0 to not call
into the buffer_head code when it doesn't exist";Christoph Hellwig;2023-08-01;0;1
C_kwDOACN7MtoAKDkyNWM4NmExOWJhY2Y4Y2UxMGViNjY2MzI4ZmIzZmE1YWZmN2I5NTE;Otherwise this is just Kconfig and ifdef changes.;Christoph Hellwig;2023-08-01;1;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;mm: Make pte_mkwrite() take a VMA;Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"The x86 Shadow stack feature includes a new type of memory called shadow
stack";Rick Edgecombe;2023-06-13;1;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"This shadow stack memory has some unusual properties, which requires
some core mm changes to function properly";Rick Edgecombe;2023-06-13;0;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"One of these unusual properties is that shadow stack memory is writable,
but only in limited ways";Rick Edgecombe;2023-06-13;0;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"These limits are applied via a specific PTE
bit combination";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Nevertheless, the memory is writable, and core mm code
will need to apply the writable permissions in the typical paths that
call pte_mkwrite()";Rick Edgecombe;2023-06-13;0;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Future patches will make pte_mkwrite() take a VMA, so
that the x86 implementation of it can know whether to create regular
writable or shadow stack mappings";Rick Edgecombe;2023-06-13;1;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;But there are a couple of challenges to this;Rick Edgecombe;2023-06-13;0;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Modifying the signatures of
each arch pte_mkwrite() implementation would be error prone because some
are generated with macros and would need to be re-implemented";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Also, some
pte_mkwrite() callers operate on kernel memory without a VMA";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;So this can be done in a three step process;Rick Edgecombe;2023-06-13;1;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"First pte_mkwrite() can be
renamed to pte_mkwrite_novma() in each arch, with a generic pte_mkwrite()
added that just calls pte_mkwrite_novma()";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Next callers without a VMA can
be moved to pte_mkwrite_novma()";Rick Edgecombe;2023-06-13;0;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"And lastly, pte_mkwrite() and all callers
can be changed to take/pass a VMA";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"Previous work pte_mkwrite() renamed pte_mkwrite_novma() and converted
callers that don't have a VMA were to use pte_mkwrite_novma()";Rick Edgecombe;2023-06-13;0;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;"So now
change pte_mkwrite() to take a VMA and change the remaining callers to
pass a VMA";Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;Apply the same changes for pmd_mkwrite();Rick Edgecombe;2023-06-13;1;1
C_kwDOACN7MtoAKDE2MWUzOTNjMGY2MzU5MmEzYjk1YmRkOGI1NTc1MjY1Mzc2M2ZjNmQ;No functional change.;Rick Edgecombe;2023-06-13;1;0
C_kwDOACN7MtoAKDk5NGVjNGUyOWIzZGUxODhkMTFmZTYwZDE3NDAzMjg1ZmNjODkxN2E;mm: remove unnecessary pagevec includes;Matthew Wilcox (Oracle);2023-06-21;1;1
C_kwDOACN7MtoAKDk5NGVjNGUyOWIzZGUxODhkMTFmZTYwZDE3NDAzMjg1ZmNjODkxN2E;"These files no longer need pagevec.h, mostly due to function declarations
being moved out of it.";Matthew Wilcox (Oracle);2023-06-21;0;1
C_kwDOACN7MtoAKDBiNTJjNDIwMzUwZThmOTg3M2JhNjI3NjhjZDgyNDY4MjcxODQ0MDg;mm: fix shmem THP counters on migration;Jan Glauber;2023-06-19;1;1
C_kwDOACN7MtoAKDBiNTJjNDIwMzUwZThmOTg3M2JhNjI3NjhjZDgyNDY4MjcxODQ0MDg;"The per node numa_stat values for shmem don't change on page migration for
THP";Jan Glauber;2023-06-19;0;0
C_kwDOACN7MtoAKDBiNTJjNDIwMzUwZThmOTg3M2JhNjI3NjhjZDgyNDY4MjcxODQ0MDg;  grep shmem /sys/fs/cgroup/machine.slice/.../memory.numa_stat;Jan Glauber;2023-06-19;1;0
C_kwDOACN7MtoAKDBiNTJjNDIwMzUwZThmOTg3M2JhNjI3NjhjZDgyNDY4MjcxODQ0MDg;"    shmem N0=1092616192 N1=10485760
    shmem_thp N0=1092616192 N1=10485760
  migratepages 9181 0 1";Jan Glauber;2023-06-19;0;0
C_kwDOACN7MtoAKDBiNTJjNDIwMzUwZThmOTg3M2JhNjI3NjhjZDgyNDY4MjcxODQ0MDg;"    shmem N0=0 N1=1103101952
    shmem_thp N0=1092616192 N1=10485760
Fix that by updating shmem_thp counters likewise to shmem counters on page
migration.";Jan Glauber;2023-06-19;0;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;mm: ptep_get() conversion;Ryan Roberts;2023-06-12;1;0
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;"Convert all instances of direct pte_t* dereferencing to instead use
ptep_get() helper";Ryan Roberts;2023-06-12;1;0
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" This means that by default, the accesses change from a
C dereference to a READ_ONCE()";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" This is technically the correct thing to
do since where pgtables are modified by HW (for access/dirty) they are
volatile and therefore we should always ensure READ_ONCE() semantics";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;"But more importantly, by always using the helper, it can be overridden by
the architecture to fully encapsulate the contents of the pte";Ryan Roberts;2023-06-12;0;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" Arch code
is deliberately not converted, as the arch code knows best";Ryan Roberts;2023-06-12;1;0
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" It is
intended that arch code (arm64) will override the default with its own
implementation that can (e.g.) hide certain bits from the core code, or
determine young/dirty status by mixing in state from another source";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;Conversion was done using Coccinelle;Ryan Roberts;2023-06-12;1;0
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;"// $ make coccicheck \
//          COCCI=ptepget.cocci \
//          SPFLAGS=""--include-headers"" \
//          MODE=patch
virtual patch
@ depends on patch @
+ ptep_get(v)
Then reviewed and hand-edited to avoid multiple unnecessary calls to
ptep_get(), instead opting to store the result of a single call in a
variable, where it is correct to do so";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" This aims to negate any cost of
READ_ONCE() and will benefit arch-overrides that may be more complex";Ryan Roberts;2023-06-12;0;0
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;"Included is a fix for an issue in an earlier version of this patch that
was pointed out by kernel test robot";Ryan Roberts;2023-06-12;0;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" The issue arose because config
MMU=n elides definition of the ptep helper functions, including
ptep_get()";Ryan Roberts;2023-06-12;0;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" HUGETLB_PAGE=n configs still define a simple
huge_ptep_clear_flush() for linking purposes, which dereferences the ptep";Ryan Roberts;2023-06-12;0;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;"So when both configs are disabled, this caused a build error because
ptep_get() is not defined";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" Fix by continuing to do a direct dereference
when MMU=n";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKGMzM2M3OTQ4MjhmMjEyMTdmNzJjZTZmYzE0MGUwZDM0ZTBkNTZiZmY;" This is safe because for this config the arch code cannot be
trying to virtualize the ptes because none of the ptep helpers are
defined.";Ryan Roberts;2023-06-12;1;1
C_kwDOACN7MtoAKDA0ZGVlOWU4NWNmNTBhMmYyNDczOGU0NTZkNjZiODhkZTEwOWI4MDY;mm/various: give up if pte_offset_map[_lock]() fails;Hugh Dickins;2023-06-09;1;1
C_kwDOACN7MtoAKDA0ZGVlOWU4NWNmNTBhMmYyNDczOGU0NTZkNjZiODhkZTEwOWI4MDY;"Following the examples of nearby code, various functions can just give up
if pte_offset_map() or pte_offset_map_lock() fails";Hugh Dickins;2023-06-09;1;0
C_kwDOACN7MtoAKDA0ZGVlOWU4NWNmNTBhMmYyNDczOGU0NTZkNjZiODhkZTEwOWI4MDY;" And there's no need
for a preliminary pmd_trans_unstable() or other such check, since such
cases are now safely handled inside.";Hugh Dickins;2023-06-09;1;1
C_kwDOACN7MtoAKDBjYjhmZDRkMTQxNjVhN2U2NTQwNDhlNDM5ODNkODZmNzViOTA4Nzk;mm/migrate: remove cruft from migration_entry_wait()s;Hugh Dickins;2023-06-09;1;0
C_kwDOACN7MtoAKDBjYjhmZDRkMTQxNjVhN2U2NTQwNDhlNDM5ODNkODZmNzViOTA4Nzk;"migration_entry_wait_on_locked() does not need to take a mapped pte
pointer, its callers can do the unmap first";Hugh Dickins;2023-06-09;0;0
C_kwDOACN7MtoAKDBjYjhmZDRkMTQxNjVhN2U2NTQwNDhlNDM5ODNkODZmNzViOTA4Nzk;" Annotate it with
__releases(ptl) to reduce sparse warnings";Hugh Dickins;2023-06-09;0;1
C_kwDOACN7MtoAKDBjYjhmZDRkMTQxNjVhN2U2NTQwNDhlNDM5ODNkODZmNzViOTA4Nzk;Fold __migration_entry_wait_huge() into migration_entry_wait_huge();Hugh Dickins;2023-06-09;1;0
C_kwDOACN7MtoAKDBjYjhmZDRkMTQxNjVhN2U2NTQwNDhlNDM5ODNkODZmNzViOTA4Nzk;" Fold
__migration_entry_wait() into migration_entry_wait(), preferring the
tighter pte_offset_map_lock() to pte_offset_map() and pte_lockptr().";Hugh Dickins;2023-06-09;0;0
C_kwDOACN7MtoAKDRlMDk2YWUxODAxZTI0YjMzOGUwMjcxNWM2NWMzZmZhODg4M2JhNWQ;mm: convert migrate_pages() to work on folios;Matthew Wilcox (Oracle);2023-05-13;1;0
C_kwDOACN7MtoAKDRlMDk2YWUxODAxZTI0YjMzOGUwMjcxNWM2NWMzZmZhODg4M2JhNWQ;"Almost all of the callers & implementors of migrate_pages() were already
converted to use folios";Matthew Wilcox (Oracle);2023-05-13;0;1
C_kwDOACN7MtoAKDRlMDk2YWUxODAxZTI0YjMzOGUwMjcxNWM2NWMzZmZhODg4M2JhNWQ;" compaction_alloc() & compaction_free() are
trivial to convert a part of this patch and not worth splitting out.";Matthew Wilcox (Oracle);2023-05-13;0;0
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;migrate_pages_batch: simplify retrying and failure counting of large folios;Huang Ying;2023-05-10;1;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;"After recent changes to the retrying and failure counting in
migrate_pages_batch(), it was found that it's unnecessary to count
retrying and failure for normal, large, and THP folios separately";Huang Ying;2023-05-10;0;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;"
Because we don't use retrying and failure number of large folios directly";Huang Ying;2023-05-10;1;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;"So, in this patch, we simplified retrying and failure counting of large
folios via counting retrying and failure of normal and large folios
together";Huang Ying;2023-05-10;0;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE; This results in the reduced line number;Huang Ying;2023-05-10;0;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;"Previously, in migrate_pages_batch we need to track whether the source
folio is large/THP before splitting";Huang Ying;2023-05-10;0;0
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;" So is_large is used to cache
folio_test_large() result";Huang Ying;2023-05-10;0;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;" Now, we don't need that variable any more
because we don't count retrying and failure of large folios (only counting
that of THP folios)";Huang Ying;2023-05-10;1;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;" So, in this patch, is_large is removed to simplify
the code";Huang Ying;2023-05-10;1;1
C_kwDOACN7MtoAKDEyNGFiY2VkNjQ3MzA2YWEzYmFkYjVkNDcyYzM2MTZkZTIzZjE4MGE;This is just code cleanup, no functionality changes are expected.;Huang Ying;2023-05-10;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;migrate_pages: avoid blocking for IO in MIGRATE_SYNC_LIGHT;Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"The MIGRATE_SYNC_LIGHT mode is intended to block for things that will
finish quickly but not for things that will take a long time";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" Exactly how
long is too long is not well defined, but waits of tens of milliseconds is
likely non-ideal";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"When putting a Chromebook under memory pressure (opening over 90 tabs on a
4GB machine) it was fairly easy to see delays waiting for some locks in
the kcompactd code path of > 100 ms";Douglas Anderson;2023-04-28;0;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" While the laptop wasn't amazingly
usable in this state, it was still limping along and this state isn't
something artificial";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" Sometimes we simply end up with a lot of memory
pressure";Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"Putting the same Chromebook under memory pressure while it was running
Android apps (though not stressing them) showed a much worse result (NOTE";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;this was on a older kernel but the codepaths here are similar);Douglas Anderson;2023-04-28;0;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" Android
apps on ChromeOS currently run from a 128K-block, zlib-compressed,
loopback-mounted squashfs disk";Douglas Anderson;2023-04-28;0;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" If we get a page fault from something
backed by the squashfs filesystem we could end up holding a folio lock
while reading enough from disk to decompress 128K (and then decompressing
it using the somewhat slow zlib algorithms)";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" That reading goes through
the ext4 subsystem (because it's a loopback mount) before eventually
ending up in the block subsystem";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI; This extra jaunt adds extra overhead;Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"
Without much work I could see cases where we ended up blocked on a folio
lock for over a second";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" With more extreme memory pressure I could see up
to 25 seconds";Douglas Anderson;2023-04-28;0;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"We considered adding a timeout in the case of MIGRATE_SYNC_LIGHT for the
two locks that were seen to be slow [1] and that generated much
discussion";Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" After discussion, it was decided that we should avoid waiting
for the two locks during MIGRATE_SYNC_LIGHT if they were being held for
IO";Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI; We'll continue with the unbounded wait for the more full SYNC modes;Douglas Anderson;2023-04-28;1;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"With this change, I couldn't see any slow waits on these locks with my
previous testcases";Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;"NOTE: The reason I stated digging into this originally isn't because some
benchmark had gone awry, but because we've received in-the-field crash
reports where we have a hung task waiting on the page lock (which is the
equivalent code path on old kernels)";Douglas Anderson;2023-04-28;0;0
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" While the root cause of those
crashes is likely unrelated and won't be fixed by this patch, analyzing
those crash reports did point out these very long waits seemed like
something good to fix";Douglas Anderson;2023-04-28;0;1
C_kwDOACN7MtoAKDRiYjZkYzc5ZDk4N2IyNDNkNjVjNzBjNTAyOWU1MWU3MTljZmI5NGI;" With this patch we should no longer hang waiting
on these locks, but presumably the system will still be in a bad shape and
hang somewhere else.";Douglas Anderson;2023-04-28;1;1
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;mm: don't check VMA write permissions if the PTE/PMD indicates write permissions;David Hildenbrand;2023-04-18;1;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"Staring at the comment ""Recheck VMA as permissions can change since
migration started"" in remove_migration_pte() can result in confusion,
because if the source PTE/PMD indicates write permissions, then there
should be no need to check VMA write permissions when restoring migration
entries or PTE-mapping a PMD";David Hildenbrand;2023-04-18;1;1
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"Commit d3cb8bf6081b (""mm: migrate: Close race between migration completion
and mprotect"") introduced the maybe_mkwrite() handling in
remove_migration_pte() in 2014, stating that a race between mprotect() and
migration finishing would be possible, and that we could end up with a
writable PTE that should be readable";David Hildenbrand;2023-04-18;0;1
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"However, mprotect() code first updates vma->vm_flags / vma->vm_page_prot
and then walks the page tables to (a) set all present writable PTEs to
read-only and (b) convert all writable migration entries to readable
migration entries";David Hildenbrand;2023-04-18;1;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;" While walking the page tables and modifying the
entries, migration code has to grab the PT locks to synchronize against
concurrent page table modifications";David Hildenbrand;2023-04-18;0;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"Assuming migration would find a writable migration entry (while holding
the PT lock) and replace it with a writable present PTE, surely mprotect()
code didn't stumble over the writable migration entry yet (converting it
into a readable migration entry) and would instead wait for the PT lock to
convert the now present writable PTE into a read-only PTE";David Hildenbrand;2023-04-18;1;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;" As mprotect()
didn't finish yet, the behavior is just like migration didn't happen: a
writable PTE will be converted to a read-only PTE";David Hildenbrand;2023-04-18;1;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"So it's fine to rely on the writability information in the source PTE/PMD
and not recheck against the VMA as long as we're holding the PT lock to
synchronize with anyone who concurrently wants to downgrade write
permissions (like mprotect()) by first adjusting vma->vm_flags /
vma->vm_page_prot to then walk over the page tables to adjust the page
table entries";David Hildenbrand;2023-04-18;0;0
C_kwDOACN7MtoAKGYzZWJkZjA0MmRmNGUwOGJhYjFkNWY4YmYxYzRiOTU5ZDg3NDFjMTA;"Running test cases that should reveal such races -- mprotect(PROT_READ)
racing with page migration or THP splitting -- for multiple hours did not
reveal an issue with this cleanup.";David Hildenbrand;2023-04-18;0;1
C_kwDOACN7MtoAKDg1MWFlNjQyNDY5N2QxYzRmMDg1Y2I4NzhjODgxNjg5MjNlYmNhZDE;migrate_pages_batch: fix statistics for longterm pin retry;Huang Ying;2023-04-16;1;1
C_kwDOACN7MtoAKDg1MWFlNjQyNDY5N2QxYzRmMDg1Y2I4NzhjODgxNjg5MjNlYmNhZDE;"In commit fd4a7ac32918 (""mm: migrate: try again if THP split is failed due
to page refcnt""), if the THP splitting fails due to page reference count,
we will retry to improve migration successful rate";Huang Ying;2023-04-16;1;1
C_kwDOACN7MtoAKDg1MWFlNjQyNDY5N2QxYzRmMDg1Y2I4NzhjODgxNjg5MjNlYmNhZDE;" But the failed
splitting is counted as migration failure and migration retry, which will
cause duplicated failure counting";Huang Ying;2023-04-16;0;1
C_kwDOACN7MtoAKDg1MWFlNjQyNDY5N2QxYzRmMDg1Y2I4NzhjODgxNjg5MjNlYmNhZDE;" So, in this patch, this is fixed via
undoing the failure counting if we decide to retry";Huang Ying;2023-04-16;1;1
C_kwDOACN7MtoAKDg1MWFlNjQyNDY5N2QxYzRmMDg1Y2I4NzhjODgxNjg5MjNlYmNhZDE;" The patch is tested
via failure injection.";Huang Ying;2023-04-16;1;1
C_kwDOACN7MtoAKDNjODExZjc4ODNjNGVlNWEzNGJhNDM1NDM4MWJkZTA2Mjg4OGRkMzE;"mm/migrate: revert ""mm/migrate: fix wrongly apply write bit after mkdirty on sparc64""";David Hildenbrand;2023-04-11;1;1
C_kwDOACN7MtoAKDNjODExZjc4ODNjNGVlNWEzNGJhNDM1NDM4MWJkZTA2Mjg4OGRkMzE;"This reverts commit 96a9c287e25d (""mm/migrate: fix wrongly apply write bit
after mkdirty on sparc64"")";David Hildenbrand;2023-04-11;1;1
C_kwDOACN7MtoAKDNjODExZjc4ODNjNGVlNWEzNGJhNDM1NDM4MWJkZTA2Mjg4OGRkMzE;"Now that sparc64 mkdirty handling is fixed and no longer sets a PTE/PMD
writable that shouldn't be writable, let's revert the temporary fix";David Hildenbrand;2023-04-11;1;1
C_kwDOACN7MtoAKDNjODExZjc4ODNjNGVlNWEzNGJhNDM1NDM4MWJkZTA2Mjg4OGRkMzE;The mkdirty mm selftest still passes with this change on sparc64;David Hildenbrand;2023-04-11;1;0
C_kwDOACN7MtoAKDNjODExZjc4ODNjNGVlNWEzNGJhNDM1NDM4MWJkZTA2Mjg4OGRkMzE;"Note that loongarch handling was fixed in commit bf2f34a506e6 (""LoongArch:";David Hildenbrand;2023-04-11;1;0
C_kwDOACN7MtoAKDFkYTI4ZjFiNWFiMWRkMTU5MTllNzViYjlhNDdlMTc5MTJmZDc1NmM;mm/migrate: drop pte_mkhuge() in remove_migration_pte();Anshuman Khandual;2023-03-02;1;0
C_kwDOACN7MtoAKDFkYTI4ZjFiNWFiMWRkMTU5MTllNzViYjlhNDdlMTc5MTJmZDc1NmM;"Since the following commit, arch_make_huge_pte() should be used directly
in generic memory subsystem as a platform provided page table helper,
instead of pte_mkhuge()";Anshuman Khandual;2023-03-02;1;1
C_kwDOACN7MtoAKDFkYTI4ZjFiNWFiMWRkMTU5MTllNzViYjlhNDdlMTc5MTJmZDc1NmM;" This just drops pte_mkhuge() from
remove_migration_pte(), which has now become redundant";Anshuman Khandual;2023-03-02;1;1
C_kwDOACN7MtoAKDFkYTI4ZjFiNWFiMWRkMTU5MTllNzViYjlhNDdlMTc5MTJmZDc1NmM;"'commit 16785bd77431 (""mm: merge pte_mkhuge() call into arch_make_huge_pte()"")'";Anshuman Khandual;2023-03-02;1;0
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;mm: Introduce untagged_addr_remote();Kirill A. Shutemov;2023-03-12;1;1
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"untagged_addr() removes tags/metadata from the address and brings it to
the canonical form";Kirill A. Shutemov;2023-03-12;1;1
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;The helper is implemented on arm64 and sparc;Kirill A. Shutemov;2023-03-12;1;1
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"Both of
them do untagging based on global rules";Kirill A. Shutemov;2023-03-12;0;0
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"However, Linear Address Masking (LAM) on x86 introduces per-process
settings for untagging";Kirill A. Shutemov;2023-03-12;0;0
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"As a result, untagged_addr() is now only
suitable for untagging addresses for the current proccess";Kirill A. Shutemov;2023-03-12;1;1
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"The new helper untagged_addr_remote() has to be used when the address
targets remote process";Kirill A. Shutemov;2023-03-12;1;1
C_kwDOACN7MtoAKDQyOGUxMDZhZTFhZDRlNDVkM2ZkNjk3OGE3NTNkYjQ3NWQwZDBlYzk;"It requires the mmap lock for target mm to be
taken.";Kirill A. Shutemov;2023-03-12;0;0
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;migrate_pages: try migrate in batch asynchronously firstly;Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;"When we have locked more than one folios, we cannot wait the lock or bit
(e.g., page lock, buffer head lock, writeback bit) synchronously";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;"
Otherwise deadlock may be triggered";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;" This make it hard to batch the
synchronous migration directly";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;"This patch re-enables batching synchronous migration via trying to migrate
in batch asynchronously firstly";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;" And any folios that are failed to be
migrated asynchronously will be migrated synchronously one by one";Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKDJlZjdkYmIyNjk5MDJiZGUzNGM4MmYwMjc4MDY5OTIxOTVkMWQxZWU;"Test shows that this can restore the TLB flushing batching performance for
synchronous migration effectively.";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGEyMWQyMTMzMjE1YjU4ZmJmMjU0ZWEyYmI3N2ViMzE0M2ZmZWRmNjA;migrate_pages: move split folios processing out of migrate_pages_batch();Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGEyMWQyMTMzMjE1YjU4ZmJmMjU0ZWEyYmI3N2ViMzE0M2ZmZWRmNjA;To simplify the code logic and reduce the line number.;Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;migrate_pages: fix deadlock in batched migration;Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"Patch series ""migrate_pages: fix deadlock in batched synchronous
migration"", v2";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;Two deadlock bugs were reported for the migrate_pages() batching series;Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"
Thanks Hugh and Pengfei";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" Analysis shows that if we have locked some other
folios except the one we are migrating, it's not safe in general to wait
synchronously, for example, to wait the writeback to complete or wait to
lock the buffer head";Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"So 1/3 fixes the deadlock in a simple way, where the batching support for
the synchronous migration is disabled";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" The change is straightforward and
easy to be understood";Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" While 3/3 re-introduce the batching for
synchronous migration via trying to migrate asynchronously in batch
optimistically, then fall back to migrate synchronously one by one for
fail-to-migrate folios";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" Test shows that this can restore the TLB flushing
batching performance for synchronous migration effectively";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;This patch (of 3);Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;Two deadlock bugs were reported for the migrate_pages() batching series;Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"
Thanks Hugh and Pengfei!  For example, in the following deadlock trace
snippet,
 INFO: task kworker/u4:0:9 blocked for more than 147 seconds";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"       Not tainted 6.2.0-rc4-kvm+ #1314
 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" task:kworker/u4:0    state:D stack:0     pid:9     ppid:2      flags:0x00004000
 Workqueue: loop4 loop_rootcg_workfn
  INFO: task repro:1023 blocked for more than 147 seconds";Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"       Not tainted 6.2.0-rc4-kvm+ #1314
 ""echo 0 > /proc/sys/kernel/hung_task_timeout_secs"" disables this message";Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" task:repro           state:D stack:0     pid:1023  ppid:360    flags:0x00004004
 Call Trace";Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"  <TASK>
  __schedule+0x43b/0xd00
  schedule+0x6a/0xf0
  io_schedule+0x4a/0x80
  folio_wait_bit_common+0x1b5/0x4e0
  ? compaction_alloc+0x77/0x1150
  ? __pfx_wake_page_function+0x10/0x10
  folio_wait_bit+0x30/0x40
  folio_wait_writeback+0x2e/0x1e0
  migrate_pages_batch+0x555/0x1ac0
  ? __pfx_compaction_alloc+0x10/0x10
  ? __pfx_compaction_free+0x10/0x10
  ? __this_cpu_preempt_check+0x17/0x20
  ? lock_is_held_type+0xe6/0x140
  migrate_pages+0x100e/0x1180
  ? __pfx_compaction_free+0x10/0x10
  ? __pfx_compaction_alloc+0x10/0x10
  compact_zone+0xe10/0x1b50
  ? lock_is_held_type+0xe6/0x140
  ? check_preemption_disabled+0x80/0xf0
  compact_node+0xa3/0x100
  ? __sanitizer_cov_trace_const_cmp8+0x1c/0x30
  ? _find_first_bit+0x7b/0x90
  sysctl_compaction_handler+0x5d/0xb0
  proc_sys_call_handler+0x29d/0x420
  proc_sys_write+0x2b/0x40
  vfs_write+0x3a3/0x780
  ksys_write+0xb7/0x180
  __x64_sys_write+0x26/0x30
  do_syscall_64+0x3b/0x90
  entry_SYSCALL_64_after_hwframe+0x72/0xdc
 RIP: 0033:0x7f3a2471f59d
 RSP: 002b:00007ffe567f7288 EFLAGS: 00000217 ORIG_RAX: 0000000000000001
 RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f3a2471f59d
 RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000005
 RBP: 00007ffe567f72a0 R08: 0000000000000010 R09: 0000000000000010
 R10: 0000000000000010 R11: 0000000000000217 R12: 00000000004012e0
 R13: 00007ffe567f73e0 R14: 0000000000000000 R15: 0000000000000000
  </TASK>
The page migration task has held the lock of the shmem folio A, and is
waiting the writeback of the folio B of the file system on the loop block
device to complete";Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" While the loop worker task which writes back the
folio B is waiting to lock the shmem folio A, because the folio A backs
the folio B in the loop device";Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M; Thus deadlock is triggered;Huang Ying;2023-03-03;0;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"In general, if we have locked some other folios except the one we are
migrating, it's not safe to wait synchronously, for example, to wait the
writeback to complete or wait to lock the buffer head";Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;"To fix the deadlock, in this patch, we avoid to batch the page migration
except for MIGRATE_ASYNC mode";Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;" In MIGRATE_ASYNC mode, synchronous waiting
is avoided";Huang Ying;2023-03-03;1;0
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M;The fix can be improved further;Huang Ying;2023-03-03;1;1
C_kwDOACN7MtoAKGZiMzU5MmM0MWE0NDI3NjAxZjk2NDNiMmE4NGU1NWJiOTlmNWNkN2M; We will do that as soon as possible.;Huang Ying;2023-03-03;0;0
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;mm: avoid gcc complaint about pointer casting;Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"The migration code ends up temporarily stashing information of the wrong
type in unused fields of the newly allocated destination folio";Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;" That
all works fine, but gcc does complain about the pointer type mis-use";Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;    mm/migrate.c: In function __migrate_folio_extract;Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"    mm/migrate.c:1050:20: note: randstruct: casting between randomized structure pointer types (ssa): struct anon_vma and struct address_space
and gcc is actually right to complain since it really doesn't understand
that this is a very temporary special case where this is ok";Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"This could be fixed in different ways by just obfuscating the assignment
sufficiently that gcc doesn't see what is going on, but the truly
""proper C"" way to do this is by explicitly using a union";Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"Using unions for type conversions like this is normally hugely ugly and
syntactically nasty, but this really is one of the few cases where we
want to make it clear that we're not doing type conversion, we're really
re-using the value bit-for-bit just using another type";Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"IOW, this should not become a common pattern, but in this one case using
that odd union is probably the best way to document to the compiler what
is conceptually going on here";Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"[ Side note: there are valid cases where we convert pointers to other
  pointer types, notably the whole ""folio vs page"" situation, where the
  types actually have fundamental commonalities";Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"  The fact that the gcc note is limited to just randomized structures
  means that we don't see equivalent warnings for those cases, but it
  migth also mean that we miss other cases where we do play these kinds
  of dodgy games, and this kind of explicit conversion might be a good
  idea";Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"]
I verified that at least for an allmodconfig build on x86-64, this
generates the exact same code, apart from line numbers and assembler
comment changes";Linus Torvalds;2023-03-04;1;1
C_kwDOACN7MtoAKGU3N2Q1ODdhMmMwNGU4MmM2YTBkZmZhNGEzMmM4NzRhNDAyOTM4NWQ;"Fixes: 64c8902ed441 (""migrate_pages: split unmap_and_move() to _unmap() and _move()"")";Linus Torvalds;2023-03-04;0;1
C_kwDOACN7MtoAKGNkNzc1NTgwMGViNTRlODUyMmY1ZTUxZjRlNzFlNjQ5NGMxZjE1NzI;mm: change to return bool for isolate_movable_page();Baolin Wang;2023-02-15;0;0
C_kwDOACN7MtoAKGNkNzc1NTgwMGViNTRlODUyMmY1ZTUxZjRlNzFlNjQ5NGMxZjE1NzI;"Now the isolate_movable_page() can only return 0 or -EBUSY, and no users
will care about the negative return value, thus we can convert the
isolate_movable_page() to return a boolean value to make the code more
clear when checking the movable page isolation state";Baolin Wang;2023-02-15;1;1
C_kwDOACN7MtoAKGNkNzc1NTgwMGViNTRlODUyMmY1ZTUxZjRlNzFlNjQ5NGMxZjE1NzI;No functional changes intended.;Baolin Wang;2023-02-15;1;0
C_kwDOACN7MtoAKDk3NDdiOWU5MjQxOGI2MWMyMjgxNTYxZTA2NTE4MDNmMWZhZDAxNTk;mm: hugetlb: change to return bool for isolate_hugetlb();Baolin Wang;2023-02-15;0;0
C_kwDOACN7MtoAKDk3NDdiOWU5MjQxOGI2MWMyMjgxNTYxZTA2NTE4MDNmMWZhZDAxNTk;"Now the isolate_hugetlb() only returns 0 or -EBUSY, and most users did not
care about the negative value, thus we can convert the isolate_hugetlb()
to return a boolean value to make code more clear when checking the
hugetlb isolation state";Baolin Wang;2023-02-15;1;1
C_kwDOACN7MtoAKDk3NDdiOWU5MjQxOGI2MWMyMjgxNTYxZTA2NTE4MDNmMWZhZDAxNTk;" Moreover converts 2 users which will consider
the negative value returned by isolate_hugetlb()";Baolin Wang;2023-02-15;0;1
C_kwDOACN7MtoAKDk3NDdiOWU5MjQxOGI2MWMyMjgxNTYxZTA2NTE4MDNmMWZhZDAxNTk;No functional changes intended.;Baolin Wang;2023-02-15;1;0
C_kwDOACN7MtoAKGY3ZjljMDBkZmFmZmZkN2E1YTFhNTY4NWUyZDg3NGM2NDkxM2UyZWQ;mm: change to return bool for isolate_lru_page();Baolin Wang;2023-02-15;1;0
C_kwDOACN7MtoAKGY3ZjljMDBkZmFmZmZkN2E1YTFhNTY4NWUyZDg3NGM2NDkxM2UyZWQ;"The isolate_lru_page() can only return 0 or -EBUSY, and most users did not
care about the negative error of isolate_lru_page(), except one user in
add_page_for_migration()";Baolin Wang;2023-02-15;1;0
C_kwDOACN7MtoAKGY3ZjljMDBkZmFmZmZkN2E1YTFhNTY4NWUyZDg3NGM2NDkxM2UyZWQ;" So we can convert the isolate_lru_page() to
return a boolean value, which can help to make the code more clear when
checking the return value of isolate_lru_page()";Baolin Wang;2023-02-15;1;1
C_kwDOACN7MtoAKGY3ZjljMDBkZmFmZmZkN2E1YTFhNTY4NWUyZDg3NGM2NDkxM2UyZWQ;Also convert all users' logic of checking the isolation state;Baolin Wang;2023-02-15;1;1
C_kwDOACN7MtoAKGY3ZjljMDBkZmFmZmZkN2E1YTFhNTY4NWUyZDg3NGM2NDkxM2UyZWQ;No functional changes intended.;Baolin Wang;2023-02-15;1;0
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;mm/migrate: fix wrongly apply write bit after mkdirty on sparc64;Peter Xu;2023-02-16;1;1
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;"Nick Bowler reported another sparc64 breakage after the young/dirty
persistent work for page migration (per ""Link:"" below)";Peter Xu;2023-02-16;0;1
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;" That's after a
similar report [2]";Peter Xu;2023-02-16;0;0
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;"It turns out page migration was overlooked, and it wasn't failing before
because page migration was not enabled in the initial report test
environment";Peter Xu;2023-02-16;0;1
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;"David proposed another way [2] to fix this from sparc64 side, but that
patch didn't land somehow";Peter Xu;2023-02-16;1;1
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;" Neither did I check whether there's any other
arch that has similar issues";Peter Xu;2023-02-16;0;0
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;"Let's fix it for now as simple as moving the write bit handling to be
after dirty, like what we did before";Peter Xu;2023-02-16;1;1
C_kwDOACN7MtoAKDk2YTljMjg3ZTI1ZDY5MGZkOTYyM2I1MTMzNzAzYjhlMzEwZmJlZDE;"Note: this is based on mm-unstable, because the breakage was since 6.1 and
we're at a very late stage of 6.2 (-rc8), so I assume for this specific
case we should target this at 6.3.";Peter Xu;2023-02-16;1;1
C_kwDOACN7MtoAKDZmN2Q3NjBlODZmYTg0ODYyZDc0OWUzNmViZDI5YWJmMzFmNGY4ODM;migrate_pages: move THP/hugetlb migration support check to simplify code;Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDZmN2Q3NjBlODZmYTg0ODYyZDc0OWUzNmViZDI5YWJmMzFmNGY4ODM;This is a code cleanup patch, no functionality change is expected;Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDZmN2Q3NjBlODZmYTg0ODYyZDc0OWUzNmViZDI5YWJmMzFmNGY4ODM;" After
the change, the line number reduces especially in the long
migrate_pages_batch().";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;migrate_pages: batch flushing TLB;Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;"The TLB flushing will cost quite some CPU cycles during the folio
migration in some situations";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;" For example, when migrate a folio of a
process with multiple active threads that run on multiple CPUs";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;" After
batching the _unmap and _move in migrate_pages(), the TLB flushing can be
batched easily with the existing TLB flush batching mechanism";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;" This patch
implements that";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;We use the following test case to test the patch;Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;"On a 2-socket Intel server,
- Run pmbench memory accessing benchmark
- Run `migratepages` to migrate pages of pmbench between node 0 and
  node 1 back and forth";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;"With the patch, the TLB flushing IPI reduces 99.1% during the test and the
number of pages migrated successfully per second increases 291.7%";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;"Haoxin helped to test the patchset on an ARM64 server with 128 cores, 2
NUMA nodes";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;" Test results show that the page migration performance
increases up to 78%";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;NOTE: TLB flushing is batched only for normal folios, not for THP folios;Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDdlMTJiZWI4Y2EyYWM5OGIyZWM0MmUwZWE0Yjc2Y2RjOTNiNTg2NTQ;"
Because the overhead of TLB flushing for THP folios is much lower than
that for normal folios (about 1/512 on x86 platform).";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKGViZTc1ZTQ3NTEwNjNkY2U2ZjYxYjU3OWI0M2RlODZkY2Y3Yjc0NjI;migrate_pages: share more code between _unmap and _move;Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKGViZTc1ZTQ3NTEwNjNkY2U2ZjYxYjU3OWI0M2RlODZkY2Y3Yjc0NjI;"This is a code cleanup patch to reduce the duplicated code between the
_unmap and _move stages of migrate_pages()";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKGViZTc1ZTQ3NTEwNjNkY2U2ZjYxYjU3OWI0M2RlODZkY2Y3Yjc0NjI;" No functionality change is
expected.";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDgwNTYyYmEwZDgzNzhlODlmZTU4MzZjMjhlYTU2YzJhYWIzMDE0ZTg;migrate_pages: move migrate_folio_unmap();Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDgwNTYyYmEwZDgzNzhlODlmZTU4MzZjMjhlYTU2YzJhYWIzMDE0ZTg;Just move the position of the functions;Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDgwNTYyYmEwZDgzNzhlODlmZTU4MzZjMjhlYTU2YzJhYWIzMDE0ZTg;" There's no any functionality
change";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDgwNTYyYmEwZDgzNzhlODlmZTU4MzZjMjhlYTU2YzJhYWIzMDE0ZTg;" This is to make it easier to review the next patch via putting
code near its position in the next patch.";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDVkZmFiMTA5ZDUxOTNlNmMyMjRkOTZjYWJmOTBlOWNjMmMwMzk4ODQ;migrate_pages: batch _unmap and _move;Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDVkZmFiMTA5ZDUxOTNlNmMyMjRkOTZjYWJmOTBlOWNjMmMwMzk4ODQ;"In this patch the _unmap and _move stage of the folio migration is
batched";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDVkZmFiMTA5ZDUxOTNlNmMyMjRkOTZjYWJmOTBlOWNjMmMwMzk4ODQ;" That for, previously, it is,
  for each folio
    _unmap()
    _move()
Now, it is,
  for each folio
    _unmap()
  for each folio
    _move()
Based on this, we can batch the TLB flushing and use some hardware
accelerator to copy folios between batched _unmap and batched _move
stages.";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDY0Yzg5MDJlZDQ0MTgzMTdjZDQxNmM1NjZmODk2YmQ0YTkyYjJlZmM;migrate_pages: split unmap_and_move() to _unmap() and _move();Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDY0Yzg5MDJlZDQ0MTgzMTdjZDQxNmM1NjZmODk2YmQ0YTkyYjJlZmM;This is a preparation patch to batch the folio unmapping and moving;Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDY0Yzg5MDJlZDQ0MTgzMTdjZDQxNmM1NjZmODk2YmQ0YTkyYjJlZmM;"In this patch, unmap_and_move() is split to migrate_folio_unmap() and
migrate_folio_move()";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDY0Yzg5MDJlZDQ0MTgzMTdjZDQxNmM1NjZmODk2YmQ0YTkyYjJlZmM;" So, we can batch _unmap() and _move() in different
loops later";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDY0Yzg5MDJlZDQ0MTgzMTdjZDQxNmM1NjZmODk2YmQ0YTkyYjJlZmM;" To pass some information between unmap and move, the
original unused dst->mapping and dst->private are used.";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;migrate_pages: restrict number of pages to migrate in batch;Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;"This is a preparation patch to batch the folio unmapping and moving for
non-hugetlb folios";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;"If we had batched the folio unmapping, all folios to be migrated would be
unmapped before copying the contents and flags of the folios";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;" If the
folios that were passed to migrate_pages() were too many in unit of pages,
the execution of the processes would be stopped for too long time, thus
too long latency";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;" For example, migrate_pages() syscall will call
migrate_pages() with all folios of a process";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;" To avoid this possible
issue, in this patch, we restrict the number of pages to be migrated to be
no more than HPAGE_PMD_NR";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDQyMDEyZTA0MzZkNDRhZWIyZTY4ZjExYTI4ZGRkMGFkM2YzOGI2MWY;" That is, the influence is at the same level of
THP migration.";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKGU1YmZmZjhiMTBlNDk2Mzc4ZGE0Yjc4NjM0NzlkZDZmYjkwN2Q0ZWE;migrate_pages: separate hugetlb folios migration;Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKGU1YmZmZjhiMTBlNDk2Mzc4ZGE0Yjc4NjM0NzlkZDZmYjkwN2Q0ZWE;"This is a preparation patch to batch the folio unmapping and moving for
the non-hugetlb folios";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKGU1YmZmZjhiMTBlNDk2Mzc4ZGE0Yjc4NjM0NzlkZDZmYjkwN2Q0ZWE;" Based on that we can batch the TLB shootdown
during the folio migration and make it possible to use some hardware
accelerator for the folio copying";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKGU1YmZmZjhiMTBlNDk2Mzc4ZGE0Yjc4NjM0NzlkZDZmYjkwN2Q0ZWE;"In this patch the hugetlb folios and non-hugetlb folios migration is
separated in migrate_pages() to make it easy to change the non-hugetlb
folios migration implementation.";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;migrate_pages: organize stats with struct migrate_pages_stats;Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"Patch series ""migrate_pages(): batch TLB flushing"", v5";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"Now, migrate_pages() migrates folios one by one, like the fake code as
follows,
  for each folio
    unmap
    flush TLB
    copy
    restore map
If multiple folios are passed to migrate_pages(), there are opportunities
to batch the TLB flushing and copying";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" That is, we can change the code to
something as follows,
  for each folio
    unmap
  for each folio
    flush TLB
  for each folio
    copy
  for each folio
    restore map
The total number of TLB flushing IPI can be reduced considerably";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" And we
may use some hardware accelerator such as DSA to accelerate the folio
copying";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"So in this patch, we refactor the migrate_pages() implementation and
implement the TLB flushing batching";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" Base on this, hardware accelerated
folio copying can be implemented";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"If too many folios are passed to migrate_pages(), in the naive batched
implementation, we may unmap too many folios at the same time";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" The
possibility for a task to wait for the migrated folios to be mapped again
increases";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc; So the latency may be hurt;Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" To deal with this issue, the max
number of folios be unmapped in batch is restricted to no more than
HPAGE_PMD_NR in the unit of page";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" That is, the influence is at the same
level of THP migration";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"We use the following test to measure the performance impact of the
patchset,
On a 2-socket Intel server,
 - Run pmbench memory accessing benchmark
 - Run `migratepages` to migrate pages of pmbench between node 0 and
   node 1 back and forth";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"With the patch, the TLB flushing IPI reduces 99.1% during the test and
the number of pages migrated successfully per second increases 291.7%";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"Xin Hao helped to test the patchset on an ARM64 server with 128 cores,
2 NUMA nodes";Huang Ying;2023-02-13;0;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" Test results show that the page migration performance
increases up to 78%";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;This patch (of 9);Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;"Define struct migrate_pages_stats to organize the various statistics in
migrate_pages()";Huang Ying;2023-02-13;1;0
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" This makes it easier to collect and consume the
statistics in multiple functions";Huang Ying;2023-02-13;0;1
C_kwDOACN7MtoAKDViODU1OTM3MDk2YWVhN2Y4MWU3M2FkNmQ0MGQ0MzNjOWRkNDk1Nzc;" This will be needed in the following
patches in the series.";Huang Ying;2023-02-13;1;1
C_kwDOACN7MtoAKDI4MGQ3MjRhYzIwZjljYzQ2M2Q0YWI4ZTIyNjlmNTk4NDc2YjA3MGY;mm/migrate: convert putback_movable_pages() to use folios;Vishal Moola (Oracle);2023-01-30;1;0
C_kwDOACN7MtoAKDI4MGQ3MjRhYzIwZjljYzQ2M2Q0YWI4ZTIyNjlmNTk4NDc2YjA3MGY;"Removes 6 calls to compound_head(), and replaces putback_movable_page()
with putback_movable_folio() as well.";Vishal Moola (Oracle);2023-01-30;1;1
C_kwDOACN7MtoAKDE5OTc5NDk3YzAyYTM2NWVkOWQ4Mjc2YjVmNGNjMzY1NTdhMTNjZWQ;mm/migrate: convert isolate_movable_page() to use folios;Vishal Moola (Oracle);2023-01-30;1;0
C_kwDOACN7MtoAKDE5OTc5NDk3YzAyYTM2NWVkOWQ4Mjc2YjVmNGNjMzY1NTdhMTNjZWQ;"Removes 6 calls to compound_head() and prepares the function to take in a
folio instead of page argument.";Vishal Moola (Oracle);2023-01-30;1;1
C_kwDOACN7MtoAKGRhNzA3YTZkMTg0YThhNmVmMGI3NTZjM2JhNDk4ODhmZWMyMjM3OTM;mm/migrate: add folio_movable_ops();Vishal Moola (Oracle);2023-01-30;1;1
C_kwDOACN7MtoAKGRhNzA3YTZkMTg0YThhNmVmMGI3NTZjM2JhNDk4ODhmZWMyMjM3OTM;"folio_movable_ops() does the same as page_movable_ops() except uses folios
instead of pages";Vishal Moola (Oracle);2023-01-30;0;0
C_kwDOACN7MtoAKGRhNzA3YTZkMTg0YThhNmVmMGI3NTZjM2JhNDk4ODhmZWMyMjM3OTM;" This function will help make folio conversions in
migrate.c more readable.";Vishal Moola (Oracle);2023-01-30;0;1
C_kwDOACN7MtoAKGVhOGU3MmY0MTE2YTk5NWMyYWJhM2ZiNzM4YWMzNzJjNDExNTM3NWE;mm/hugetlb: convert putback_active_hugepage to take in a folio;Sidhartha Kumar;2023-01-25;1;0
C_kwDOACN7MtoAKGVhOGU3MmY0MTE2YTk5NWMyYWJhM2ZiNzM4YWMzNzJjNDExNTM3NWE;"Convert putback_active_hugepage() to folio_putback_active_hugetlb(), this
removes one user of the Huge Page macros which take in a page";Sidhartha Kumar;2023-01-25;1;0
C_kwDOACN7MtoAKGVhOGU3MmY0MTE2YTk5NWMyYWJhM2ZiNzM4YWMzNzJjNDExNTM3NWE;" The
callers in migrate.c are also cleaned up by being able to directly use the
src and dst folio variables.";Sidhartha Kumar;2023-01-25;0;1
C_kwDOACN7MtoAKGUzN2QzZTgzOGQ5MDc4NTM4ZjkyMDk1N2QxZTg5NjgyYjY3NjQ5Nzc;mm/hugetlb: convert alloc_migrate_huge_page to folios;Sidhartha Kumar;2023-01-13;1;0
C_kwDOACN7MtoAKGUzN2QzZTgzOGQ5MDc4NTM4ZjkyMDk1N2QxZTg5NjgyYjY3NjQ5Nzc;"Change alloc_huge_page_nodemask() to alloc_hugetlb_folio_nodemask() and
alloc_migrate_huge_page() to alloc_migrate_hugetlb_folio()";Sidhartha Kumar;2023-01-13;0;0
C_kwDOACN7MtoAKGUzN2QzZTgzOGQ5MDc4NTM4ZjkyMDk1N2QxZTg5NjgyYjY3NjQ5Nzc;" Both
functions now return a folio rather than a page.";Sidhartha Kumar;2023-01-13;0;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;mm/hugetlb: convert isolate_hugetlb to folios;Sidhartha Kumar;2023-01-13;1;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;"Patch series ""continue hugetlb folio conversion"", v3";Sidhartha Kumar;2023-01-13;1;1
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;"This series continues the conversion of core hugetlb functions to use
folios";Sidhartha Kumar;2023-01-13;0;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;"This series converts many helper funtions in the hugetlb fault
path";Sidhartha Kumar;2023-01-13;0;1
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;"This is in preparation for another series to convert the hugetlb
fault code paths to operate on folios";Sidhartha Kumar;2023-01-13;0;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;This patch (of 8);Sidhartha Kumar;2023-01-13;1;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;"Convert isolate_hugetlb() to take in a folio and convert its callers to
pass a folio";Sidhartha Kumar;2023-01-13;1;0
C_kwDOACN7MtoAKDZhYTNhOTIwMTI1ZTlmNTg4OTFlMmI1ZGMyZWZkNGQwYzFmZjA1YTY;" Use page_folio() to convert the callers to use a folio is
safe as isolate_hugetlb() operates on a head page.";Sidhartha Kumar;2023-01-13;1;1
C_kwDOACN7MtoAKDk2Zjk3YzQzOGY2MWRkYmE5NDExN2RjZDFhMWViMGFhYWZhMjIzMDk;mm: mlock: update the interface to use folios;Lorenzo Stoakes;2023-01-12;1;1
C_kwDOACN7MtoAKDk2Zjk3YzQzOGY2MWRkYmE5NDExN2RjZDFhMWViMGFhYWZhMjIzMDk;"Update the mlock interface to accept folios rather than pages, bringing
the interface in line with the internal implementation";Lorenzo Stoakes;2023-01-12;0;1
C_kwDOACN7MtoAKDk2Zjk3YzQzOGY2MWRkYmE5NDExN2RjZDFhMWViMGFhYWZhMjIzMDk;"munlock_vma_page() still requires a page_folio() conversion, however this
is consistent with the existent mlock_vma_page() implementation and a
product of rmap still dealing in pages rather than folios.";Lorenzo Stoakes;2023-01-12;1;1
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;mm/hugetlb: move swap entry handling into vma lock when faulted;Peter Xu;2022-12-16;1;0
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;"In hugetlb_fault(), there used to have a special path to handle swap entry
at the entrance using huge_pte_offset()";Peter Xu;2022-12-16;1;0
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;" That's unsafe because
huge_pte_offset() for a pmd sharable range can access freed pgtables if
without any lock to protect the pgtable from being freed after pmd
unshare";Peter Xu;2022-12-16;0;0
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;"Here the simplest solution to make it safe is to move the swap handling to
be after the vma lock being held";Peter Xu;2022-12-16;1;1
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;" We may need to take the fault mutex on
either migration or hwpoison entries now (also the vma lock, but that's
really needed), however neither of them is hot path";Peter Xu;2022-12-16;0;1
C_kwDOACN7MtoAKGZjZDQ4NTQwZDE4ODg3NmM5MTdhMzc3ZDgxY2QyNGMxMDAzMzJhNjI;"Note that the vma lock cannot be released in hugetlb_fault() when the
migration entry is detected, because in migration_entry_wait_huge() the
pgtable page will be used again (by taking the pgtable lock), so that also
so that it must be called with vma read lock held, and properly release
the lock in __migration_entry_wait_huge().";Peter Xu;2022-12-16;1;1
C_kwDOACN7MtoAKGUyNjM1NWUyMTUyMGU1NjczMmY1ODU5MGI4MTJjYTYxOWI4OGFjNzI;mm: export buffer_migrate_folio_norefs();Jan Kara;2022-12-07;0;0
C_kwDOACN7MtoAKGUyNjM1NWUyMTUyMGU1NjczMmY1ODU5MGI4MTJjYTYxOWI4OGFjNzI;"Ext4 needs this function to allow safe migration for journalled data
pages.";Jan Kara;2022-12-07;1;1
C_kwDOACN7MtoAKDRjNzRiNjVmNDc4ZGM5MzUzNzgwYTZiZTE3ZmM4MmYxYjA2Y2VhODA;mm/migrate.c: stop using 0 as NULL pointer;Yang Li;2022-11-16;0;0
C_kwDOACN7MtoAKDRjNzRiNjVmNDc4ZGM5MzUzNzgwYTZiZTE3ZmM4MmYxYjA2Y2VhODA;mm/migrate.c:1198:24: warning: Using plain integer as NULL pointer;Yang Li;2022-11-16;0;0
C_kwDOACN7MtoAKGVhZWM0ZTYzOWYxMTQxM2NlNzVmYmYzOGFmZmQxYWE1YzQwOTc5ZTk;migrate: convert migrate_pages() to use folios;Huang Ying;2022-11-09;1;0
C_kwDOACN7MtoAKGVhZWM0ZTYzOWYxMTQxM2NlNzVmYmYzOGFmZmQxYWE1YzQwOTc5ZTk;"Quite straightforward, the page functions are converted to corresponding
folio functions";Huang Ying;2022-11-09;0;0
C_kwDOACN7MtoAKGVhZWM0ZTYzOWYxMTQxM2NlNzVmYmYzOGFmZmQxYWE1YzQwOTc5ZTk; Same for comments;Huang Ying;2022-11-09;0;1
C_kwDOACN7MtoAKGVhZWM0ZTYzOWYxMTQxM2NlNzVmYmYzOGFmZmQxYWE1YzQwOTc5ZTk;THP specific code are converted to be large folio.;Huang Ying;2022-11-09;0;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;migrate: convert unmap_and_move() to use folios;Huang Ying;2022-11-09;1;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;"Patch series ""migrate: convert migrate_pages()/unmap_and_move() to use
folios"", v2";Huang Ying;2022-11-09;1;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;"The conversion is quite straightforward, just replace the page API to the
corresponding folio API";Huang Ying;2022-11-09;1;1
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;" migrate_pages() and unmap_and_move() mostly work
with folios (head pages) only";Huang Ying;2022-11-09;0;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;This patch (of 2);Huang Ying;2022-11-09;1;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk;"Quite straightforward, the page functions are converted to corresponding
folio functions";Huang Ying;2022-11-09;0;0
C_kwDOACN7MtoAKDQ5ZjUxODU5MjIxYTNkZmVlMjc0ODhlYWVhZmY4MDA0NTljYWM2YTk; Same for comments.;Huang Ying;2022-11-09;0;1
C_kwDOACN7MtoAKDE2ZmQ2YjMxZGQ5YjI0YWNmODNkNDM5YTczYTQxYzQxMzgxOTk0MjQ;"Revert ""mm: migration: fix the FOLL_GET failure on following huge page""";Baolin Wang;2022-11-09;1;1
C_kwDOACN7MtoAKDE2ZmQ2YjMxZGQ5YjI0YWNmODNkNDM5YTczYTQxYzQxMzgxOTk0MjQ;"Revert commit 831568214883 (""mm: migration: fix the FOLL_GET failure on
following huge page""), since after commit 1a6baaa0db73 (""s390/hugetlb";Baolin Wang;2022-11-09;1;1
C_kwDOACN7MtoAKDE2ZmQ2YjMxZGQ5YjI0YWNmODNkNDM5YTczYTQxYzQxMzgxOTk0MjQ;"switch to generic version of follow_huge_pud()"") and commit 57a196a58421
(""hugetlb: simplify hugetlb handling in follow_page_mask"") were merged,
now all the following huge page routines can support FOLL_GET operation.";Baolin Wang;2022-11-09;1;1
C_kwDOACN7MtoAKDM0NWM2MmQxNjM0OTZhZTRiNWMxY2U1MzBiMTU4ODA2N2Q4ZjVhOGI;mm/hugetlb: convert move_hugetlb_state() to folios;Sidhartha Kumar;2022-11-01;1;0
C_kwDOACN7MtoAKDM0NWM2MmQxNjM0OTZhZTRiNWMxY2U1MzBiMTU4ODA2N2Q4ZjVhOGI;"Clean up unmap_and_move_huge_page() by converting move_hugetlb_state() to
take in folios.";Sidhartha Kumar;2022-11-01;0;0
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;mm/migrate: make isolate_movable_page() skip slab pages;Vlastimil Babka;2022-11-04;1;0
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;"In the next commit we want to rearrange struct slab fields to allow a larger
rcu_head";Vlastimil Babka;2022-11-04;1;1
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;"Afterwards, the page->mapping field will overlap with SLUB's ""struct
list_head slab_list"", where the value of prev pointer can become LIST_POISON2,
which is 0x122 + POISON_POINTER_DELTA";Vlastimil Babka;2022-11-04;1;1
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;" Unfortunately the bit 1 being set can
confuse PageMovable() to be a false positive and cause a GPF as reported by lkp
To fix this, make isolate_movable_page() skip pages with the PageSlab flag set";Vlastimil Babka;2022-11-04;1;1
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;"This is a bit tricky as we need to add memory barriers to SLAB and SLUB's page
allocation and freeing, and their counterparts to isolate_movable_page()";Vlastimil Babka;2022-11-04;1;1
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;Based on my RFC from [2];Vlastimil Babka;2022-11-04;0;0
C_kwDOACN7MtoAKDhiODgxNzYzMGFlODAwMzJlODBiMmVhZjMzNGRlNzU2YWMxZmY2YTM;"Added a comment update from Matthew's variant in [3]
and, as done there, moved the PageSlab checks to happen before trying to take
the page lock.";Vlastimil Babka;2022-11-04;0;0
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;mm: migrate: try again if THP split is failed due to page refcnt;Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"When creating a virtual machine, we will use memfd_create() to get a file
descriptor which can be used to create share memory mappings using the
mmap function, meanwhile the mmap() will set the MAP_POPULATE flag to
allocate physical pages for the virtual machine";Baolin Wang;2022-10-24;1;0
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"When allocating physical pages for the guest, the host can fallback to
allocate some CMA pages for the guest when over half of the zone's free
memory is in the CMA area";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"In guest os, when the application wants to do some data transaction with
DMA, our QEMU will call VFIO_IOMMU_MAP_DMA ioctl to do longterm-pin and
create IOMMU mappings for the DMA pages";Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;" However, when calling
VFIO_IOMMU_MAP_DMA ioctl to pin the physical pages, we found it will be
failed to longterm-pin sometimes";Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"After some invetigation, we found the pages used to do DMA mapping can
contain some CMA pages, and these CMA pages will cause a possible failure
of the longterm-pin, due to failed to migrate the CMA pages";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;" The reason
of migration failure may be temporary reference count or memory allocation
failure";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;" So that will cause the VFIO_IOMMU_MAP_DMA ioctl returns error,
which makes the application failed to start";Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"I observed one migration failure case (which is not easy to reproduce) is
that, the 'thp_migration_fail' count is 1 and the 'thp_split_page_failed'
count is also 1";Baolin Wang;2022-10-24;1;0
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"That means when migrating a THP which is in CMA area, but can not allocate
a new THP due to memory fragmentation, so it will split the THP";Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;" However
THP split is also failed, probably the reason is temporary reference count
of this THP";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;" And the temporary reference count can be caused by dropping
page caches (I observed the drop caches operation in the system), but we
can not drop the shmem page caches due to they are already dirty at that
time";Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKGZkNGE3YWMzMjkxOGQzZDdhMmQxN2RjMDZjNTUyMGY0NWUzNmViNTI;"Especially for THP split failure, which is caused by temporary reference
count, we can try again to mitigate the failure of migration in this case
according to previous discussion [1].";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKGU1MWRhM2E5YjZjMmY2Nzg3OTg4MDI1OWEyNWM1MWRiZGEwMWM0NjI;mm/hugetlb: add folio_hstate();Sidhartha Kumar;2022-09-22;1;1
C_kwDOACN7MtoAKGU1MWRhM2E5YjZjMmY2Nzg3OTg4MDI1OWEyNWM1MWRiZGEwMWM0NjI;Helper function to retrieve hstate information from a hugetlb folio.;Sidhartha Kumar;2022-09-22;0;1
C_kwDOACN7MtoAKDAzZTVmODJlYTYzMmFmMzI5ZTMyZWMwM2Q5NTJiMmQ5OTQ5N2VlYWE;mm: migrate: fix return value if all subpages of THPs are migrated successfully;Baolin Wang;2022-10-24;1;1
C_kwDOACN7MtoAKDAzZTVmODJlYTYzMmFmMzI5ZTMyZWMwM2Q5NTJiMmQ5OTQ5N2VlYWE;"During THP migration, if THPs are not migrated but they are split and all
subpages are migrated successfully, migrate_pages() will still return the
number of THP pages that were not migrated";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKDAzZTVmODJlYTYzMmFmMzI5ZTMyZWMwM2Q5NTJiMmQ5OTQ5N2VlYWE;" This will confuse the callers
of migrate_pages()";Baolin Wang;2022-10-24;1;0
C_kwDOACN7MtoAKDAzZTVmODJlYTYzMmFmMzI5ZTMyZWMwM2Q5NTJiMmQ5OTQ5N2VlYWE;" For example, the longterm pinning will failed though
all pages are migrated successfully";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKDAzZTVmODJlYTYzMmFmMzI5ZTMyZWMwM2Q5NTJiMmQ5OTQ5N2VlYWE;"Thus we should return 0 to indicate that all pages are migrated in this
case";Baolin Wang;2022-10-24;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;mm/memory.c: fix race when faulting a device private page;Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"Patch series ""Fix several device private page reference counting issues"",
This series aims to fix a number of page reference counting issues in
drivers dealing with device private ZONE_DEVICE pages";Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" These result in
use-after-free type bugs, either from accessing a struct page which no
longer exists because it has been removed or accessing fields within the
struct page which are no longer valid because the page has been freed";Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;During normal usage it is unlikely these will cause any problems;Alistair Popple;2022-09-28;0;0
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" However
without these fixes it is possible to crash the kernel from userspace";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"
These crashes can be triggered either by unloading the kernel module or
unbinding the device from the driver prior to a userspace task exiting";Alistair Popple;2022-09-28;0;0
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"
In modules such as Nouveau it is also possible to trigger some of these
issues by explicitly closing the device file-descriptor prior to the task
exiting and then accessing device private memory";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;This involves some minor changes to both PowerPC and AMD GPU code;Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"
Unfortunately I lack hardware to test either of those so any help there
would be appreciated";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" The changes mimic what is done in for both Nouveau
and hmm-tests though so I doubt they will cause problems";Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;This patch (of 8);Alistair Popple;2022-09-28;1;0
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"When the CPU tries to access a device private page the migrate_to_ram()
callback associated with the pgmap for the page is called";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" However no
reference is taken on the faulting page";Alistair Popple;2022-09-28;0;0
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" Therefore a concurrent migration
of the device private page can free the page and possibly the underlying
pgmap";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" This results in a race which can crash the kernel due to the
migrate_to_ram() function pointer becoming invalid";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" It also means drivers
can't reliably read the zone_device_data field because the page may have
been freed with memunmap_pages()";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;"Close the race by getting a reference on the page while holding the ptl to
ensure it has not been freed";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" Unfortunately the elevated reference count
will cause the migration required to handle the fault to fail";Alistair Popple;2022-09-28;1;1
C_kwDOACN7MtoAKDE2Y2UxMDFkYjg1ZGI2OTRhOTEzODBhYTRjODliMjU1MzA4NzFkMzM;" To avoid
this failure pass the faulting page into the migrate_vma functions so that
if an elevated reference count is found it can be checked to see if it's
expected or not.";Alistair Popple;2022-09-28;0;1
C_kwDOACN7MtoAKDI5ZWVhOWI1YTljOWVjZjIxMTY0YTA4MmE0MmJmYWJlMDZmZGNiMzA;mm: convert page_get_anon_vma() to folio_get_anon_vma();Matthew Wilcox (Oracle);2022-09-02;1;0
C_kwDOACN7MtoAKDI5ZWVhOWI1YTljOWVjZjIxMTY0YTA4MmE0MmJmYWJlMDZmZGNiMzA;"With all callers now passing in a folio, rename the function and convert
all callers";Matthew Wilcox (Oracle);2022-09-02;1;0
C_kwDOACN7MtoAKDI5ZWVhOWI1YTljOWVjZjIxMTY0YTA4MmE0MmJmYWJlMDZmZGNiMzA;" Removes a couple of calls to compound_head() and a reference
to page->mapping.";Matthew Wilcox (Oracle);2022-09-02;0;1
C_kwDOACN7MtoAKGMzM2RiMjkyMzFhZDI0MmIwYzM4MWM2MGIxNjAzZjVlMWRlYzdlNDY;migrate: convert unmap_and_move_huge_page() to use folios;Matthew Wilcox (Oracle);2022-09-02;1;0
C_kwDOACN7MtoAKGMzM2RiMjkyMzFhZDI0MmIwYzM4MWM2MGIxNjAzZjVlMWRlYzdlNDY;"Saves several calls to compound_head() and removes a couple of uses of
page->lru.";Matthew Wilcox (Oracle);2022-09-02;0;1
C_kwDOACN7MtoAKDY4MmE3MWExYjZiMzYzYmZmNzE0NDBmNGVjYTY0OThmODI3YTgzOWQ;migrate: convert __unmap_and_move() to use folios;Matthew Wilcox (Oracle);2022-09-02;1;0
C_kwDOACN7MtoAKDY4MmE3MWExYjZiMzYzYmZmNzE0NDBmNGVjYTY0OThmODI3YTgzOWQ;Removes a lot of calls to compound_head();Matthew Wilcox (Oracle);2022-09-02;1;0
C_kwDOACN7MtoAKDY4MmE3MWExYjZiMzYzYmZmNzE0NDBmNGVjYTY0OThmODI3YTgzOWQ;" Also remove a VM_BUG_ON that
can never trigger as the PageAnon bit is the bottom bit of page->mapping.";Matthew Wilcox (Oracle);2022-09-02;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;mm: fix the handling Non-LRU pages returned by follow_page;Haiyue Wang;2022-08-23;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;"The handling Non-LRU pages returned by follow_page() jumps directly, it
doesn't call put_page() to handle the reference count, since 'FOLL_GET'
flag for follow_page() has get_page() called";Haiyue Wang;2022-08-23;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;" Fix the zone device page
check by handling the page reference count correctly before returning";Haiyue Wang;2022-08-23;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;"And as David reviewed, ""device pages are never PageKsm pages""";Haiyue Wang;2022-08-23;0;0
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;" Drop this
zone device page check for break_ksm()";Haiyue Wang;2022-08-23;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI;"Since the zone device page can't be a transparent huge page, so drop the
redundant zone device page check for split_huge_pages_pid()";Haiyue Wang;2022-08-23;1;1
C_kwDOACN7MtoAKGY3MDkxZWQ2NGVjODMxMWIwYzM1ODY1ODc1ZjhjM2UwNGU1ZWE1MzI; (by Miaohe);Haiyue Wang;2022-08-23;0;0
C_kwDOACN7MtoAKDQ2N2IxNzFhZjg4MTI4MmZjNjI3MzI4ZTZjMTY0ZjA0NGE2ZGY4ODg;mm/demotion: update node_is_toptier to work with memory tiers;Aneesh Kumar K.V;2022-08-18;0;1
C_kwDOACN7MtoAKDQ2N2IxNzFhZjg4MTI4MmZjNjI3MzI4ZTZjMTY0ZjA0NGE2ZGY4ODg;"With memory tier support we can have memory only NUMA nodes in the top
tier from which we want to avoid promotion tracking NUMA faults";Aneesh Kumar K.V;2022-08-18;1;1
C_kwDOACN7MtoAKDQ2N2IxNzFhZjg4MTI4MmZjNjI3MzI4ZTZjMTY0ZjA0NGE2ZGY4ODg;" Update
node_is_toptier to work with memory tiers";Aneesh Kumar K.V;2022-08-18;0;1
C_kwDOACN7MtoAKDQ2N2IxNzFhZjg4MTI4MmZjNjI3MzI4ZTZjMTY0ZjA0NGE2ZGY4ODg;" All NUMA nodes are by default
top tier nodes";Aneesh Kumar K.V;2022-08-18;0;0
C_kwDOACN7MtoAKDQ2N2IxNzFhZjg4MTI4MmZjNjI3MzI4ZTZjMTY0ZjA0NGE2ZGY4ODg;" With lower(slower) memory tiers added we consider all
memory tiers above a memory tier having CPU NUMA nodes as a top memory
tier";Aneesh Kumar K.V;2022-08-18;0;1
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;mm/demotion: build demotion targets based on explicit memory tiers;Aneesh Kumar K.V;2022-08-18;1;1
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;"This patch switch the demotion target building logic to use memory tiers
instead of NUMA distance";Aneesh Kumar K.V;2022-08-18;1;1
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;" All N_MEMORY NUMA nodes will be placed in the
default memory tier and additional memory tiers will be added by drivers
like dax kmem";Aneesh Kumar K.V;2022-08-18;0;0
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;"This patch builds the demotion target for a NUMA node by looking at all
memory tiers below the tier to which the NUMA node belongs";Aneesh Kumar K.V;2022-08-18;0;0
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;" The closest
node in the immediately following memory tier is used as a demotion
target";Aneesh Kumar K.V;2022-08-18;0;0
C_kwDOACN7MtoAKDZjNTQyYWI3NTcxNGZlOTBkYWUyOTJhZWIzZTkxYWM1M2Y1ZmY1OTk;"Since we are now only building demotion target for N_MEMORY NUMA nodes the
CPU hotplug calls are removed in this patch.";Aneesh Kumar K.V;2022-08-18;0;0
C_kwDOACN7MtoAKDkxOTUyNDQwMjI3ODg5MzVlYWMwZGYxNjEzMjM5NGZmYTU2MTM1NDI;mm/demotion: move memory demotion related code;Aneesh Kumar K.V;2022-08-18;1;1
C_kwDOACN7MtoAKDkxOTUyNDQwMjI3ODg5MzVlYWMwZGYxNjEzMjM5NGZmYTU2MTM1NDI;This moves memory demotion related code to mm/memory-tiers.c;Aneesh Kumar K.V;2022-08-18;0;1
C_kwDOACN7MtoAKDkxOTUyNDQwMjI3ODg5MzVlYWMwZGYxNjEzMjM5NGZmYTU2MTM1NDI;" No
functional change in this patch.";Aneesh Kumar K.V;2022-08-18;1;0
C_kwDOACN7MtoAKDcwNDdiNWE0MGJjZTc0ODgzYTEwNTQ5YmQ2MDk2NDNiOGJjNGEzZmI;mm: migrate: do not retry 10 times for the subpages of fail-to-migrate THP;Baolin Wang;2022-08-17;1;1
C_kwDOACN7MtoAKDcwNDdiNWE0MGJjZTc0ODgzYTEwNTQ5YmQ2MDk2NDNiOGJjNGEzZmI;"If THP is failed to migrate due to -ENOSYS or -ENOMEM case, the THP will
be split, and the subpages of fail-to-migrate THP will be tried to migrate
again, so we should not account the retry counter in the second loop,
since we already accounted 'nr_thp_failed' in the first loop";Baolin Wang;2022-08-17;1;1
C_kwDOACN7MtoAKDcwNDdiNWE0MGJjZTc0ODgzYTEwNTQ5YmQ2MDk2NDNiOGJjNGEzZmI;"Moreover we also do not need retry 10 times for -EAGAIN case for the
subpages of fail-to-migrate THP in the second loop, since we already
regarded the THP as migration failure, and save some migration time (for
the worst case, will try 512 * 10 times) according to previous discussion";Baolin Wang;2022-08-17;1;1
C_kwDOACN7MtoAKDA3NzMwOWJjMWViOGY0MWRkNDE0OTAyNjM0YzIxMjYwNjAwOGJkNTQ;migrate_pages(): fix failure counting for retry;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDA3NzMwOWJjMWViOGY0MWRkNDE0OTAyNjM0YzIxMjYwNjAwOGJkNTQ;"After 10 retries, we will give up and the remaining pages will be counted
as failure in nr_failed and nr_thp_failed";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDA3NzMwOWJjMWViOGY0MWRkNDE0OTAyNjM0YzIxMjYwNjAwOGJkNTQ;" We should count the failure in
nr_failed_pages too";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDA3NzMwOWJjMWViOGY0MWRkNDE0OTAyNjM0YzIxMjYwNjAwOGJkNTQ; This is done in this patch.;Huang Ying;2022-08-17;1;0
C_kwDOACN7MtoAKGU2ZmE4YTc5ZmUwM2UxNzM0YzI2Mjg3NDc0YjFhYzA5Mjg3ZmRlYjc;migrate_pages(): fix failure counting for THP splitting;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGU2ZmE4YTc5ZmUwM2UxNzM0YzI2Mjg3NDc0YjFhYzA5Mjg3ZmRlYjc;If THP is failed to be migrated, it may be split and retry;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGU2ZmE4YTc5ZmUwM2UxNzM0YzI2Mjg3NDc0YjFhYzA5Mjg3ZmRlYjc;" But after
splitting, the head page will be left in ""from"" list, although THP
migration failure has been counted already";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGU2ZmE4YTc5ZmUwM2UxNzM0YzI2Mjg3NDc0YjFhYzA5Mjg3ZmRlYjc;" If the head page is failed to
be migrated too, the failure will be counted twice incorrectly";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGU2ZmE4YTc5ZmUwM2UxNzM0YzI2Mjg3NDc0YjFhYzA5Mjg3ZmRlYjc;" So this
is fixed in this patch via moving the head page of THP after splitting to
""thp_split_pages"" too.";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;migrate_pages(): fix failure counting for THP on -ENOSYS;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;"If THP or hugetlbfs page migration isn't supported, unmap_and_move() or
unmap_and_move_huge_page() will return -ENOSYS";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;" For THP, splitting will
be tried, but if splitting doesn't succeed, the THP will be left in ""from""
list wrongly";Huang Ying;2022-08-17;1;0
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;" If some other pages are retried, the THP migration failure
will counted again";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;" This is fixed via moving the failure THP from ""from""
to ""ret_pages""";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;"Another issue of the original code is that the unsupported failure
processing isn't consistent between THP and hugetlbfs page";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKDU3N2JlMDVjODkyN2FhNTkzY2Y3ZTI5ZTJiNDk0MDYwN2Y1NzU2ZmY;" Make them
consistent in this patch to make the code easier to be understood too.";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDVmYzMwOTE2YjVjZGE2OTdhN2ViOGYxMTY3YzM4YzI3MTAwYTc5M2E;migrate_pages(): fix failure counting for THP subpages retrying;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDVmYzMwOTE2YjVjZGE2OTdhN2ViOGYxMTY3YzM4YzI3MTAwYTc5M2E;"If THP is failed to be migrated for -ENOSYS and -ENOMEM, the THP will be
split into thp_split_pages, and after other pages are migrated, pages in
thp_split_pages will be migrated with no_subpage_counting == true, because
its failure have been counted already";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKDVmYzMwOTE2YjVjZGE2OTdhN2ViOGYxMTY3YzM4YzI3MTAwYTc5M2E;" If some pages in thp_split_pages
are retried during migration, we should not count their failure if
no_subpage_counting == true too";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDVmYzMwOTE2YjVjZGE2OTdhN2ViOGYxMTY3YzM4YzI3MTAwYTc5M2E;" This is done this patch to fix the
failure counting for THP subpages retrying.";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;migrate_pages(): fix THP failure counting for -ENOMEM;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;"In unmap_and_move(), if the new THP cannot be allocated, -ENOMEM will be
returned, and migrate_pages() will try to split the THP unless ""reason"" is
MR_NUMA_MISPLACED (that is, nosplit == true)";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;" But when nosplit == true,
the THP migration failure will not be counted";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;"This is incorrect, so in this patch, the THP migration failure will be
counted for -ENOMEM regardless of nosplit is true or false";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;" The nr_failed
counting isn't fixed because it's not used";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGZiZWQ1M2I0Nzc3MGI5NzhiZTI5MGNlYzBmNGYyMjU3Nzc2NmMxMmQ;" Added some comments for it
per Baolin's suggestion.";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;migrate_pages(): remove unnecessary list_safe_reset_next();Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;"Before commit b5bade978e9b (""mm: migrate: fix the return value of
migrate_pages()""), the tail pages of THP will be put in the ""from""
list directly";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;" So one of the loop cursors (page2) needs to be reset,
as is done in try_split_thp() via list_safe_reset_next()";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;" But after
the commit, the tail pages of THP will be put in a dedicated
list (thp_split_pages)";Huang Ying;2022-08-17;1;0
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;" That is, the ""from"" list will not be changed
during splitting";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;" So, it's unnecessary to call list_safe_reset_next()
anymore";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDljNjJmZjAwNWZjNzc0ZmIyYmExNDIyM2IwZDg2NWE4YWNhNDhmYjU;This is a code cleanup, no functionality changes are expected.;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;migrate: fix syscall move_pages() return value for failure;Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"Patch series ""migrate_pages(): fix several bugs in error path"", v3";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"During review the code of migrate_pages() and build a test program for
it";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;" Several bugs in error path are identified and fixed in this
series";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"Most patches are tested via
- Apply error-inject.patch in Linux kernel
- Compile test-migrate.c (with -lnuma)
- Test with test-migrate.sh
error-inject.patch, test-migrate.c, and test-migrate.sh are as below";Huang Ying;2022-08-17;1;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"It turns out that error injection is an important tool to fix bugs in
error path";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;This patch (of 8);Huang Ying;2022-08-17;1;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"The return value of move_pages() syscall is incorrect when counting
the remaining pages to be migrated";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;" For example, for the following
test program,
 #define _GNU_SOURCE
 #include <stdbool.h>
 #include <stdio.h>
 #include <string.h>
 #include <stdlib.h>
 #include <errno.h>
 #include <fcntl.h>
 #include <sys/uio.h>
 #include <sys/mman.h>
 #include <sys/types.h>
 #include <unistd.h>
 #include <numaif.h>
 #include <numa.h>
 #ifndef MADV_FREE
 #endif
 #define ONE_MB		(1024 * 1024)
 #define MAP_SIZE	(16 * ONE_MB)
 #define THP_SIZE	(2 * ONE_MB)
 #define THP_MASK	(THP_SIZE - 1)
 #define ERR_EXIT_ON(cond, msg)					\
	 do {							\
		 int __cond_in_macro = (cond);			\
		 if (__cond_in_macro)				\
			 error_exit(__cond_in_macro, (msg));	\
	 } while (0)
 void error_msg(int ret, int nr, int *status, const char *msg)
	 fprintf(stderr, ""Error: %s, ret : %d, error: %s\n"",
	 for (i = 0; i < nr; i++)
 void error_exit(int ret, const char *msg)
 void prepare()
	 addr = mmap(NULL, MAP_SIZE, PROT_READ | PROT_WRITE,
 void test_migrate()
 int main(int argc, char *argv[])
The output of the current kernel is,
Error: move 1 page, ret : 0, error: Success
status: 1
Error: move 2 pages, page 1 not mapped, ret : 0, error: Success
status: 1 -14
Error: move 2 pages, ret : 0, error: Success
status: 1 1
Error: move 2 pages, page 1 to node 0, ret : 0, error: Success
status: 1 0
Make page 0 cannot be migrated";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"Error: move 1 page, ret : 0, error: Success
status: 1024
Error: move 2 pages, page 1 not mapped, ret : 1, error: Success
status: 1024 -14
Error: move 2 pages, ret : 0, error: Success
status: 1024 1024
Error: move 2 pages, page 1 to node 0, ret : 1, error: Success
status: 1024 1024
While the expected output is,
Error: move 1 page, ret : 0, error: Success
status: 1
Error: move 2 pages, page 1 not mapped, ret : 0, error: Success
status: 1 -14
Error: move 2 pages, ret : 0, error: Success
status: 1 1
Error: move 2 pages, page 1 to node 0, ret : 0, error: Success
status: 1 0
Make page 0 cannot be migrated";Huang Ying;2022-08-17;0;0
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;"Error: move 1 page, ret : 1, error: Success
status: 1024
Error: move 2 pages, page 1 not mapped, ret : 1, error: Success
status: 1024 -14
Error: move 2 pages, ret : 1, error: Success
status: 1024 1024
Error: move 2 pages, page 1 to node 0, ret : 2, error: Success
status: 1024 1024
Fix this via correcting the remaining pages counting";Huang Ying;2022-08-17;0;1
C_kwDOACN7MtoAKGE3NTA0ZWQxNGY5YjVlODczNTk5YjI0ODdlYjk1MDYyZGQwYjY1Zjg;" With the fix,
the output for the test program as above is expected.";Huang Ying;2022-08-17;1;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;mm: remember young/dirty bit for page migrations;Peter Xu;2022-08-11;0;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;"When page migration happens, we always ignore the young/dirty bit settings
in the old pgtable, and marking the page as old in the new page table
using either pte_mkold() or pmd_mkold(), and keeping the pte clean";Peter Xu;2022-08-11;1;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;"That's fine from functional-wise, but that's not friendly to page reclaim
because the moving page can be actively accessed within the procedure";Peter Xu;2022-08-11;0;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;"
Not to mention hardware setting the young bit can bring quite some
overhead on some systems, e.g";Peter Xu;2022-08-11;1;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;" x86_64 needs a few hundreds nanoseconds to
set the bit";Peter Xu;2022-08-11;1;0
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;" The same slowdown problem to dirty bits when the memory is
first written after page migration happened";Peter Xu;2022-08-11;0;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;"Actually we can easily remember the A/D bit configuration and recover the
information after the page is migrated";Peter Xu;2022-08-11;0;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;" To achieve it, define a new set
of bits in the migration swap offset field to cache the A/D bits for old
pte";Peter Xu;2022-08-11;1;0
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;" Then when removing/recovering the migration entry, we can recover
the A/D bits even if the page changed";Peter Xu;2022-08-11;1;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM;"One thing to mention is that here we used max_swapfile_size() to detect
how many swp offset bits we have, and we'll only enable this feature if we
know the swp offset is big enough to store both the PFN value and the A/D
bits";Peter Xu;2022-08-11;1;1
C_kwDOACN7MtoAKDJlMzQ2ODc3OGRiZTNlYzM4OWExMGMyMWE3MDNiYjhlNWJlNWNmYmM; Otherwise the A/D bits are dropped like before.;Peter Xu;2022-08-11;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;memory tiering: hot page selection with hint page fault latency;Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"Patch series ""memory tiering: hot page selection"", v4";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"To optimize page placement in a memory tiering system with NUMA balancing,
the hot pages in the slow memory nodes need to be identified";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"
Essentially, the original NUMA balancing implementation selects the mostly
recently accessed (MRU) pages to promote";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But this isn't a perfect
algorithm to identify the hot pages";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" Because the pages with quite low
access frequency may be accessed eventually given the NUMA balancing page
table scanning period could be quite long (e.g";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM; 60 seconds);Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So in this
patchset, we implement a new hot page identification algorithm based on
the latency between NUMA balancing page table scanning and hint page
fault";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM; Which is a kind of mostly frequently accessed (MFU) algorithm;Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"In NUMA balancing memory tiering mode, if there are hot pages in slow
memory node and cold pages in fast memory node, we need to promote/demote
hot/cold pages between the fast and cold memory nodes";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;A choice is to promote/demote as fast as possible;Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But the CPU cycles and
memory bandwidth consumed by the high promoting/demoting throughput will
hurt the latency of some workload because of accessing inflating and slow
memory bandwidth contention";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"A way to resolve this issue is to restrict the max promoting/demoting
throughput";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM; It will take longer to finish the promoting/demoting;Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But
the workload latency will be better";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" This is implemented in this patchset
as the page promotion rate limit mechanism";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"The promotion hot threshold is workload and system configuration
dependent";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So in this patchset, a method to adjust the hot threshold
automatically is implemented";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" The basic idea is to control the number of
the candidate promotion pages to match the promotion rate limit";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"We used the pmbench memory accessing benchmark tested the patchset on a
2-socket server system with DRAM and PMEM installed";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" The test results are
as follows,
		pmbench score		promote rate
		 (accesses/s)			MB/s
base		  146887704.1		       725.6
hot selection     165695601.2		       544.0
rate limit	  162814569.8		       165.2
auto adjustment	  170495294.0                  136.9
From the results above,
With hot page selection patch [1/3], the pmbench score increases about
12.8%, and promote rate (overhead) decreases about 25.0%, compared with
base kernel";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"With rate limit patch [2/3], pmbench score decreases about 1.7%, and
promote rate decreases about 69.6%, compared with hot page selection
patch";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"With threshold auto adjustment patch [3/3], pmbench score increases about
4.7%, and promote rate decrease about 17.1%, compared with rate limit
patch";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"Baolin helped to test the patchset with MySQL on a machine which contains
1 DRAM node (30G) and 1 PMEM node (126G)";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"sysbench /usr/share/sysbench/oltp_read_write.lua \
--tables=200 \
--table-size=1000000 \
--report-interval=10 \
--threads=16 \
--time=120
The tps can be improved about 5%";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;This patch (of 3);Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"To optimize page placement in a memory tiering system with NUMA balancing,
the hot pages in the slow memory node need to be identified";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" Essentially,
the original NUMA balancing implementation selects the mostly recently
accessed (MRU) pages to promote";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But this isn't a perfect algorithm to
identify the hot pages";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" Because the pages with quite low access frequency
may be accessed eventually given the NUMA balancing page table scanning
period could be quite long (e.g";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM; 60 seconds);Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" The most frequently
accessed (MFU) algorithm is better";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;So, in this patch we implemented a better hot page selection algorithm;Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"
Which is based on NUMA balancing page table scanning and hint page fault
as follows,
- When the page tables of the processes are scanned to change PTE/PMD
  to be PROT_NONE, the current time is recorded in struct page as scan
  time";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;- When the page is accessed, hint page fault will occur;Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" The scan
  time is gotten from the struct page";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" And The hint page fault
  latency is defined as
    hint page fault time - scan time
The shorter the hint page fault latency of a page is, the higher the
probability of their access frequency to be higher";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So the hint page
fault latency is a better estimation of the page hot/cold";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;It's hard to find some extra space in struct page to hold the scan time;Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"
Fortunately, we can reuse some bits used by the original NUMA balancing";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"NUMA balancing uses some bits in struct page to store the page accessing
CPU and PID (referring to page_cpupid_xchg_last())";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" Which is used by the
multi-stage node selection algorithm to avoid to migrate pages shared
accessed by the NUMA nodes back and forth";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But for pages in the slow
memory node, even if they are shared accessed by multiple NUMA nodes, as
long as the pages are hot, they need to be promoted to the fast memory
node";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So the accessing CPU and PID information are unnecessary for the
slow memory pages";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" We can reuse these bits in struct page to record the
scan time";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM; For the fast memory pages, these bits are used as before;Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"For the hot threshold, the default value is 1 second, which works well in
our performance test";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" All pages with hint page fault latency < hot
threshold will be considered hot";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;It's hard for users to determine the hot threshold;Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So we don't provide a
kernel ABI to set it, just provide a debugfs interface for advanced users
to experiment";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" We will continue to work on a hot threshold automatic
adjustment mechanism";Huang Ying;2022-07-13;0;1
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"The downside of the above method is that the response time to the workload
hot spot changing may be much longer";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" For example,
- A previous cold memory area becomes hot
- The hint page fault will be triggered";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" But the hint page fault
  latency isn't shorter than the hot threshold";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;" So the pages will
  not be promoted";Huang Ying;2022-07-13;0;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"- When the memory area is scanned again, maybe after a scan period,
  the hint page fault latency measured will be shorter than the hot
  threshold and the pages will be promoted";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"To mitigate this, if there are enough free space in the fast memory node,
the hot threshold will not be used, all pages will be promoted upon the
hint page fault for fast response";Huang Ying;2022-07-13;1;0
C_kwDOACN7MtoAKDMzMDI0NTM2YmFmZDkxMjlmMWQxNmFkZTA5NzQ2NzFjNjQ4NzAwYWM;"Thanks Zhong Jiang reported and tested the fix for a bug when disabling
memory tiering mode dynamically.";Huang Ying;2022-07-13;1;1
C_kwDOACN7MtoAKDgzMTU2ODIxNDg4M2UwYTk5NDBmNzc2NzcxMzQzNDIwMzA2ZDIzNDE;mm: migration: fix the FOLL_GET failure on following huge page;Haiyue Wang;2022-08-12;1;1
C_kwDOACN7MtoAKDgzMTU2ODIxNDg4M2UwYTk5NDBmNzc2NzcxMzQzNDIwMzA2ZDIzNDE;"Not all huge page APIs support FOLL_GET option, so move_pages() syscall
will fail to get the page node information for some huge pages";Haiyue Wang;2022-08-12;1;1
C_kwDOACN7MtoAKDgzMTU2ODIxNDg4M2UwYTk5NDBmNzc2NzcxMzQzNDIwMzA2ZDIzNDE;"Like x86 on linux 5.19 with 1GB huge page API follow_huge_pud(), it will
return NULL page for FOLL_GET when calling move_pages() syscall with the
NULL 'nodes' parameter, the 'status' parameter has '-2' error in array";Haiyue Wang;2022-08-12;0;1
C_kwDOACN7MtoAKDgzMTU2ODIxNDg4M2UwYTk5NDBmNzc2NzcxMzQzNDIwMzA2ZDIzNDE;Note: follow_huge_pud() now supports FOLL_GET in linux 6.0.;Haiyue Wang;2022-08-12;1;0
C_kwDOACN7MtoAKDlkMGRkYzBjYjU3NWZkNDFmZjE2MTMxYjA2ZTA4ZTFmZWFjNDNiODE;fs: Remove aops->migratepage();Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDlkMGRkYzBjYjU3NWZkNDFmZjE2MTMxYjA2ZTA4ZTFmZWFjNDNiODE;With all users converted to migrate_folio(), remove this operation.;Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKGI4OTBlYzJhMmMyZDk2MmY3MWJhMzFhZTI5MWY4ZmQyNTJiNDYyNTg;hugetlb: Convert to migrate_folio;Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKGI4OTBlYzJhMmMyZDk2MmY3MWJhMzFhZTI5MWY4ZmQyNTJiNDYyNTg;This involves converting migrate_huge_page_move_mapping();Matthew Wilcox (Oracle);2022-06-06;1;1
C_kwDOACN7MtoAKGI4OTBlYzJhMmMyZDk2MmY3MWJhMzFhZTI5MWY4ZmQyNTJiNDYyNTg;" We also need a
folio variant of hugetlb_set_page_subpool(), but that's for a later patch.";Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKDJlYzgxMGQ1OTYwMmYwZTA4ODQ3Zjk4NmVmOGUxNjQ2OTcyMjQ5NmY;mm/migrate: Add filemap_migrate_folio();Matthew Wilcox (Oracle);2022-06-06;1;1
C_kwDOACN7MtoAKDJlYzgxMGQ1OTYwMmYwZTA4ODQ3Zjk4NmVmOGUxNjQ2OTcyMjQ5NmY;"There is nothing iomap-specific about iomap_migratepage(), and it fits
a pattern used by several other filesystems, so move it to mm/migrate.c,
convert it to be filemap_migrate_folio() and convert the iomap filesystems
to use it.";Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDU0MTg0NjUwMmY0ZmU4MjZjZDdjMTZlNDc4NDY5NWFjOTA3MzY1ODU;mm/migrate: Convert migrate_page() to migrate_folio();Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDU0MTg0NjUwMmY0ZmU4MjZjZDdjMTZlNDc4NDY5NWFjOTA3MzY1ODU;Convert all callers to pass a folio;Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDU0MTg0NjUwMmY0ZmU4MjZjZDdjMTZlNDc4NDY5NWFjOTA3MzY1ODU;" Most have the folio
already available";Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKDU0MTg0NjUwMmY0ZmU4MjZjZDdjMTZlNDc4NDY5NWFjOTA3MzY1ODU;" Switch all users from aops->migratepage to
aops->migrate_folio";Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKDU0MTg0NjUwMmY0ZmU4MjZjZDdjMTZlNDc4NDY5NWFjOTA3MzY1ODU; Also turn the documentation into kerneldoc.;Matthew Wilcox (Oracle);2022-06-06;1;1
C_kwDOACN7MtoAKDEwOGNhODM1ODEzOWJlYzQyMzIzMTlkZWJmYjIwYmFmZGFmNGY4Nzc;mm/migrate: Convert expected_page_refs() to folio_expected_refs();Matthew Wilcox (Oracle);2022-06-06;0;1
C_kwDOACN7MtoAKDEwOGNhODM1ODEzOWJlYzQyMzIzMTlkZWJmYjIwYmFmZGFmNGY4Nzc;"Now that both callers have a folio, convert this function to
take a folio & rename it.";Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDY3MjM1MTgyYTQxYzFiZDZiMzI4MDZhMTU1NmExZDI5OWI4NDIxMmI;mm/migrate: Convert buffer_migrate_page() to buffer_migrate_folio();Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDY3MjM1MTgyYTQxYzFiZDZiMzI4MDZhMTU1NmExZDI5OWI4NDIxMmI;"Use a folio throughout __buffer_migrate_folio(), add kernel-doc for
buffer_migrate_folio() and buffer_migrate_folio_norefs(), move their
declarations to buffer.h and switch all filesystems that have wired
them up.";Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDJiZTdmYTEwYzAyODAxOWY3YjJmZWUxMTIzODk4Nzc2MjU2N2Q0MWU;mm/migrate: Convert writeout() to take a folio;Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDJiZTdmYTEwYzAyODAxOWY3YjJmZWUxMTIzODk4Nzc2MjU2N2Q0MWU;Use a folio throughout this function.;Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKDhmYWE4ZWY1ZGQxMWFiZTExOWFkMGM4Y2NkMzlmMjA2NGNhN2VkMGU;mm/migrate: Convert fallback_migrate_page() to fallback_migrate_folio();Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDhmYWE4ZWY1ZGQxMWFiZTExOWFkMGM4Y2NkMzlmMjA2NGNhN2VkMGU;Use a folio throughout;Matthew Wilcox (Oracle);2022-06-06;1;1
C_kwDOACN7MtoAKDhmYWE4ZWY1ZGQxMWFiZTExOWFkMGM4Y2NkMzlmMjA2NGNhN2VkMGU;" migrate_page() will be converted to
migrate_folio() later.";Matthew Wilcox (Oracle);2022-06-06;0;1
C_kwDOACN7MtoAKDU0OTBkYTRmMDZkMTgyYmE5NDQ3MDY4NzUwMjllOThmZTdmNmI4MjE;fs: Add aops->migrate_folio;Matthew Wilcox (Oracle);2022-06-06;1;0
C_kwDOACN7MtoAKDU0OTBkYTRmMDZkMTgyYmE5NDQ3MDY4NzUwMjllOThmZTdmNmI4MjE;Provide a folio-based replacement for aops->migratepage;Matthew Wilcox (Oracle);2022-06-06;0;0
C_kwDOACN7MtoAKDU0OTBkYTRmMDZkMTgyYmE5NDQ3MDY4NzUwMjllOThmZTdmNmI4MjE;" Update the
documentation to document migrate_folio instead of migratepage.";Matthew Wilcox (Oracle);2022-06-06;1;1
C_kwDOACN7MtoAKDY4ZjI3MzZhODU4MzI0YzNlYzg1MmY2YzJjZGRkOWQxYzc3NzM1N2Q;mm: Convert all PageMovable users to movable_operations;Matthew Wilcox (Oracle);2022-06-07;1;0
C_kwDOACN7MtoAKDY4ZjI3MzZhODU4MzI0YzNlYzg1MmY2YzJjZGRkOWQxYzc3NzM1N2Q;"These drivers are rather uncomfortably hammered into the
address_space_operations hole";Matthew Wilcox (Oracle);2022-06-07;0;1
C_kwDOACN7MtoAKDY4ZjI3MzZhODU4MzI0YzNlYzg1MmY2YzJjZGRkOWQxYzc3NzM1N2Q;" They aren't filesystems and don't behave
like filesystems";Matthew Wilcox (Oracle);2022-06-07;1;1
C_kwDOACN7MtoAKDY4ZjI3MzZhODU4MzI0YzNlYzg1MmY2YzJjZGRkOWQxYzc3NzM1N2Q;" They just need their own movable_operations structure,
which we can point to directly from page->mapping.";Matthew Wilcox (Oracle);2022-06-07;1;1
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;mm: handling Non-LRU pages returned by vm_normal_pages;Alex Sierra;2022-07-15;1;0
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;"With DEVICE_COHERENT, we'll soon have vm_normal_pages() return
device-managed anonymous pages that are not LRU pages";Alex Sierra;2022-07-15;0;0
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;" Although they
behave like normal pages for purposes of mapping in CPU page, and for COW";Alex Sierra;2022-07-15;0;0
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;They do not support LRU lists, NUMA migration or THP;Alex Sierra;2022-07-15;1;1
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;"Callers to follow_page() currently don't expect ZONE_DEVICE pages,
however, with DEVICE_COHERENT we might now return ZONE_DEVICE";Alex Sierra;2022-07-15;0;1
C_kwDOACN7MtoAKDMyMThmODcxMmQ2YmJhMTgxMmVmZDVlMGQ2NmMxZTE1MTM0ZjJhOTE;" Check for
ZONE_DEVICE pages in applicable users of follow_page() as well.";Alex Sierra;2022-07-15;0;0
C_kwDOACN7MtoAKGFkMWFjNTk2ZThhOGM0YjA2NzE1ZGZiZDg5ODUzZWI3M2M5ODg2YjI;mm/migration: fix potential pte_unmap on an not mapped pte;Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKGFkMWFjNTk2ZThhOGM0YjA2NzE1ZGZiZDg5ODUzZWI3M2M5ODg2YjI;"__migration_entry_wait and migration_entry_wait_on_locked assume pte is
always mapped from caller";Miaohe Lin;2022-05-30;1;0
C_kwDOACN7MtoAKGFkMWFjNTk2ZThhOGM0YjA2NzE1ZGZiZDg5ODUzZWI3M2M5ODg2YjI;" But this is not the case when it's called from
migration_entry_wait_huge and follow_huge_pmd";Miaohe Lin;2022-05-30;0;0
C_kwDOACN7MtoAKGFkMWFjNTk2ZThhOGM0YjA2NzE1ZGZiZDg5ODUzZWI3M2M5ODg2YjI;" Add a hugetlbfs variant
that calls hugetlb_migration_entry_wait(ptep == NULL) to fix this issue.";Miaohe Lin;2022-05-30;0;0
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;mm/migration: return errno when isolate_huge_page failed;Miaohe Lin;2022-05-30;0;1
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;We might fail to isolate huge page due to e.g;Miaohe Lin;2022-05-30;0;1
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;" the page is under
migration which cleared HPageMigratable";Miaohe Lin;2022-05-30;0;1
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;" We should return errno in this
case rather than always return 1 which could confuse the user, i.e";Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;" the
caller might think all of the memory is migrated while the hugetlb page is
left behind";Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKDdjZTgyZjRjM2YzZWFkMTNhOWQ5NDk4NzY4ZTNiMWE3OTk3NWM0ZDg;" We make the prototype of isolate_huge_page consistent with
isolate_lru_page as suggested by Huang Ying and rename isolate_huge_page
to isolate_hugetlb as suggested by Muchun to improve the readability.";Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKDE2MDA4OGIzYjZkNzk0NmU0NTZjYWEzNzlkY2RmYzg3MDJjNjYyNzQ;mm/migration: remove unneeded lock page and PageMovable check;Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKDE2MDA4OGIzYjZkNzk0NmU0NTZjYWEzNzlkY2RmYzg3MDJjNjYyNzQ;"When non-lru movable page was freed from under us, __ClearPageMovable must
have been done";Miaohe Lin;2022-05-30;0;1
C_kwDOACN7MtoAKDE2MDA4OGIzYjZkNzk0NmU0NTZjYWEzNzlkY2RmYzg3MDJjNjYyNzQ;" So we can remove unneeded lock page and PageMovable check
here";Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKDE2MDA4OGIzYjZkNzk0NmU0NTZjYWEzNzlkY2RmYzg3MDJjNjYyNzQ;" Also free_pages_prepare() will clear PG_isolated for us, so we can
further remove ClearPageIsolated as suggested by David.";Miaohe Lin;2022-05-30;1;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;mm: Clear page->private when splitting or migrating a page;Matthew Wilcox (Oracle);2022-06-19;0;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;"In our efforts to remove uses of PG_private, we have found folios with
the private flag clear and folio->private not-NULL";Matthew Wilcox (Oracle);2022-06-19;0;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;" That is the root
cause behind 642d51fb0775 (""ceph: check folio PG_private bit instead
of folio->private"")";Matthew Wilcox (Oracle);2022-06-19;1;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;" It can also affect a few other filesystems that
haven't yet reported a problem";Matthew Wilcox (Oracle);2022-06-19;0;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;"compaction_alloc() can return a page with uninitialised page->private,
and rather than checking all the callers of migrate_pages(), just zero
page->private after calling get_new_page()";Matthew Wilcox (Oracle);2022-06-19;1;1
C_kwDOACN7MtoAKGI2NTNkYjc3MzUwYzczMDdhNTEzYjgxODU2ZmU1M2U5NGNmNDI0NDY;" Similarly, the tail pages
from split_huge_page() may also have an uninitialised page->private.";Matthew Wilcox (Oracle);2022-06-19;0;1
C_kwDOACN7MtoAKGU3ZTNmZmViMjc0ZjFmZjViYzY4YmI5MTM1MTI4ZTFiYTE0YTdkNTM;mm/migrate: convert move_to_new_page() into move_to_new_folio();Matthew Wilcox (Oracle);2022-05-13;1;0
C_kwDOACN7MtoAKGU3ZTNmZmViMjc0ZjFmZjViYzY4YmI5MTM1MTI4ZTFiYTE0YTdkNTM;Pass in the folios that we already have in each caller;Matthew Wilcox (Oracle);2022-05-13;1;0
C_kwDOACN7MtoAKGU3ZTNmZmViMjc0ZjFmZjViYzY4YmI5MTM1MTI4ZTFiYTE0YTdkNTM;" Saves a
lot of calls to compound_head().";Matthew Wilcox (Oracle);2022-05-13;0;0
C_kwDOACN7MtoAKDcxN2FlYWI0Mjk0M2VmYTdjZmE4NzZiM2I2ODdjNmZmMzZlYWU4Njc;mm: convert sysfs input to bool using kstrtobool();Jagdish Gediya;2022-05-13;1;0
C_kwDOACN7MtoAKDcxN2FlYWI0Mjk0M2VmYTdjZmE4NzZiM2I2ODdjNmZmMzZlYWU4Njc;Sysfs input conversion to corrosponding bool value e.g;Jagdish Gediya;2022-05-13;0;0
C_kwDOACN7MtoAKDcxN2FlYWI0Mjk0M2VmYTdjZmE4NzZiM2I2ODdjNmZmMzZlYWU4Njc;" ""false"" or ""0"" to
false, ""true"" or ""1"" to true are currently handled through strncmp at
multiple places";Jagdish Gediya;2022-05-13;0;1
C_kwDOACN7MtoAKDcxN2FlYWI0Mjk0M2VmYTdjZmE4NzZiM2I2ODdjNmZmMzZlYWU4Njc; Use kstrtobool() to convert sysfs input to bool value.;Jagdish Gediya;2022-05-13;1;0
C_kwDOACN7MtoAKDY4MTg5ZmVmODhjN2QwMmViOTJlMDM4YmUzZDY0MjhlYmQwZDI5NDU;fs: Change try_to_free_buffers() to take a folio;Matthew Wilcox (Oracle);2022-05-01;1;0
C_kwDOACN7MtoAKDY4MTg5ZmVmODhjN2QwMmViOTJlMDM4YmUzZDY0MjhlYmQwZDI5NDU;"All but two of the callers already have a folio; pass a folio into
try_to_free_buffers()";Matthew Wilcox (Oracle);2022-05-01;1;0
C_kwDOACN7MtoAKDY4MTg5ZmVmODhjN2QwMmViOTJlMDM4YmUzZDY0MjhlYmQwZDI5NDU;" This removes the last user of cancel_dirty_page()
so remove that wrapper function too.";Matthew Wilcox (Oracle);2022-05-01;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;mm: remember exclusively mapped anonymous pages with PG_anon_exclusive;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Let's mark exclusively mapped anonymous pages with PG_anon_exclusive as
exclusive, and use that information to make GUP pins reliable and stay
consistent with the page mapped into the page table even if the page table
entry gets write-protected";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"With that information at hand, we can extend our COW logic to always reuse
anonymous pages that are exclusive";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" For anonymous pages that might be
shared, the existing logic applies";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"As already documented, PG_anon_exclusive is usually only expressive in
combination with a page table entry";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE; Especially PTE vs;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" PMD-mapped
anonymous pages require more thought, some examples: due to mremap() we
can easily have a single compound page PTE-mapped into multiple page
tables exclusively in a single process -- multiple page table locks apply";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Further, due to MADV_WIPEONFORK we might not necessarily write-protect
all PTEs, and only some subpages might be pinned";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" Long story short: once
PTE-mapped, we have to track information about exclusivity per sub-page,
but until then, we can just track it for the compound page in the head
page and not having to update a whole bunch of subpages all of the time
for a simple PMD mapping of a THP";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"For simplicity, this commit mostly talks about ""anonymous pages"", while
it's for THP actually ""the part of an anonymous folio referenced via a
page table entry""";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"To not spill PG_anon_exclusive code all over the mm code-base, we let the
anon rmap code to handle all PG_anon_exclusive logic it can easily handle";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"If a writable, present page table entry points at an anonymous (sub)page,
that (sub)page must be PG_anon_exclusive";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" If GUP wants to take a reliably
pin (FOLL_PIN) on an anonymous page references via a present page table
entry, it must only pin if PG_anon_exclusive is set for the mapped
(sub)page";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"This commit doesn't adjust GUP, so this is only implicitly handled for
FOLL_WRITE, follow-up commits will teach GUP to also respect it for
FOLL_PIN without FOLL_WRITE, to make all GUP pins of anonymous pages fully
reliable";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Whenever an anonymous page is to be shared (fork(), KSM), or when
temporarily unmapping an anonymous page (swap, migration), the relevant
PG_anon_exclusive bit has to be cleared to mark the anonymous page
possibly shared";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE; Clearing will fail if there are GUP pins on the page;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;  share it;David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" fork() protects against concurrent GUP using the PT lock and
  the src_mm->write_protect_seq";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;  will fail, For migration this means, migration will fail early;David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" All
  three cases protect against concurrent GUP using the PT lock and a
  proper clear/invalidate+flush of the relevant page table entry";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"This fixes memory corruptions reported for FOLL_PIN | FOLL_WRITE, when a
pinned page gets mapped R/O and the successive write fault ends up
replacing the page instead of reusing it";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" It improves the situation for
O_DIRECT/vmsplice/..";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" that still use FOLL_GET instead of FOLL_PIN, if
fork() is *not* involved, however swapout and fork() are still
problematic";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" Properly using FOLL_PIN instead of FOLL_GET for these GUP
users will fix the issue for them";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Details about basic handling
I.1";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Fresh anonymous pages
page_add_new_anon_rmap() and hugepage_add_new_anon_rmap() will mark the
given page exclusive via __page_set_anon_rmap(exclusive=1)";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" As that is
the mechanism fresh anonymous pages come into life (besides migration code
where we copy the page->mapping), all fresh anonymous pages will start out
as exclusive";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.2;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"COW reuse handling of anonymous pages
When a COW handler stumbles over a (sub)page that's marked exclusive, it
simply reuses it";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" Otherwise, the handler tries harder under page lock to
detect if the (sub)page is exclusive and can be reused";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" If exclusive,
page_move_anon_rmap() will mark the given (sub)page exclusive";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Note that hugetlb code does not yet check for PageAnonExclusive(), as it
still uses the old COW logic that is prone to the COW security issue
because hugetlb code cannot really tolerate unnecessary/wrong COW as huge
pages are a scarce resource";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.3;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Migration handling
try_to_migrate() has to try marking an exclusive anonymous page shared via
page_try_share_anon_rmap()";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" If it fails because there are GUP pins on the
page, unmap fails";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" migrate_vma_collect_pmd() and
__split_huge_pmd_locked() are handled similarly";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;Writable migration entries implicitly point at shared anonymous pages;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"
For readable migration entries that information is stored via a new
""readable-exclusive"" migration entry, specific to anonymous pages";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"When restoring a migration entry in remove_migration_pte(), information
about exlusivity is detected via the migration entry type, and
RMAP_EXCLUSIVE is set accordingly for
page_add_anon_rmap()/hugepage_add_anon_rmap() to restore that information";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.4;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Swapout handling
try_to_unmap() has to try marking the mapped page possibly shared via
page_try_share_anon_rmap()";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" If it fails because there are GUP pins on the
page, unmap fails";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE; For now, information about exclusivity is lost;David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" In
the future, we might want to remember that information in the swap entry
in some cases, however, it requires more thought, care, and a way to store
that information in swap entries";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.5;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Swapin handling
do_swap_page() will never stumble over exclusive anonymous pages in the
swap cache, as try_to_migrate() prohibits that";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" do_swap_page() always has
to detect manually if an anonymous page is exclusive and has to set
RMAP_EXCLUSIVE for page_add_anon_rmap() accordingly";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.6;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"THP handling
__split_huge_pmd_locked() has to move the information about exclusivity
from the PMD to the PTEs";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"a) In case we have a readable-exclusive PMD migration entry, simply
   insert readable-exclusive PTE migration entries";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"b) In case we have a present PMD entry and we don't want to freeze
   (""convert to migration entries""), simply forward PG_anon_exclusive to
   all sub-pages, no need to temporarily clear the bit";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"c) In case we have a present PMD entry and want to freeze, handle it
   similar to try_to_migrate(): try marking the page shared first";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" In
   case we fail, we ignore the ""freeze"" instruction and simply split
   ordinarily";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" try_to_migrate() will properly fail because the THP is
   still mapped via PTEs";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"When splitting a compound anonymous folio (THP), the information about
exclusivity is implicitly handled via the migration entries: no need to
replicate PG_anon_exclusive manually";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.7;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" fork() handling fork() handling is relatively easy, because
PG_anon_exclusive is only expressive for some page table entry types";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"a) Present anonymous pages
page_try_dup_anon_rmap() will mark the given subpage shared -- which will
fail if the page is pinned";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" If it failed, we have to copy (or PTE-map a
PMD to handle it on the PTE level)";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Note that device exclusive entries are just a pointer at a PageAnon()
page";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;" fork() will first convert a device exclusive entry to a present
page table and handle it just like present anonymous pages";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"b) Device private entry
Device private entries point at PageAnon() pages that cannot be mapped
directly and, therefore, cannot get pinned";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"page_try_dup_anon_rmap() will mark the given subpage shared, which cannot
fail because they cannot get pinned";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"c) HW poison entries
PG_anon_exclusive will remain untouched and is stale -- the page table
entry is just a placeholder after all";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"d) Migration entries
Writable and readable-exclusive entries are converted to readable entries";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;possibly shared;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;I.8;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"mprotect() handling
mprotect() only has to properly handle the new readable-exclusive
migration entry";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"When write-protecting a migration entry that points at an anonymous page,
remember the information about exclusivity via the ""readable-exclusive""
migration entry type";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;II;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Migration and GUP-fast
Whenever replacing a present page table entry that maps an exclusive
anonymous page by a migration entry, we have to mark the page possibly
shared and synchronize against GUP-fast by a proper clear/invalidate+flush
to make the following scenario impossible";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;1;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"try_to_migrate() places a migration entry after checking for GUP pins
   and marks the page possibly shared";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;2;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"GUP-fast pins the page due to lack of synchronization
3";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"fork() converts the ""writable/readable-exclusive"" migration entry into a
   readable migration entry
4";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Migration fails due to the GUP pin (failing to freeze the refcount)
5";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;Migration entries are restored;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"PG_anon_exclusive is lost
-> We have a pinned page that is not marked exclusive anymore";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Note that we move information about exclusivity from the page to the
migration entry as it otherwise highly overcomplicates fork() and
PTE-mapping a THP";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;III;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"Swapout and GUP-fast
Whenever replacing a present page table entry that maps an exclusive
anonymous page by a swap entry, we have to mark the page possibly shared
and synchronize against GUP-fast by a proper clear/invalidate+flush to
make the following scenario impossible";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;1;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"try_to_unmap() places a swap entry after checking for GUP pins and
   clears exclusivity information on the page";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;2;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;GUP-fast pins the page due to lack of synchronization;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;-> We have a pinned page that is not marked exclusive anymore;David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE;"If we'd ever store information about exclusivity in the swap entry,
similar to migration handling, the same considerations as in II would
apply";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKDZjMjg3NjA1ZmQ1NjQ2NmU2NDU2OTNlZmYzYWU3YzA4ZmJhNTZlMGE; This is future work.;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDI4YzUyMDlkZmQ1Zjg2ZjQzOThjZTAxYmZhYzg1MDhiMmM0ZDQwNTA;mm/rmap: pass rmap flags to hugepage_add_anon_rmap();David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDI4YzUyMDlkZmQ1Zjg2ZjQzOThjZTAxYmZhYzg1MDhiMmM0ZDQwNTA;"Let's prepare for passing RMAP_EXCLUSIVE, similarly as we do for
page_add_anon_rmap() now";David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKDI4YzUyMDlkZmQ1Zjg2ZjQzOThjZTAxYmZhYzg1MDhiMmM0ZDQwNTA;" RMAP_COMPOUND is implicit for hugetlb pages and
ignored.";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKGYxZTJkYjEyZTQ1YmFhYTJkMzY2Zjg3Yzg4NTk2ODA5NmMyZmY1YWE;mm/rmap: remove do_page_add_anon_rmap();David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKGYxZTJkYjEyZTQ1YmFhYTJkMzY2Zjg3Yzg4NTk2ODA5NmMyZmY1YWE;..;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKGYxZTJkYjEyZTQ1YmFhYTJkMzY2Zjg3Yzg4NTk2ODA5NmMyZmY1YWE;and instead convert page_add_anon_rmap() to accept flags;David Hildenbrand;2022-05-10;1;0
C_kwDOACN7MtoAKGYxZTJkYjEyZTQ1YmFhYTJkMzY2Zjg3Yzg4NTk2ODA5NmMyZmY1YWE;"Passing flags instead of bools is usually nicer either way, and we want to
more often also pass RMAP_EXCLUSIVE in follow up patches when detecting
that an anonymous page is exclusive: for example, when restoring an
anonymous page from a writable migration entry";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKGYxZTJkYjEyZTQ1YmFhYTJkMzY2Zjg3Yzg4NTk2ODA5NmMyZmY1YWE;"This is a preparation for marking an anonymous page inside
page_add_anon_rmap() as exclusive when RMAP_EXCLUSIVE is passed.";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;mm/rmap: split page_dup_rmap() into page_dup_file_rmap() and page_try_dup_anon_rmap();David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;..;David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;" and move the special check for pinned pages into
page_try_dup_anon_rmap() to prepare for tracking exclusive anonymous pages
via a new pageflag, clearing it only after making sure that there are no
GUP pins on the anonymous page";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;"We really only care about pins on anonymous pages, because they are prone
to getting replaced in the COW handler once mapped R/O";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;" For !anon pages
in cow-mappings (!VM_SHARED && VM_MAYWRITE) we shouldn't really care about
that, at least not that I could come up with an example";David Hildenbrand;2022-05-10;0;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;"Let's drop the is_cow_mapping() check from page_needs_cow_for_dma(), as we
know we're dealing with anonymous pages";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;" Also, drop the handling of
pinned pages from copy_huge_pud() and add a comment if ever supporting
anonymous pages on the PUD level";David Hildenbrand;2022-05-10;1;1
C_kwDOACN7MtoAKGZiM2Q4MjRkMWE0NmM1YmIwNTg0ZWE4OGYzMmRjMjQ5NTU0NGFlYmY;"This is a preparation for tracking exclusivity of anonymous pages in the
rmap code, and disallowing marking a page shared (-> failing to duplicate)
if there are GUP pins on a page.";David Hildenbrand;2022-05-10;0;0
C_kwDOACN7MtoAKDdkNmUyZDk2Mzg0NTU2YTRmMzA1NDc4MDNiZTFmNjA2ZWI4MDVhNjI;mm: untangle config dependencies for demote-on-reclaim;Oscar Salvador;2022-04-29;1;0
C_kwDOACN7MtoAKDdkNmUyZDk2Mzg0NTU2YTRmMzA1NDc4MDNiZTFmNjA2ZWI4MDVhNjI;"At the time demote-on-reclaim was introduced, it was tied to
CONFIG_HOTPLUG_CPU + CONFIG_MIGRATE, but that is not really accurate";Oscar Salvador;2022-04-29;0;1
C_kwDOACN7MtoAKDdkNmUyZDk2Mzg0NTU2YTRmMzA1NDc4MDNiZTFmNjA2ZWI4MDVhNjI;"The only two things we need to depend on are CONFIG_NUMA + CONFIG_MIGRATE,
so clean this up";Oscar Salvador;2022-04-29;1;1
C_kwDOACN7MtoAKDdkNmUyZDk2Mzg0NTU2YTRmMzA1NDc4MDNiZTFmNjA2ZWI4MDVhNjI;" Furthermore, we only register the hotplug memory
notifier when the system has CONFIG_MEMORY_HOTPLUG.";Oscar Salvador;2022-04-29;1;0
C_kwDOACN7MtoAKDljNDJmZTRlMzBhOWI5MzRiMWRlNjZjMmVkY2ExOTY1NjMyMjEzOTI;mm: migrate: simplify the refcount validation when migrating hugetlb mapping;Baolin Wang;2022-04-29;1;1
C_kwDOACN7MtoAKDljNDJmZTRlMzBhOWI5MzRiMWRlNjZjMmVkY2ExOTY1NjMyMjEzOTI;"There is no need to validate the hugetlb page's refcount before trying to
freeze the hugetlb page's expected refcount, instead we can just rely on
the page_ref_freeze() to simplify the validation";Baolin Wang;2022-04-29;1;1
C_kwDOACN7MtoAKDljNDJmZTRlMzBhOWI5MzRiMWRlNjZjMmVkY2ExOTY1NjMyMjEzOTI;"Moreover we are always under the page lock when migrating the hugetlb page
mapping, which means nowhere else can remove it from the page cache, so we
can remove the xas_load() validation under the i_pages lock.";Baolin Wang;2022-04-29;0;1
C_kwDOACN7MtoAKDRjZDYxNDg0MWMwNjMzOGEwODc3NjllZTNjZmE5NjcxODc4NGQxZjU;mm/migration: fix possible do_pages_stat_array racing with memory offline;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDRjZDYxNDg0MWMwNjMzOGEwODc3NjllZTNjZmE5NjcxODc4NGQxZjU;"When follow_page peeks a page, the page could be migrated and then be
offlined while it's still being used by the do_pages_stat_array()";Miaohe Lin;2022-04-29;0;0
C_kwDOACN7MtoAKDRjZDYxNDg0MWMwNjMzOGEwODc3NjllZTNjZmE5NjcxODc4NGQxZjU;" Use
FOLL_GET to hold the page refcnt to fix this potential race.";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDNmMjZjODhiZDY2Y2Q4YWIxNzMxNzYzYzY4ZGY3ZmUyM2E3NjcxYzA;mm/migration: fix potential invalid node access for reclaim-based migration;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDNmMjZjODhiZDY2Y2Q4YWIxNzMxNzYzYzY4ZGY3ZmUyM2E3NjcxYzA;"If we failed to setup hotplug state callbacks for mm/demotion:online in
some corner cases, node_demotion will be left uninitialized";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDNmMjZjODhiZDY2Y2Q4YWIxNzMxNzYzYzY4ZGY3ZmUyM2E3NjcxYzA;" Invalid node
might be returned from the next_demotion_node() when doing reclaim-based
migration";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDNmMjZjODhiZDY2Y2Q4YWIxNzMxNzYzYzY4ZGY3ZmUyM2E3NjcxYzA; Use kcalloc to allocate node_demotion to fix the issue.;Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDY5YTA0MWZmNTA1ODA2Yzk1YjI0YjhkNWNhYjQzZTY2YWFjZDkxZTY;mm/migration: fix potential page refcounts leak in migrate_pages;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDY5YTA0MWZmNTA1ODA2Yzk1YjI0YjhkNWNhYjQzZTY2YWFjZDkxZTY;"In -ENOMEM case, there might be some subpages of fail-to-migrate THPs left
in thp_split_pages list";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDY5YTA0MWZmNTA1ODA2Yzk1YjI0YjhkNWNhYjQzZTY2YWFjZDkxZTY;" We should move them back to migration list so
that they could be put back to the right list by the caller otherwise the
page refcnt will be leaked here";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDY5YTA0MWZmNTA1ODA2Yzk1YjI0YjhkNWNhYjQzZTY2YWFjZDkxZTY;" Also adjust nr_failed and nr_thp_failed
accordingly to make vm events account more accurate.";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKGY0MzA4OTNiMDFlNzhlMGIyZTIxZjliZDE2MzNhNzc4YzA2Mzk5M2U;mm/migration: remove some duplicated codes in migrate_pages;Miaohe Lin;2022-04-29;1;0
C_kwDOACN7MtoAKGY0MzA4OTNiMDFlNzhlMGIyZTIxZjliZDE2MzNhNzc4YzA2Mzk5M2U;Remove the duplicated codes in migrate_pages to simplify the code;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKGY0MzA4OTNiMDFlNzhlMGIyZTIxZjliZDE2MzNhNzc4YzA2Mzk5M2U;" Minor
readability improvement";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKGY0MzA4OTNiMDFlNzhlMGIyZTIxZjliZDE2MzNhNzc4YzA2Mzk5M2U; No functional change intended.;Miaohe Lin;2022-04-29;1;0
C_kwDOACN7MtoAKDkxOTI1YWI4Y2MyYTA1YWIwZTUyNDgzMDI0N2UxZDY2YmE0ZTRlMTk;mm/migration: avoid unneeded nodemask_t initialization;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDkxOTI1YWI4Y2MyYTA1YWIwZTUyNDgzMDI0N2UxZDY2YmE0ZTRlMTk;"Avoid unneeded next_pass and this_pass initialization as they're always
set before using to save possible cpu cycles when there are plenty of
nodes in the system.";Miaohe Lin;2022-04-29;1;0
C_kwDOACN7MtoAKDNlZWZiODI2YzVhNjI3MDg0ZWNjZWM3ODhmMDIzNmEwNzA5ODhkYWU;mm/migration: use helper macro min in do_pages_stat;Miaohe Lin;2022-04-29;1;0
C_kwDOACN7MtoAKDNlZWZiODI2YzVhNjI3MDg0ZWNjZWM3ODhmMDIzNmEwNzA5ODhkYWU;"We could use helper macro min to help set the chunk_nr to simplify the
code.";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKGNiMWMzN2IxYzY1ZDllNTQ1MGFmMmVhNmVjODkxNmM1Y2QyM2EyZTc;mm/migration: use helper function vma_lookup() in add_page_for_migration;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKGNiMWMzN2IxYzY1ZDllNTQ1MGFmMmVhNmVjODkxNmM1Y2QyM2EyZTc;"We could use helper function vma_lookup() to lookup the needed vma to
simplify the code.";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKGI3NTQ1NGUxMDEwMWFmM2ExMWIyNGNhN2UxNDkxN2I2Yzg4NzQ2ODg;mm/migration: remove unneeded local variable page_lru;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKGI3NTQ1NGUxMDEwMWFmM2ExMWIyNGNhN2UxNDkxN2I2Yzg4NzQ2ODg;"We can use page_is_file_lru() directly to help account the isolated pages
to simplify the code a bit.";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;mm/migration: remove unneeded local variable mapping_locked;Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;"Patch series ""A few cleanup and fixup patches for migration"", v2";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;"This series contains a few patches to remove unneeded variables, jump
label and use helper to simplify the code";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;" Also we fix some bugs such as
page refcounts leak , invalid node access and so on";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;" More details can be
found in the respective changelogs";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;This patch (of 11);Miaohe Lin;2022-04-29;1;0
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;When mapping_locked is true, TTU_RMAP_LOCKED is always set to ttu;Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;" We can
check ttu instead so mapping_locked can be removed";Miaohe Lin;2022-04-29;1;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ;" And ttu is either 0
or TTU_RMAP_LOCKED now";Miaohe Lin;2022-04-29;0;1
C_kwDOACN7MtoAKDUyMDI5NzhiNDg3ODY3MDNhNmNlYzk0NTk2MDY3YjNmYWZkMWY3MzQ; Change '|=' to '=' to reflect this.;Miaohe Lin;2022-04-29;0;0
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;mm/vmscan: make sure wakeup_kswapd with managed zone;Wei Yang;2022-04-29;1;0
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;wakeup_kswapd() only wake up kswapd when the zone is managed;Wei Yang;2022-04-29;1;0
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;For two callers of wakeup_kswapd(), they are node perspective;Wei Yang;2022-04-29;0;1
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;If we picked up a !managed zone, this is not we expected;Wei Yang;2022-04-29;1;1
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;This patch makes sure we pick up a managed zone for wakeup_kswapd();Wei Yang;2022-04-29;1;1
C_kwDOACN7MtoAKGJjNTMwMDhlZWE1NTMzMGY0ODVjOTU2MzM4ZDNjNTlmOTZjNzBjMDg;" And
it also use managed_zone in migrate_balanced_pgdat() to get the proper
zone.";Wei Yang;2022-04-29;1;0
C_kwDOACN7MtoAKGMxODVlNDk0YWUwY2ViMTI2ZDg5YjhlMzQxM2VkMGExMTMyZTA1ZDM;mm/migrate: Use a folio in migrate_misplaced_transhuge_page();Matthew Wilcox (Oracle);2021-07-06;1;0
C_kwDOACN7MtoAKGMxODVlNDk0YWUwY2ViMTI2ZDg5YjhlMzQxM2VkMGExMTMyZTA1ZDM;Unify alloc_misplaced_dst_page() and alloc_misplaced_dst_page_thp();Matthew Wilcox (Oracle);2021-07-06;0;1
C_kwDOACN7MtoAKGMxODVlNDk0YWUwY2ViMTI2ZDg5YjhlMzQxM2VkMGExMTMyZTA1ZDM;Removes an assumption that compound pages are HPAGE_PMD_ORDER.;Matthew Wilcox (Oracle);2021-07-06;1;0
C_kwDOACN7MtoAKGZmZTA2Nzg2YjU0MDM5ZWRjZWNiNTFhNTQwNjFlZThkODEwMzZhMTk;mm/migrate: Use a folio in alloc_migration_target();Matthew Wilcox (Oracle);2022-04-04;1;1
C_kwDOACN7MtoAKGZmZTA2Nzg2YjU0MDM5ZWRjZWNiNTFhNTQwNjFlZThkODEwMzZhMTk;"This removes an assumption that a large folio is HPAGE_PMD_ORDER
as well as letting us remove the call to prep_transhuge_page()
and a few hidden calls to compound_head().";Matthew Wilcox (Oracle);2022-04-04;1;1
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE;mm/munlock: protect the per-CPU pagevec by a local_lock_t;Sebastian Andrzej Siewior;2022-04-01;0;1
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE;"The access to mlock_pvec is protected by disabling preemption via
get_cpu_var() or implicit by having preemption disabled by the caller
(in mlock_page_drain() case)";Sebastian Andrzej Siewior;2022-04-01;0;1
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE;" This breaks on PREEMPT_RT since
folio_lruvec_lock_irq() acquires a sleeping lock in this section";Sebastian Andrzej Siewior;2022-04-01;0;1
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE;"Create struct mlock_pvec which consits of the local_lock_t and the
pagevec";Sebastian Andrzej Siewior;2022-04-01;1;0
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE; Acquire the local_lock() before accessing the per-CPU pagevec;Sebastian Andrzej Siewior;2022-04-01;0;1
C_kwDOACN7MtoAKGFkYjExZTc4YzVkYzVlMjY3NzRhY2IwNWY5ODNkYTM2NDQ3Zjc5MTE;"Replace mlock_page_drain() with a _local() version which is invoked on
the local CPU and acquires the local_lock_t and a _remote() version
which uses the pagevec from a remote CPU which offline.";Sebastian Andrzej Siewior;2022-04-01;0;1
C_kwDOACN7MtoAKDRjYzc5YjMzMDNmMjI0YTkyMGYzYWZmMjFmM2QyMzE3NDlkNzMzODQ;mm/migration: add trace events for base page and HugeTLB migrations;Anshuman Khandual;2022-03-25;1;0
C_kwDOACN7MtoAKDRjYzc5YjMzMDNmMjI0YTkyMGYzYWZmMjFmM2QyMzE3NDlkNzMzODQ;This adds two trace events for base page and HugeTLB page migrations;Anshuman Khandual;2022-03-25;1;1
C_kwDOACN7MtoAKDRjYzc5YjMzMDNmMjI0YTkyMGYzYWZmMjFmM2QyMzE3NDlkNzMzODQ;"These events, closely follow the implementation details like setting and
removing of PTE migration entries, which are essential operations for
migration";Anshuman Khandual;2022-03-25;1;0
C_kwDOACN7MtoAKDRjYzc5YjMzMDNmMjI0YTkyMGYzYWZmMjFmM2QyMzE3NDlkNzMzODQ;" The new CREATE_TRACE_POINTS in <mm/rmap.c> covers both
<events/migration.h> and <events/tlb.h> based trace events";Anshuman Khandual;2022-03-25;1;0
C_kwDOACN7MtoAKDRjYzc5YjMzMDNmMjI0YTkyMGYzYWZmMjFmM2QyMzE3NDlkNzMzODQ;" Hence drop
redundant CREATE_TRACE_POINTS from other places which could have otherwise
conflicted during build.";Anshuman Khandual;2022-03-25;1;0
C_kwDOACN7MtoAKDczNGMxNTcwMGNkZjkwNjJhZTk4ZDhiMTMxYzZmZTg3M2RmYWQyNmQ;mm: only re-generate demotion targets when a numa node changes its N_CPU state;Oscar Salvador;2022-03-22;0;0
C_kwDOACN7MtoAKDczNGMxNTcwMGNkZjkwNjJhZTk4ZDhiMTMxYzZmZTg3M2RmYWQyNmQ;"Abhishek reported that after patch [1], hotplug operations are taking
roughly double the expected time";Oscar Salvador;2022-03-22;1;1
C_kwDOACN7MtoAKDczNGMxNTcwMGNkZjkwNjJhZTk4ZDhiMTMxYzZmZTg3M2RmYWQyNmQ;" [2]
The reason behind is that the CPU callbacks that
migrate_on_reclaim_init() sets always call set_migration_target_nodes()
whenever a CPU is brought up/down";Oscar Salvador;2022-03-22;0;1
C_kwDOACN7MtoAKDczNGMxNTcwMGNkZjkwNjJhZTk4ZDhiMTMxYzZmZTg3M2RmYWQyNmQ;"But we only care about numa nodes going from having cpus to become
cpuless, and vice versa, as that influences the demotion_target order";Oscar Salvador;2022-03-22;0;0
C_kwDOACN7MtoAKDczNGMxNTcwMGNkZjkwNjJhZTk4ZDhiMTMxYzZmZTg3M2RmYWQyNmQ;"We do already have two CPU callbacks (vmstat_cpu_online() and
vmstat_cpu_dead()) that check exactly that, so get rid of the CPU
callbacks in migrate_on_reclaim_init() and only call
numa node change its N_CPU state.";Oscar Salvador;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;NUMA balancing: optimize page placement for memory tiering system;Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"With the advent of various new memory types, some machines will have
multiple types of memory, e.g";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg; DRAM and PMEM (persistent memory);Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" The
memory subsystem of these machines can be called memory tiering system,
because the performance of the different types of memory are usually
different";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"In such system, because of the memory accessing pattern changing etc,
some pages in the slow memory may become hot globally";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So in this
patch, the NUMA balancing mechanism is enhanced to optimize the page
placement among the different memory types according to hot/cold
dynamically";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"In a typical memory tiering system, there are CPUs, fast memory and slow
memory in each physical NUMA node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" The CPUs and the fast memory will be
put in one logical node (called fast memory node), while the slow memory
will be put in another (faked) logical node (called slow memory node)";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"That is, the fast memory is regarded as local while the slow memory is
regarded as remote";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So it's possible for the recently accessed pages in
the slow memory node to be promoted to the fast memory node via the
existing NUMA balancing mechanism";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"The original NUMA balancing mechanism will stop to migrate pages if the
free memory of the target node becomes below the high watermark";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" This
is a reasonable policy if there's only one memory type";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" But this makes
the original NUMA balancing mechanism almost do not work to optimize
page placement among different memory types";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg; Details are as follows;Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"It's the common cases that the working-set size of the workload is
larger than the size of the fast memory nodes";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" Otherwise, it's
unnecessary to use the slow memory at all";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So, there are almost always
no enough free pages in the fast memory nodes, so that the globally hot
pages in the slow memory node cannot be promoted to the fast memory
node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" To solve the issue, we have 2 choices as follows,
a";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"Ignore the free pages watermark checking when promoting hot pages
   from the slow memory node to the fast memory node";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" This will
   create some memory pressure in the fast memory node, thus trigger
   the memory reclaiming";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So that, the cold pages in the fast memory
   node will be demoted to the slow memory node";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;b;Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"Define a new watermark called wmark_promo which is higher than
   wmark_high, and have kswapd reclaiming pages until free pages reach
   such watermark";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" The scenario is as follows: when we want to promote
   hot-pages from a slow memory to a fast memory, but fast memory's free
   pages would go lower than high watermark with such promotion, we wake
   up kswapd with wmark_promo watermark in order to demote cold pages and
   free us up some space";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So, next time we want to promote hot-pages we
   might have a chance of doing so";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"The choice ""a"" may create high memory pressure in the fast memory node";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"If the memory pressure of the workload is high, the memory pressure
may become so high that the memory allocation latency of the workload
is influenced, e.g";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg; the direct reclaiming may be triggered;Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"The choice ""b"" works much better at this aspect";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" If the memory
pressure of the workload is high, the hot pages promotion will stop
earlier because its allocation watermark is higher than that of the
normal memory allocation";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So in this patch, choice ""b"" is implemented";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;A new zone watermark (WMARK_PROMO) is added;Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" Which is larger than the
high watermark and can be controlled via watermark_scale_factor";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;"In addition to the original page placement optimization among sockets,
the NUMA balancing mechanism is extended to be used to optimize page
placement according to hot/cold among different memory types";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" So the
sysctl user space interface (numa_balancing) is extended in a backward
compatible way as follow, so that the users can enable/disable these
functionality individually";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;The sysctl is converted from a Boolean value to a bits field;Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" The
definition of the flags is,
- 0: NUMA_BALANCING_DISABLED
- 1: NUMA_BALANCING_NORMAL
- 2: NUMA_BALANCING_MEMORY_TIERING
We have tested the patch with the pmbench memory accessing benchmark
with the 80:20 read/write ratio and the Gauss access address
distribution on a 2 socket Intel server with Optane DC Persistent
Memory Model";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;" The test results shows that the pmbench score can
improve up to 95.9%";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGM1NzRiYmU5MTcwMzZjODk2OGI5ODRjODJjN2IxMzE5NGZlNWNlOTg;Thanks Andrew Morton to help fix the document format error.;Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;NUMA Balancing: add page promotion counter;Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"Patch series ""NUMA balancing: optimize memory placement for memory tiering system"", v13
With the advent of various new memory types, some machines will have
multiple types of memory, e.g";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ; DRAM and PMEM (persistent memory);Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" The
memory subsystem of these machines can be called memory tiering system,
because the performance of the different types of memory are different";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"After commit c221c0b0308f (""device-dax: ""Hotplug"" persistent memory for
use like normal RAM""), the PMEM could be used as the cost-effective
volatile memory in separate NUMA nodes";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" In a typical memory tiering
system, there are CPUs, DRAM and PMEM in each physical NUMA node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" The
CPUs and the DRAM will be put in one logical node, while the PMEM will
be put in another (faked) logical node";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"To optimize the system overall performance, the hot pages should be
placed in DRAM node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" To do that, we need to identify the hot pages in
the PMEM node and migrate them to DRAM node via NUMA migration";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"In the original NUMA balancing, there are already a set of existing
mechanisms to identify the pages recently accessed by the CPUs in a node
and migrate the pages to the node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" So we can reuse these mechanisms to
build the mechanisms to optimize the page placement in the memory
tiering system";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ; This is implemented in this patchset;Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;At the other hand, the cold pages should be placed in PMEM node;Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" So, we
also need to identify the cold pages in the DRAM node and migrate them
to PMEM node";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"In commit 26aa2d199d6f (""mm/migrate: demote pages during reclaim""), a
mechanism to demote the cold DRAM pages to PMEM node under memory
pressure is implemented";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" Based on that, the cold DRAM pages can be
demoted to PMEM node proactively to free some memory space on DRAM node
to accommodate the promoted hot PMEM pages";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" This is implemented in this
patchset too";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"We have tested the solution with the pmbench memory accessing benchmark
with the 80:20 read/write ratio and the Gauss access address
distribution on a 2 socket Intel server with Optane DC Persistent Memory
Model";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" The test results shows that the pmbench score can improve up to
This patch (of 3)";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;In a system with multiple memory types, e.g;Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" DRAM and PMEM, the CPU
and DRAM in one socket will be put in one NUMA node as before, while
the PMEM will be put in another NUMA node as described in the
description of the commit c221c0b0308f (""device-dax: ""Hotplug""
persistent memory for use like normal RAM"")";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" So, the NUMA balancing
mechanism will identify all PMEM accesses as remote access and try to
promote the PMEM pages to DRAM";Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;"To distinguish the number of the inter-type promoted pages from that of
the inter-socket migrated pages";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ; A new vmstat count is added;Huang Ying;2022-03-22;1;0
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" The
counter is per-node (count in the target node)";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGUzOWJiNmJlOWYyYjM5YTZkYmFlZmY0ODQzNjFkZTc2MDIxYjE3NWQ;" So this can be used to
identify promotion imbalance among the NUMA nodes.";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKDM1NmVhMzg2NTY4NzkyNmU1ZGE3NTc5ZDFmMzM1MWQzZjBhMzIyYTE;mm/migrate: fix race between lock page and clear PG_Isolated;andrew.yang;2022-03-22;1;1
C_kwDOACN7MtoAKDM1NmVhMzg2NTY4NzkyNmU1ZGE3NTc5ZDFmMzM1MWQzZjBhMzIyYTE;"When memory is tight, system may start to compact memory for large
continuous memory demands";andrew.yang;2022-03-22;0;0
C_kwDOACN7MtoAKDM1NmVhMzg2NTY4NzkyNmU1ZGE3NTc5ZDFmMzM1MWQzZjBhMzIyYTE;" If one process tries to lock a memory page
that is being locked and isolated for compaction, it may wait a long time
or even forever";andrew.yang;2022-03-22;0;0
C_kwDOACN7MtoAKDM1NmVhMzg2NTY4NzkyNmU1ZGE3NTc5ZDFmMzM1MWQzZjBhMzIyYTE;" This is because compaction will perform non-atomic
PG_Isolated clear while holding page lock, this may overwrite PG_waiters
set by the process that can't obtain the page lock and add itself to the
waiting queue to wait for the lock to be unlocked";andrew.yang;2022-03-22;0;1
C_kwDOACN7MtoAKDM1NmVhMzg2NTY4NzkyNmU1ZGE3NTc5ZDFmMzM1MWQzZjBhMzIyYTE;"  CPU1                            CPU2
  lock_page(page); (successful)
                                  lock_page(); (failed)
  __ClearPageIsolated(page);      SetPageWaiters(page) (may be overwritten)
The solution is to not perform non-atomic operation on page flags while
holding page lock.";andrew.yang;2022-03-22;0;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;mm,migrate: fix establishing demotion target;Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;"In commit ac16ec835314 (""mm: migrate: support multiple target nodes
demotion""), after the first demotion target node is found, we will
continue to check the next candidate obtained via find_next_best_node()";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;This is to find all demotion target nodes with same NUMA distance;Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" But
one side effect of find_next_best_node() is that the candidate node
returned will be set in ""used"" parameter, even if the candidate node isn't
passed in the following NUMA distance checking, the candidate node will
not be used as demotion target node for the following nodes";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" For example,
for system as follows,
node distances";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;"node   0   1   2   3
when we establish demotion target node for node 0, in the first round node
2 is added to the demotion target node set";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" Then in the second round,
node 3 is checked and failed because distance(0, 3) > distance(0, 2)";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" But
node 3 is set in ""used"" nodemask too";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" When we establish demotion target
node for node 1, there is no available node";Huang Ying;2022-03-22;0;0
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" This is wrong, node 3 should
be set as the demotion target of node 1";Huang Ying;2022-03-22;0;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;"To fix this, if the candidate node is failed to pass the distance
checking, it will be cleared in ""used"" nodemask";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;" So that it can be used
for the following node";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGZjODkyMTNhNjM2YzM3MzVlYjMzODZmMTBhMzRjMDgyMjcxYjQxOTI;"The bug can be reproduced and fixed with this patch on a 2 socket server
machine with DRAM and PMEM.";Huang Ying;2022-03-22;1;1
C_kwDOACN7MtoAKGI2OThmMGExNzczZjdkZjczZjJiYjRiZmUwZTU5N2VhMWJiMzg4MWY;mm/fs: delete PF_SWAPWRITE;Hugh Dickins;2022-03-22;1;0
C_kwDOACN7MtoAKGI2OThmMGExNzczZjdkZjczZjJiYjRiZmUwZTU5N2VhMWJiMzg4MWY;"PF_SWAPWRITE has been redundant since v3.2 commit ee72886d8ed5 (""mm";Hugh Dickins;2022-03-22;0;1
C_kwDOACN7MtoAKGI2OThmMGExNzczZjdkZjczZjJiYjRiZmUwZTU5N2VhMWJiMzg4MWY;"vmscan: do not writeback filesystem pages in direct reclaim"")";Hugh Dickins;2022-03-22;0;0
C_kwDOACN7MtoAKGI2OThmMGExNzczZjdkZjczZjJiYjRiZmUwZTU5N2VhMWJiMzg4MWY;"Coincidentally, NeilBrown's current patch ""remove inode_congested()""
deletes may_write_to_inode(), which appeared to be the one function which
took notice of PF_SWAPWRITE";Hugh Dickins;2022-03-22;1;0
C_kwDOACN7MtoAKGI2OThmMGExNzczZjdkZjczZjJiYjRiZmUwZTU5N2VhMWJiMzg4MWY;" But if you study the old logic, and the
conditions under which may_write_to_inode() was called, you discover that
flag and function have been pointless for a decade.";Hugh Dickins;2022-03-22;0;0
C_kwDOACN7MtoAKDg3ZDI3NjJlMjJmM2VhNjg4NTg2MmNiMWZkNDE5Yjc3YTViY2Q4Zjc;mm: remove unneeded local variable follflags;Miaohe Lin;2022-03-22;1;1
C_kwDOACN7MtoAKDg3ZDI3NjJlMjJmM2VhNjg4NTg2MmNiMWZkNDE5Yjc3YTViY2Q4Zjc;"We can pass FOLL_GET | FOLL_DUMP to follow_page directly to simplify the
code a bit in add_page_for_migration and split_huge_pages_pid.";Miaohe Lin;2022-03-22;1;1
C_kwDOACN7MtoAKDMxNTBiZThmYTg5ZTRkMTA2NGQyNTBiYjNmOGVhMzY2NWQxZWM1ZTk;mm: replace multiple dcache flush with flush_dcache_folio();Muchun Song;2022-03-22;1;1
C_kwDOACN7MtoAKDMxNTBiZThmYTg5ZTRkMTA2NGQyNTBiYjNmOGVhMzY2NWQxZWM1ZTk;Simplify the code by using flush_dcache_folio().;Muchun Song;2022-03-22;1;1
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;mm: fix missing cache flush for all tail pages of compound page;Muchun Song;2022-03-22;1;1
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;"The D-cache maintenance inside move_to_new_page() only consider one
page, there is still D-cache maintenance issue for tail pages of
compound page (e.g";Muchun Song;2022-03-22;1;0
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;THP or HugeTLB);Muchun Song;2022-03-22;0;0
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;"THP migration is only enabled on x86_64, ARM64 and powerpc, while
powerpc and arm64 need to maintain the consistency between I-Cache and
D-Cache, which depends on flush_dcache_page() to maintain the
consistency between I-Cache and D-Cache";Muchun Song;2022-03-22;0;1
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;"But there is no issues on arm64 and powerpc since they already considers
the compound page cache flushing in their icache flush function";Muchun Song;2022-03-22;0;1
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;"HugeTLB migration is enabled on arm, arm64, mips, parisc, powerpc,
riscv, s390 and sh, while arm has handled the compound page cache flush
in flush_dcache_page(), but most others do not";Muchun Song;2022-03-22;0;1
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;In theory, the issue exists on many architectures;Muchun Song;2022-03-22;0;0
C_kwDOACN7MtoAKDI3NzE3MzlhNzE2Mjc4MmMwYWE2NDI0YjJlM2RkODc0ZTg4NGExNWQ;" Fix this by not
using flush_dcache_folio() since it is not backportable.";Muchun Song;2022-03-22;1;1
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc;mm/gup: follow_pfn_pte(): -EEXIST cleanup;John Hubbard;2022-03-22;1;1
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc;"Remove a quirky special case from follow_pfn_pte(), and adjust its
callers to match";John Hubbard;2022-03-22;1;0
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc; Caller changes include;John Hubbard;2022-03-22;1;0
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc;"__get_user_pages(): Regardless of any FOLL_* flags, get_user_pages() and
its variants should handle PFN-only entries by stopping early, if the
caller expected **pages to be filled in";John Hubbard;2022-03-22;0;1
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc;" This makes for a more reliable
API, as compared to the previous approach of skipping over such entries
(and thus leaving them silently unwritten)";John Hubbard;2022-03-22;0;1
C_kwDOACN7MtoAKDY1NDYyNDYyZmZiMjhmZGRmMTNkNDZjNjI4YzRmYzU1ODc4YWIzOTc;"move_pages(): squash the -EEXIST error return from follow_page() into
-EFAULT, because -EFAULT is listed in the man page, whereas -EEXIST is
not.";John Hubbard;2022-03-22;0;0
C_kwDOACN7MtoAKDJmMDMxYzZmMDQyY2I4YTliMjIxYThiNmI4MGU2OWRlNTE3MGY4MzA;mm/rmap: Convert rmap_walk() to take a folio;Matthew Wilcox (Oracle);2022-01-29;1;0
C_kwDOACN7MtoAKDJmMDMxYzZmMDQyY2I4YTliMjIxYThiNmI4MGU2OWRlNTE3MGY4MzA;"This ripples all the way through to every calling and called function
from rmap.";Matthew Wilcox (Oracle);2022-01-29;1;0
C_kwDOACN7MtoAKDRlZWNiOGI5MTYzZGY4MmM4N2M5MTc2NGEwMmZmZjIyOGVmMjVmNmQ;mm/migrate: Convert remove_migration_ptes() to folios;Matthew Wilcox (Oracle);2022-01-29;1;0
C_kwDOACN7MtoAKDRlZWNiOGI5MTYzZGY4MmM4N2M5MTc2NGEwMmZmZjIyOGVmMjVmNmQ;Convert the implementation and all callers.;Matthew Wilcox (Oracle);2022-01-29;1;0
C_kwDOACN7MtoAKDRiODU1NGM1MjdmM2NmYTE4M2Y2YzA2ZDIzMWE5Mzg3ODczMjA1YTA;mm/rmap: Convert try_to_migrate() to folios;Matthew Wilcox (Oracle);2022-01-28;1;0
C_kwDOACN7MtoAKDRiODU1NGM1MjdmM2NmYTE4M2Y2YzA2ZDIzMWE5Mzg3ODczMjA1YTA;"Convert the callers to pass a folio and the try_to_migrate_one()
worker to use a folio throughout";Matthew Wilcox (Oracle);2022-01-28;1;0
C_kwDOACN7MtoAKDRiODU1NGM1MjdmM2NmYTE4M2Y2YzA2ZDIzMWE5Mzg3ODczMjA1YTA;" Fixes an assumption that a
folio must be <= PMD size.";Matthew Wilcox (Oracle);2022-01-28;1;1
C_kwDOACN7MtoAKDJhZmY3YTQ3NTViZWQyODcwZWUyM2I3NWJjODhjZGM4ZDc2Y2RkMDM;mm: Convert page_vma_mapped_walk to work on PFNs;Matthew Wilcox (Oracle);2022-02-03;1;0
C_kwDOACN7MtoAKDJhZmY3YTQ3NTViZWQyODcwZWUyM2I3NWJjODhjZGM4ZDc2Y2RkMDM;"page_mapped_in_vma() really just wants to walk one page, but as the
code stands, if passed the head page of a compound page, it will
walk every page in the compound page";Matthew Wilcox (Oracle);2022-02-03;1;1
C_kwDOACN7MtoAKDJhZmY3YTQ3NTViZWQyODcwZWUyM2I3NWJjODhjZGM4ZDc2Y2RkMDM;" Extract pfn/nr_pages/pgoff
from the struct page early, so they can be overridden by
page_mapped_in_vma().";Matthew Wilcox (Oracle);2022-02-03;1;0
C_kwDOACN7MtoAKGVlZDA1ZTU0ZDI3NWIzY2ZjNWQ4Yzc5ODQzYzUyNzZhNTg3OGU5NGE;mm: Add DEFINE_PAGE_VMA_WALK and DEFINE_FOLIO_VMA_WALK;Matthew Wilcox (Oracle);2022-02-03;1;1
C_kwDOACN7MtoAKGVlZDA1ZTU0ZDI3NWIzY2ZjNWQ4Yzc5ODQzYzUyNzZhNTg3OGU5NGE;"Instead of declaring a struct page_vma_mapped_walk directly,
use these helpers to allow us to transition to a PFN approach in the
following patches.";Matthew Wilcox (Oracle);2022-02-03;1;1
C_kwDOACN7MtoAKDc2Y2JiZWFkMjUzZGRjYWU5ODc4YmUwZDcwMjIwOGJiMWU0ZmFjNmY;mm: move the migrate_vma_* device migration code into its own file;Christoph Hellwig;2022-02-16;1;0
C_kwDOACN7MtoAKDc2Y2JiZWFkMjUzZGRjYWU5ODc4YmUwZDcwMjIwOGJiMWU0ZmFjNmY;"Split the code used to migrate to and from ZONE_DEVICE memory from
migrate.c into a new file.";Christoph Hellwig;2022-02-16;0;1
C_kwDOACN7MtoAKGFhZjdkNzBjYzU5NWM3OGQyN2U5MTU0NTFlOTNhNDQ1OWNmYzM2ZjM;mm: refactor the ZONE_DEVICE handling in migrate_vma_pages;Christoph Hellwig;2022-02-16;1;0
C_kwDOACN7MtoAKGFhZjdkNzBjYzU5NWM3OGQyN2U5MTU0NTFlOTNhNDQ1OWNmYzM2ZjM;"Make the flow a little more clear and prepare for adding a new
ZONE_DEVICE memory type.";Christoph Hellwig;2022-02-16;1;1
C_kwDOACN7MtoAKDE3NzZjMGQxMDI0ODJkNGFlY2NkNTZlNDA0Mjg1YmM0N2E0ODFiZTg;mm: refactor the ZONE_DEVICE handling in migrate_vma_insert_page;Christoph Hellwig;2022-02-16;1;1
C_kwDOACN7MtoAKDE3NzZjMGQxMDI0ODJkNGFlY2NkNTZlNDA0Mjg1YmM0N2E0ODFiZTg;"Make the flow a little more clear and prepare for adding a new
ZONE_DEVICE memory type.";Christoph Hellwig;2022-02-16;1;1
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;mm: remove the extra ZONE_DEVICE struct page refcount;Christoph Hellwig;2022-02-16;1;0
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;"ZONE_DEVICE struct pages have an extra reference count that complicates
the code for put_page() and several places in the kernel that need to
check the reference count to see that a page is not being used (gup,
compaction, migration, etc.)";Christoph Hellwig;2022-02-16;0;0
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;"Clean up the code so the reference count
doesn't need to be treated specially for ZONE_DEVICE pages";Christoph Hellwig;2022-02-16;1;1
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;"Note that this excludes the special idle page wakeup for fsdax pages,
which still happens at refcount 1";Christoph Hellwig;2022-02-16;1;0
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;" This is a separate issue and will
be sorted out later";Christoph Hellwig;2022-02-16;0;0
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;" Given that only fsdax pages require the
notifiacation when the refcount hits 1 now, the PAGEMAP_OPS Kconfig
symbol can go away and be replaced with a FS_DAX check for this hook
in the put_page fastpath";Christoph Hellwig;2022-02-16;1;1
C_kwDOACN7MtoAKDI3Njc0ZWY2YzczZjBjOTA5NmE5ODI3ZGM1ZDZiYTlmYzc4MDg0MjI;Based on an earlier patch from Ralph Campbell <rcampbell@nvidia.com>.;Christoph Hellwig;2022-02-16;0;0
C_kwDOACN7MtoAKGI3NDM1NTA3OGI2NTU0MjcxMzcxNTMyYTVkYWEzYjFhM2RiNjIwZjk;mm/munlock: page migration needs mlock pagevec drained;Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGI3NDM1NTA3OGI2NTU0MjcxMzcxNTMyYTVkYWEzYjFhM2RiNjIwZjk;"Page migration of a VM_LOCKED page tends to fail, because when the old
page is unmapped, it is put on the mlock pagevec with raised refcount,
which then fails the freeze";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGI3NDM1NTA3OGI2NTU0MjcxMzcxNTMyYTVkYWEzYjFhM2RiNjIwZjk;"At first I thought this would be fixed by a local mlock_page_drain() at
the upper rmap_walk() level - which would have nicely batched all the
munlocks of that page; but tests show that the task can too easily move
to another cpu, leaving pagevec residue behind which fails the migration";Hugh Dickins;2022-02-15;0;1
C_kwDOACN7MtoAKGI3NDM1NTA3OGI2NTU0MjcxMzcxNTMyYTVkYWEzYjFhM2RiNjIwZjk;"So try_to_migrate_one() drain the local pagevec after page_remove_rmap()
from a VM_LOCKED vma; and do the same in try_to_unmap_one(), whose
TTU_IGNORE_MLOCK users would want the same treatment; and do the same
in remove_migration_pte() - not important when successfully inserting
a new page, but necessary when hoping to retry after failure";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGI3NDM1NTA3OGI2NTU0MjcxMzcxNTMyYTVkYWEzYjFhM2RiNjIwZjk;"Any new pagevec runs the risk of adding a new way of stranding, and we
might discover other corners where mlock_page_drain() or lru_add_drain()
would now help.";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGMzMDk2ZTY3ODJiNzMzMTU4YmYzNGY2YmJiNDU2NzgwOGQ0ZTA3NDA;mm/migrate: __unmap_and_move() push good newpage to LRU;Hugh Dickins;2022-02-15;1;0
C_kwDOACN7MtoAKGMzMDk2ZTY3ODJiNzMzMTU4YmYzNGY2YmJiNDU2NzgwOGQ0ZTA3NDA;"Compaction, NUMA page movement, THP collapse/split, and memory failure
do isolate unevictable pages from their ""LRU"", losing the record of
mlock_count in doing so (isolators are likely to use page->lru for their
own private lists, so mlock_count has to be presumed lost)";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGMzMDk2ZTY3ODJiNzMzMTU4YmYzNGY2YmJiNDU2NzgwOGQ0ZTA3NDA;"That's unfortunate, and we should put in some work to correct that: one
can imagine a function to build up the mlock_count again - but it would
require i_mmap_rwsem for read, so be careful where it's called";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGMzMDk2ZTY3ODJiNzMzMTU4YmYzNGY2YmJiNDU2NzgwOGQ0ZTA3NDA;" Or
page_referenced_one() and try_to_unmap_one() might do that extra work";Hugh Dickins;2022-02-15;0;1
C_kwDOACN7MtoAKGMzMDk2ZTY3ODJiNzMzMTU4YmYzNGY2YmJiNDU2NzgwOGQ0ZTA3NDA;"But one place that can very easily be improved is page migration's
__unmap_and_move(): a small adjustment to where the successful new page
is put back on LRU, and its mlock_count (if any) is built back up by
remove_migration_ptes().";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;mm/munlock: rmap call mlock_vma_page() munlock_vma_page();Hugh Dickins;2022-02-15;1;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"Add vma argument to mlock_vma_page() and munlock_vma_page(), make them
inline functions which check (vma->vm_flags & VM_LOCKED) before calling
mlock_page() and munlock_page() in mm/mlock.c";Hugh Dickins;2022-02-15;1;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"Add bool compound to mlock_vma_page() and munlock_vma_page(): this is
because we have understandable difficulty in accounting pte maps of THPs,
and if passed a PageHead page, mlock_page() and munlock_page() cannot
tell whether it's a pmd map to be counted or a pte map to be ignored";Hugh Dickins;2022-02-15;0;1
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"Add vma arg to page_add_file_rmap() and page_remove_rmap(), like the
others, and use that to call mlock_vma_page() at the end of the page
adds, and munlock_vma_page() at the end of page_remove_rmap() (end or
beginning? unimportant, but end was easier for assertions in testing)";Hugh Dickins;2022-02-15;1;1
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;No page lock is required (although almost all adds happen to hold it);Hugh Dickins;2022-02-15;1;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"delete the ""Serialize with page migration"" BUG_ON(!PageLocked(page))s";Hugh Dickins;2022-02-15;0;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"Certainly page lock did serialize with page migration, but I'm having
difficulty explaining why that was ever important";Hugh Dickins;2022-02-15;0;1
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"Mlock accounting on THPs has been hard to define, differed between anon
and file, involved PageDoubleMap in some places and not others, required
clear_page_mlock() at some points";Hugh Dickins;2022-02-15;0;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;" Keep it simple now: just count the
pmds and ignore the ptes, there is no reason for ptes to undo pmd mlocks";Hugh Dickins;2022-02-15;1;0
C_kwDOACN7MtoAKGNlYTg2ZmUyNDZiNjk0YTE5MTgwNGI0NzM3OGViOWQ3N2FlZmFiZWM;"page_add_new_anon_rmap() callers unchanged: they have long been calling
lru_cache_add_inactive_or_unevictable(), which does its own VM_LOCKED
handling (it also checks for not VM_SPECIAL: I think that's overcautious,
and inconsistent with other checks, that mmap_region() already prevents
VM_LOCKED on VM_SPECIAL; but haven't quite convinced myself to change it).";Hugh Dickins;2022-02-15;0;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;mm/migrate.c: rework migration_entry_wait() to not take a pageref;Alistair Popple;2022-01-22;0;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;This fixes the FIXME in migrate_vma_check_page();Alistair Popple;2022-01-22;1;1
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;"Before migrating a page migration code will take a reference and check
there are no unexpected page references, failing the migration if there
are";Alistair Popple;2022-01-22;1;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;" When a thread faults on a migration entry it will take a temporary
reference to the page to wait for the page to become unlocked signifying
the migration entry has been removed";Alistair Popple;2022-01-22;0;1
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;"This reference is dropped just prior to waiting on the page lock,
however the extra reference can cause migration failures so it is
desirable to avoid taking it";Alistair Popple;2022-01-22;1;1
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;"As migration code already has a reference to the migrating page an extra
reference to wait on PG_locked is unnecessary so long as the reference
can't be dropped whilst setting up the wait";Alistair Popple;2022-01-22;1;1
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;"When faulting on a migration entry the ptl is taken to check the
migration entry";Alistair Popple;2022-01-22;0;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;" Removing a migration entry also requires the ptl, and
migration code won't drop its page reference until after the migration
entry has been removed";Alistair Popple;2022-01-22;1;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;" Therefore retaining the ptl of a migration
entry is sufficient to ensure the page has a reference";Alistair Popple;2022-01-22;0;0
C_kwDOACN7MtoAKGZmYTY1NzUzYzQzMTQyZjNiODAzNDg2NDQyODEzNzQ0ZGE3MWNmZjI;" Reworking
migration_entry_wait() to hold the ptl until the wait setup is complete
means the extra page reference is no longer needed.";Alistair Popple;2022-01-22;0;0
C_kwDOACN7MtoAKGYxZThkYjA0YjY4Y2M1NmVkYzViYWVlNWM3Y2IxZjliNzljM2RhN2U;mm/migrate: remove redundant variables used in a for-loop;Colin Ian King;2022-01-14;1;1
C_kwDOACN7MtoAKGYxZThkYjA0YjY4Y2M1NmVkYzViYWVlNWM3Y2IxZjliNzljM2RhN2U;"The variable addr is being set and incremented in a for-loop but not
actually being used";Colin Ian King;2022-01-14;0;0
C_kwDOACN7MtoAKGYxZThkYjA0YjY4Y2M1NmVkYzViYWVlNWM3Y2IxZjliNzljM2RhN2U;" It is redundant and so addr and also variable
start can be removed.";Colin Ian King;2022-01-14;1;1
C_kwDOACN7MtoAKGRjZWU5YmY1YmYyZjU5YzE3M2YzNjQ1YWMyMjc0NTk1YWM2YzZhZWE;mm/migrate: move node demotion code to near its user;Huang Ying;2022-01-14;0;0
C_kwDOACN7MtoAKGRjZWU5YmY1YmYyZjU5YzE3M2YzNjQ1YWMyMjc0NTk1YWM2YzZhZWE;"Now, node_demotion and next_demotion_node() are placed between
__unmap_and_move() and unmap_and_move()";Huang Ying;2022-01-14;0;0
C_kwDOACN7MtoAKGRjZWU5YmY1YmYyZjU5YzE3M2YzNjQ1YWMyMjc0NTk1YWM2YzZhZWE; This hurts code readability;Huang Ying;2022-01-14;0;1
C_kwDOACN7MtoAKGRjZWU5YmY1YmYyZjU5YzE3M2YzNjQ1YWMyMjc0NTk1YWM2YzZhZWE;So move them near their users in the file;Huang Ying;2022-01-14;1;0
C_kwDOACN7MtoAKGRjZWU5YmY1YmYyZjU5YzE3M2YzNjQ1YWMyMjc0NTk1YWM2YzZhZWE;" There's no functionality
change in this patch.";Huang Ying;2022-01-14;1;0
C_kwDOACN7MtoAKDc4MTNhMWI1MjU3YjhlYjJjYjkxNWNkMDhlN2JhODU3MDcwZmRmZDM;mm: migrate: add more comments for selecting target node randomly;Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKDc4MTNhMWI1MjU3YjhlYjJjYjkxNWNkMDhlN2JhODU3MDcwZmRmZDM;"As Yang Shi suggested [1], it will be helpful to explain why we should
select target node randomly now if there are multiple target nodes.";Baolin Wang;2022-01-14;0;1
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;mm: migrate: support multiple target nodes demotion;Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"We have some machines with multiple memory types like below, which have
one fast (DRAM) memory node and two slow (persistent memory) memory
nodes";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;" According to current node demotion policy, if node 0 fills up,
its memory should be migrated to node 1, when node 1 fills up, its
memory will be migrated to node 2: node 0 -> node 1 -> node 2 ->stop";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"But this is not efficient and suitbale memory migration route for our
machine with multiple slow memory nodes";Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;" Since the distance between
node 0 to node 1 and node 0 to node 2 is equal, and memory migration
between slow memory nodes will increase persistent memory bandwidth
greatly, which will hurt the whole system's performance";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"Thus for this case, we can treat the slow memory node 1 and node 2 as a
whole slow memory region, and we should migrate memory from node 0 to
node 1 and node 2 if node 0 fills up";Baolin Wang;2022-01-14;0;1
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"This patch changes the node_demotion data structure to support multiple
target nodes, and establishes the migration path to support multiple
target nodes with validating if the node distance is the best or not";Baolin Wang;2022-01-14;1;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"  available: 3 nodes (0-2)
  node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
  node 0 size: 62153 MB
  node 0 free: 55135 MB
  node 1 cpus";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"  node 1 size: 127007 MB
  node 1 free: 126930 MB
  node 2 cpus";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;"  node 2 size: 126968 MB
  node 2 free: 126878 MB
  node distances";Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKGFjMTZlYzgzNTMxNDY3N2RkNzQwNWRmYjVhNWUwMDdjM2NhNDI0Yzc;  node   0   1   2;Baolin Wang;2022-01-14;0;0
C_kwDOACN7MtoAKDVkMzlhN2ViYzhiZTcwZTMwMTc2YWVkNmY5OGY3OTliZmE3NDM5ZDY;mm: migrate: correct the hugetlb migration stats;Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKDVkMzlhN2ViYzhiZTcwZTMwMTc2YWVkNmY5OGY3OTliZmE3NDM5ZDY;"Correct the migration stats for hugetlb with using compound_nr() instead
of thp_nr_pages(), meanwhile change 'nr_failed_pages' to record the
number of normal pages failed to migrate, including THP and hugetlb, and
'nr_succeeded' will record the number of normal pages migrated
successfully.";Baolin Wang;2022-01-14;0;1
C_kwDOACN7MtoAKGI1YmFkZTk3OGU5YjhmNDI1MjFjY2VmNzExNjQyYmQyMTMxM2NmNDQ;mm: migrate: fix the return value of migrate_pages();Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKGI1YmFkZTk3OGU5YjhmNDI1MjFjY2VmNzExNjQyYmQyMTMxM2NmNDQ;"Patch series ""Improve the migration stats""";Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKGI1YmFkZTk3OGU5YjhmNDI1MjFjY2VmNzExNjQyYmQyMTMxM2NmNDQ;"According to talk with Zi Yan [1], this patch set changes the return
value of migrate_pages() to avoid returning a number which is larger
than the number of pages the users tried to migrate by move_pages()
syscall";Baolin Wang;2022-01-14;0;1
C_kwDOACN7MtoAKGI1YmFkZTk3OGU5YjhmNDI1MjFjY2VmNzExNjQyYmQyMTMxM2NmNDQ;" Also fix the hugetlb migration stats and migration stats in
trace_mm_compaction_migratepages().";Baolin Wang;2022-01-14;1;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;mm: change page type prior to adding page table entry;Pasha Tatashin;2022-01-14;1;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"Patch series ""page table check"", v3";Pasha Tatashin;2022-01-14;1;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"Ensure that some memory corruptions are prevented by checking at the
time of insertion of entries into user page tables that there is no
illegal sharing";Pasha Tatashin;2022-01-14;0;0
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;We have recently found a problem [1] that existed in kernel since 4.14;Pasha Tatashin;2022-01-14;0;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"The problem was caused by broken page ref count and led to memory
leaking from one process into another";Pasha Tatashin;2022-01-14;0;0
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;" The problem was accidentally
detected by studying a dump of one process and noticing that one page
contains memory that should not belong to this process";Pasha Tatashin;2022-01-14;0;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"There are some other page->_refcount related problems that were recently
fixed: [2], [3] which potentially could also lead to illegal sharing";Pasha Tatashin;2022-01-14;0;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"In addition to hardening refcount [4] itself, this work is an attempt to
prevent this class of memory corruption issues";Pasha Tatashin;2022-01-14;1;1
C_kwDOACN7MtoAKDFlYmE4NmMwOTZlMzVlM2NjODNkZTFhZDJjMjZmMmQ3MDQ3MDIxMWI;"It uses a simple state machine that is independent from regular MM logic
to check for illegal sharing at time pages are inserted and removed from
page tables.";Pasha Tatashin;2022-01-14;0;1
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;mm: Use multi-index entries in the page cache;Matthew Wilcox (Oracle);2020-06-28;1;1
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;We currently store large folios as 2^N consecutive entries;Matthew Wilcox (Oracle);2020-06-28;0;0
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;" While this
consumes rather more memory than necessary, it also turns out to be buggy";Matthew Wilcox (Oracle);2020-06-28;1;1
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;"A writeback operation which starts within a tail page of a dirty folio will
not write back the folio as the xarray's dirty bit is only set on the
head index";Matthew Wilcox (Oracle);2020-06-28;1;1
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;" With multi-index entries, the dirty bit will be found no
matter where in the folio the operation starts";Matthew Wilcox (Oracle);2020-06-28;1;1
C_kwDOACN7MtoAKDZiMjRjYTRhMWE4ZDRlZTMyMjFkNmQ0NGRkYmI5OWY1NDJlNGJkYTM;"This does end up simplifying the page cache slightly, although not as
much as I had hoped.";Matthew Wilcox (Oracle);2020-06-28;0;1
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA;filemap: Add folio_put_wait_locked();Matthew Wilcox (Oracle);2021-08-17;1;1
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA;"Convert all three callers of put_and_wait_on_page_locked() to
folio_put_wait_locked()";Matthew Wilcox (Oracle);2021-08-17;1;0
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA; This shrinks the kernel overall by 19 bytes;Matthew Wilcox (Oracle);2021-08-17;0;0
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA;"filemap_update_page() shrinks by 19 bytes while __migration_entry_wait()
is unchanged";Matthew Wilcox (Oracle);2021-08-17;1;1
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA;" folio_put_wait_locked() is 14 bytes smaller than
put_and_wait_on_page_locked(), but pmd_migration_entry_wait() grows by
14 bytes";Matthew Wilcox (Oracle);2021-08-17;1;0
C_kwDOACN7MtoAKDlmMmIwNGEyNWE0MWIxZjQxYjNjZWFkNGY1Njg1NGE0MTkyZWM1YjA;" It removes the assumption from pmd_migration_entry_wait()
that pages cannot be larger than a PMD (which is true today, but
may be interesting to explore in the future).";Matthew Wilcox (Oracle);2021-08-17;0;0
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;mm/migrate.c: remove MIGRATE_PFN_LOCKED;Alistair Popple;2021-11-11;1;1
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;"MIGRATE_PFN_LOCKED is used to indicate to migrate_vma_prepare() that a
source page was already locked during migrate_vma_collect()";Alistair Popple;2021-11-11;1;1
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;" If it
wasn't then the a second attempt is made to lock the page";Alistair Popple;2021-11-11;0;1
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;" However if
the first attempt failed it's unlikely a second attempt will succeed,
and the retry adds complexity";Alistair Popple;2021-11-11;1;1
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;" So clean this up by removing the retry
and MIGRATE_PFN_LOCKED flag";Alistair Popple;2021-11-11;1;1
C_kwDOACN7MtoAKGFiMDkyNDNhYTk1YTcyYmFjNWM3MWU4NTI3NzNkZTM0MTE2ZjhkMGY;"Destination pages are also meant to have the MIGRATE_PFN_LOCKED flag
set, but nothing actually checks that.";Alistair Popple;2021-11-11;0;0
C_kwDOACN7MtoAKDBlZjAyNDYyMTQxN2ZhM2ZjZGViMmMzMzIwZjkwZWUzNGUxOGE1ZDk;mm: migrate: simplify the file-backed pages validation when migrating its mapping;Baolin Wang;2021-11-11;1;1
C_kwDOACN7MtoAKDBlZjAyNDYyMTQxN2ZhM2ZjZGViMmMzMzIwZjkwZWUzNGUxOGE1ZDk;"There is no need to validate the file-backed page's refcount before
trying to freeze the page's expected refcount, instead we can rely on
the folio_ref_freeze() to validate if the page has the expected refcount
before migrating its mapping";Baolin Wang;2021-11-11;1;1
C_kwDOACN7MtoAKDBlZjAyNDYyMTQxN2ZhM2ZjZGViMmMzMzIwZjkwZWUzNGUxOGE1ZDk;"Moreover we are always under the page lock when migrating the page
mapping, which means nowhere else can remove it from the page cache, so
we can remove the xas_load() validation under the i_pages lock.";Baolin Wang;2021-11-11;0;1
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;mm: migrate: make demotion knob depend on migration;Yang Shi;2021-11-05;1;0
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;The memory demotion needs to call migrate_pages() to do the jobs;Yang Shi;2021-11-05;0;1
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;" And
it is controlled by a knob, however, the knob doesn't depend on
CONFIG_MIGRATION";Yang Shi;2021-11-05;0;1
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;" The knob could be truned on even though MIGRATION is
disabled, this will not cause any crash since migrate_pages() would just
return -ENOSYS";Yang Shi;2021-11-05;0;1
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;" But it is definitely not optimal to go through demotion
path then retry regular swap every time";Yang Shi;2021-11-05;1;0
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;"And it doesn't make too much sense to have the knob visible to the users
when !MIGRATION";Yang Shi;2021-11-05;0;1
C_kwDOACN7MtoAKDIwZjliYTRmOTk1MjQ3YmI3OWUyNDM3NDFiOGZkZGRiZDc2ZGQ5MjM;" Move the related code from mempolicy.[h|c] to
migrate.[h|c].";Yang Shi;2021-11-05;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;mm/migrate: fix CPUHP state to update node demotion order;Huang Ying;2021-10-18;1;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;The node demotion order needs to be updated during CPU hotplug;Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" Because
whether a NUMA node has CPU may influence the demotion order";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" The
update function should be called during CPU online/offline after the
node_states[N_CPU] has been updated";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" That is done in
CPUHP_AP_ONLINE_DYN during CPU online and in CPUHP_MM_VMSTAT_DEAD during
CPU offline";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" But in commit 884a6e5d1f93 (""mm/migrate: update node
demotion order on hotplug events""), the function to update node demotion
order is called in CPUHP_AP_ONLINE_DYN during CPU online/offline";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" This
doesn't satisfy the order requirement";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;"For example, there are 4 CPUs (P0, P1, P2, P3) in 2 sockets (P0, P1 in S0
and P2, P3 in S1), the demotion order is
 - S0 -> NUMA_NO_NODE
 - S1 -> NUMA_NO_NODE
After P2 and P3 is offlined, because S1 has no CPU now, the demotion
order should have been changed to
 - S0 -> S1
 - S1 -> NO_NODE
but it isn't changed, because the order updating callback for CPU
hotplug doesn't see the new nodemask";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;" After that, if P1 is offlined,
the demotion order is changed to the expected order as above";Huang Ying;2021-10-18;0;1
C_kwDOACN7MtoAKGE2YTAyNTFjNmZjZTQ5Njc0NDEyMWI0ZTA4Yzg5OWY0NTI3MGRiY2M;"So in this patch, we added CPUHP_AP_MM_DEMOTION_ONLINE and
CPUHP_MM_DEMOTION_DEAD to be called after CPUHP_AP_ONLINE_DYN and
CPUHP_MM_VMSTAT_DEAD during CPU online and offline, and register the
update function on them.";Huang Ying;2021-10-18;0;0
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;mm/migrate: add CPU hotplug to demotion #ifdef;Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;"Once upon a time, the node demotion updates were driven solely by memory
hotplug events";Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;" But now, there are handlers for both CPU and memory
hotplug";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;However, the #ifdef around the code checks only memory hotplug;Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;" A
system that has HOTPLUG_CPU=y but MEMORY_HOTPLUG=n would miss CPU
hotplug events";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;Update the #ifdef around the common code;Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;" Add memory and CPU-specific
#ifdefs for their handlers";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDc2YWY2YTA1NGRhNDA1NTMwNWRkYjI4YzVlYjE1MWI5ZWU0Zjc0Zjk;" These memory/CPU #ifdefs avoid unused
function warnings when their Kconfig option is off.";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;mm/migrate: optimize hotplug-time demotion order updates;Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"Patch series ""mm/migrate: 5.15 fixes for automatic demotion"", v2";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"This contains two fixes for the ""automatic demotion"" code which was
merged into 5.15";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;   suppressing any real action on irrelevant hotplug events;Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;   is disabled;Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;This patch (of 2);Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"== tl;dr ==
Automatic demotion opted for a simple, lazy approach to handling hotplug
events";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ; This noticeably slows down memory hotplug[1];Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" Optimize away
updates to the demotion order when memory hotplug events should have no
effect";Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;This has no effect on CPU hotplug;Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" There is no known problem on the CPU
side and any work there will be in a separate series";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"== Background ==
Automatic demotion is a memory migration strategy to ensure that new
allocations have room in faster memory tiers on tiered memory systems";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"The kernel maintains an array (node_demotion[]) to drive these
migrations";Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"The node_demotion[] path is calculated by starting at nodes with CPUs
and then ""walking"" to nodes with memory";Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" Only hotplug events which
online or offline a node with memory (N_ONLINE) or CPUs (N_CPU) will
actually affect the migration order";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"== Problem ==
However, the current code is lazy";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" It completely regenerates the
migration order on *any* CPU or memory hotplug event";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" The logic was
that these events are extremely rare and that the overhead from
indiscriminate order regeneration is minimal";Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"Part of the update logic involves a synchronize_rcu(), which is a pretty
big hammer";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" Its overhead was large enough to be detected by some 0day
tests that watch memory hotplug performance[1]";Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"== Solution ==
Add a new helper (node_demotion_topo_changed()) which can differentiate
between superfluous and impactful hotplug events";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" Skip the expensive
update operation for superfluous events";Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"== Aside: Locking ==
It took me a few moments to declare the locking to be safe enough for
node_demotion_topo_changed() to work";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" It all hinges on the memory
hotplug lock";Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;During memory hotplug events, 'mem_hotplug_lock' is held for write;Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"This ensures that two memory hotplug events can not be called
simultaneously";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"CPU hotplug has a similar lock (cpuhp_state_mutex) which also provides
mutual exclusion between CPU hotplug events";Dave Hansen;2021-10-18;1;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" In addition, the demotion
code acquire and hold the mem_hotplug_lock for read during its CPU
hotplug handlers";Dave Hansen;2021-10-18;0;0
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;" This provides mutual exclusion between the demotion
memory hotplug callbacks and the CPU hotplug callbacks";Dave Hansen;2021-10-18;0;1
C_kwDOACN7MtoAKDI5NWJlOTFmN2VmMDAyN2ZjYTJmMmU0Nzg4ZTk5NzMxYWE5MzE4MzQ;"This effectively allows treating the migration target generation code to
act as if it is single-threaded.";Dave Hansen;2021-10-18;1;0
C_kwDOACN7MtoAKDcxNWNiZmQ2YzVjNTk1YmM4YjdhNmY5YWQxZmU5ZmVjMDEyMmJiMjA;mm/migrate: Add folio_migrate_copy();Matthew Wilcox (Oracle);2021-05-07;1;1
C_kwDOACN7MtoAKDcxNWNiZmQ2YzVjNTk1YmM4YjdhNmY5YWQxZmU5ZmVjMDEyMmJiMjA;"This is the folio equivalent of migrate_page_copy(), which is retained
as a wrapper for filesystems which are not yet converted to folios";Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDcxNWNiZmQ2YzVjNTk1YmM4YjdhNmY5YWQxZmU5ZmVjMDEyMmJiMjA;Also convert copy_huge_page() to folio_copy().;Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDE5MTM4MzQ5ZWQ1OWI5MGNlNThhY2EzMTliODczZWNhMmUwNGFkNDM;mm/migrate: Add folio_migrate_flags();Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDE5MTM4MzQ5ZWQ1OWI5MGNlNThhY2EzMTliODczZWNhMmUwNGFkNDM;Turn migrate_page_states() into a wrapper around folio_migrate_flags();Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDE5MTM4MzQ5ZWQ1OWI5MGNlNThhY2EzMTliODczZWNhMmUwNGFkNDM;"Also convert two functions only called from folio_migrate_flags() to
be folio-based";Matthew Wilcox (Oracle);2021-05-07;0;0
C_kwDOACN7MtoAKDE5MTM4MzQ5ZWQ1OWI5MGNlNThhY2EzMTliODczZWNhMmUwNGFkNDM;" ksm_migrate_page() becomes folio_migrate_ksm() and
copy_page_owner() becomes folio_copy_owner()";Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDE5MTM4MzQ5ZWQ1OWI5MGNlNThhY2EzMTliODczZWNhMmUwNGFkNDM;" folio_migrate_flags()
alone shrinks by two thirds -- 1967 bytes down to 642 bytes.";Matthew Wilcox (Oracle);2021-05-07;0;0
C_kwDOACN7MtoAKDM0MTcwMTNlMGQxODNiZTliNDJkNzk0MDgyZWVjMGVjMWM1YjVmMTU;mm/migrate: Add folio_migrate_mapping();Matthew Wilcox (Oracle);2021-05-07;1;1
C_kwDOACN7MtoAKDM0MTcwMTNlMGQxODNiZTliNDJkNzk0MDgyZWVjMGVjMWM1YjVmMTU;"Reimplement migrate_page_move_mapping() as a wrapper around
folio_migrate_mapping()";Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKDM0MTcwMTNlMGQxODNiZTliNDJkNzk0MDgyZWVjMGVjMWM1YjVmMTU; Saves 193 bytes of kernel text.;Matthew Wilcox (Oracle);2021-05-07;1;0
C_kwDOACN7MtoAKGQyMWJiYTJiN2QwYWUxOWRkMTI3OWUxMGFlZTYxYzM3YTE3YWJhNzQ;mm/memcg: Convert mem_cgroup_migrate() to take folios;Matthew Wilcox (Oracle);2021-05-06;1;0
C_kwDOACN7MtoAKGQyMWJiYTJiN2QwYWUxOWRkMTI3OWUxMGFlZTYxYzM3YTE3YWJhNzQ;Convert all callers of mem_cgroup_migrate() to call page_folio() first;Matthew Wilcox (Oracle);2021-05-06;1;0
C_kwDOACN7MtoAKGQyMWJiYTJiN2QwYWUxOWRkMTI3OWUxMGFlZTYxYzM3YTE3YWJhNzQ;They all look like they're using head pages already, but this proves it.;Matthew Wilcox (Oracle);2021-05-06;1;0
C_kwDOACN7MtoAKDhmNDI1ZTRlZDBlYjNlZjBiMmQ4NWE5ZWZjY2Y5NDdjYTZhYTliMWM;mm/memcg: Convert mem_cgroup_charge() to take a folio;Matthew Wilcox (Oracle);2021-06-25;1;0
C_kwDOACN7MtoAKDhmNDI1ZTRlZDBlYjNlZjBiMmQ4NWE5ZWZjY2Y5NDdjYTZhYTliMWM;"Convert all callers of mem_cgroup_charge() to call page_folio() on the
page they're currently passing in";Matthew Wilcox (Oracle);2021-06-25;1;0
C_kwDOACN7MtoAKDhmNDI1ZTRlZDBlYjNlZjBiMmQ4NWE5ZWZjY2Y5NDdjYTZhYTliMWM;" Many of them will be converted to
use folios themselves soon.";Matthew Wilcox (Oracle);2021-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OWFiODQ0ZWVkOWM2YjAxZDMyZGNiMjdiNTdhY2NjMjM3NzFiMzI0;compat: remove some compat entry points;Arnd Bergmann;2021-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo1OWFiODQ0ZWVkOWM2YjAxZDMyZGNiMjdiNTdhY2NjMjM3NzFiMzI0;"These are all handled correctly when calling the native system call entry
point, so remove the special cases.";Arnd Bergmann;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YjFiNTYxYmE3M2M4YWI5Yzk4ZTVkZmQxNGRjN2VlNDdlZmI2NTMw;mm: simplify compat_sys_move_pages;Arnd Bergmann;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YjFiNTYxYmE3M2M4YWI5Yzk4ZTVkZmQxNGRjN2VlNDdlZmI2NTMw;"The compat move_pages() implementation uses compat_alloc_user_space() for
converting the pointer array";Arnd Bergmann;2021-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YjFiNTYxYmE3M2M4YWI5Yzk4ZTVkZmQxNGRjN2VlNDdlZmI2NTMw;" Moving the compat handling into the
function itself is a bit simpler and lets us avoid the
compat_alloc_user_space() call.";Arnd Bergmann;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMTNlY2IzMTU3NTE0NDg2YTlhZTY4NDhhMjk4YjkxYTc5Y2MyZTJh;mm: migrate: change to use bool type for 'page_was_mapped';Baolin Wang;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMTNlY2IzMTU3NTE0NDg2YTlhZTY4NDhhMjk4YjkxYTc5Y2MyZTJh;"Change to use bool type for 'page_was_mapped' variable making it more
readable.";Baolin Wang;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OGE5ODQzZjE0YjZiMGQxY2UwMjM3MjE4MTQ0MDMyNTNkOGU5MTUz;mm: migrate: fix the incorrect function name in comments;Baolin Wang;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OGE5ODQzZjE0YjZiMGQxY2UwMjM3MjE4MTQ0MDMyNTNkOGU5MTUz;"since commit a98a2f0c8ce1 (""mm/rmap: split migration into its own
function""), the migration ptes establishment has been split into a
separate try_to_migrate() function, thus update the related comments.";Baolin Wang;2021-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyYjliNjI0ZjVhZWY2YWY2MDhlZGY1NDFmZWQ5NzM5NDhlMjcwMDRj;mm: migrate: introduce a local variable to get the number of pages;Baolin Wang;2021-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyYjliNjI0ZjVhZWY2YWY2MDhlZGY1NDFmZWQ5NzM5NDhlMjcwMDRj;"Use thp_nr_pages() instead of compound_nr() to get the number of pages for
THP page, meanwhile introducing a local variable 'nr_pages' to avoid
getting the number of pages repeatedly.";Baolin Wang;2021-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpjOWJkN2QxODM2NzNiNTEzNmU1NjIxMDAwM2UxZDk0MzM4ZDQ3YzQ1;mm/migrate: correct kernel-doc notation;Randy Dunlap;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjOWJkN2QxODM2NzNiNTEzNmU1NjIxMDAwM2UxZDk0MzM4ZDQ3YzQ1;"Use the expected ""Return:"" format to prevent a kernel-doc warning";Randy Dunlap;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOWJkN2QxODM2NzNiNTEzNmU1NjIxMDAwM2UxZDk0MzM4ZDQ3YzQ1;mm/migrate.c:1157: warning: Excess function parameter 'returns' description in 'next_demotion_node';Randy Dunlap;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm;mm/migrate: enable returning precise migrate_pages() success count;Yang Shi;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm;"Under normal circumstances, migrate_pages() returns the number of pages
migrated";Yang Shi;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm; In error conditions, it returns an error code;Yang Shi;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm;" When returning
an error code, there is no way to know how many pages were migrated or not
migrated";Yang Shi;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm;"Make migrate_pages() return how many pages are demoted successfully for
all cases, including when encountering errors";Yang Shi;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YWM5NTg4NGE3ODRlODIyYjhjYmUzZDRiZDZlOWY5NmIzYjcxZTNm;" Page reclaim behavior will
depend on this in subsequent patches.";Yang Shi;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;mm/migrate: update node demotion order on hotplug events;Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;"Reclaim-based migration is attempting to optimize data placement in memory
based on the system topology";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;" If the system changes, so must the
migration ordering";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;The implementation is conceptually simple and entirely unoptimized;Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;" On
any memory or CPU hotplug events, assume that a node was added or removed
and recalculate all migration targets";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;" This ensures that the
node_demotion[] array is always ready to be used in case the new reclaim
mode is enabled";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;"This recalculation is far from optimal, most glaringly that it does not
even attempt to figure out the hotplug event would have some *actual*
effect on the demotion order";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODRhNmU1ZDFmOTNiNTAzMmU1ZDZkZDJhMTgzZjhiM2YwMDg0MTZi;" But, given the expected paucity of hotplug
events, this should be fine.";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;mm/numa: automatically generate node migration order;Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"Patch series ""Migrate Pages in lieu of discard"", v11";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"We're starting to see systems with more and more kinds of memory such as
Intel's implementation of persistent memory";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;Let's say you have a system with some DRAM and some persistent memory;Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"Today, once DRAM fills up, reclaim will start and some of the DRAM
contents will be thrown out";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" Allocations will, at some point, start
falling over to the slower persistent memory";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;That has two nasty properties;Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" First, the newer allocations can end up in
the slower persistent memory";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" Second, reclaimed data in DRAM are just
discarded even if there are gobs of space in persistent memory that could
be used";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;This patchset implements a solution to these problems;Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" At the end of the
reclaim process in shrink_page_list() just before the last page refcount
is dropped, the page is migrated to persistent memory instead of being
dropped";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"While I've talked about a DRAM/PMEM pairing, this approach would function
in any environment where memory tiers exist";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;This is not perfect;Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" It ""strands"" pages in slower memory and never brings
them back to fast DRAM";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" Huang Ying has follow-on work which repurposes
NUMA balancing to promote hot pages back to DRAM";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"This is also all based on an upstream mechanism that allows persistent
memory to be onlined and used as if it were volatile";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"With that, the DRAM and PMEM in each socket will be represented as 2
separate NUMA nodes, with the CPUs sit in the DRAM node";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" So the
general inter-NUMA demotion mechanism introduced in the patchset can
migrate the cold DRAM pages to the PMEM node";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;We have tested the patchset with the postgresql and pgbench;Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" On a
2-socket server machine with DRAM and PMEM, the kernel with the patchset
can improve the score of pgbench up to 22.1% compared with that of the
DRAM only + disk case";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" This comes from the reduced disk read throughput
(which reduces up to 70.8%)";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"== Open Issues ==
   to DRAM can be demoted to PMEM whenever they opt in to this
   new mechanism";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" A cgroup-level API to opt-in or opt-out of
   these migrations will likely be required as a follow-on";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;   since it no longer necessarily involves I/O;Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" get_scan_count()
   for instance says: ""If we have no swap space, do not bother
   scanning anon pages""
This patch (of 9)";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"Prepare for the kernel to auto-migrate pages to other memory nodes with a
node migration table";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" This allows creating single migration target for
each NUMA node to enable the kernel to do NUMA page migrations instead of
simply discarding colder pages";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" A node with no target is a ""terminal
node"", so reclaim acts normally there";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" The migration target does not
fundamentally _need_ to be a single node, but this implementation starts
there to limit complexity";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"When memory fills up on a node, memory contents can be automatically
migrated to another node";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" The biggest problems are knowing when to
migrate and to where the migration should be targeted";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"The most straightforward way to generate the ""to where"" list would be to
follow the page allocator fallback lists";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" Those lists already tell us if
memory is full where to look next";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" It would also be logical to move
memory in that order";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"But, the allocator fallback lists have a fatal flaw: most nodes appear in
all the lists";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" This would potentially lead to migration cycles (A->B,
B->A, A->B, ...)";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"Instead of using the allocator fallback lists directly, keep a separate
node migration ordering";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" But, reuse the same data used to generate page
allocator fallback in the first place: find_next_best_node()";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"This means that the firmware data used to populate node distances
essentially dictates the ordering for now";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" It should also be
architecture-neutral since all NUMA architectures have a working
find_next_best_node()";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"RCU is used to allow lock-less read of node_demotion[] and prevent
demotion cycles been observed";Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" If multiple reads of node_demotion[] are
performed, a single rcu_read_lock() must be held over all reads to ensure
no cycles are observed";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0; Details are as follows;Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"=== What does RCU provide? ===
Imagine a simple loop which walks down the demotion path looking
for the last node";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;The initial values are;Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;and are updated to;Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;What guarantees that the cycle is not observed;Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"and would loop forever?
With RCU, a rcu_read_lock/unlock() can be placed around the loop";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" Since
the write side does a synchronize_rcu(), the loop that observed the old
contents is known to be complete before the synchronize_rcu() has
completed";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"RCU, combined with disable_all_migrate_targets(), ensures that the old
migration state is not visible by the time __set_migration_target_nodes()
is called";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"=== What does READ_ONCE() provide? ===
READ_ONCE() forbids the compiler from merging or reordering successive
reads of node_demotion[]";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" This ensures that any updates are *eventually*
observed";Dave Hansen;2021-09-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;Consider the above loop again;Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" The compiler could theoretically read the
entirety of node_demotion[] into local storage (registers) and never go
back to memory, and *permanently* observe bad values for node_demotion[]";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;"Note: RCU does not provide any universal compiler-ordering
guarantees";Dave Hansen;2021-09-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;This code is unused for now;Dave Hansen;2021-09-02;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMyOGE0MTY3MjI3ODI4M2ZhNzJlMDNkMGJmODBlNjY0NGQ0YWM0;" It will be called later in the
series.";Dave Hansen;2021-09-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNTkxNmMwMjU0MzJiN2M3NzZiNmJiMTM2MTc0ODVmYmMwYmQzZWJk;mm/migrate: fix NR_ISOLATED corruption on 64-bit;Aneesh Kumar K.V;2021-07-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTkxNmMwMjU0MzJiN2M3NzZiNmJiMTM2MTc0ODVmYmMwYmQzZWJk;"Similar to commit 2da9f6305f30 (""mm/vmscan: fix NR_ISOLATED_FILE
corruption on 64-bit"") avoid using unsigned int for nr_pages";Aneesh Kumar K.V;2021-07-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTkxNmMwMjU0MzJiN2M3NzZiNmJiMTM2MTc0ODVmYmMwYmQzZWJk;" With
unsigned int type the large unsigned int converts to a large positive
signed long";Aneesh Kumar K.V;2021-07-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNTkxNmMwMjU0MzJiN2M3NzZiNmJiMTM2MTc0ODVmYmMwYmQzZWJk;"Symptoms include CMA allocations hanging forever due to
alloc_contig_range->...->isolate_migratepages_block waiting forever in
""while (unlikely(too_many_isolated(pgdat)))"".";Aneesh Kumar K.V;2021-07-29;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OTc4OWRiMDNmZGQ3NzUxMGNmYjM1Y2I0YjNiZDUyYjZjNTBjOTAx;mm: Make copy_huge_page() always available;Matthew Wilcox (Oracle);2021-07-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OTc4OWRiMDNmZGQ3NzUxMGNmYjM1Y2I0YjNiZDUyYjZjNTBjOTAx;"Rewrite copy_huge_page() and move it into mm/util.c so it's always
available";Matthew Wilcox (Oracle);2021-07-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OTc4OWRiMDNmZGQ3NzUxMGNmYjM1Y2I0YjNiZDUyYjZjNTBjOTAx;" Fixes an exposure of uninitialised memory on configurations
with HUGETLB and UFFD enabled and MIGRATION disabled";Matthew Wilcox (Oracle);2021-07-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OTc4OWRiMDNmZGQ3NzUxMGNmYjM1Y2I0YjNiZDUyYjZjNTBjOTAx;"Fixes: 8cc5fcbb5be8 (""mm, hugetlb: fix racy resv_huge_pages underflow on UFFDIO_COPY"")";Matthew Wilcox (Oracle);2021-07-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YjQ5YmY2ZGRiYjBkNzk5MmM4MTY4NDZhY2ZhNWZkMWNmNzUxYzM2;mm: rename migrate_pgmap_owner;Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YjQ5YmY2ZGRiYjBkNzk5MmM4MTY4NDZhY2ZhNWZkMWNmNzUxYzM2;"MMU notifier ranges have a migrate_pgmap_owner field which is used by
drivers to store a pointer";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YjQ5YmY2ZGRiYjBkNzk5MmM4MTY4NDZhY2ZhNWZkMWNmNzUxYzM2;" This is subsequently used by the driver
callback to filter MMU_NOTIFY_MIGRATE events";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YjQ5YmY2ZGRiYjBkNzk5MmM4MTY4NDZhY2ZhNWZkMWNmNzUxYzM2;" Other notifier event types
can also benefit from this filtering, so rename the 'migrate_pgmap_owner'
field to 'owner' and create a new notifier initialisation function to
initialise this field.";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;mm/rmap: split migration into its own function;Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;"Migration is currently implemented as a mode of operation for
try_to_unmap_one() generally specified by passing the TTU_MIGRATION flag
or in the case of splitting a huge anonymous page TTU_SPLIT_FREEZE";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;"However it does not have much in common with the rest of the unmap
functionality of try_to_unmap_one() and thus splitting it into a separate
function reduces the complexity of try_to_unmap_one() making it more
readable";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;"Several simplifications can also be made in try_to_migrate_one() based on
the following observations";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5; - All users of TTU_MIGRATION also set TTU_IGNORE_MLOCK;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5; - No users of TTU_MIGRATION ever set TTU_IGNORE_HWPOISON;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5; - No users of TTU_MIGRATION ever set TTU_BATCH_FLUSH;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;"TTU_SPLIT_FREEZE is a special case of migration used when splitting an
anonymous page";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphOThhMmYwYzhjZTFiMjEzOGNiOGUzYWU0MTA0NDRkZWRjYzE0ODA5;" This is most easily dealt with by calling the correct
function from unmap_page() in mm/huge_memory.c - either try_to_migrate()
for PageAnon or try_to_unmap().";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo0ZGQ4NDViNWEzZTU3YWQwN2YyNmVmODA4NzA3YjA2NDY5NmZlMzRi;mm/swapops: rework swap entry manipulation code;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZGQ4NDViNWEzZTU3YWQwN2YyNmVmODA4NzA3YjA2NDY5NmZlMzRi;"Both migration and device private pages use special swap entries that are
manipluated by a range of inline functions";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ZGQ4NDViNWEzZTU3YWQwN2YyNmVmODA4NzA3YjA2NDY5NmZlMzRi;" The arguments to these are
somewhat inconsistent so rework them to remove flag type arguments and to
make the arguments similar for both read and write entry creation.";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;mm: remove special swap entry functions;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patch series ""Add support for SVM atomics in Nouveau"", v11";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Introduction
Some devices have features such as atomic PTE bits that can be used to
implement atomic access to system memory";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" To support atomic operations to
a shared virtual memory page such a device needs access to that page which
is exclusive of the CPU";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" This series introduces a mechanism to
temporarily unmap pages granting exclusive access to a device";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"These changes are required to support OpenCL atomic operations in Nouveau
to shared virtual memory (SVM) regions allocated with the
CL_MEM_SVM_ATOMICS clSVMAlloc flag";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" A more complete description of the
OpenCL SVM feature is available at
OpenCL_API.html#_shared_virtual_memory ";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Implementation
Exclusive device access is implemented by adding a new swap entry type
(SWAP_DEVICE_EXCLUSIVE) which is similar to a migration entry";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" The main
difference is that on fault the original entry is immediately restored by
the fault handler instead of waiting";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Restoring the entry triggers calls to MMU notifers which allows a device
driver to revoke the atomic access permission from the GPU prior to the
CPU finalising the entry";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patches
Patches 1 & 2 refactor existing migration and device private entry
functions";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patches 3 & 4 rework try_to_unmap_one() by splitting out unrelated
functionality into separate functions - try_to_migrate_one() and
try_to_munlock_one()";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;Patch 5 renames some existing code but does not introduce functionality;Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;Patch 6 is a small clean-up to swap entry handling in copy_pte_range();Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patch 7 contains the bulk of the implementation for device exclusive
memory";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patch 8 contains some additions to the HMM selftests to ensure everything
works as expected";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;Patch 9 is a cleanup for the Nouveau SVM implementation;Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Patch 10 contains the implementation of atomic access for the Nouveau
driver";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Testing
This has been tested with upstream Mesa 21.1.0 and a simple OpenCL program
which checks that GPU atomic accesses to system memory are atomic";Alistair Popple;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Without this series the test fails as there is no way of write-protecting
the page mapping which results in the device clobbering CPU writes";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" For
reference the test is available at
Further testing has been performed by adding support for testing exclusive
access to the hmm-tests kselftests";Alistair Popple;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;This patch (of 10);Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Remove multiple similar inline functions for dealing with different types
of special swap entries";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Both migration and device private swap entries use the swap offset to
store a pfn";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;" Instead of multiple inline functions to obtain a struct page
for each swap entry type use a common function pfn_swap_entry_to_page()";Alistair Popple;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZjVjZGFmODIyMzhmYjM2MzdhMGQwZmZmNDY3MGU1YmU3MWM2MTFj;"Also open-code the various entry_to_pfn() functions as this results is
shorter code that is easier to understand.";Alistair Popple;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo2NjJhZWVhNzUzNmQ4NGQ3ZTFkMDE3Mzk2OTRlNDc0OGJhMjk0Y2Uw;mm: migrate: check mapcount for THP instead of refcount;Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2NjJhZWVhNzUzNmQ4NGQ3ZTFkMDE3Mzk2OTRlNDc0OGJhMjk0Y2Uw;"The generic migration path will check refcount, so no need check refcount
here";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2NjJhZWVhNzUzNmQ4NGQ3ZTFkMDE3Mzk2OTRlNDc0OGJhMjk0Y2Uw;" But the old code actually prevents from migrating shared THP
(mapped by multiple processes), so bail out early if mapcount is > 1 to
keep the behavior.";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMGI1MTViZmIzZjRmM2RjMjA4ODYyOTg5ZTM4ZWU1MjY4YTEwMDNm;mm: migrate: don't split THP for misplaced NUMA page;Yang Shi;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMGI1MTViZmIzZjRmM2RjMjA4ODYyOTg5ZTM4ZWU1MjY4YTEwMDNm;"The old behavior didn't split THP if migration is failed due to lack of
memory on the target node";Yang Shi;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMGI1MTViZmIzZjRmM2RjMjA4ODYyOTg5ZTM4ZWU1MjY4YTEwMDNm;" But the THP migration does split THP, so keep
the old behavior for misplaced NUMA page migration.";Yang Shi;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNWZjNWMzYWUwYzg0OWM3MTNjNDI5MWFkZGI1ZmNlNjk5YWQwOTcy;mm: migrate: account THP NUMA migration counters correctly;Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNWZjNWMzYWUwYzg0OWM3MTNjNDI5MWFkZGI1ZmNlNjk5YWQwOTcy;"Now both base page and THP NUMA migration is done via
migrate_misplaced_page(), keep the counters correctly for THP.";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;mm: thp: refactor NUMA fault handling;Yang Shi;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"When the THP NUMA fault support was added THP migration was not supported
yet";Yang Shi;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx; So the ad hoc THP migration was implemented in NUMA fault handling;Yang Shi;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"Since v4.14 THP migration has been supported so it doesn't make too much
sense to still keep another THP migration implementation rather than using
the generic migration code";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"This patch reworks the NUMA fault handling to use generic migration
implementation to migrate misplaced page";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx; There is no functional change;Yang Shi;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"After the refactor the flow of NUMA fault handling looks just like its
PTE counterpart";Yang Shi;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"  Acquire ptl
  Prepare for migration (elevate page refcount)
  Release ptl
  Isolate page from lru and elevate page refcount
  Migrate the misplaced THP
If migration fails just restore the old normal PMD";Yang Shi;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"In the old code anon_vma lock was needed to serialize THP migration
against THP split, but since then the THP code has been reworked a lot, it
seems anon_vma lock is not required anymore to avoid the race";Yang Shi;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"The page refcount elevation when holding ptl should prevent from THP
split";Yang Shi;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNWI1YTNkZDJjMWZhNjEwNDliNzc4OWNlNTk2ZmFmZjRkNjU5YTYx;"Use migrate_misplaced_page() for both base page and THP NUMA hinting fault
and remove all the dead and duplicate code.";Yang Shi;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;mm: migrate: fix missing update page_private to hugetlb_page_subpool;Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;"Since commit d6995da31122 (""hugetlb: use page.private for hugetlb specific
page flags"") converts page.private for hugetlb specific page flags";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" We
should use hugetlb_page_subpool() to get the subpool pointer instead of
page_private()";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;This 'could' prevent the migration of hugetlb pages;Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" page_private(hpage)
is now used for hugetlb page specific flags";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" At migration time, the only
flag which could be set is HPageVmemmapOptimized";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" This flag will only be
set if the new vmemmap reduction feature is enabled";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" In addition,
!page_mapping() implies an anonymous mapping";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" So, this will prevent
migration of hugetb pages in anonymous mappings if the vmemmap reduction
feature is enabled";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;"In addition, that if statement checked for the rare race condition of a
page being migrated while in the process of being freed";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;" Since that check
is now wrong, we could leak hugetlb subpool usage counts";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj;The commit forgot to update it in the page migration routine;Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWNmYjViYTE1MGNmNzUwMDVjZTg1ZTBlMjVkNzllZjJmZWMyODdj; So fix it.;Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;mm, hugetlb: fix racy resv_huge_pages underflow on UFFDIO_COPY;Mina Almasry;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;"On UFFDIO_COPY, if we fail to copy the page contents while holding the
hugetlb_fault_mutex, we will drop the mutex and return to the caller after
allocating a page that consumed a reservation";Mina Almasry;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;" In this case there may be
a fault that double consumes the reservation";Mina Almasry;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;" To handle this, we free the
allocated page, fix the reservations, and allocate a temporary hugetlb
page and return that to the caller";Mina Almasry;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;" When the caller does the copy outside
of the lock, we again check the cache, and allocate a page consuming the
reservation, and copy over the contents";Mina Almasry;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;Test;Mina Almasry;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;"Hacked the code locally such that resv_huge_pages underflows produce
a warning and the copy_huge_page_from_user() always fails, then";Mina Almasry;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;"./tools/testing/selftests/vm/userfaultfd hugetlb_shared 10
        2 /tmp/kokonut_test/huge/userfaultfd_test && echo test success
./tools/testing/selftests/vm/userfaultfd hugetlb 10
	2 /tmp/kokonut_test/huge/userfaultfd_test && echo test success
Both tests succeed and produce no warnings";Mina Almasry;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2M1ZmNiYjViZTgxNGMxMTUwODU1NDliNzAwZTQ3MzY4NWIxMWU5;"After the
test runs number of free/resv hugepages is correct.";Mina Almasry;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;mm/hugetlb: change parameters of arch_make_huge_pte();Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"Patch series ""Subject: [PATCH v2 0/5] Implement huge VMAP and VMALLOC on powerpc 8xx"", v2";Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;This series implements huge VMAP and VMALLOC on powerpc 8xx;Christophe Leroy;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;Powerpc 8xx has 4 page sizes;Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"- 16k
- 512k
At the time being, vmalloc and vmap only support huge pages which are
leaf at PMD level";Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"Here the PMD level is 4M, it doesn't correspond to any supported
page size";Christophe Leroy;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"For now, implement use of 16k and 512k pages which is done
at PTE level";Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"Support of 8M pages will be implemented later, it requires use of
hugepd tables";Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;To allow this, the architecture provides two functions;Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"- arch_vmap_pte_range_map_size() which tells vmap_pte_range() what
page size to use";Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"A stub returning PAGE_SIZE is provided when the
architecture doesn't provide this function";Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;what page shift to use for a given area size;Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"A stub returning
PAGE_SHIFT is provided when the architecture doesn't provide this
function";Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;This patch (of 5);Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;At the time being, arch_make_huge_pte() has the following prototype;Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"  pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,
vma is used to get the pages shift or size";Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;vma is also used on Sparc to get vm_flags;Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;page is not used;Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;writable is not used;Christophe Leroy;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj;"In order to use this function without a vma, replace vma by shift and
flags";Christophe Leroy;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWMxYzU5NGY0OWE4OGZiYTk3NDRjYjVjODU5NzhjNmIxYjM2NWVj; Also remove the used parameters.;Christophe Leroy;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;mm: hugetlb: alloc the vmemmap pages associated with each HugeTLB page;Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;"When we free a HugeTLB page to the buddy allocator, we need to allocate
the vmemmap pages associated with it";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" However, we may not be able to
allocate the vmemmap pages when the system is under memory pressure";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" In
this case, we just refuse to free the HugeTLB page";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" This changes behavior
in some corner cases as listed below";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi; 1) Failing to free a huge page triggered by the user (decrease nr_pages);Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;    User needs to try again later;Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi; 2) Failing to free a surplus huge page when freed by the application;Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;    Try again later when freeing a huge page next time;Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" 3) Failing to dissolve a free huge page on ZONE_MOVABLE via
    offline_pages()";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;"    This can happen when we have plenty of ZONE_MOVABLE memory, but
    not enough kernel memory to allocate vmemmmap pages";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" We may even
    be able to migrate huge page contents, but will not be able to
    dissolve the source huge page";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" This will prevent an offline
    operation and is unfortunate as memory offlining is expected to
    succeed on movable zones";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" Users that depend on memory hotplug
    to succeed for movable zones should carefully consider whether the
    memory savings gained from this feature are worth the risk of
    possibly not being able to offline memory in certain situations";Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" 4) Failing to dissolve a huge page on CMA/ZONE_MOVABLE via
    alloc_contig_range() - once we have that handling in place";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;"Mainly
    affects CMA and virtio-mem";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;    Similar to 3);Muchun Song;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;virito-mem will handle migration errors gracefully;Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;"    CMA might be able to fallback on other free areas within the CMA
    region";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;Vmemmap pages are allocated from the page freeing context;Muchun Song;2021-07-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" In order for
those allocations to be not disruptive (e.g";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" trigger oom killer)
__GFP_NORETRY is used";Muchun Song;2021-07-01;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" hugetlb_lock is dropped for the allocation because
a non sleeping allocation would be too fragile and it could fail too
easily under memory pressure";Muchun Song;2021-07-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDJmYTM3MTdiNzQ5OTRhMjI1MTlkYmUwNDU3NTcxMzVkYjAwZGJi;" GFP_ATOMIC or other modes to access memory
reserves is not used because we want to prevent consuming reserves under
heavy hugetlb freeing.";Muchun Song;2021-07-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNTliOGI0ODc1YjNjMDQ2NzcwZTRmOWZiNTUzZWNlNDBiMjE3YjQw;mm/migrate: use vma_lookup() in do_pages_stat_array();Liam Howlett;2021-06-29;1;1
MDY6Q29tbWl0MjMyNTI5ODowNTliOGI0ODc1YjNjMDQ2NzcwZTRmOWZiNTUzZWNlNDBiMjE3YjQw;"will return NULL if the address is not within any VMA, the start address
no longer needs to be validated.";Liam Howlett;2021-06-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;mm, thp: use head page in __migration_entry_wait();Xu Yu;2021-06-16;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;"We notice that hung task happens in a corner but practical scenario when
CONFIG_PREEMPT_NONE is enabled, as follows";Xu Yu;2021-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;"Process 0                       Process 1                     Process 2..Inf
split_huge_page_to_list
    unmap_page
        split_huge_pmd_address
                                __migration_entry_wait(head)
                                                              __migration_entry_wait(tail)
    remap_page (roll back)
        remove_migration_ptes
            rmap_walk_anon
                cond_resched
Where __migration_entry_wait(tail) is occurred in kernel space, e.g.,
copy_to_user in fstat, which will immediately fault again without
rescheduling, and thus occupy the cpu fully";Xu Yu;2021-06-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;"When there are too many processes performing __migration_entry_wait on
tail page, remap_page will never be done after cond_resched";Xu Yu;2021-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;"This makes __migration_entry_wait operate on the compound head page,
thus waits for remap_page to complete, whether the THP is split
successfully or roll back";Xu Yu;2021-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;"Note that put_and_wait_on_page_locked helps to drop the page reference
acquired with get_page_unless_zero, as soon as the page is on the wait
queue, before actually waiting";Xu Yu;2021-06-16;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZmM5MGNiYjI5NzBhYjg4YjY2ZWE1MWRkNTgwNDY5ZWVkZTU3YjY3;" So splitting the THP is only prevented
for a brief interval.";Xu Yu;2021-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpmMDk1M2ExYmJhY2E3MWUxZWJiY2I5ODY0ZWIxYjI3MzE1NjE1N2Vk;mm: fix typos in comments;Ingo Molnar;2021-05-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpmMDk1M2ExYmJhY2E3MWUxZWJiY2I5ODY0ZWIxYjI3MzE1NjE1N2Vk;"Fix ~94 single-word typos in locking code comments, plus a few
very obvious grammar mistakes.";Ingo Molnar;2021-05-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YmMxYWVjNWUyODc2NWFkMTg3NDI4MjRiM2I5NzI0NzE4MDdhNjMy;mm: cma: add trace events for CMA alloc perf testing;Liam Mark;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YmMxYWVjNWUyODc2NWFkMTg3NDI4MjRiM2I5NzI0NzE4MDdhNjMy;"Add cma and migrate trace events to enable CMA allocation performance to
be measured via ftrace.";Liam Mark;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"Revert ""mm: migrate: skip shared exec THP for NUMA balancing""";Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;This reverts commit c77c5cbafe549eb330e8909861a3e16cbda2c848;Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"Since commit c77c5cbafe54 (""mm: migrate: skip shared exec THP for NUMA
balancing""), the NUMA balancing would skip shared exec transhuge page";Miaohe Lin;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;But this enhancement is not suitable for transhuge page;Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;" Because it's
required that page_mapcount() must be 1 due to no migration pte dance is
done here";Miaohe Lin;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;" On the other hand, the shared exec transhuge page will leave
the migrate_misplaced_page() with pte entry untouched and page locked";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"Thus pagefault for NUMA will be triggered again and deadlock occurs when
we start waiting for the page lock held by ourselves";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;Yang Shi said;Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;" ""Thanks for catching this";Miaohe Lin;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"By relooking the code I think the other
  important reason for removing this is
  migrate_misplaced_transhuge_page() actually can't see shared exec
  file THP at all since page_lock_anon_vma_read() is called before
  and if page is not anonymous page it will just restore the PMD
  without migrating anything";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"  The pages for private mapped file vma may be anonymous pages due to
  COW but they can't be THP so it won't trigger THP numa fault at all";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"I
  think this is why no bug was reported";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWU4MjBlZTcyMzg4Mjc5YTM3MDc3ZjQxOGUzMjY0M2EyOTgyNDNh;"I overlooked this in the first
  place.""";Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NDNlMWJlMTA4YjkxMzBlNWVjNWE3OGExNGY3N2RjMjM3YzgzZTFl;mm/migrate.c: use helper migrate_vma_collect_skip() in migrate_vma_collect_hole();Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDNlMWJlMTA4YjkxMzBlNWVjNWE3OGExNGY3N2RjMjM3YzgzZTFl;"It's more recommended to use helper function migrate_vma_collect_skip() to
skip the unexpected case and it also helps remove some duplicated codes";Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDNlMWJlMTA4YjkxMzBlNWVjNWE3OGExNGY3N2RjMjM3YzgzZTFl;"Move migrate_vma_collect_skip() above migrate_vma_collect_hole() to avoid
compiler warning.";Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODozNGY1ZTliOWQxOTkwZDI4NjE5OTA4NGVmYTc1MjUzMGVlM2Q4Mjk3;mm/migrate.c: fix potential indeterminate pte entry in migrate_vma_insert_page();Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODozNGY1ZTliOWQxOTkwZDI4NjE5OTA4NGVmYTc1MjUzMGVlM2Q4Mjk3;"If the zone device page does not belong to un-addressable device memory,
the variable entry will be uninitialized and lead to indeterminate pte
entry ultimately";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODozNGY1ZTliOWQxOTkwZDI4NjE5OTA4NGVmYTc1MjUzMGVlM2Q4Mjk3; Fix this unexpected case and warn about it.;Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODphMDQ4NDBjNjg0MWJiMjY2YzM4ZjUxYWRjODczMjUzMDhhYjhkNTc1;mm/migrate.c: remove unnecessary rc != MIGRATEPAGE_SUCCESS check in 'else' case;Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODphMDQ4NDBjNjg0MWJiMjY2YzM4ZjUxYWRjODczMjUzMDhhYjhkNTc1;"It's guaranteed that in the 'else' case of the rc == MIGRATEPAGE_SUCCESS
check, rc does not equal to MIGRATEPAGE_SUCCESS";Miaohe Lin;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODphMDQ4NDBjNjg0MWJiMjY2YzM4ZjUxYWRjODczMjUzMDhhYjhkNTc1;" Remove this unnecessary
check.";Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;mm/migrate.c: make putback_movable_page() static;Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;"Patch series ""Cleanup and fixup for mm/migrate.c"", v3";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;"This series contains cleanups to remove unnecessary VM_BUG_ON_PAGE and rc
!= MIGRATEPAGE_SUCCESS check";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;" Also use helper function to remove some
duplicated codes";Miaohe Lin;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;" What's more, this fixes potential deadlock in NUMA
balancing shared exec THP case and so on";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;" More details can be found in
the respective changelogs";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;This patch (of 5);Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;"The putback_movable_page() is just called by putback_movable_pages() and
we know the page is locked and both PageMovable() and PageIsolated() is
checked right before calling putback_movable_page()";Miaohe Lin;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MDZhNmY3MWEyNWFjY2ZjOTYwYTUwNjNjMjM3MTdmZjA3YWE0M2Ez;" So we make it static
and remove all the 3 VM_BUG_ON_PAGE().";Miaohe Lin;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODozNjFhMmEyMjlmYTMxYWI3ZjJiMjM2YjU5NDZlNDM0OTY0ZDAwNzYy;mm: replace migrate_[prep|finish] with lru_cache_[disable|enable];Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODozNjFhMmEyMjlmYTMxYWI3ZjJiMjM2YjU5NDZlNDM0OTY0ZDAwNzYy;"Currently, migrate_[prep|finish] is merely a wrapper of
lru_cache_[disable|enable]";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODozNjFhMmEyMjlmYTMxYWI3ZjJiMjM2YjU5NDZlNDM0OTY0ZDAwNzYy;" There is not much to gain from having
additional abstraction";Minchan Kim;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODozNjFhMmEyMjlmYTMxYWI3ZjJiMjM2YjU5NDZlNDM0OTY0ZDAwNzYy;"Use lru_cache_[disable|enable] instead of migrate_[prep|finish], which
would be more descriptive";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODozNjFhMmEyMjlmYTMxYWI3ZjJiMjM2YjU5NDZlNDM0OTY0ZDAwNzYy;"note: migrate_prep_local in compaction.c changed into lru_add_drain to
avoid CPU schedule cost with involving many other CPUs to keep old
behavior.";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;mm: disable LRU pagevec during the migration temporarily;Minchan Kim;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;LRU pagevec holds refcount of pages until the pagevec are drained;Minchan Kim;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" It
could prevent migration since the refcount of the page is greater than
the expection in migration logic";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" To mitigate the issue, callers of
migrate_pages drains LRU pagevec via migrate_prep or lru_add_drain_all
before migrate_pages call";Minchan Kim;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;"However, it's not enough because pages coming into pagevec after the
draining call still could stay at the pagevec so it could keep
preventing page migration";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" Since some callers of migrate_pages have
retrial logic with LRU draining, the page would migrate at next trail
but it is still fragile in that it doesn't close the fundamental race
between upcoming LRU pages into pagvec and migration so the migration
failure could cause contiguous memory allocation failure in the end";Minchan Kim;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;"To close the race, this patch disables lru caches(i.e, pagevec) during
ongoing migration until migrate is done";Minchan Kim;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;"Since it's really hard to reproduce, I measured how many times
migrate_pages retried with force mode(it is about a fallback to a sync
migration) with below debug code";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;"int migrate_pages(struct list_head *from, new_page_t get_new_page,
The test was repeating android apps launching with cma allocation in
background every five seconds";Minchan Kim;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" Total cma allocation count was about 500
during the testing";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" With this patch, the dump_page count was reduced
from 400 to 30";Minchan Kim;2021-05-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;"The new interface is also useful for memory hotplug which currently
drains lru pcp caches after each migration failure";Minchan Kim;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" This is rather
suboptimal as it has to disrupt others running during the operation";Minchan Kim;2021-05-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;With the new interface the operation happens only once;Minchan Kim;2021-05-05;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDc5OTYwZTQ0ZjI3ZTBlNTJiYTMxYjIxNzQwYjcwM2M1MzgwMjdj;" This is also in
line with pcp allocator cache which are disabled for the offlining as
well.";Minchan Kim;2021-05-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NDE3MmY0YmI3NTI0MjQ0MTU3NTYzNTFhNDBmOGRhNTcxNGUxNTU0;mm/page_alloc: combine __alloc_pages and __alloc_pages_nodemask;Matthew Wilcox (Oracle);2021-04-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDE3MmY0YmI3NTI0MjQ0MTU3NTYzNTFhNDBmOGRhNTcxNGUxNTU0;"There are only two callers of __alloc_pages() so prune the thicket of
alloc_page variants by combining the two functions together";Matthew Wilcox (Oracle);2021-04-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NDE3MmY0YmI3NTI0MjQ0MTU3NTYzNTFhNDBmOGRhNTcxNGUxNTU0;" Current
callers of __alloc_pages() simply add an extra 'NULL' parameter and
current callers of __alloc_pages_nodemask() call __alloc_pages() instead.";Matthew Wilcox (Oracle);2021-04-30;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;mm: memcg: add swapcache stat for memcg v2;Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;This patch adds swapcache stat for the cgroup v2;Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" The swapcache
represents the memory that is accounted against both the memory and the
swap limit of the cgroup";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" The main motivation behind exposing the
swapcache stat is for enabling users to gracefully migrate from cgroup
v1's memsw counter to cgroup v2's memory and swap counters";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;"Cgroup v1's memsw limit allows users to limit the memory+swap usage of a
workload but without control on the exact proportion of memory and swap";Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;"Cgroup v2 provides separate limits for memory and swap which enables more
control on the exact usage of memory and swap individually for the
workload";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;"With some little subtleties, the v1's memsw limit can be switched with the
sum of the v2's memory and swap limits";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" However the alternative for memsw
usage is not yet available in cgroup v2";Shakeel Butt;2021-02-24;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" Exposing per-cgroup swapcache
stat enables that alternative";Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" Adding the memory usage and swap usage and
subtracting the swapcache will approximate the memsw usage";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" This will
help in the transparent migration of the workloads depending on memsw
usage and limit to v2' memory and swap counters";Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;"The reasons these applications are still interested in this approximate
memsw usage are: (1) these applications are not really interested in two
separate memory and swap usage metrics";Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" A single usage metric is more
simple to use and reason about for them";Shakeel Butt;2021-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;"(2) The memsw usage metric hides the underlying system's swap setup from
the applications";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNjAzODk0MjQ4MGU1NzRjNjk3ZWExYTgwMDE5YmJlNTg2YzFkNjU0;" Applications with multiple instances running in a
datacenter with heterogeneous systems (some have swap and some don't) will
keep seeing a consistent view of their usage.";Shakeel Butt;2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ODA1NDYyNTk4MTEzZjM1MDgzOGQ2MTJkMDg5NWRiMmRiYjM5OTJi;mm/filemap: pass a sleep state to put_and_wait_on_page_locked;Matthew Wilcox (Oracle);2021-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ODA1NDYyNTk4MTEzZjM1MDgzOGQ2MTJkMDg5NWRiMmRiYjM5OTJi;"This is prep work for the next patch, but I think at least one of the
current callers would prefer a killable sleep to an uninterruptible one.";Matthew Wilcox (Oracle);2021-02-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MWE2NGY2MThiZTk1OTRjZDA2NDUxMDVjMDk4OTg1NWMwZjg2ZDkw;mm: migrate: do not migrate HugeTLB page whose refcount is one;Muchun Song;2021-02-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MWE2NGY2MThiZTk1OTRjZDA2NDUxMDVjMDk4OTg1NWMwZjg2ZDkw;"All pages isolated for the migration have an elevated reference count and
therefore seeing a reference count equal to 1 means that the last user of
the page has dropped the reference and the page has became unused and
there doesn't make much sense to migrate it anymore";Muchun Song;2021-02-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MWE2NGY2MThiZTk1OTRjZDA2NDUxMDVjMDk4OTg1NWMwZjg2ZDkw;"This has been done for regular pages and this patch does the same for
hugetlb pages";Muchun Song;2021-02-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MWE2NGY2MThiZTk1OTRjZDA2NDUxMDVjMDk4OTg1NWMwZjg2ZDkw;" Although the likelihood of the race is rather small for
hugetlb pages it makes sense the two code paths in sync.";Muchun Song;2021-02-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YzQ0N2QyNzRmMzc0NmZiZWQ2ZTY5NWU3YjlhMmQ3YmQ4YjMxYjcx;mm: fix numa stats for thp migration;Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzQ0N2QyNzRmMzc0NmZiZWQ2ZTY5NWU3YjlhMmQ3YmQ4YjMxYjcx;"Currently the kernel is not correctly updating the numa stats for
NR_FILE_PAGES and NR_SHMEM on THP migration";Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzQ0N2QyNzRmMzc0NmZiZWQ2ZTY5NWU3YjlhMmQ3YmQ4YjMxYjcx; Fix that;Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzQ0N2QyNzRmMzc0NmZiZWQ2ZTY5NWU3YjlhMmQ3YmQ4YjMxYjcx;"For NR_FILE_DIRTY and NR_ZONE_WRITE_PENDING, although at the moment
there is no need to handle THP migration as kernel still does not have
write support for file THP but to be more future proof, this patch adds
the THP support for those stats as well.";Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YTg3OTJmNjAwYWJhY2Q3ZTFiOWJiNjY3NzU5ZGNhMWMxNTNmNjRj;mm: memcg: fix memcg file_dirty numa stat;Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YTg3OTJmNjAwYWJhY2Q3ZTFiOWJiNjY3NzU5ZGNhMWMxNTNmNjRj;"The kernel updates the per-node NR_FILE_DIRTY stats on page migration
but not the memcg numa stats";Shakeel Butt;2021-01-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo4YTg3OTJmNjAwYWJhY2Q3ZTFiOWJiNjY3NzU5ZGNhMWMxNTNmNjRj;"That was not an issue until recently the commit 5f9a4f4a7096 (""mm";Shakeel Butt;2021-01-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo4YTg3OTJmNjAwYWJhY2Q3ZTFiOWJiNjY3NzU5ZGNhMWMxNTNmNjRj;"memcontrol: add the missing numa_stat interface for cgroup v2"") exposed
numa stats for the memcg";Shakeel Butt;2021-01-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTg3OTJmNjAwYWJhY2Q3ZTFiOWJiNjY3NzU5ZGNhMWMxNTNmNjRj;So fix the file_dirty per-memcg numa stat.;Shakeel Butt;2021-01-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTU4YjI0OTExMDRkN2YyNTRjZmYwNjk4NTA1MzkyNTgyZGJjMTNh;mm: fix some spelling mistakes in comments;Haitao Shi;2020-12-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTU4YjI0OTExMDRkN2YyNTRjZmYwNjk4NTA1MzkyNTgyZGJjMTNh;Fix some spelling mistakes in comments;Haitao Shi;2020-12-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTU4YjI0OTExMDRkN2YyNTRjZmYwNjk4NTA1MzkyNTgyZGJjMTNh;"	udpate ==> update
	succesful ==> successful
	exmaple ==> example
	unneccessary ==> unnecessary
	stoping ==> stopping
	uknown ==> unknown";Haitao Shi;2020-12-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpkODVjNmRiNGNjNjFiZDgyOTlmNjg1MzRiZjdlYTJmNzE3ZjQ5NTM5;mm: migrate: remove unused parameter in migrate_vma_insert_page();Stephen Zhang;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkODVjNmRiNGNjNjFiZDgyOTlmNjg1MzRiZjdlYTJmNzE3ZjQ5NTM5;"""dst"" parameter to migrate_vma_insert_page() is not used anymore.";Stephen Zhang;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;mm: migrate: return -ENOSYS if THP migration is unsupported;Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;"In the current implementation unmap_and_move() would return -ENOMEM if THP
migration is unsupported, then the THP will be split";Yang Shi;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;" If split is failed
just exit without trying to migrate other pages";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;" It doesn't make too much
sense since there may be enough free memory to migrate other pages and
there may be a lot base pages on the list";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;Return -ENOSYS to make consistent with hugetlb;Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;" And if THP split is
failed just skip and try other pages on the list";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNTMyZTJlNTdlM2M1M2NlNzRlNTE5YTA3ZDdkMjI0NDQ4MmI3YmQ4;Just skip the whole list and exit when free memory is really low.;Yang Shi;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODoyMzZjMzJlYjEwOTY5NjU5MGI3NDI4OTU3ZWRhNTBjYzA1ZTIyYWY4;mm: migrate: clean up migrate_prep{_local};Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMzZjMzJlYjEwOTY5NjU5MGI3NDI4OTU3ZWRhNTBjYzA1ZTIyYWY4;"The migrate_prep{_local} never fails, so it is pointless to have return
value and check the return value.";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;mm: migrate: skip shared exec THP for NUMA balancing;Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;The NUMA balancing skip shared exec base page;Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;" Since
CONFIG_READ_ONLY_THP_FOR_FS was introduced, there are probably shared exec
THP, so skip such THPs for NUMA balancing as well";Yang Shi;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;"And Willy's regular filesystem THP support patches could create shared
exec THP wven without that config";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;"In addition, the page_is_file_lru() is used to tell if the page is file
cache or not, but it filters out shmem page";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNzdjNWNiYWZlNTQ5ZWIzMzBlODkwOTg2MWEzZTE2Y2JkYTJjODQ4;" It sounds like a typical
usecase by putting executables in shmem to achieve performance gain via
using shmem-THP, so it sounds worth skipping migration for such case too.";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;mm: migrate: simplify the logic for handling permanent failure;Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;"!MIGRATEPAGE_SUCCESS, the page would be put back to LRU or proper list if
it is non-LRU movable page";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;" But, the callers always call
putback_movable_pages() to put the failed pages back later on, so it seems
not very efficient to put every single page back immediately, and the code
looks convoluted";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;"Put the failed page on a separate list, then splice the list to migrate
list when all pages are tried";Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;" It is the caller's responsibility to call
putback_movable_pages() to handle failures";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;" This also makes the code
simpler and more readable";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;After the change the rules are;Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm;"               back
The from list would be empty iff all pages are migrated successfully, it
was not so before";Yang Shi;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZDRhZTc4YTIxZmMwNWQ5MWQ4NDFlNDk5ZGRkZDA1N2FkNjRhNGRm; This has no impact to current existing callsites.;Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMTJiODk1MWFkMTdjZDg0NWM3ZTY3NGE4MzlhZjg0ODQ0OTU0NzA2;mm: truncate_complete_page() does not exist any more;Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMTJiODk1MWFkMTdjZDg0NWM3ZTY3NGE4MzlhZjg0ODQ0OTU0NzA2;"Patch series ""mm: misc migrate cleanup and improvement"", v3";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMTJiODk1MWFkMTdjZDg0NWM3ZTY3NGE4MzlhZjg0ODQ0OTU0NzA2;This patch (of 5);Yang Shi;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMTJiODk1MWFkMTdjZDg0NWM3ZTY3NGE4MzlhZjg0ODQ0OTU0NzA2;"The commit 9f4e41f4717832e (""mm: refactor truncate_complete_page()"")
refactored truncate_complete_page(), and it is not existed anymore,
correct the comment in vmscan and migrate to avoid confusion.";Yang Shi;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;mm/migrate.c: optimize migrate_vma_pages() mmu notifier;Ralph Campbell;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;"When migrating a zero page or pte_none() anonymous page to device private
memory, migrate_vma_setup() will initialize the src[] array with a NULL
PFN";Ralph Campbell;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;" This lets the device driver allocate device private memory and clear
it instead of DMAing a page of zeros over the device bus";Ralph Campbell;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;"Since the source page didn't exist at the time, no struct page was locked
nor a migration PTE inserted into the CPU page tables";Ralph Campbell;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;" The actual PTE
insertion happens in migrate_vma_pages() when it tries to insert the
device private struct page PTE into the CPU page tables";Ralph Campbell;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;"migrate_vma_pages() has to call the mmu notifiers again since another
device could fault on the same page before the page table locks are
acquired";Ralph Campbell;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTVkZGE4MWEwZGZiODJkZTE3NTdhYjg3OGQ5ZmZkMjMzOWM5YjJh;"Allow device drivers to optimize the invalidation similar to
migrate_vma_setup() by calling mmu_notifier_range_init() which sets struct
mmu_notifier_range event type to MMU_NOTIFY_MIGRATE and the
migrate_pgmap_owner field.";Ralph Campbell;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjlkZDRmOGExNjc1Yjg2YjY0YTdkMWY0MjFjMjUxODI4MTlmN2Ey;mm/migrate.c: fix comment spelling;Long Li;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODphYjlkZDRmOGExNjc1Yjg2YjY0YTdkMWY0MjFjMjUxODI4MTlmN2Ey;"The word in the comment is misspelled, it should be ""include"".";Long Li;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;mm/rmap: always do TTU_IGNORE_ACCESS;Shakeel Butt;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;"Since commit 369ea8242c0f (""mm/rmap: update to new mmu_notifier semantic
v2""), the code to check the secondary MMU's page table access bit is
broken for !(TTU_IGNORE_ACCESS) because the page is unmapped from the
secondary MMU's page table before the check";Shakeel Butt;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;" More specifically for those
secondary MMUs which unmap the memory in
mmu_notifier_invalidate_range_start() like kvm";Shakeel Butt;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;"However memory reclaim is the only user of !(TTU_IGNORE_ACCESS) or the
absence of TTU_IGNORE_ACCESS and it explicitly performs the page table
access check before trying to unmap the page";Shakeel Butt;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;" So, at worst the reclaim
will miss accesses in a very short window if we remove page table access
check in unmapping code";Shakeel Butt;2020-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;"There is an unintented consequence of !(TTU_IGNORE_ACCESS) for the memcg
reclaim";Shakeel Butt;2020-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;" From memcg reclaim the page_referenced() only account the
accesses from the processes which are in the same memcg of the target page
but the unmapping code is considering accesses from all the processes, so,
decreasing the effectiveness of memcg reclaim";Shakeel Butt;2020-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODowMTMzMzlkZjExNmMyZWUwZDc5NmRkOGJmYjhmMjkzYTIwMzBjMDYz;"The simplest solution is to always assume TTU_IGNORE_ACCESS in unmapping
code.";Shakeel Butt;2020-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;hugetlbfs: fix anon huge page migration race;Mike Kravetz;2020-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"Qian Cai reported the following BUG in [1]
  LTP: starting move_pages12
  BUG: unable to handle page fault for address: ffffffffffffffe0
  RIP: 0010:anon_vma_interval_tree_iter_first+0xa2/0x170 avc_start_pgoff at mm/interval_tree.c:63
  Hugh Dickins diagnosed this as a migration bug caused by code introduced
to use i_mmap_rwsem for pmd sharing synchronization";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;" Specifically, the
routine unmap_and_move_huge_page() is always passing the TTU_RMAP_LOCKED
flag to try_to_unmap() while holding i_mmap_rwsem";Mike Kravetz;2020-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;" This is wrong for
anon pages as the anon_vma_lock should be held in this case";Mike Kravetz;2020-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;" Further
analysis suggested that i_mmap_rwsem was not required to he held at all
when calling try_to_unmap for anon pages as an anon page could never be
part of a shared pmd mapping";Mike Kravetz;2020-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"Discussion also revealed that the hack in hugetlb_page_mapping_lock_write
to drop page lock and acquire i_mmap_rwsem is wrong";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;" There is no way to
keep mapping valid while dropping page lock";Mike Kravetz;2020-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;This patch does the following;Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;" - Do not take i_mmap_rwsem and set TTU_RMAP_LOCKED for anon pages when
   calling try_to_unmap";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3; - Remove the hacky code in hugetlb_page_mapping_lock_write;Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"The routine
   will now simply do a 'trylock' while still holding the page lock";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"If
   the trylock fails, it will return NULL";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"This could impact the
   callers";Mike Kravetz;2020-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"    - migration calling code will receive -EAGAIN and retry up to the
      hard coded limit (10)";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;    - memory error code will treat the page as BUSY;Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"This will force
      killing (SIGKILL) instead of SIGBUS any mapping tasks";Mike Kravetz;2020-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"   Do note that this change in behavior only happens when there is a
   race";Mike Kravetz;2020-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODozMzZiZjMwZWI3NjU4MGI1NzlkYzcxMWRlZDVkNTk5ZDkwNWMwMjE3;"None of the standard kernel testing suites actually hit this
   race, but it is possible.";Mike Kravetz;2020-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo0ZGMyMDBjZWUxOTUwYWMzZjliOTlmMGM4ZDRhNzUwYjYyOTU4Zjgx;mm/migrate: avoid possible unnecessary process right check in kernel_move_pages();Miaohe Lin;2020-10-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZGMyMDBjZWUxOTUwYWMzZjliOTlmMGM4ZDRhNzUwYjYyOTU4Zjgx;"There is no need to check if this process has the right to modify the
specified process when they are same";Miaohe Lin;2020-10-17;0;1
MDY6Q29tbWl0MjMyNTI5ODo0ZGMyMDBjZWUxOTUwYWMzZjliOTlmMGM4ZDRhNzUwYjYyOTU4Zjgx;" And we could also skip the security
hook call if a process is modifying its own pages";Miaohe Lin;2020-10-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZGMyMDBjZWUxOTUwYWMzZjliOTlmMGM4ZDRhNzUwYjYyOTU4Zjgx;" Add helper function to
handle these.";Miaohe Lin;2020-10-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;mm,hwpoison: rework soft offline for in-use pages;Oscar Salvador;2020-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;This patch changes the way we set and handle in-use poisoned pages;Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;" Until
now, poisoned pages were released to the buddy allocator, trusting that
the checks that take place at allocation time would act as a safe net and
would skip that page";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"This has proved to be wrong, as we got some pfn walkers out there, like
compaction, that all they care is the page to be in a buddy freelist";Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"Although this might not be the only user, having poisoned pages in the
buddy allocator seems a bad idea as we should only have free pages that
are ready and meant to be used as such";Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"Before explaining the taken approach, let us break down the kind of pages
we can soft offline";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"- Anonymous THP (after the split, they end up being 4K pages)
- Hugetlb
- Order-0 pages (that can be either migrated or invalited)
  - If they are clean and unmapped page cache pages, we invalidate
    then by means of invalidate_inode_page()";Oscar Salvador;2020-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;  - If they are mapped/dirty, we do the isolate-and-migrate dance;Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;Either way, do not call put_page directly from those paths;Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;" Instead, we
keep the page and send it to page_handle_poison to perform the right
handling";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;page_handle_poison sets the HWPoison flag and does the last put_page;Oscar Salvador;2020-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"Down the chain, we placed a check for HWPoison page in
free_pages_prepare, that just skips any poisoned page, so those pages
do not end up in any pcplist/freelist";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"After that, we set the refcount on the page to 1 and we increment
the poisoned pages counter";Oscar Salvador;2020-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"If we see that the check in free_pages_prepare creates trouble, we can
always do what we do for free pages";Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"  - wait until the page hits buddy's freelists
  - take it off, and flag it
The downside of the above approach is that we could race with an
allocation, so by the time we  want to take the page off the buddy, the
page has been already allocated so we cannot soft offline it";Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;But the user could always retry it;Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"  - We isolate-and-migrate them
After the migration has been successful, we call dissolve_free_huge_page,
and we set HWPoison on the page if we succeed";Oscar Salvador;2020-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;Hugetlb has a slightly different handling though;Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"While for non-hugetlb pages we cared about closing the race with an
allocation, doing so for hugetlb pages requires quite some additional
and intrusive code (we would need to hook in free_huge_page and some other
places)";Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"So I decided to not make the code overly complicated and just fail
normally if the page we allocated in the meantime";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;We can always build on top of this;Oscar Salvador;2020-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OWY1ZjhmYWI0ODJkZmZmNjI5NDgyMTQ0NjhhYzRlYmJmMGEwMTZm;"As a bonus, because of the way we handle now in-use pages, we no longer
need the put-as-isolation-migratetype dance, that was guarding for poisoned
pages to end up in pcplists.";Oscar Salvador;2020-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODpmMWY0ZjNhYjU0ZTlhNTJjNzYxMGM5OThmZjgyNTVmMDE5NzQyZTY3;mm/migrate: remove obsolete comment about device public;Ralph Campbell;2020-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpmMWY0ZjNhYjU0ZTlhNTJjNzYxMGM5OThmZjgyNTVmMDE5NzQyZTY3;"Device public memory never had an in tree consumer and was removed in
commit 25b2995a35b6 (""mm: remove MEMORY_DEVICE_PUBLIC support"")";Ralph Campbell;2020-10-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpmMWY0ZjNhYjU0ZTlhNTJjNzYxMGM5OThmZjgyNTVmMDE5NzQyZTY3;" Delete
the obsolete comment.";Ralph Campbell;2020-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MjU3ODg5MTI0Y2NlNDUyNmViZjI5MzI5YmFlODc5NGU5N2I0NTVh;mm/migrate: remove cpages-- in migrate_vma_finalize();Ralph Campbell;2020-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MjU3ODg5MTI0Y2NlNDUyNmViZjI5MzI5YmFlODc5NGU5N2I0NTVh;"The variable struct migrate_vma->cpages is only used in
migrate_vma_setup()";Ralph Campbell;2020-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MjU3ODg5MTI0Y2NlNDUyNmViZjI5MzI5YmFlODc5NGU5N2I0NTVh;" There is no need to decrement it in
migrate_vma_finalize() since it is never checked.";Ralph Campbell;2020-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzVjN2I5ZjMzNTI3MjhhZjQ5MDE4M2Y3MWQzNTBiYjY1OGZmYjc1;mm/migrate: correct thp migration stats;Zi Yan;2020-09-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzVjN2I5ZjMzNTI3MjhhZjQ5MDE4M2Y3MWQzNTBiYjY1OGZmYjc1;"PageTransHuge returns true for both thp and hugetlb, so thp stats was
counting both thp and hugetlb migrations";Zi Yan;2020-09-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzVjN2I5ZjMzNTI3MjhhZjQ5MDE4M2Y3MWQzNTBiYjY1OGZmYjc1;" Exclude hugetlb migration by
setting is_thp variable right";Zi Yan;2020-09-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YzVjN2I5ZjMzNTI3MjhhZjQ5MDE4M2Y3MWQzNTBiYjY1OGZmYjc1;Clean up thp handling code too when we are there;Zi Yan;2020-09-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzVjN2I5ZjMzNTI3MjhhZjQ5MDE4M2Y3MWQzNTBiYjY1OGZmYjc1;"Fixes: 1a5bae25e3cf (""mm/vmstat: add events for THP migration without split"")";Zi Yan;2020-09-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNTY3NTNhYzJhOTA4MTA3MjYzMzRkZjA0ZDczNWU5ZjhmNWEzMmQ5;bdi: replace BDI_CAP_NO_{WRITEBACK,ACCT_DIRTY} with a single flag;Christoph Hellwig;2020-09-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNTY3NTNhYzJhOTA4MTA3MjYzMzRkZjA0ZDczNWU5ZjhmNWEzMmQ5;"Replace the two negative flags that are always used together with a
single positive flag that indicates the writeback capability instead
of two related non-capabilities";Christoph Hellwig;2020-09-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNTY3NTNhYzJhOTA4MTA3MjYzMzRkZjA0ZDczNWU5ZjhmNWEzMmQ5;" Also remove the pointless wrappers
to just check the flag.";Christoph Hellwig;2020-09-24;1;1
MDY6Q29tbWl0MjMyNTI5ODphMzMzZTNlNzNiNjY0OGQzYmQzZWY2Yjk3MWE1OWE2MzYzYmZjZmM1;mm: migration of hugetlbfs page skip memcg;Hugh Dickins;2020-09-19;1;0
MDY6Q29tbWl0MjMyNTI5ODphMzMzZTNlNzNiNjY0OGQzYmQzZWY2Yjk3MWE1OWE2MzYzYmZjZmM1;"hugetlbfs pages do not participate in memcg: so although they do find most
of migrate_page_states() useful, it would be better if they did not call
into mem_cgroup_migrate() - where Qian Cai reported that LTP's
move_pages12 triggers the warning in Alex Shi's prospective commit
""mm/memcg: warning on !memcg after readahead page charged"".";Hugh Dickins;2020-09-19;1;1
MDY6Q29tbWl0MjMyNTI5ODozZDMyMWJmODJjNGJlOGUzMzI2MTc1NGE1Nzc1YmM2NWZjNWQyMTg0;mm/migrate: preserve soft dirty in remove_migration_pte();Ralph Campbell;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODozZDMyMWJmODJjNGJlOGUzMzI2MTc1NGE1Nzc1YmM2NWZjNWQyMTg0;"The code to remove a migration PTE and replace it with a device private
PTE was not copying the soft dirty bit from the migration entry";Ralph Campbell;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODozZDMyMWJmODJjNGJlOGUzMzI2MTc1NGE1Nzc1YmM2NWZjNWQyMTg0;" This
could lead to page contents not being marked dirty when faulting the page
back from device private memory.";Ralph Campbell;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk;mm/migrate: remove unnecessary is_zone_device_page() check;Ralph Campbell;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk;"Patch series ""mm/migrate: preserve soft dirty in remove_migration_pte()""";Ralph Campbell;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk;"I happened to notice this from code inspection after seeing Alistair
Popple's patch (""mm/rmap: Fixup copying of soft dirty and uffd ptes"")";Ralph Campbell;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk;This patch (of 2);Ralph Campbell;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk;"The check for is_zone_device_page() and is_device_private_page() is
unnecessary since the latter is sufficient to determine if the page is a
device private page";Ralph Campbell;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MTI4NzYzZmMzMjQ0ZDFiNDg2OGU1ZjBhYTQwMWY3Zjk4N2I1YzRk; Simplify the code for easier reading.;Ralph Campbell;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;mm/rmap: fixup copying of soft dirty and uffd ptes;Alistair Popple;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;"During memory migration a pte is temporarily replaced with a migration
swap pte";Alistair Popple;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;" Some pte bits from the existing mapping such as the soft-dirty
and uffd write-protect bits are preserved by copying these to the
temporary migration swap pte";Alistair Popple;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;"However these bits are not stored at the same location for swap and
non-swap ptes";Alistair Popple;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;" Therefore testing these bits requires using the
appropriate helper function for the given pte type";Alistair Popple;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;"Unfortunately several code locations were found where the wrong helper
function is being used to test soft_dirty and uffd_wp bits which leads to
them getting incorrectly set or cleared during page-migration";Alistair Popple;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;Fix these by using the correct tests based on pte type;Alistair Popple;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODphZDdkZjc2NGI3ZTFjN2RjNjRlMDE2ZGE3YWRhMmUzZTFiYjkwNzAw;"Fixes: a5430dda8a3a (""mm/migrate: support un-addressable ZONE_DEVICE page in migration"")
Fixes: 8c3328f1f36a (""mm/migrate: migrate_vma() unmap page from vma while collecting pages"")
Fixes: f45ec5ff16a7 (""userfaultfd: wp: support swap and page migration"")";Alistair Popple;2020-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmODMyMWVlZWI2MjNhZWQ2MGY3ZWQxNmY3NDQ1MzYzMjMwMTE4;mm/migrate: fixup setting UFFD_WP flag;Alistair Popple;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmODMyMWVlZWI2MjNhZWQ2MGY3ZWQxNmY3NDQ1MzYzMjMwMTE4;"Commit f45ec5ff16a75 (""userfaultfd: wp: support swap and page migration"")
introduced support for tracking the uffd wp bit during page migration";Alistair Popple;2020-09-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmODMyMWVlZWI2MjNhZWQ2MGY3ZWQxNmY3NDQ1MzYzMjMwMTE4;"However the non-swap PTE variant was used to set the flag for zone device
private pages which are a type of swap page";Alistair Popple;2020-09-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmODMyMWVlZWI2MjNhZWQ2MGY3ZWQxNmY3NDQ1MzYzMjMwMTE4;"This leads to corruption of the swap offset if the original PTE has the
uffd_wp flag set";Alistair Popple;2020-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmODMyMWVlZWI2MjNhZWQ2MGY3ZWQxNmY3NDQ1MzYzMjMwMTE4;"Fixes: f45ec5ff16a75 (""userfaultfd: wp: support swap and page migration"")";Alistair Popple;2020-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzM1Nzg0OGI0NGI0MDE2Y2E0MjIxNzhhYTM2OGE3NDcyMjQ1ZjZm;mm: replace hpage_nr_pages with thp_nr_pages;Matthew Wilcox (Oracle);2020-08-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzM1Nzg0OGI0NGI0MDE2Y2E0MjIxNzhhYTM2OGE3NDcyMjQ1ZjZm;"The thp prefix is more frequently used than hpage and we should be
consistent between the various functions.";Matthew Wilcox (Oracle);2020-08-15;1;1
MDY6Q29tbWl0MjMyNTI5ODphMDk3NjMxMTYwYzNiNzFhMTJkYjIyNGVjZTdkNzIyMDhjOWI2ODQ2;mm/mempolicy: use a standard migration target allocation callback;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODphMDk3NjMxMTYwYzNiNzFhMTJkYjIyNGVjZTdkNzIyMDhjOWI2ODQ2;There is a well-defined migration target allocation callback;Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODphMDk3NjMxMTYwYzNiNzFhMTJkYjIyNGVjZTdkNzIyMDhjOWI2ODQ2; Use it.;Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;mm/migrate: introduce a standard migration target allocation function;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;There are some similar functions for migration target allocation;Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" Since
there is no fundamental difference, it's better to keep just one rather
than keeping all variants";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" This patch implements base migration target
allocation function";Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" In the following patches, variants will be converted
to use this function";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;"Changes should be mechanical, but, unfortunately, there are some
differences";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" First, some callers' nodemask is assgined to NULL since NULL
nodemask will be considered as all available nodes, that is,
&node_states[N_MEMORY]";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" Second, for hugetlb page allocation, gfp_mask is
redefined as regular hugetlb allocation gfp_mask plus __GFP_THISNODE if
user provided gfp_mask has it";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" This is because future caller of this
function requires to set this node constaint";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;" Lastly, if provided nodeid
is NUMA_NO_NODE, nodeid is set up to the node where migration source
lives";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1; It helps to remove simple wrappers for setting up the nodeid;Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZjN2JlZDI1MmMxNmFjZTI5NDkxZTRjZmEyYmFmYjI2NGViNTA1;"Note that PageHighmem() call in previous function is changed to open-code
""is_highmem_idx()"" since it provides more readability.";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;mm/migrate: clear __GFP_RECLAIM to make the migration callback consistent with regular THP allocations;Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;"new_page_nodemask is a migration callback and it tries to use a common gfp
flags for the target page allocation whether it is a base page or a THP";Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;The later only adds GFP_TRANSHUGE to the given mask;Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;" This results in the
allocation being slightly more aggressive than necessary because the
resulting gfp mask will contain also __GFP_RECLAIM_KSWAPD";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;" THP
allocations usually exclude this flag to reduce over eager background
reclaim during a high THP allocation load which has been seen during large
mmaps initialization";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;" There is no indication that this is a problem for
migration as well but theoretically the same might happen when migrating
large mappings to a different node";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTMzYTBjOGE1Mzk2ZGY2ZGJhZWY4MDlhOWVlNGFkNjRlYmMzYWJl;" Make the migration callback
consistent with regular THP allocations.";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;mm/hugetlb: unify migration callbacks;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;"There is no difference between two migration callback functions,
alloc_huge_page_node() and alloc_huge_page_nodemask(), except
__GFP_THISNODE handling";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;" It's redundant to have two almost similar
functions in order to handle this flag";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;" So, this patch tries to remove
one by introducing a new argument, gfp_mask, to
alloc_huge_page_nodemask()";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;"After introducing gfp_mask argument, it's caller's job to provide correct
gfp_mask";Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;" So, every callsites for alloc_huge_page_nodemask() are changed
to provide gfp_mask";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOTJiYmMyNzE5YmQyYmUyMzdlZTMzNjExM2I2MzQ5MmE2YmFjYTNi;"Note that it's safe to remove a node id check in alloc_huge_page_node()
since there is no caller passing NUMA_NO_NODE as a node id.";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNGIzODIyMzhlZDJmOTRmMGQzODYwZjkxMjBiNjY0MDRmYTk5NDYz;mm/migrate: move migration helper from .h to .c;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNGIzODIyMzhlZDJmOTRmMGQzODYwZjkxMjBiNjY0MDRmYTk5NDYz;It's not performance sensitive function;Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNGIzODIyMzhlZDJmOTRmMGQzODYwZjkxMjBiNjY0MDRmYTk5NDYz; Move it to .c;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNGIzODIyMzhlZDJmOTRmMGQzODYwZjkxMjBiNjY0MDRmYTk5NDYz;" This is a
preparation step for future change.";Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODplYWY0NDRkZWVkNDgyZjkwODM5NWRlZmE0ZDliNjdjZTU1YjcyMjA4;mm/migrate.c: delete duplicated word;Randy Dunlap;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODplYWY0NDRkZWVkNDgyZjkwODM5NWRlZmE0ZDliNjdjZTU1YjcyMjA4;"Drop the repeated word ""and"".";Randy Dunlap;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;mm/vmstat: add events for THP migration without split;Anshuman Khandual;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;"Add following new vmstat events which will help in validating THP
migration without split";Anshuman Khandual;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;" Statistics reported through these new VM events
will help in performance debugging";Anshuman Khandual;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;1;Anshuman Khandual;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;"THP_MIGRATION_SUCCESS
2";Anshuman Khandual;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;"THP_MIGRATION_FAILURE
3";Anshuman Khandual;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;"THP_MIGRATION_SPLIT
In addition, these new events also update normal page migration statistics
appropriately via PGMIGRATE_SUCCESS and PGMIGRATE_FAILURE";Anshuman Khandual;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYTViYWUyNWUzY2Y5NWM0ZTgzYTk3Zjg3YTZiNTI4MGQ5YWNiYjIy;" While here,
this updates current trace event 'mm_migrate_pages' to accommodate now
available THP statistics.";Anshuman Khandual;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;mm/migrate: optimize migrate_vma_setup() for holes;Ralph Campbell;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;"Patch series ""mm/migrate: optimize migrate_vma_setup() for holes""";Ralph Campbell;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;"A simple optimization for migrate_vma_*() when the source vma is not an
anonymous vma and a new test case to exercise it";Ralph Campbell;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;This patch (of 2);Ralph Campbell;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;"When migrating system memory to device private memory, if the source
address range is a valid VMA range and there is no memory or a zero page,
the source PFN array is marked as valid but with no PFN";Ralph Campbell;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;"This lets the device driver allocate private memory and clear it, then
insert the new device private struct page into the CPU's page tables when
migrate_vma_pages() is called";Ralph Campbell;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;" migrate_vma_pages() only inserts the new
page if the VMA is an anonymous range";Ralph Campbell;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;"There is no point in telling the device driver to allocate device private
memory and then not migrate the page";Ralph Campbell;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzQ0ZjI4MDdhNGZmYjExMWEzMWMwNzY5ODc4ZWM4YjEyNWI1MjQx;" Instead, mark the source PFN array
entries as not migrating to avoid this overhead.";Ralph Campbell;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;mm/vmscan: protect the workingset on anonymous LRU;Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"In current implementation, newly created or swap-in anonymous page is
started on active list";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;" Growing active list results in rebalancing
active/inactive list so old pages on active list are demoted to inactive
list";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl; Hence, the page on active list isn't protected at all;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;Following is an example of this situation;Joonsoo Kim;2020-08-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;Assume that 50 hot pages on active list;Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;" Numbers denote the number of
pages on active/inactive list (active | inactive)";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;1;Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"50 hot pages on active list
50(h) | 0
2";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"workload: 50 newly created (used-once) pages
50(uo) | 50(h)
3";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"workload: another 50 newly created (used-once) pages
50(uo) | 50(uo), swap-out 50(h)
This patch tries to fix this issue";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;" Like as file LRU, newly created or
swap-in anonymous pages will be inserted to the inactive list";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;" They are
promoted to active list if enough reference happens";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;" This simple
modification changes the above example as following";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;1;Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"50 hot pages on active list
50(h) | 0
2";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"workload: 50 newly created (used-once) pages
50(h) | 50(uo)
3";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"workload: another 50 newly created (used-once) pages
50(h) | 50(uo), swap-out 50(uo)
As you can see, hot pages on active list would be protected";Joonsoo Kim;2020-08-12;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"Note that, this implementation has a drawback that the page cannot be
promoted and will be swapped-out if re-access interval is greater than the
size of inactive list but less than the size of total(active+inactive)";Joonsoo Kim;2020-08-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNTE4MTU0ZTU5YWFiM2FkMDc4MGExNjljNWNjODRiZDRlZTQzNTdl;"To solve this potential issue, following patch will apply workingset
detection similar to the one that's already applied to file LRU.";Joonsoo Kim;2020-08-12;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMWEwNmRmNmViZjZjYTk4ZmI3YTY3MmZlNDQ3Yzc0NjlkNmMxOTY4;mm/migrate: fix migrate_pgmap_owner w/o CONFIG_MMU_NOTIFIER;Ralph Campbell;2020-08-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMWEwNmRmNmViZjZjYTk4ZmI3YTY3MmZlNDQ3Yzc0NjlkNmMxOTY4;"On x86_64, when CONFIG_MMU_NOTIFIER is not set/enabled, there is a
compiler error";Ralph Campbell;2020-08-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMWEwNmRmNmViZjZjYTk4ZmI3YTY3MmZlNDQ3Yzc0NjlkNmMxOTY4;   mm/migrate.c: In function 'migrate_vma_collect';Ralph Campbell;2020-08-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMWEwNmRmNmViZjZjYTk4ZmI3YTY3MmZlNDQ3Yzc0NjlkNmMxOTY4;   mm/migrate.c:2481:7: error: 'struct mmu_notifier_range' has no member named 'migrate_pgmap_owner';Ralph Campbell;2020-08-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5OTg0MjdiM2FkMmM3NjkwODI4NTM4ODBjZjM1MzU1N2VjMGVjNzdk;mm/notifier: add migration invalidation type;Ralph Campbell;2020-07-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo5OTg0MjdiM2FkMmM3NjkwODI4NTM4ODBjZjM1MzU1N2VjMGVjNzdk;"which flushes all device private page mappings whether or not a page is
being migrated to/from device private memory";Ralph Campbell;2020-07-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTg0MjdiM2FkMmM3NjkwODI4NTM4ODBjZjM1MzU1N2VjMGVjNzdk;"In order to not disrupt device mappings that are not being migrated, shift
the responsibility for clearing device private mappings to the device
driver and leave CPU page table unmapping handled by
migrate_vma_setup()";Ralph Campbell;2020-07-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTg0MjdiM2FkMmM3NjkwODI4NTM4ODBjZjM1MzU1N2VjMGVjNzdk;"To support this, the caller of migrate_vma_setup() should always set
struct migrate_vma::pgmap_owner to a non NULL value that matches the
device private page->pgmap->owner";Ralph Campbell;2020-07-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OTg0MjdiM2FkMmM3NjkwODI4NTM4ODBjZjM1MzU1N2VjMGVjNzdk;"This value is then passed to the struct
mmu_notifier_range with a new event type which the driver's invalidation
function can use to avoid device MMU invalidations.";Ralph Campbell;2020-07-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MTQzMTkyY2Q0MTBjNGZjODNiZTA5YTJlNzM0MjM3NjVhZWUwNzJi;mm/migrate: add a flags parameter to migrate_vma;Ralph Campbell;2020-07-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MTQzMTkyY2Q0MTBjNGZjODNiZTA5YTJlNzM0MjM3NjVhZWUwNzJi;"The src_owner field in struct migrate_vma is being used for two purposes,
it acts as a selection filter for which types of pages are to be migrated
and it identifies device private pages owned by the caller";Ralph Campbell;2020-07-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1MTQzMTkyY2Q0MTBjNGZjODNiZTA5YTJlNzM0MjM3NjVhZWUwNzJi;"Split this into separate parameters so the src_owner field can be used
just to identify device private pages owned by the caller of
migrate_vma_setup()";Ralph Campbell;2020-07-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MTQzMTkyY2Q0MTBjNGZjODNiZTA5YTJlNzM0MjM3NjVhZWUwNzJi;"Rename the src_owner field to pgmap_owner to reflect it is now used only
to identify which device private pages to migrate.";Ralph Campbell;2020-07-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;Raise gcc version requirement to 4.9;Linus Torvalds;2020-07-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"I realize that we fairly recently raised it to 4.8, but the fact is, 4.9
is a much better minimum version to target";Linus Torvalds;2020-07-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"We have a number of workarounds for actual bugs in pre-4.9 gcc versions
(including things like internal compiler errors on ARM), but we also
have some syntactic workarounds for lacking features";Linus Torvalds;2020-07-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"In particular, raising the minimum to 4.9 means that we can now just
assume _Generic() exists, which is likely the much better replacement
for a lot of very convoluted built-time magic with conditionals on
sizeof and/or __builtin_choose_expr() with same_type() etc";Linus Torvalds;2020-07-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"Using _Generic also means that you will need to have a very recent
version of 'sparse', but thats easy to build yourself, and much less of
a hassle than some old gcc version can be";Linus Torvalds;2020-07-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"The latest (in a long string) of reasons for minimum compiler version
upgrades was commit 5435f73d5c4a (""efi/x86: Fix build with gcc 4"")";Linus Torvalds;2020-07-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"Ard points out that RHEL 7 uses gcc-4.8, but the people who stay back on
old RHEL versions persumably also don't build their own kernels anyway";Linus Torvalds;2020-07-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZWM0NDc2YWM4MjUxMmYwOWM5NGFmZjU5NzI2NTRiNzBmMzc3MmIy;"And maybe they should cross-built or just have a little side affair with
a newer compiler?";Linus Torvalds;2020-07-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMWU4ZDdjNmE3YTY4MmUxNDA1ZTNlMjQyZDMyZmMzNzdmZDE5NmZm;mmap locking API: convert mmap_sem comments;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMWU4ZDdjNmE3YTY4MmUxNDA1ZTNlMjQyZDMyZmMzNzdmZDE5NmZm;Convert comments that reference mmap_sem to reference mmap_lock instead.;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODozZTRlMjhjNWE4ZjAxZWU0MTc0ZDYzOWUzNmVkMTU1YWRlNDg5YTZm;mmap locking API: convert mmap_sem API comments;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODozZTRlMjhjNWE4ZjAxZWU0MTc0ZDYzOWUzNmVkMTU1YWRlNDg5YTZm;"Convert comments that reference old mmap_sem APIs to reference
corresponding new mmap locking APIs instead.";Michel Lespinasse;2020-06-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkOGVkNDVjNWRjZDQ1NWZjNTg0OGQ0N2Y4Njg4M2ExYjg3MmFjMGQw;mmap locking API: use coccinelle to convert mmap_sem rwsem call sites;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOGVkNDVjNWRjZDQ1NWZjNTg0OGQ0N2Y4Njg4M2ExYjg3MmFjMGQw;"This change converts the existing mmap_sem rwsem calls to use the new mmap
locking API instead";Michel Lespinasse;2020-06-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkOGVkNDVjNWRjZDQ1NWZjNTg0OGQ0N2Y4Njg4M2ExYjg3MmFjMGQw;The change is generated using coccinelle with the following rule;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOGVkNDVjNWRjZDQ1NWZjNTg0OGQ0N2Y4Njg4M2ExYjg3MmFjMGQw;// spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir ;Michel Lespinasse;2020-06-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOGVkNDVjNWRjZDQ1NWZjNTg0OGQ0N2Y4Njg4M2ExYjg3MmFjMGQw;"-init_rwsem
+mmap_init_lock
-down_write
+mmap_write_lock
-down_write_killable
+mmap_write_lock_killable
-down_write_trylock
+mmap_write_trylock
-up_write
+mmap_write_unlock
-downgrade_write
+mmap_write_downgrade
-down_read
+mmap_read_lock
-down_read_killable
+mmap_read_lock_killable
-down_read_trylock
+mmap_read_trylock
-up_read
+mmap_read_unlock
-(&mm->mmap_sem)
+(mm)";Michel Lespinasse;2020-06-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkOWViMWVhMmJmODczNGFmZDhlYzdkOTk1MjcwNDM3YTcyNDJmODJi;mm: memcontrol: delete unused lrucare handling;Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpkOWViMWVhMmJmODczNGFmZDhlYzdkOTk1MjcwNDM3YTcyNDJmODJi;"Swapin faults were the last event to charge pages after they had already
been put on the LRU list";Johannes Weiner;2020-06-03;0;0
MDY6Q29tbWl0MjMyNTI5ODpkOWViMWVhMmJmODczNGFmZDhlYzdkOTk1MjcwNDM3YTcyNDJmODJi;" Now that we charge directly on swapin, the
lrucare portion of the charge code is unused.";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDgyYzY5NDM4ZDBkZmY4ODA5MDYxZWRiY2NlNDNhNWE0YmNmMDlm;mm: memcontrol: convert anon and file-thp to new mem_cgroup_charge() API;Johannes Weiner;2020-06-03;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZDgyYzY5NDM4ZDBkZmY4ODA5MDYxZWRiY2NlNDNhNWE0YmNmMDlm;"With the page->mapping requirement gone from memcg, we can charge anon and
file-thp pages in one single step, right after they're allocated";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDgyYzY5NDM4ZDBkZmY4ODA5MDYxZWRiY2NlNDNhNWE0YmNmMDlm;"This removes two out of three API calls - especially the tricky commit
step that needed to happen at just the right time between when the page is
""set up"" and when it's ""published"" - somewhat vague and fluid concepts
that varied by page type";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDgyYzY5NDM4ZDBkZmY4ODA5MDYxZWRiY2NlNDNhNWE0YmNmMDlm;" All we need is a freshly allocated page and a
memcg context to charge";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDgyYzY5NDM4ZDBkZmY4ODA5MDYxZWRiY2NlNDNhNWE0YmNmMDlm;v2: prevent double charges on pre-allocated hugepages in khugepaged;Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;mm: memcontrol: switch to native NR_ANON_MAPPED counter;Johannes Weiner;2020-06-03;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;Memcg maintains a private MEMCG_RSS counter;Johannes Weiner;2020-06-03;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;" This divergence from the
generic VM accounting means unnecessary code overhead, and creates a
dependency for memcg that page->mapping is set up at the time of charging,
so that page types can be told apart";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;"Convert the generic accounting sites to mod_lruvec_page_state and friends
to maintain the per-cgroup vmstat counter of NR_ANON_MAPPED";Johannes Weiner;2020-06-03;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;" We use
lock_page_memcg() to stabilize page->mem_cgroup during rmap changes, the
same way we do for NR_FILE_MAPPED";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;"With the previous patch removing MEMCG_CACHE and the private NR_SHMEM
counter, this patch finally eliminates the need to have page->mapping set
up at charge time";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;" However, we need to have page->mem_cgroup set up by
the time rmap runs and does the accounting, so switch the commit and the
rmap callbacks around";Johannes Weiner;2020-06-03;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZTVkMGE3NGM2MmQ4ZGE0M2Y5NTI2YTViMDhjZGQxOGUyYmJjMzdh;v2: fix temporary accounting bug by switching rmap<->commit (Joonsoo);Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;mm: memcontrol: switch to native NR_FILE_PAGES and NR_SHMEM counters;Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;Memcg maintains private MEMCG_CACHE and NR_SHMEM counters;Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;" This
divergence from the generic VM accounting means unnecessary code overhead,
and creates a dependency for memcg that page->mapping is set up at the
time of charging, so that page types can be told apart";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;"Convert the generic accounting sites to mod_lruvec_page_state and friends
to maintain the per-cgroup vmstat counters of NR_FILE_PAGES and NR_SHMEM";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;"we only need minimal tweaks of two mem_cgroup_migrate() calls to ensure
it's set up in time";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODowZDFjMjA3MjJhYjMzM2FjMGFjMDNhZTIxODg5MjJjMTAyMWQzYWJj;"Then replace MEMCG_CACHE with NR_FILE_PAGES and delete the private
NR_SHMEM accounting sites.";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;mm: memcontrol: drop @compound parameter from memcg charging API;Johannes Weiner;2020-06-03;1;0
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;"The memcg charging API carries a boolean @compound parameter that tells
whether the page we're dealing with is a hugepage";Johannes Weiner;2020-06-03;0;0
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;"mem_cgroup_commit_charge() has another boolean @lrucare that indicates
whether the page needs LRU locking or not while charging";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;" The majority of
callsites know those parameters at compile time, which results in a lot of
naked ""false, false"" argument lists";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;" This makes for cryptic code and is a
breeding ground for subtle mistakes";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;"Thankfully, the huge page state can be inferred from the page itself and
doesn't need to be passed along";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;" This is safe because charging completes
before the page is published and somebody may split it";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;"Simplify the callsites by removing @compound, and let memcg infer the
state by using hpage_nr_pages() unconditionally";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;" That function does
PageTransHuge() to identify huge pages, which also helpfully asserts that
nobody passes in tail pages by accident";Johannes Weiner;2020-06-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmJhNjlhNTZlMTZlOGRjZjE4MmZlNmNhNzc3MzVkZDY1YTg5OGFh;"The following patches will introduce a new charging API, best not to carry
over unnecessary weight.";Johannes Weiner;2020-06-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTY2MzkxZmE1NzZlMWZiMjcwMWJlOGJjY2ExOTdkOGY3MjczN2I3;mm/migrate.c: attach_page_private already does the get_page;Hugh Dickins;2020-06-02;1;0
MDY6Q29tbWl0MjMyNTI5ODoxOTY2MzkxZmE1NzZlMWZiMjcwMWJlOGJjY2ExOTdkOGY3MjczN2I3;"Just finished bisecting mmotm, to find why a test which used to take
four minutes now took more than an hour: the __buffer_migrate_page()
cleanup left behind a get_page() which attach_page_private() now does";Hugh Dickins;2020-06-02;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTY2MzkxZmE1NzZlMWZiMjcwMWJlOGJjY2ExOTdkOGY3MjczN2I3;"Fixes: cd0f37154443 (""mm/migrate.c: call detach_page_private to cleanup code"")";Hugh Dickins;2020-06-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZDBmMzcxNTQ0NDM4NDQyNTY3MDlmNzM2NzU0YjFiYWNlNWIyNGQ4;mm/migrate.c: call detach_page_private to cleanup code;Guoqing Jiang;2020-06-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZDBmMzcxNTQ0NDM4NDQyNTY3MDlmNzM2NzU0YjFiYWNlNWIyNGQ4;We can cleanup code a little by call detach_page_private here.;Guoqing Jiang;2020-06-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM4ODM0MGFlMGJjODM5N2VmNWIyNDM0MjI3OWY3NzM5OTgyOTE4;fs: convert mpage_readpages to mpage_readahead;Matthew Wilcox (Oracle);2020-06-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDM4ODM0MGFlMGJjODM5N2VmNWIyNDM0MjI3OWY3NzM5OTgyOTE4;"Implement the new readahead aop and convert all callers (block_dev,
exfat, ext2, fat, gfs2, hpfs, isofs, jfs, nilfs2, ocfs2, omfs, qnx6,
reiserfs & udf)";Matthew Wilcox (Oracle);2020-06-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDM4ODM0MGFlMGJjODM5N2VmNWIyNDM0MjI3OWY3NzM5OTgyOTE4;The callers are all trivial except for GFS2 & OCFS2.;Matthew Wilcox (Oracle);2020-06-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;userfaultfd: wp: support swap and page migration;Peter Xu;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;"For either swap and page migration, we all use the bit 2 of the entry to
identify whether this entry is uffd write-protected";Peter Xu;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;" It plays a similar
role as the existing soft dirty bit in swap entries but only for keeping
the uffd-wp tracking for a specific PTE/PMD";Peter Xu;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;"Something special here is that when we want to recover the uffd-wp bit
from a swap/migration entry to the PTE bit we'll also need to take care of
the _PAGE_RW bit and make sure it's cleared, otherwise even with the
_PAGE_UFFD_WP bit we can't trap it at all";Peter Xu;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;In change_pte_range() we do nothing for uffd if the PTE is a swap entry;Peter Xu;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;"That can lead to data mismatch if the page that we are going to write
protect is swapped out when sending the UFFDIO_WRITEPROTECT";Peter Xu;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNDVlYzVmZjE2YTc1Zjk2ZGFjOGM4OTg2MmQ3NWYxZDg3MzllZmQ0;" This patch
also applies/removes the uffd-wp bit even for the swap entries.";Peter Xu;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZGU0ZjIyYTYwZjczMTk0M2YwNTBmNDQ0OGJmMjkzM2VkM2ZhNzBi;mm: code cleanup for MADV_FREE;Huang Ying;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZGU0ZjIyYTYwZjczMTk0M2YwNTBmNDQ0OGJmMjkzM2VkM2ZhNzBi;"Some comments for MADV_FREE is revised and added to help people understand
the MADV_FREE code, especially the page flag, PG_swapbacked";Huang Ying;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZGU0ZjIyYTYwZjczMTk0M2YwNTBmNDQ0OGJmMjkzM2VkM2ZhNzBi;" This makes
page_is_file_cache() isn't consistent with its comments";Huang Ying;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ZGU0ZjIyYTYwZjczMTk0M2YwNTBmNDQ0OGJmMjkzM2VkM2ZhNzBi;" So the function
is renamed to page_is_file_lru() to make them consistent again";Huang Ying;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZGU0ZjIyYTYwZjczMTk0M2YwNTBmNDQ0OGJmMjkzM2VkM2ZhNzBi;" All these
are put in one patch as one logical change.";Huang Ying;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YWVmZjI0MWZlNmM0NTYxYTg0MmIzNDRjMWNhMTRhNzAwZWM4NDQx;mm/migrate.c: migrate PG_readahead flag;Yang Shi;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWVmZjI0MWZlNmM0NTYxYTg0MmIzNDRjMWNhMTRhNzAwZWM4NDQx;Currently the migration code doesn't migrate PG_readahead flag;Yang Shi;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWVmZjI0MWZlNmM0NTYxYTg0MmIzNDRjMWNhMTRhNzAwZWM4NDQx;"Theoretically this would incur slight performance loss as the application
might have to ramp its readahead back up again";Yang Shi;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWVmZjI0MWZlNmM0NTYxYTg0MmIzNDRjMWNhMTRhNzAwZWM4NDQx;" Even though such problem
happens, it might be hidden by something else since migration is typically
triggered by compaction and NUMA balancing, any of which should be more
noticeable";Yang Shi;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWVmZjI0MWZlNmM0NTYxYTg0MmIzNDRjMWNhMTRhNzAwZWM4NDQx;"Migrate the flag after end_page_writeback() since it may clear PG_reclaim
flag, which is the same bit as PG_readahead, for the new page.";Yang Shi;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;"mm/migrate.c: unify ""not queued for migration"" handling in do_pages_move()";Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;It can currently happen that we store the status of a page twice;Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;  target node is contained in the current interval;Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;"Let's simplify the code and always call do_move_pages_to_node() in case we
did not queue a page for migration";Wei Yang;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;" Note that pages that are already on
the target node are not added to the pagelist and are, therefore, ignored
by do_move_pages_to_node() - there is no functional change";Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;The status of such a page is now only stored once;Wei Yang;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDgyMjFhMDgwN2IwNDg5ZjAwODE0NzZiY2QzNmRhODg3MjI1NjBi;[david@redhat.com rephrase changelog];Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDdhZTg5MWNkYzY4ZjMwZThiNmRiMzViYmQ3ZjBiMWFlZWUxODlj;mm/migrate.c: check pagelist in move_pages_and_store_status();Wei Yang;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDdhZTg5MWNkYzY4ZjMwZThiNmRiMzViYmQ3ZjBiMWFlZWUxODlj;When pagelist is empty, it is not necessary to do the move and store;Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDdhZTg5MWNkYzY4ZjMwZThiNmRiMzViYmQ3ZjBiMWFlZWUxODlj;Also it consolidate the empty list check in one place.;Wei Yang;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2E4NzgzYWQ4MTY5NzJmMGM2NGExODliOGJhNjYxNWM2Yjc4NTIx;mm/migrate.c: wrap do_move_pages_to_node() and store_status();Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2E4NzgzYWQ4MTY5NzJmMGM2NGExODliOGJhNjYxNWM2Yjc4NTIx;"Usually, do_move_pages_to_node() and store_status() are used in
combination";Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2E4NzgzYWQ4MTY5NzJmMGM2NGExODliOGJhNjYxNWM2Yjc4NTIx; We have three similar call sites;Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2E4NzgzYWQ4MTY5NzJmMGM2NGExODliOGJhNjYxNWM2Yjc4NTIx;"Let's provide a wrapper for both function calls -
move_pages_and_store_status - to make the calling code easier to maintain
and fix (as noted by Yang Shi, the return value handling of
do_move_pages_to_node() has a flaw)";Wei Yang;2020-04-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2E4NzgzYWQ4MTY5NzJmMGM2NGExODliOGJhNjYxNWM2Yjc4NTIx;[david@redhat.com rephrase changelog];Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;mm/migrate.c: no need to check for i > start in do_pages_move();Wei Yang;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;"Patch series ""cleanup on do_pages_move()"", v5";Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;"The logic in do_pages_move() is a little mess for audience to read and has
some potential error on handling the return value";Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;"Especially there are
three calls on do_move_pages_to_node() and store_status() with almost the
same form";Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;"This patch set tries to make the code a little friendly for audience by
consolidate the calls";Wei Yang;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;This patch (of 4);Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;At this point, we always have i >= start;Wei Yang;2020-04-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;" If i == start, store_status()
will return 0";Wei Yang;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh; So we can drop the check for i > start;Wei Yang;2020-04-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YWZkYWNlYzJlMzEzNTA5YmY5YzI1ZjMzNzBiZTI5OWEwZDYwYjlh;[david@redhat.com rephrase changelog];Wei Yang;2020-04-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization;Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"Patch series ""hugetlbfs: use i_mmap_rwsem for more synchronization"", v2";Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"While discussing the issue with huge_pte_offset [1], I remembered that
there were more outstanding hugetlb races";Mike Kravetz;2020-04-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl; These issues are;Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"1) For shared pmds, huge PTE pointers returned by huge_pte_alloc can become
   invalid via a call to huge_pmd_unshare by another thread";Mike Kravetz;2020-04-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"2) hugetlbfs page faults can race with truncation causing invalid global
   reserve counts and state";Mike Kravetz;2020-04-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"A previous attempt was made to use i_mmap_rwsem in this manner as
described at [2]";Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" However, those patches were reverted starting with [3]
due to locking issues";Mike Kravetz;2020-04-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"To effectively use i_mmap_rwsem to address the above issues it needs to be
held (in read mode) during page fault processing";Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" However, during fault
processing we need to lock the page we will be adding";Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" Lock ordering
requires we take page lock before i_mmap_rwsem";Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" Waiting until after
taking the page lock is too late in the fault process for the
synchronization we want to do";Mike Kravetz;2020-04-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"To address this lock ordering issue, the following patches change the lock
ordering for hugetlb pages";Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" This is not too invasive as hugetlbfs
processing is done separate from core mm in many places";Mike Kravetz;2020-04-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" However, I don't
really like this idea";Mike Kravetz;2020-04-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" Much ugliness is contained in the new routine
hugetlb_page_mapping_lock_write() of patch 1";Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;"The only other way I can think of to address these issues is by catching
all the races";Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl; After catching a race, cleanup, backout, retry ..;Mike Kravetz;2020-04-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" etc,
as needed";Mike Kravetz;2020-04-02;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" This can get really ugly, especially for huge page
reservations";Mike Kravetz;2020-04-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" At one time, I started writing some of the reservation
backout code for page faults and it got so ugly and complicated I went
down the path of adding synchronization to avoid the races";Mike Kravetz;2020-04-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMGQwMzgxYWRlNzk4ODVjMDRhMDRjMzAzMjg0YjA0MDYxNmIxMTZl;" Any other
suggestions would be welcome.";Mike Kravetz;2020-04-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MDBiYjFjOGRjODBiYjQxMjE0NDZiNTY4MTMwNjdmM2VhNDRlZGVl;mm: handle multiple owners of device private pages in migrate_vma;Christoph Hellwig;2020-03-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDBiYjFjOGRjODBiYjQxMjE0NDZiNTY4MTMwNjdmM2VhNDRlZGVl;Add a new src_owner field to struct migrate_vma;Christoph Hellwig;2020-03-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDBiYjFjOGRjODBiYjQxMjE0NDZiNTY4MTMwNjdmM2VhNDRlZGVl;" If the field is set,
only device private pages with page->pgmap->owner equal to that field are
migrated";Christoph Hellwig;2020-03-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MDBiYjFjOGRjODBiYjQxMjE0NDZiNTY4MTMwNjdmM2VhNDRlZGVl;" If the field is not set only ""normal"" pages are migrated";Christoph Hellwig;2020-03-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MDBiYjFjOGRjODBiYjQxMjE0NDZiNTY4MTMwNjdmM2VhNDRlZGVl;"Fixes: df6ad69838fc (""mm/device-public-memory: device memory cache coherent with CPU"")";Christoph Hellwig;2020-03-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;mm: pagewalk: add 'depth' parameter to pte_hole;Steven Price;2020-02-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;The pte_hole() callback is called at multiple levels of the page tables;Steven Price;2020-02-04;0;0
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;"Code dumping the kernel page tables needs to know what at what depth the
missing entry is";Steven Price;2020-02-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm; Add this is an extra parameter to pte_hole();Steven Price;2020-02-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;" When the
depth isn't know (e.g";Steven Price;2020-02-04;0;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm; processing a vma) then -1 is passed;Steven Price;2020-02-04;0;0
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;"The depth that is reported is the actual level where the entry is missing
(ignoring any folding that is in place), i.e";Steven Price;2020-02-04;0;1
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;" any levels where
PTRS_PER_P?D is set to 1 are ignored";Steven Price;2020-02-04;1;0
MDY6Q29tbWl0MjMyNTI5ODpiN2ExNmM3YWQ3OTBkMGVjYjQ0ZGNiMDhhNmE3NWQwZDA0NTVhYjVm;"Note that depth starts at 0 for a PGD so that PUD/PMD/PTE retain their
natural numbers as levels 2/3/4.";Steven Price;2020-02-04;1;0
MDY6Q29tbWl0MjMyNTI5ODozNDI5MGUyYzY0MTlkM2E2MTM5MTQxNmI1YWI2Y2ViMzdmODU3ZmRl;mm/migrate: add stable check in migrate_vma_insert_page();Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODozNDI5MGUyYzY0MTlkM2E2MTM5MTQxNmI1YWI2Y2ViMzdmODU3ZmRl;migrate_vma_insert_page() closely follows the code in;Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODozNDI5MGUyYzY0MTlkM2E2MTM5MTQxNmI1YWI2Y2ViMzdmODU3ZmRl;"  __handle_mm_fault()
    handle_pte_fault()
      do_anonymous_page()
Add a call to check_stable_address_space() after locking the page table
entry before inserting a ZONE_DEVICE private zero page mapping similar
to page faulting a new anonymous page.";Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMjNhMGM5OTc5M2ZiN2UyZGFhYzBmOGZjN2ZmM2JkN2Q3N2M5MjUw;mm/migrate: clean up some minor coding style;Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMjNhMGM5OTc5M2ZiN2UyZGFhYzBmOGZjN2ZmM2JkN2Q3N2M5MjUw;"Fix some comment typos and coding style clean up in preparation for the
next patch";Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMjNhMGM5OTc5M2ZiN2UyZGFhYzBmOGZjN2ZmM2JkN2Q3N2M5MjUw; No functional changes.;Ralph Campbell;2020-01-31;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NzJlYTcwNzUxMWFkNDNhZmUyMDJmNDAxY2NiOGRlMDFlMmIyYTNi;mm/migrate: remove useless mask of start address;Ralph Campbell;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NzJlYTcwNzUxMWFkNDNhZmUyMDJmNDAxY2NiOGRlMDFlMmIyYTNi;"Addresses passed to walk_page_range() callback functions are already
page aligned and don't need to be masked with PAGE_MASK.";Ralph Campbell;2020-01-31;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;mm: move_pages: report the number of non-attempted pages;Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;"Since commit a49bd4d71637 (""mm, numa: rework do_pages_move""), the
semantic of move_pages() has changed to return the number of
non-migrated pages if they were result of a non-fatal reasons (usually a
busy page)";Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;"This was an unintentional change that hasn't been noticed except for LTP
tests which checked for the documented behavior";Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;There are two ways to go around this change;Yang Shi;2020-01-31;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;" We can even get back to
the original behavior and return -EAGAIN whenever migrate_pages is not
able to migrate pages due to non-fatal reasons";Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;" Another option would be
to simply continue with the changed semantic and extend move_pages
documentation to clarify that -errno is returned on an invalid input or
when migration simply cannot succeed (e.g";Yang Shi;2020-01-31;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;" -ENOMEM, -EBUSY) or the
number of pages that couldn't have been migrated due to ephemeral
reasons (e.g";Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl; page is pinned or locked for other reasons);Yang Shi;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;"This patch implements the second option because this behavior is in
place for some time without anybody complaining and possibly new users
depending on it";Yang Shi;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;" Also it allows to have a slightly easier error
handling as the caller knows that it is worth to retry when err > 0";Yang Shi;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTg0ZmFiYjZlODJkOWFiNGU2MzA1Y2I5OTY5NGM4NWQ0NmRlOGFl;"But since the new semantic would be aborted immediately if migration is
failed due to ephemeral reasons, need include the number of
non-attempted pages in the return value too.";Yang Shi;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZmU5YWEyM2NhYjc4ODBhNzk0ZGI5ZWIyZDE3NmMwMGVkMDY0ZWI2;mm/migrate.c: also overwrite error when it is bigger than zero;Wei Yang;2020-01-31;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZmU5YWEyM2NhYjc4ODBhNzk0ZGI5ZWIyZDE3NmMwMGVkMDY0ZWI2;"If we get here after successfully adding page to list, err would be 1 to
indicate the page is queued in the list";Wei Yang;2020-01-31;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZmU5YWEyM2NhYjc4ODBhNzk0ZGI5ZWIyZDE3NmMwMGVkMDY0ZWI2;Current code has two problems;Wei Yang;2020-01-31;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZmU5YWEyM2NhYjc4ODBhNzk0ZGI5ZWIyZDE3NmMwMGVkMDY0ZWI2;"    from do_move_pages_to_node() is set, the err1 is not returned since err
And these behaviors break the user interface.";Wei Yang;2020-01-31;0;1
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;mm: move_pages: return valid node id in status if the page is already on the target node;Yang Shi;2020-01-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;"Felix Abecassis reports move_pages() would return random status if the
pages are already on the target node by the below test program";Yang Shi;2020-01-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;"  int main(void)
		pages[i] = mmap(NULL, page_size, PROT_WRITE | PROT_READ,
				MAP_PRIVATE | MAP_POPULATE | MAP_ANONYMOUS,
	for (int i = 0; i < num_pages; ++i)
Then running the program would return nonsense status values";Yang Shi;2020-01-04;1;0
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;"  $ ./move_pages_bug
  move_pages: 0
  status[0] = 208
  status[1] = 208
  status[2] = 208
  status[3] = 208
  status[4] = 208
  status[5] = 208
  status[6] = 208
  status[7] = 208
This is because the status is not set if the page is already on the
target node, but move_pages() should return valid status as long as it
succeeds";Yang Shi;2020-01-04;0;1
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2; The valid status may be errno or node id;Yang Shi;2020-01-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;"We can't simply initialize status array to zero since the pages may be
not on node 0";Yang Shi;2020-01-04;0;1
MDY6Q29tbWl0MjMyNTI5ODplMDE1M2ZjMmM3NjA2ZjEwMTM5MmI2ODJlNzIwYTdhNDU2ZDZjNzY2;" Fix it by updating status with node id which the page is
already on.";Yang Shi;2020-01-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;autonuma: fix watermark checking in migrate_balanced_pgdat();Huang Ying;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;"When zone_watermark_ok() is called in migrate_balanced_pgdat() to check
migration target node, the parameter classzone_idx (for requested zone)
is specified as 0 (ZONE_DMA)";Huang Ying;2019-12-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" But when allocating memory for autonuma
in alloc_misplaced_dst_page(), the requested zone from GFP flags is
ZONE_MOVABLE";Huang Ying;2019-12-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4; That is, the requested zone is different;Huang Ying;2019-12-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" The size of
lowmem_reserve for the different requested zone is different";Huang Ying;2019-12-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" And this
may cause some issues";Huang Ying;2019-12-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;"For example, in the zoneinfo of a test machine as below,
Node 0, zone    DMA32
  pages free     61592
        min      29
        low      454
        high     879
        spanned  1044480
        present  442306
        managed  425921
        protection: (0, 0, 62457, 62457, 62457)
The free page number of ZONE_DMA32 is greater than ""high watermark +
lowmem_reserve[ZONE_DMA]"", but less than ""high watermark +
lowmem_reserve[ZONE_MOVABLE]""";Huang Ying;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" And because __alloc_pages_node() in
alloc_misplaced_dst_page() requests ZONE_MOVABLE, the
zone_watermark_ok() on ZONE_DMA32 in migrate_balanced_pgdat() may always
return true";Huang Ying;2019-12-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" So, autonuma may not stop even when memory pressure in
node 0 is heavy";Huang Ying;2019-12-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;"To fix the issue, ZONE_MOVABLE is used as parameter to call
zone_watermark_ok() in migrate_balanced_pgdat()";Huang Ying;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" This makes it same as
requested zone in alloc_misplaced_dst_page()";Huang Ying;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZmU5ZDAwNmM5NzFhNWRhZWZlN2E4YjI3ODE5Y2NkNDk3MDkwZmQ4;" So that
migrate_balanced_pgdat() returns false when memory pressure is heavy.";Huang Ying;2019-12-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NGQ0YTU3OTdiODkwNDhhNWIyMDc0NmRhN2U4MGFmMWU3M2I4NTQ3;mm/migrate.c: handle freed page at the first place;Yang Shi;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NGQ0YTU3OTdiODkwNDhhNWIyMDc0NmRhN2U4MGFmMWU3M2I4NTQ3;"When doing migration if the freed page is met, we just return without
migrating it since it is pointless to migrate a freed page";Yang Shi;2019-12-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NGQ0YTU3OTdiODkwNDhhNWIyMDc0NmRhN2U4MGFmMWU3M2I4NTQ3;" But, the
current code allocates target page unconditionally before handling freed
page, if the page is freed, the newly allocated will be just freed";Yang Shi;2019-12-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NGQ0YTU3OTdiODkwNDhhNWIyMDc0NmRhN2U4MGFmMWU3M2I4NTQ3;" It
doesn't make too much sense and is just a waste of time although
migrating freed page is rare";Yang Shi;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NGQ0YTU3OTdiODkwNDhhNWIyMDc0NmRhN2U4MGFmMWU3M2I4NTQ3;"So, handle freed page at the before that to avoid unnecessary page
allocation and free.";Yang Shi;2019-12-01;1;1
MDY6Q29tbWl0MjMyNTI5ODowNTdkMzM4OTEwOGVkYThhMjBjN2Y0OTZmMDExODQ2OTMyNjgwZDg4;mm: untag user pointers passed to memory syscalls;Andrey Konovalov;2019-09-25;1;1
MDY6Q29tbWl0MjMyNTI5ODowNTdkMzM4OTEwOGVkYThhMjBjN2Y0OTZmMDExODQ2OTMyNjgwZDg4;"This patch is a part of a series that extends kernel ABI to allow to pass
tagged user pointers (with the top byte set to something else other than
0x00) as syscall arguments";Andrey Konovalov;2019-09-25;1;1
MDY6Q29tbWl0MjMyNTI5ODowNTdkMzM4OTEwOGVkYThhMjBjN2Y0OTZmMDExODQ2OTMyNjgwZDg4;"This patch allows tagged pointers to be passed to the following memory
syscalls: get_mempolicy, madvise, mbind, mincore, mlock, mlock2, mprotect,
mremap, msync, munlock, move_pages";Andrey Konovalov;2019-09-25;1;1
MDY6Q29tbWl0MjMyNTI5ODowNTdkMzM4OTEwOGVkYThhMjBjN2Y0OTZmMDExODQ2OTMyNjgwZDg4;The mmap and mremap syscalls do not currently accept tagged addresses;Andrey Konovalov;2019-09-25;0;0
MDY6Q29tbWl0MjMyNTI5ODowNTdkMzM4OTEwOGVkYThhMjBjN2Y0OTZmMDExODQ2OTMyNjgwZDg4;"Architectures may interpret the tag as a background colour for the
corresponding vma.";Andrey Konovalov;2019-09-25;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNzZmNzU2ZDcwMDJjZWI2NmY4YWUzZjdiZDgwMjhlN2FlM2RlZWQ4;mm/migrate.c: clean up useless code in migrate_vma_collect_pmd();Pingfan Liu;2019-09-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNzZmNzU2ZDcwMDJjZWI2NmY4YWUzZjdiZDgwMjhlN2FlM2RlZWQ4;Remove unused 'pfn' variable.;Pingfan Liu;2019-09-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MTAxMTk2YjE5ZDdmOTA1ZGNhNWRjZjQ2Y2QzNWViNzU4Y2YwNmMw;mm: page cache: store only head pages in i_pages;Matthew Wilcox (Oracle);2019-09-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo0MTAxMTk2YjE5ZDdmOTA1ZGNhNWRjZjQ2Y2QzNWViNzU4Y2YwNmMw;"Transparent Huge Pages are currently stored in i_pages as pointers to
consecutive subpages";Matthew Wilcox (Oracle);2019-09-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo0MTAxMTk2YjE5ZDdmOTA1ZGNhNWRjZjQ2Y2QzNWViNzU4Y2YwNmMw;" This patch changes that to storing consecutive
pointers to the head page in preparation for storing huge pages more
efficiently in i_pages";Matthew Wilcox (Oracle);2019-09-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo0MTAxMTk2YjE5ZDdmOTA1ZGNhNWRjZjQ2Y2QzNWViNzU4Y2YwNmMw;"Large parts of this are ""inspired"" by Kirill's patch
Kirill and Huang Ying contributed several fixes.";Matthew Wilcox (Oracle);2019-09-23;0;1
MDY6Q29tbWl0MjMyNTI5ODpkOGM2NTQ2YjFhZWE4NDNmYmViNGQ1NGExMjAyZjFhZGRhNjUwNGJl;mm: introduce compound_nr();Matthew Wilcox (Oracle);2019-09-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpkOGM2NTQ2YjFhZWE4NDNmYmViNGQ1NGExMjAyZjFhZGRhNjUwNGJl;Replace 1 << compound_order(page) with compound_nr(page);Matthew Wilcox (Oracle);2019-09-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpkOGM2NTQ2YjFhZWE4NDNmYmViNGQ1NGExMjAyZjFhZGRhNjUwNGJl;" Minor
improvements in readability.";Matthew Wilcox (Oracle);2019-09-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Yjg2YWMzMzcxYjcwYzNmZDhmZDk1NTAxNzE5YmViMWZhYWI3MTlm;pagewalk: separate function pointers from iterator data;Christoph Hellwig;2019-08-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Yjg2YWMzMzcxYjcwYzNmZDhmZDk1NTAxNzE5YmViMWZhYWI3MTlm;The mm_walk structure currently mixed data and code;Christoph Hellwig;2019-08-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Yjg2YWMzMzcxYjcwYzNmZDhmZDk1NTAxNzE5YmViMWZhYWI3MTlm;" Split out the
operations vectors into a new mm_walk_ops structure, and while we are
changing the API also declare the mm_walk structure inside the
walk_page_range and walk_page_vma functions";Christoph Hellwig;2019-08-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Yjg2YWMzMzcxYjcwYzNmZDhmZDk1NTAxNzE5YmViMWZhYWI3MTlm;Based on patch from Linus Torvalds.;Christoph Hellwig;2019-08-28;0;0
MDY6Q29tbWl0MjMyNTI5ODphNTIwMTEwZTRhMTVjZWIzODUzMDRkOWNhYjIyYmI1MTQzOGY2MDgw;mm: split out a new pagewalk.h header from mm.h;Christoph Hellwig;2019-08-28;1;1
MDY6Q29tbWl0MjMyNTI5ODphNTIwMTEwZTRhMTVjZWIzODUzMDRkOWNhYjIyYmI1MTQzOGY2MDgw;"Add a new header for the two handful of users of the walk_page_range /
walk_page_vma interface instead of polluting all users of mm.h with it.";Christoph Hellwig;2019-08-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YjJlZDljYjk3NWM2M2Y1NTM0ZGFhZWJlYjIyNWFiNTJiNTg5Mzcy;mm: remove CONFIG_MIGRATE_VMA_HELPER;Christoph Hellwig;2019-08-14;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YjJlZDljYjk3NWM2M2Y1NTM0ZGFhZWJlYjIyNWFiNTJiNTg5Mzcy;"CONFIG_MIGRATE_VMA_HELPER guards helpers that are required for proper
devic private memory support";Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YjJlZDljYjk3NWM2M2Y1NTM0ZGFhZWJlYjIyNWFiNTJiNTg5Mzcy;" Remove the option and just check for
CONFIG_DEVICE_PRIVATE instead.";Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODowNmQ0NjJiZWI0NzBkMzYxZmZhOGJkN2IzZDg2NTUwOWE4NjA2OTg3;mm: remove the unused MIGRATE_PFN_DEVICE flag;Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODowNmQ0NjJiZWI0NzBkMzYxZmZhOGJkN2IzZDg2NTUwOWE4NjA2OTg3;"No one ever checks this flag, and we could easily get that information
from the page if needed.";Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODphN2QxZjIyYmI3NGYzMmNmM2NkOTNmNTI3NzYwMDdlMTYxZjFhNzM4;mm: turn migrate_vma upside down;Christoph Hellwig;2019-08-14;1;0
MDY6Q29tbWl0MjMyNTI5ODphN2QxZjIyYmI3NGYzMmNmM2NkOTNmNTI3NzYwMDdlMTYxZjFhNzM4;There isn't any good reason to pass callbacks to migrate_vma;Christoph Hellwig;2019-08-14;1;0
MDY6Q29tbWl0MjMyNTI5ODphN2QxZjIyYmI3NGYzMmNmM2NkOTNmNTI3NzYwMDdlMTYxZjFhNzM4;" Instead
we can just export the three steps done by this function to drivers and
let them sequence the operation without callbacks";Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODphN2QxZjIyYmI3NGYzMmNmM2NkOTNmNTI3NzYwMDdlMTYxZjFhNzM4;" This removes a lot
of boilerplate code as-is, and will allow the drivers to drastically
improve code flow and error handling further on.";Christoph Hellwig;2019-08-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjM1OGM2ZjEyZGM4MjM2NGY2ZDMxN2Y4YzhmMWQ3OTRhZGJjM2Y1;mm/migrate.c: initialize pud_entry in migrate_vma();Ralph Campbell;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjM1OGM2ZjEyZGM4MjM2NGY2ZDMxN2Y4YzhmMWQ3OTRhZGJjM2Y1;"When CONFIG_MIGRATE_VMA_HELPER is enabled, migrate_vma() calls
migrate_vma_collect() which initializes a struct mm_walk but didn't
initialize mm_walk.pud_entry";Ralph Campbell;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjM1OGM2ZjEyZGM4MjM2NGY2ZDMxN2Y4YzhmMWQ3OTRhZGJjM2Y1;" (Found by code inspection) Use a C
structure initialization to make sure it is set to NULL.";Ralph Campbell;2019-08-03;1;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;mm: migrate: fix reference check race between __find_get_block() and migration;Jan Kara;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;"buffer_migrate_page_norefs() can race with bh users in the following
way";Jan Kara;2019-08-03;0;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;"CPU1                                    CPU2
buffer_migrate_page_norefs()
  buffer_migrate_lock_buffers()
  checks bh refs
  spin_unlock(&mapping->private_lock)
                                        __find_get_block()
                                          spin_lock(&mapping->private_lock)
                                          grab bh ref
                                          spin_unlock(&mapping->private_lock)
  move page                               do bh work
This can result in various issues like lost updates to buffers (i.e";Jan Kara;2019-08-03;0;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;metadata corruption) or use after free issues for the old page;Jan Kara;2019-08-03;0;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;"This patch closes the race by holding mapping->private_lock while the
mapping is being moved to a new page";Jan Kara;2019-08-03;0;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;" Ordinarily, a reference can be
taken outside of the private_lock using the per-cpu BH LRU but the
references are checked and the LRU invalidated if necessary";Jan Kara;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;" The
private_lock is held once the references are known so the buffer lookup
slow path will spin on the private_lock";Jan Kara;2019-08-03;1;0
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;" Between the page lock and
private_lock, it should be impossible for other references to be
acquired and updates to happen during the migration";Jan Kara;2019-08-03;0;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;"A user had reported data corruption issues on a distribution kernel with
a similar page migration implementation as mainline";Jan Kara;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;" The data
corruption could not be reproduced with this patch applied";Jan Kara;2019-08-03;1;1
MDY6Q29tbWl0MjMyNTI5ODplYmRmNGRlNTY0MmZiNjU4MGIwNzYzMTU4YjZiNGI3OTFjNGQ2YTRk;" A small
number of migration-intensive tests were run and no performance problems
were noted.";Jan Kara;2019-08-03;0;1
MDY6Q29tbWl0MjMyNTI5ODozNzEwOTY5NDlmMGFkMzk1MGIwNjcyOTk4OWJkMjdkZTUxYjhjNWY1;mm: migrate: remove unused mode argument;Keith Busch;2019-07-18;1;1
MDY6Q29tbWl0MjMyNTI5ODozNzEwOTY5NDlmMGFkMzk1MGIwNjcyOTk4OWJkMjdkZTUxYjhjNWY1;migrate_page_move_mapping() doesn't use the mode argument;Keith Busch;2019-07-18;1;0
MDY6Q29tbWl0MjMyNTI5ODozNzEwOTY5NDlmMGFkMzk1MGIwNjcyOTk4OWJkMjdkZTUxYjhjNWY1;" Remove it
and update callers accordingly.";Keith Busch;2019-07-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OWJmNGI2YjU0ZmI3ZjUyYjdlYTljZTI4ZDRhMzYwY2Q1ZWM5NTZk;"Revert ""mm: page cache: store only head pages in i_pages""";Linus Torvalds;2019-07-06;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OWJmNGI2YjU0ZmI3ZjUyYjdlYTljZTI4ZDRhMzYwY2Q1ZWM5NTZk;This reverts commit 5fd4ca2d84b249f0858ce28cf637cf25b61a398f;Linus Torvalds;2019-07-06;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OWJmNGI2YjU0ZmI3ZjUyYjdlYTljZTI4ZDRhMzYwY2Q1ZWM5NTZk;"Mikhail Gavrilov reports that it causes the VM_BUG_ON_PAGE() in
__delete_from_swap_cache() to trigger";Linus Torvalds;2019-07-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo2OWJmNGI2YjU0ZmI3ZjUyYjdlYTljZTI4ZDRhMzYwY2Q1ZWM5NTZk;"   page:ffffd6d34dff0000 refcount:1 mapcount:1 mapping:ffff97812323a689 index:0xfecec363
   anon
   flags: 0x17fffe00080034(uptodate|lru|active|swapbacked)
   raw: 0017fffe00080034 ffffd6d34c67c508 ffffd6d3504b8d48 ffff97812323a689
   raw: 00000000fecec363 0000000000000000 0000000100000000 ffff978433ace000
   page dumped because: VM_BUG_ON_PAGE(entry != page)
   page->mem_cgroup:ffff978433ace000
   ------------[ cut here ]------------
   kernel BUG at mm/swap_state.c:170!
   invalid opcode: 0000 [#1] SMP NOPTI
   CPU: 1 PID: 221 Comm: kswapd0 Not tainted 5.2.0-0.rc2.git0.1.fc31.x86_64 #1
   Hardware name: System manufacturer System Product Name/ROG STRIX X470-I GAMING, BIOS 2202 04/11/2019
   RIP: 0010:__delete_from_swap_cache+0x20d/0x240
   Code: 30 65 48 33 04 25 28 00 00 00 75 4a 48 83 c4 38 5b 5d 41 5c 41 5d 41 5e 41 5f c3 48 c7 c6 2f dc 0f 8a 48 89 c7 e8 93 1b fd ff <0f> 0b 48 c7 c6 a8 74 0f 8a e8 85 1b fd ff 0f 0b 48 c7 c6 a8 7d 0f
   RSP: 0018:ffffa982036e7980 EFLAGS: 00010046
   RAX: 0000000000000021 RBX: 0000000000000040 RCX: 0000000000000006
   RDX: 0000000000000000 RSI: 0000000000000086 RDI: ffff97843d657900
   RBP: 0000000000000001 R08: ffffa982036e7835 R09: 0000000000000535
   R10: ffff97845e21a46c R11: ffffa982036e7835 R12: ffff978426387120
   R13: 0000000000000000 R14: ffffd6d34dff0040 R15: ffffd6d34dff0000
   FS:  0000000000000000(0000) GS:ffff97843d640000(0000) knlGS:0000000000000000
   CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
   CR2: 00002cba88ef5000 CR3: 000000078a97c000 CR4: 00000000003406e0
   and it's not immediately obvious why it happens";Linus Torvalds;2019-07-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo2OWJmNGI2YjU0ZmI3ZjUyYjdlYTljZTI4ZDRhMzYwY2Q1ZWM5NTZk;" It's too late in the
rc cycle to do anything but revert for now.";Linus Torvalds;2019-07-06;0;1
MDY6Q29tbWl0MjMyNTI5ODoyNWIyOTk1YTM1YjYwOTExOWNmOTZmNmI2MmVjY2Q1NmMwMjM0Yzdk;mm: remove MEMORY_DEVICE_PUBLIC support;Christoph Hellwig;2019-06-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNWIyOTk1YTM1YjYwOTExOWNmOTZmNmI2MmVjY2Q1NmMwMjM0Yzdk;"The code hasn't been used since it was added to the tree, and doesn't
appear to actually be usable.";Christoph Hellwig;2019-06-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MjY5Zjk5OTkzNGIyODlkYTc5NzJlOTc1Yjc4MTQxN2IwN2VmODM2;mm/mmu_notifier: use correct mmu_notifier events for each invalidation;Jrme Glisse;2019-05-14;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MjY5Zjk5OTkzNGIyODlkYTc5NzJlOTc1Yjc4MTQxN2IwN2VmODM2;"This updates each existing invalidation to use the correct mmu notifier
event that represent what is happening to the CPU page table";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MjY5Zjk5OTkzNGIyODlkYTc5NzJlOTc1Yjc4MTQxN2IwN2VmODM2;" See the
patch which introduced the events to see the rational behind this.";Jrme Glisse;2019-05-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;mm/mmu_notifier: contextual information for event triggering invalidation;Jrme Glisse;2019-05-14;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"CPU page table update can happens for many reasons, not only as a result
of a syscall (munmap(), mprotect(), mremap(), madvise(), ...) but also as
a result of kernel activities (memory compression, reclaim, migration,
Users of mmu notifier API track changes to the CPU page table and take
specific action for them";Jrme Glisse;2019-05-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;" While current API only provide range of virtual
address affected by the change, not why the changes is happening";Jrme Glisse;2019-05-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"This patchset do the initial mechanical convertion of all the places that
calls mmu_notifier_range_init to also provide the default MMU_NOTIFY_UNMAP
event as well as the vma if it is know (most invalidation happens against
a given vma)";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;" Passing down the vma allows the users of mmu notifier to
inspect the new vma page protection";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"The MMU_NOTIFY_UNMAP is always the safe default as users of mmu notifier
should assume that every for the range is going away when that event
happens";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;" A latter patch do convert mm call path to use a more appropriate
events for each call";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"This is done as 2 patches so that no call site is forgotten especialy
as it uses this following coccinelle patch";Jrme Glisse;2019-05-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"static inline void mmu_notifier_range_init(struct mmu_notifier_range *I1,
+enum mmu_notifier_event event,
+unsigned flags,
+struct vm_area_struct *vma,
struct mm_struct *I2, unsigned long I3, unsigned long I4) { ..";Jrme Glisse;2019-05-14;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZjRmMTNlOGQ5ZTI3Y2VmZDJjZDg4ZGQ0ZmQ4MGFhNmQ2OGI5MTMx;"}
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, I1,
I1->vm_mm, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, VMA,
E2, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, VMA,
E2, E3, E4)
mmu_notifier_range_init(E1,
+MMU_NOTIFY_UNMAP, 0, NULL,
E2, E3, E4)
Applied with:";Jrme Glisse;2019-05-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZmQ0Y2EyZDg0YjI0OWYwODU4Y2UyOGNmNjM3Y2YyNWI2MWEzOThm;mm: page cache: store only head pages in i_pages;Matthew Wilcox;2019-05-14;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZmQ0Y2EyZDg0YjI0OWYwODU4Y2UyOGNmNjM3Y2YyNWI2MWEzOThm;"Transparent Huge Pages are currently stored in i_pages as pointers to
consecutive subpages";Matthew Wilcox;2019-05-14;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZmQ0Y2EyZDg0YjI0OWYwODU4Y2UyOGNmNjM3Y2YyNWI2MWEzOThm;" This patch changes that to storing consecutive
pointers to the head page in preparation for storing huge pages more
efficiently in i_pages";Matthew Wilcox;2019-05-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZmQ0Y2EyZDg0YjI0OWYwODU4Y2UyOGNmNjM3Y2YyNWI2MWEzOThm;"Large parts of this are ""inspired"" by Kirill's patch";Matthew Wilcox;2019-05-14;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;mm/migrate.c: add missing flush_dcache_page for non-mapped page migrate;Lars Persson;2019-03-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"Our MIPS 1004Kc SoCs were seeing random userspace crashes with SIGILL
and SIGSEGV that could not be traced back to a userspace code bug";Lars Persson;2019-03-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;" They
had all the magic signs of an I/D cache coherency issue";Lars Persson;2019-03-29;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"Now recently we noticed that the /proc/sys/vm/compact_memory interface
was quite efficient at provoking this class of userspace crashes";Lars Persson;2019-03-29;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"Studying the code in mm/migrate.c there is a distinction made between
migrating a page that is mapped at the instant of migration and one that
is not mapped";Lars Persson;2019-03-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3; Our problem turned out to be the non-mapped pages;Lars Persson;2019-03-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"For the non-mapped page the code performs a copy of the page content and
all relevant meta-data of the page without doing the required D-cache
maintenance";Lars Persson;2019-03-29;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;" This leaves dirty data in the D-cache of the CPU and on
the 1004K cores this data is not visible to the I-cache";Lars Persson;2019-03-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;" A subsequent
page-fault that triggers a mapping of the page will happily serve the
process with potentially stale code";Lars Persson;2019-03-29;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"What about ARM then, this bug should have seen greater exposure? Well
ARM became immune to this flaw back in 2010, see commit c01778001a4f
(""ARM: 6379/1: Assume new page cache pages have dirty D-cache"")";Lars Persson;2019-03-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMmIyYzZkZDIyN2JhNWI4YTgwMjg1ODc0OGVjOWE3ODBjYjc1YjQ3;"My proposed fix moves the D-cache maintenance inside move_to_new_page to
make it common for both cases.";Lars Persson;2019-03-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpmOTAwNDgyZGE1NjA5NDFmOTc4YjBkMzY2NjBlOTZmNDhlYTc4NzUy;mm/migrate.c: cleanup expected_page_refs();Jan Kara;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpmOTAwNDgyZGE1NjA5NDFmOTc4YjBkMzY2NjBlOTZmNDhlYTc4NzUy;"Andrea has noted that page migration code propagates page_mapping(page)
through the whole migration stack down to migrate_page() function so it
seems stupid to then use page_mapping(page) in expected_page_refs()
instead of passed down 'mapping' argument";Jan Kara;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODpmOTAwNDgyZGE1NjA5NDFmOTc4YjBkMzY2NjBlOTZmNDhlYTc4NzUy;" I agree so let's make
expected_page_refs() more in line with the rest of the migration stack.";Jan Kara;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YmI0ZTdhMmVlMjZjMDVhOTRhZTZjYjBhZWMyZjgyYTM1MjNjZjM1;mm: fix some typos in mm directory;Wei Yang;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YmI0ZTdhMmVlMjZjMDVhOTRhZTZjYjBhZWMyZjgyYTM1MjNjZjM1;No functional change.;Wei Yang;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;mm, migrate: immediately fail migration of a page with no migration handler;Mel Gorman;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;"Pages with no migration handler use a fallback handler which sometimes
works and sometimes persistently retries";Mel Gorman;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;" A historical example was
blockdev pages but there are others such as odd refcounting when
page->private is used";Mel Gorman;2019-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;" These are retried multiple times which is
wasteful during compaction so this patch will fail migration faster
unless the caller specifies MIGRATE_SYNC";Mel Gorman;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;"This is not expected to help THP allocation success rates but it did
reduce latencies very slightly in some cases";Mel Gorman;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;"1-socket thpfioscale
                              noreserved-v2r15         failfast-v2r15
Amean     fault-both-1         0.00 (   0.00%)        0.00 *   0.00%*
Amean     fault-both-3      3839.67 (   0.00%)     3833.72 (   0.15%)
Amean     fault-both-5      5177.47 (   0.00%)     4967.15 (   4.06%)
Amean     fault-both-7      7245.03 (   0.00%)     7139.19 (   1.46%)
Amean     fault-both-12    11534.89 (   0.00%)    11326.30 (   1.81%)
Amean     fault-both-18    16241.10 (   0.00%)    16270.70 (  -0.18%)
Amean     fault-both-24    19075.91 (   0.00%)    19839.65 (  -4.00%)
Amean     fault-both-30    22712.11 (   0.00%)    21707.05 (   4.43%)
Amean     fault-both-32    21692.92 (   0.00%)    21968.16 (  -1.27%)
The 2-socket results are not materially different";Mel Gorman;2019-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MDYwMzFiYjVlYzM2ZWQ4NzlkNjQyNDlkNWE1Y2Y5YzY2NTdmODlk;" Scan rates are
similar as expected.";Mel Gorman;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;mm/hugetlb: distinguish between migratability and movability;Anshuman Khandual;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"Patch series ""arm64/mm: Enable HugeTLB migration"", v4";Anshuman Khandual;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"This patch series enables HugeTLB migration support for all supported
huge page sizes at all levels including contiguous bit implementation";Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"Following HugeTLB migration support matrix has been enabled with this
patch series";Anshuman Khandual;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh; All permutations have been tested except for the 16GB;Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"           CONT PTE    PMD    CONT PMD    PUD
  4K:         64K     2M         32M     1G
  16K:         2M    32M          1G
  64K:         2M   512M         16G
First the series adds migration support for PUD based huge pages";Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;" It
then adds a platform specific hook to query an architecture if a given
huge page size is supported for migration while also providing a default
fallback option preserving the existing semantics which just checks for
(PMD|PUD|PGDIR)_SHIFT macros";Anshuman Khandual;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;" The last two patches enables HugeTLB
migration on arm64 and subscribe to this new platform specific hook by
defining an override";Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"The second patch differentiates between movability and migratability
aspects of huge pages and implements hugepage_movable_supported() which
can then be used during allocation to decide whether to place the huge
page in movable zone or not";Anshuman Khandual;2019-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;This patch (of 5);Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"During huge page allocation it's migratability is checked to determine
if it should be placed under movable zones with GFP_HIGHUSER_MOVABLE";Anshuman Khandual;2019-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"But the movability aspect of the huge page could depend on other factors
than just migratability";Anshuman Khandual;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;" Movability in itself is a distinct property
which should not be tied with migratability alone";Anshuman Khandual;2019-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;"This differentiates these two and implements an enhanced movability check
which also considers huge page size to determine if it is feasible to be
placed under a movable zone";Anshuman Khandual;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWQyYzMxZGFiZGViM2VlNmFiZThmZjVhYWM3Mjg3ODIxYTUwY2Jh;" At present it just checks for gigantic pages
but going forward it can incorporate other enhanced checks.";Anshuman Khandual;2019-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;hugetlbfs: fix races and page leaks during migration;Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;hugetlb pages should only be migrated if they are 'active';Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" The
routines set/clear_page_huge_active() modify the active state of hugetlb
pages";Mike Kravetz;2019-03-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"When a new hugetlb page is allocated at fault time, set_page_huge_active
is called before the page is locked";Mike Kravetz;2019-03-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" Therefore, another thread could
race and migrate the page while it is being added to page table by the
fault code";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" This race is somewhat hard to trigger, but can be seen by
strategically adding udelay to simulate worst case scheduling behavior";Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;Depending on 'how' the code races, various BUG()s could be triggered;Mike Kravetz;2019-03-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"To address this issue, simply delay the set_page_huge_active call until
after the page is successfully added to the page table";Mike Kravetz;2019-03-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"Hugetlb pages can also be leaked at migration time if the pages are
associated with a file in an explicitly mounted hugetlbfs filesystem";Mike Kravetz;2019-03-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"For example, consider a two node system with 4GB worth of huge pages
available";Mike Kravetz;2019-03-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2; A program mmaps a 2G file in a hugetlbfs filesystem;Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" It
then migrates the pages associated with the file from one node to
another";Mike Kravetz;2019-03-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2; When the program exits, huge page counts are as follows;Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"  node0
  1024    free_hugepages
  1024    nr_hugepages
  node1
  0       free_hugepages
  1024    nr_hugepages
  Filesystem                         Size  Used Avail Use% Mounted on
  nodev                              4.0G  2.0G  2.0G  50% /var/opt/hugepool
That is as expected";Mike Kravetz;2019-03-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" 2G of huge pages are taken from the free_hugepages
counts, and 2G is the size of the file in the explicitly mounted
filesystem";Mike Kravetz;2019-03-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2; If the file is then removed, the counts become;Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"  node0
  1024    free_hugepages
  1024    nr_hugepages
  node1
  1024    free_hugepages
  1024    nr_hugepages
  Filesystem                         Size  Used Avail Use% Mounted on
  nodev                              4.0G  2.0G  2.0G  50% /var/opt/hugepool
Note that the filesystem still shows 2G of pages used, while there
actually are no huge pages in use";Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" The only way to 'fix' the filesystem
accounting is to unmount the filesystem
If a hugetlb page is associated with an explicitly mounted filesystem,
this information in contained in the page_private field";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" At migration
time, this information is not preserved";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" To fix, simply transfer
page_private from old to new page at migration time if necessary";Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;"There is a related race with removing a huge page from a file and
migration";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" When a huge page is removed from the pagecache, the
page_mapping() field is cleared, yet page_private remains set until the
page is actually freed by free_huge_page()";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" A page could be migrated
while in this state";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" However, since page_mapping() is not set the
hugetlbfs specific routine to transfer page_private is not called and we
leak the page count in the filesystem";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;To fix that, check for this condition before migrating a huge page;Mike Kravetz;2019-03-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjZhY2QwMWUyZTQzZmQ4YmFkMTExNTU3NTJiNzY5OWMzZDBmYjc2;" If
the condition is detected, return EBUSY for the page.";Mike Kravetz;2019-03-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;mm: migrate: don't rely on __PageMovable() of newpage after unlocking it;David Hildenbrand;2019-02-01;1;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"We had a race in the old balloon compaction code before b1123ea6d3b3
(""mm: balloon: use general non-lru movable page feature"") refactored it
that became visible after backporting 195a8c43e93d (""virtio-balloon";David Hildenbrand;2019-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"deflate via a page list"") without the refactoring";David Hildenbrand;2019-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"The bug existed from commit d6d86c0a7f8d (""mm/balloon_compaction";David Hildenbrand;2019-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"redesign ballooned pages management"") till b1123ea6d3b3 (""mm: balloon";David Hildenbrand;2019-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"use general non-lru movable page feature"")";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;" d6d86c0a7f8d
(""mm/balloon_compaction: redesign ballooned pages management"") was
backported to 3.12, so the broken kernels are stable kernels [3.12 -
There was a subtle race between dropping the page lock of the newpage in
__unmap_and_move() and checking for __is_movable_balloon_page(newpage)";David Hildenbrand;2019-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"Just after dropping this page lock, virtio-balloon could go ahead and
deflate the newpage, effectively dequeueing it and clearing PageBalloon,
in turn making __is_movable_balloon_page(newpage) fail";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"This resulted in dropping the reference of the newpage via
putback_lru_page(newpage) instead of put_page(newpage), leading to
page->lru getting modified and a !LRU page ending up in the LRU lists";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"With 195a8c43e93d (""virtio-balloon: deflate via a page list"")
backported, one would suddenly get corrupted lists in
release_pages_balloon()";David Hildenbrand;2019-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"- WARNING: CPU: 13 PID: 6586 at lib/list_debug.c:59 __list_del_entry+0xa1/0xd0
- list_del corruption";David Hildenbrand;2019-02-01;1;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"prev->next should be ffffe253961090a0, but was dead000000000100
Nowadays this race is no longer possible, but it is hidden behind very
ugly handling of __ClearPageMovable() and __PageMovable()";David Hildenbrand;2019-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"__ClearPageMovable() will not make __PageMovable() fail, only
PageMovable()";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;" So the new check (__PageMovable(newpage)) will still
hold even after newpage was dequeued by virtio-balloon";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"If anybody would ever change that special handling, the BUG would be
introduced again";David Hildenbrand;2019-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;" So instead, make it explicit and use the information
of the original isolated page before migration";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMGEzNTJmYWJjZTYxZjczMDM0MWQxMTlmYmVkZjcxZmZkYjg2NjNm;"This patch can be backported fairly easy to stable kernels (in contrast
to the refactoring).";David Hildenbrand;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDQwOWM2NWUyYzZjZDE1NDAwNDVlZTAxZmM1NWU1MGQ5NWUwOTgz;mm: migrate: make buffer_migrate_page_norefs() actually succeed;Jan Kara;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDQwOWM2NWUyYzZjZDE1NDAwNDVlZTAxZmM1NWU1MGQ5NWUwOTgz;"Currently, buffer_migrate_page_norefs() was constantly failing because
buffer_migrate_lock_buffers() grabbed reference on each buffer";Jan Kara;2019-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MDQwOWM2NWUyYzZjZDE1NDAwNDVlZTAxZmM1NWU1MGQ5NWUwOTgz;" In
fact, there's no reason for buffer_migrate_lock_buffers() to grab any
buffer references as the page is locked during all our operation and
thus nobody can reclaim buffers from the page";Jan Kara;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MDQwOWM2NWUyYzZjZDE1NDAwNDVlZTAxZmM1NWU1MGQ5NWUwOTgz;"So remove grabbing of buffer references which also makes
buffer_migrate_page_norefs() succeed.";Jan Kara;2019-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;"hugetlbfs: revert ""use i_mmap_rwsem for more pmd sharing synchronization""";Mike Kravetz;2019-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;"This reverts b43a9990055958e70347c56f90ea2ae32c67334c
The reverted commit caused issues with migration and poisoning of anon
huge pages";Mike Kravetz;2019-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;" The LTP move_pages12 test will cause an ""unable to handle
kernel NULL pointer"" BUG would occur with stack similar to";Mike Kravetz;2019-01-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;"  RIP: 0010:down_write+0x1b/0x40
  The purpose of the reverted patch was to fix some long existing races
with huge pmd sharing";Mike Kravetz;2019-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;" It used i_mmap_rwsem for this purpose with the
idea that this could also be used to address truncate/page fault races
with another patch";Mike Kravetz;2019-01-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;" Further analysis has determined that i_mmap_rwsem
can not be used to address all these hugetlbfs synchronization issues";Mike Kravetz;2019-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZGVhYWIzMmE4OWYwNGI3ZTJhMmRmODc3MTU4M2E3MTljNGFjNmI3;"Therefore, revert this patch while working an another approach to the
underlying issues.";Mike Kravetz;2019-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;mm: treewide: remove unused address argument from pte_alloc functions;Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"Patch series ""Add support for fast mremap""";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"This series speeds up the mremap(2) syscall by copying page tables at
the PMD level even for non-THP systems";Joel Fernandes (Google);2019-01-03;0;0
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;" There is concern that the extra
'address' argument that mremap passes to pte_alloc may do something
subtle architecture related in the future that may make the scheme not
work";Joel Fernandes (Google);2019-01-03;0;0
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;" Also we find that there is no point in passing the 'address' to
pte_alloc since its unused";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;" This patch therefore removes this argument
tree-wide resulting in a nice negative diff as well";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;" Also ensuring
along the way that the enabled architectures do not do anything funky
with the 'address' argument that goes unnoticed by the optimization";Joel Fernandes (Google);2019-01-03;1;0
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;Build and boot tested on x86-64;Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh; Build tested on arm64;Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;" The config
enablement patch for arm64 will be posted in the future after more
testing";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;The changes were obtained by applying the following Coccinelle script;Joel Fernandes (Google);2019-01-03;0;0
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;(thanks Julia for answering all Coccinelle questions!);Joel Fernandes (Google);2019-01-03;0;0
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;Following fix ups were done manually;Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"// Options: --include-headers --no-includes
// Note: I split the 'identifier fn' line, so if you are manually
// running it, please unsplit it so it runs for you";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"virtual patch
@pte_alloc_func_def depends on patch exists@
identifier fn =~
 fn(..";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"- , T2 E2
@pte_alloc_func_proto_noarg depends on patch exists@
@pte_alloc_func_proto depends on patch exists@
identifier fn =~
@pte_alloc_func_call depends on patch exists@
identifier fn =~
 fn(..";Joel Fernandes (Google);2019-01-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo0Y2Y1ODkyNDk1MWVmODBlZWM2MzZiODYzZTdhNTM5NzNjNDQyNjFh;"-,  E2
@pte_alloc_macro depends on patch exists@
identifier fn =~
- #define fn(a, b, c) e
+ #define fn(a, b) e
- #define fn(a, b) e
+ #define fn(a) e";Joel Fernandes (Google);2019-01-03;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization;Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;"While looking at BUGs associated with invalid huge page map counts, it was
discovered and observed that a huge pte pointer could become 'invalid' and
point to another task's page table";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj; Consider the following;Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;"A task takes a page fault on a shared hugetlbfs file and calls
huge_pte_alloc to get a ptep";Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" Suppose the returned ptep points to a
shared pmd";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;Now, another task truncates the hugetlbfs file;Mike Kravetz;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" As part of truncation, it
unmaps everyone who has the file mapped";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" If the range being truncated is
covered by a shared pmd, huge_pmd_unshare will be called";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" For all but the
last user of the shared pmd, huge_pmd_unshare will clear the pud pointing
to the pmd";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" If the task in the middle of the page fault is not the last
user, the ptep returned by huge_pte_alloc now points to another task's
page table or worse";Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" This leads to bad things such as incorrect page
map/reference counts or invalid memory references";Mike Kravetz;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;To fix, expand the use of i_mmap_rwsem as follows;Mike Kravetz;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;- i_mmap_rwsem is held in read mode whenever huge_pmd_share is called;Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;"  huge_pmd_share is only called via huge_pte_alloc, so callers of
  huge_pte_alloc take i_mmap_rwsem before calling";Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;" In addition, callers
  of huge_pte_alloc continue to hold the semaphore until finished with the
  ptep";Mike Kravetz;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDNhOTk5MDA1NTk1OGU3MDM0N2M1NmY5MGVhMmFlMzJjNjczMzRj;"- i_mmap_rwsem is held in write mode whenever huge_pmd_unshare is
  called.";Mike Kravetz;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODphYjQxZWU2ODc5OTgxYjNkM2ExNmExMDc5YTMzZmE2ZmQwNDNlYjNj;mm: migrate: drop unused argument of migrate_page_move_mapping();Jan Kara;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODphYjQxZWU2ODc5OTgxYjNkM2ExNmExMDc5YTMzZmE2ZmQwNDNlYjNj;"All callers of migrate_page_move_mapping() now pass NULL for 'head'
argument";Jan Kara;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODphYjQxZWU2ODc5OTgxYjNkM2ExNmExMDc5YTMzZmE2ZmQwNDNlYjNj; Drop it.;Jan Kara;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo4OWNiMDg4OGNhMTQ4M2FkNzI2NDg4NDRkZGQxYjgwMTg2M2E4OTQ5;mm: migrate: provide buffer_migrate_page_norefs();Jan Kara;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OWNiMDg4OGNhMTQ4M2FkNzI2NDg4NDRkZGQxYjgwMTg2M2E4OTQ5;"Provide a variant of buffer_migrate_page() that also checks whether there
are no unexpected references to buffer heads";Jan Kara;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OWNiMDg4OGNhMTQ4M2FkNzI2NDg4NDRkZGQxYjgwMTg2M2E4OTQ5;" This function will then be
safe to use for block device pages.";Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NGFkZTdjMTVjY2ZiNDZmODUyZjRlMjQ2OWIwMDdhMzM2NTE5MDRk;mm: migrate: move migrate_page_lock_buffers();Jan Kara;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NGFkZTdjMTVjY2ZiNDZmODUyZjRlMjQ2OWIwMDdhMzM2NTE5MDRk;"buffer_migrate_page() is the only caller of migrate_page_lock_buffers()
move it close to it and also drop the now unused stub for !CONFIG_BLOCK.";Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYzRmMTFlNjlmZDAwYzYxYzM4NjE5NzU5YjA3ZDAwNjMxYmRhNWNh;mm: migrate: lock buffers before migrate_page_move_mapping();Jan Kara;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYzRmMTFlNjlmZDAwYzYxYzM4NjE5NzU5YjA3ZDAwNjMxYmRhNWNh;"Lock buffers before calling into migrate_page_move_mapping() so that that
function doesn't have to know about buffers (which is somewhat unexpected
anyway) and all the buffer head logic is in buffer_migrate_page().";Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;mm: migration: factor out code to compute expected number of page references;Jan Kara;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;"Patch series ""mm: migrate: Fix page migration stalls for blkdev pages""";Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;"This patchset deals with page migration stalls that were reported by our
customer due to a block device page that had a bufferhead that was in the
bh LRU cache";Jan Kara;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;"The patchset modifies the page migration code so that bufferheads are
completely handled inside buffer_migrate_page() and then provides a new
migration helper for pages with buffer heads that is safe to use even for
block device pages and that also deals with bh lrus";Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;This patch (of 6);Jan Kara;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;"Factor out function to compute number of expected page references in
migrate_page_move_mapping()";Jan Kara;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODowYjM5MDFiMzhkOWQ5MTZmNjM0ZTkwM2NlN2NkMmE4ZGRkNWIxNTU5;" Note that we move hpage_nr_pages() and
page_has_private() checks from under xas_lock_irq() however this is safe
since we hold page lock.";Jan Kara;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzQ2ZDRmM2M0MzI0MWZmYTIzZDViZjM2MTUzYTA4MzBjMGUwMmNj;mm/mmu_notifier: use structure for invalidate_range_start/end calls v2;Jrme Glisse;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzQ2ZDRmM2M0MzI0MWZmYTIzZDViZjM2MTUzYTA4MzBjMGUwMmNj;"To avoid having to change many call sites everytime we want to add a
parameter use a structure to group all parameters for the mmu_notifier
invalidate_range_start/end cakks";Jrme Glisse;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzQ2ZDRmM2M0MzI0MWZmYTIzZDViZjM2MTUzYTA4MzBjMGUwMmNj; No functional changes with this patch.;Jrme Glisse;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;mm: put_and_wait_on_page_locked() while page is migrated;Hugh Dickins;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"Waiting on a page migration entry has used wait_on_page_locked() all along
since 2006: but you cannot safely wait_on_page_locked() without holding a
reference to the page, and that extra reference is enough to make
migrate_page_move_mapping() fail with -EAGAIN, when a racing task faults
on the entry before migrate_page_move_mapping() gets there";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"And that failure is retried nine times, amplifying the pain when trying to
migrate a popular page";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;" With a single persistent faulter, migration
sometimes succeeds; with two or three concurrent faulters, success becomes
much less likely (and the more the page was mapped, the worse the overhead
of unmapping and remapping it on each try)";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"This is especially a problem for memory offlining, where the outer level
retries forever (or until terminated from userspace), because a heavy
refault workload can trigger an endless loop of migration failures";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;wait_on_page_locked() is the wrong tool for the job;Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;David Herrmann (but was he the first?) noticed this issue in 2014;Hugh Dickins;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;Tim Chen started a thread in August 2017 which appears relevant;Hugh Dickins;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"where Kan Liang went
on to implicate __migration_entry_wait()";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"and the thread ended
up with the v4.14 commits: 2554db916586 (""sched/wait: Break up long wake
list walk"") 11a19c7b099f (""sched/wait: Introduce wakeup boomark in
wake_up_page_bit"")
Baoquan He reported ""Memory hotplug softlock issue"" 14 November 2018";Hugh Dickins;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"We have all assumed that it is essential to hold a page reference while
waiting on a page lock: partly to guarantee that there is still a struct
page when MEMORY_HOTREMOVE is configured, but also to protect against
reuse of the struct page going to someone who then holds the page locked
indefinitely, when the waiter can reasonably expect timely unlocking";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"But in fact, so long as wait_on_page_bit_common() does the put_page(), and
is careful not to rely on struct page contents thereafter, there is no
need to hold a reference to the page while waiting on it";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;" That does mean
that this case cannot go back through the loop: but that's fine for the
page migration case, and even if used more widely, is limited by the ""Stop
walking if it's locked"" optimization in wake_page_function()";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"Add interface put_and_wait_on_page_locked() to do this, using ""behavior""
enum in place of ""lock"" arg to wait_on_page_bit_common() to implement it";Hugh Dickins;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"No interruptible or killable variant needed yet, but they might follow: I
have a vague notion that reporting -EINTR should take precedence over
return from wait_on_page_bit_common() without knowing the page state, so
arrange it accordingly - but that may be nothing but pedantic";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"__migration_entry_wait() still has to take a brief reference to the page,
prior to calling put_and_wait_on_page_locked(): but now that it is dropped
before waiting, the chance of impeding page migration is very much
reduced";Hugh Dickins;2018-12-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;" Should we perhaps disable preemption across this?
shrink_page_list()'s __ClearPageLocked(): that was a surprise!  This
survived a lot of testing before that showed up";Hugh Dickins;2018-12-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;" PageWaiters may have
been set by wait_on_page_bit_common(), and the reference dropped, just
before shrink_page_list() succeeds in freezing its last page reference: in
such a case, unlock_page() must be used";Hugh Dickins;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;" Follow the suggestion from
Michal Hocko, just revert a978d6f52106 (""mm: unlockless reclaim"") now";Hugh Dickins;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"that optimization predates PageWaiters, and won't buy much these days; but
we can reinstate it for the !PageWaiters case if anyone notices";Hugh Dickins;2018-12-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTFlYTQzOWIxNmI5MjAwMmUwYTZmY2VlYmM1ZDE3OTQ5MDZlMjk3;"It does raise the question: should vmscan.c's is_page_cache_freeable() and
__remove_mapping() now treat a PageWaiters page as if an extra reference
were held?  Perhaps, but I don't think it matters much, since
shrink_page_list() already had to win its trylock_page(), so waiters are
not very common there: I noticed no difference when trying the bigger
change, and it's surely not needed while put_and_wait_on_page_locked() is
only used for page migration.";Hugh Dickins;2018-12-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWVmNWY5N2MxZjk0YzdiNzI1MjBiNDJkMzcyMDM3ZTk3YTgxYjk1;mm: thp: relocate flush_cache_range() in migrate_misplaced_transhuge_page();Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWVmNWY5N2MxZjk0YzdiNzI1MjBiNDJkMzcyMDM3ZTk3YTgxYjk1;"There should be no cache left by the time we overwrite the old transhuge
pmd with the new one";Andrea Arcangeli;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWVmNWY5N2MxZjk0YzdiNzI1MjBiNDJkMzcyMDM3ZTk3YTgxYjk1;" It's already too late to flush through the virtual
address because we already copied the page data to the new physical
address";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZWVmNWY5N2MxZjk0YzdiNzI1MjBiNDJkMzcyMDM3ZTk3YTgxYjk1;So flush the cache before the data copy;Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZWVmNWY5N2MxZjk0YzdiNzI1MjBiNDJkMzcyMDM3ZTk3YTgxYjk1;"Also delete the ""end"" variable to shutoff a ""unused variable"" warning on
x86 where flush_cache_range() is a noop.";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;mm: thp: fix mmu_notifier in migrate_misplaced_transhuge_page();Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"change_huge_pmd() after arming the numa/protnone pmd doesn't flush the TLB
right away";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;" do_huge_pmd_numa_page() flushes the TLB before calling
migrate_misplaced_transhuge_page()";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;" By the time do_huge_pmd_numa_page()
runs some CPU could still access the page through the TLB";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"change_huge_pmd() before arming the numa/protnone transhuge pmd calls
mmu_notifier_invalidate_range_start()";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;" So there's no need of
sequence in migrate_misplaced_transhuge_page() too, because by the time
migrate_misplaced_transhuge_page() runs, the pmd mapping has already been
invalidated in the secondary MMUs";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;" It has to or if a secondary MMU can
still write to the page, the migrate_page_copy() would lose data";Andrea Arcangeli;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"However an explicit mmu_notifier_invalidate_range() is needed before
migrate_misplaced_transhuge_page() starts copying the data of the
transhuge page or the below can happen for MMU notifier users sharing the
primary MMU pagetables and only implementing ->invalidate_range";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"CPU0		CPU1		GPU sharing linux pagetables using
                                only ->invalidate_range
				GPU secondary MMU writes to the page
				mapped by the transhuge pmd
change_pmd_range()
mmu..._range_start()
->invalidate_range_start() noop
change_huge_pmd()
set_pmd_at(numa/protnone)
pmd_unlock()
		do_huge_pmd_numa_page()
		CPU TLB flush globally (1)
		CPU cannot write to page
		migrate_misplaced_transhuge_page()
				GPU writes to the page..";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"		migrate_page_copy()
				...GPU stops writing to the page
CPU TLB flush (2)
mmu..._range_end() (3)
->invalidate_range_stop() noop
->invalidate_range()
				GPU secondary MMU is invalidated
				and cannot write to the page anymore
				(too late)
Just like we need a CPU TLB flush (1) because the TLB flush (2) arrives
too late, we also need a mmu_notifier_invalidate_range() before calling
migrate_misplaced_transhuge_page(), because the ->invalidate_range() in
(3) also arrives too late";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"This requirement is the result of the lazy optimization in
change_huge_pmd() that releases the pmd_lock without first flushing the
TLB and without first calling mmu_notifier_invalidate_range()";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"Even converting the removed mmu_notifier_invalidate_range_only_end() into
a mmu_notifier_invalidate_range_end() would not have been enough to fix
this, because it run after migrate_page_copy()";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"After the hugepage data copy is done migrate_misplaced_transhuge_page()
can proceed and call set_pmd_at without having to flush the TLB nor any
secondary MMUs because the secondary MMU invalidate, just like the CPU TLB
flush, has to happen before the migrate_page_copy() is called or it would
be a bug in the first place (and it was for drivers using
->invalidate_range())";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;KVM is unaffected because it doesn't implement ->invalidate_range();Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MDY2ZjBmOTMzYTFmZDcwN2JiMzg3ODE4NjY2NTc3NjljZmY3ZWZj;"The standard PAGE_SIZEd migrate_misplaced_page is less accelerated and
uses the generic migrate_pages which transitions the pte from
numa/protnone to a migration entry in try_to_unmap_one() and flushes TLBs
and all mmu notifiers there before copying the page.";Andrea Arcangeli;2018-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;mm: thp: fix MADV_DONTNEED vs migrate_misplaced_transhuge_page race condition;Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"Patch series ""migrate_misplaced_transhuge_page race conditions""";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"Aaron found a new instance of the THP MADV_DONTNEED race against
pmdp_clear_flush* variants, that was apparently left unfixed";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"While looking into the race found by Aaron, I may have found two more
issues in migrate_misplaced_transhuge_page";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"These race conditions would not cause kernel instability, but they'd
corrupt userland data or leave data non zero after MADV_DONTNEED";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"I did only minor testing, and I don't expect to be able to reproduce this
(especially the lack of ->invalidate_range before migrate_page_copy,
requires the latest iommu hardware or infiniband to reproduce)";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;" The last
patch is noop for x86 and it needs further review from maintainers of
archs that implement flush_cache_range() (not in CC yet)";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"To avoid confusion, it's not the first patch that introduces the bug fixed
in the second patch, even before removing the
pmdp_huge_clear_flush_notify, that _notify suffix was called after
migrate_page_copy already run";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;This patch (of 3);Andrea Arcangeli;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"This is a corollary of ced108037c2aa (""thp: fix MADV_DONTNEED vs";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;" numa
balancing race""), 58ceeb6bec8 (""thp: fix MADV_DONTNEED vs";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;" MADV_FREE
race"") and 5b7abeae3af8c (""thp: fix MADV_DONTNEED vs clear soft dirty
race)";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"When the above three fixes where posted Dave asked
but apparently this was missed";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"The pmdp_clear_flush* in migrate_misplaced_transhuge_page() was introduced
in a54a407fbf7 (""mm: Close races between THP migration and PMD numa
clearing"")";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"The important part of such commit is only the part where the page lock is
not released until the first do_huge_pmd_numa_page() finished disarming
the pagenuma/protnone";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"The addition of pmdp_clear_flush() wasn't beneficial to such commit and
there's no commentary about such an addition either";Andrea Arcangeli;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"I guess the pmdp_clear_flush() in such commit was added just in case for
safety, but it ended up introducing the MADV_DONTNEED race condition found
by Aaron";Andrea Arcangeli;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"At that point in time nobody thought of such kind of MADV_DONTNEED race
conditions yet (they were fixed later) so the code may have looked more
robust by adding the pmdp_clear_flush()";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;"This specific race condition won't destabilize the kernel, but it can
confuse userland because after MADV_DONTNEED the memory won't be zeroed
out";Andrea Arcangeli;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2MzMzkzNDEzZmU3ZTdkYzU0NDk4ZWEyMDBlYTk0NzQyZDYxZTE4;This also optimizes the code and removes a superfluous TLB flush.;Andrea Arcangeli;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;mm: workingset: tell cache transitions from workingset thrashing;Johannes Weiner;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;"Refaults happen during transitions between workingsets as well as in-place
thrashing";Johannes Weiner;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;" Knowing the difference between the two has a range of
applications, including measuring the impact of memory shortage on the
system performance, as well as the ability to smarter balance pressure
between the filesystem cache and the swap-backed workingset";Johannes Weiner;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;"During workingset transitions, inactive cache refaults and pushes out
established active cache";Johannes Weiner;2018-10-26;1;0
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;" When that active cache isn't stale, however,
and also ends up refaulting, that's bonafide thrashing";Johannes Weiner;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;"Introduce a new page flag that tells on eviction whether the page has been
active or not in its lifetime";Johannes Weiner;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;" This bit is then stored in the shadow
entry, to classify refaults as transitioning or thrashing";Johannes Weiner;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;"How many page->flags does this leave us with on 32-bit?
	20 bits are always page flags
	21 if you have an MMU
	23 with the zone bits for DMA, Normal, HighMem, Movable
	29 with the sparsemem section bits
	30 if PAE is enabled
	31 with this patch";Johannes Weiner;2018-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;So on 32-bit PAE, that leaves 1 bit for distinguishing two NUMA nodes;Johannes Weiner;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODk5YWQxOGM2MDcyZDY4OTg5NmJhZGFmYjgxMjY3YjBhMTA5MmE0;" If
that's not enough, the system can switch to discontigmem and re-gain the 6
or 7 sparsemem section bits.";Johannes Weiner;2018-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OWViOTQ2YTc0MzJiZTYzOWI0NTJmYWMyOTVjMGMyZTUxODZjNGE0;mm: Convert page migration to XArray;Matthew Wilcox;2017-12-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo4OWViOTQ2YTc0MzJiZTYzOWI0NTJmYWMyOTVjMGMyZTUxODZjNGE0;;Matthew Wilcox;2017-12-04;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz;mm/migrate.c: split only transparent huge pages when allocation fails;Anshuman Khandual;2018-10-05;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz;split_huge_page_to_list() fails on HugeTLB pages;Anshuman Khandual;2018-10-05;1;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz;" I was experimenting
with moving 32MB contig HugeTLB pages on arm64 (with a debug patch
applied) and hit the following stack trace when the kernel crashed";Anshuman Khandual;2018-10-05;1;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz;"When unmap_and_move[_huge_page]() fails due to lack of memory, the
splitting should happen only for transparent huge pages not for HugeTLB
pages";Anshuman Khandual;2018-10-05;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz; PageTransHuge() returns true for both THP and HugeTLB pages;Anshuman Khandual;2018-10-05;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjExMmZjMzAwNzAyZjk2Mzc0ZjM0MzY4NTEzZDU3Nzk1ZmM2ZDIz;"Hence the conditonal check should test PagesHuge() flag to make sure that
given pages is not a HugeTLB one.";Anshuman Khandual;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;mm, thp: fix mlocking THP page with migration enabled;Kirill A. Shutemov;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;A transparent huge page is represented by a single entry on an LRU list;Kirill A. Shutemov;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"Therefore, we can only make unevictable an entire compound page, not
individual subpages";Kirill A. Shutemov;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"If a user tries to mlock() part of a huge page, we want the rest of the
page to be reclaimable";Kirill A. Shutemov;2018-10-05;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"We handle this by keeping PTE-mapped huge pages on normal LRU lists: the
PMD on border of VM_LOCKED VMA will be split into PTE table";Kirill A. Shutemov;2018-10-05;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"Introduction of THP migration breaks[1] the rules around mlocking THP
pages";Kirill A. Shutemov;2018-10-05;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;" If we had a single PMD mapping of the page in mlocked VMA, the
page will get mlocked, regardless of PTE mappings of the page";Kirill A. Shutemov;2018-10-05;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"For tmpfs/shmem it's easy to fix by checking PageDoubleMap() in
remove_migration_pmd()";Kirill A. Shutemov;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;Anon THP pages can only be shared between processes via fork();Kirill A. Shutemov;2018-10-05;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;" Mlocked
page can only be shared if parent mlocked it before forking, otherwise CoW
will be triggered on mlock()";Kirill A. Shutemov;2018-10-05;0;0
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"For Anon-THP, we can fix the issue by munlocking the page on removing PTE
migration entry for the page";Kirill A. Shutemov;2018-10-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;" PTEs for the page will always come after
mlocked PMD: rmap walks VMAs from oldest to newest";Kirill A. Shutemov;2018-10-05;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;Test-case;Kirill A. Shutemov;2018-10-05;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTI1ZmU0MDVhYmVkYzFkYzhhNWIyMjI5YjgwZWU5MWMxNDM0MDE1;"	int main(void)
		addr = mmap((void *)0x20000000UL, 2UL << 20, PROT_READ | PROT_WRITE,
	        mbind(addr, 2UL << 20, MPOL_PREFERRED | MPOL_F_RELATIVE_NODES,";Kirill A. Shutemov;2018-10-05;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;mm, sched/numa: Remove rate-limiting of automatic NUMA balancing migration;Mel Gorman;2018-10-01;1;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Rate limiting of page migrations due to automatic NUMA balancing was
introduced to mitigate the worst-case scenario of migrating at high
frequency due to false sharing or slowly ping-ponging between nodes";Mel Gorman;2018-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Since then, a lot of effort was spent on correctly identifying these
pages and avoiding unnecessary migrations and the safety net may no longer
be required";Mel Gorman;2018-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Jirka Hladky reported a regression in 4.17 due to a scheduler patch that
avoids spreading STREAM tasks wide prematurely";Mel Gorman;2018-10-01;1;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"However, once the task
was properly placed, it delayed migrating the memory due to rate limiting";Mel Gorman;2018-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;Increasing the limit fixed the problem for him;Mel Gorman;2018-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Currently, the limit is hard-coded and does not account for the real
capabilities of the hardware";Mel Gorman;2018-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Even if an estimate was attempted, it would
not properly account for the number of memory controllers and it could
not account for the amount of bandwidth used for normal accesses";Mel Gorman;2018-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Rather
than fudging, this patch simply eliminates the rate limiting";Mel Gorman;2018-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"However, Jirka reports that a STREAM configuration using multiple
processes achieved similar performance to 4.16";Mel Gorman;2018-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"In local tests, this patch
improved performance of STREAM relative to the baseline but it is somewhat
machine-dependent";Mel Gorman;2018-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"Most workloads show little or not performance difference
implying that there is not a heavily reliance on the throttling mechanism
and it is safe to remove";Mel Gorman;2018-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplZmFmZmM1ZTQwYWVjZWQwYmNiNDk3ZWQ3YTBhNWI4YzE0YWJmY2Rm;"STREAM on 2-socket machine
                         4.19.0-rc5             4.19.0-rc5
                         numab-v1r1       noratelimit-v1r1
MB/sec copy     43298.52 (   0.00%)    44673.38 (   3.18%)
MB/sec scale    30115.06 (   0.00%)    31293.06 (   3.91%)
MB/sec add      32825.12 (   0.00%)    34883.62 (   6.27%)
MB/sec triad    32549.52 (   0.00%)    34906.60 (   7.24%";Mel Gorman;2018-10-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;mm/migrate: Use spin_trylock() while resetting rate limit;Srikar Dronamraju;2018-09-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;"Since this spinlock will only serialize the migrate rate limiting,
convert the spin_lock() to a spin_trylock()";Srikar Dronamraju;2018-09-21;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;"If another thread is updating, this
task can move on";Srikar Dronamraju;2018-09-21;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;"Specjbb2005 results (8 warehouses)
Higher bops are better
2 Socket - 2  Node Haswell - X86
JVMS  Prev    Current  %Change
2 Socket - 4 Node Power8 - PowerNV
JVMS  Prev    Current  %Change
2 Socket - 2  Node Power9 - PowerNV
JVMS  Prev    Current  %Change
4 Socket - 4  Node Power7 - PowerVM
JVMS  Prev     Current  %Change
Avoiding stretching of window intervals may be the reason for the
regression";Srikar Dronamraju;2018-09-21;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;Also code now uses READ_ONCE/WRITE_ONCE;Srikar Dronamraju;2018-09-21;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;"That may
also be hurting performance to some extent";Srikar Dronamraju;2018-09-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;Some events stats before and after applying the patch;Srikar Dronamraju;2018-09-21;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NTM0NjEyMTIzZTBmNWQwMjBhYmExMDc2YTZiYjUwNWRiMGU2YmZl;"perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
Event                     Before          After
cs                        14,285,708      13,818,546
migrations                1,180,621       1,149,960
faults                    339,114         385,583
cache-misses              55,205,631,894  55,259,546,768
sched:sched_move_numa     843             2,257
sched:sched_stick_numa    6               9
sched:sched_swap_numa     219             512
migrate:mm_migrate_pages  365             2,225
vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
Event                   Before  After
numa_hint_faults        26907   72692
numa_hint_faults_local  24279   62270
numa_hit                239771  238762
numa_huge_pte_updates   0       48
numa_interleave         68      75
numa_local              239688  238676
numa_other              83      86
numa_pages_migrated     363     2225
numa_pte_updates        27415   98557
perf stats 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
Event                     Before          After
cs                        3,202,779       3,173,490
migrations                37,186          36,966
faults                    106,076         108,776
cache-misses              12,024,873,744  12,200,075,320
sched:sched_move_numa     931             1,264
sched:sched_stick_numa    0               0
sched:sched_swap_numa     1               0
migrate:mm_migrate_pages  637             899
vmstat 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
Event                   Before  After
numa_hint_faults        17409   21109
numa_hint_faults_local  14367   17120
numa_hit                73953   72934
numa_huge_pte_updates   20      42
numa_interleave         25      33
numa_local              73892   72866
numa_other              61      68
numa_pages_migrated     668     915
numa_pte_updates        27276   42326
perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
Event                     Before       After
cs                        8,474,013    8,312,022
migrations                254,934      231,705
faults                    320,506      310,242
cache-misses              110,580,458  402,324,573
sched:sched_move_numa     725          193
sched:sched_stick_numa    0            0
sched:sched_swap_numa     7            3
migrate:mm_migrate_pages  145          93
vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
Event                   Before  After
numa_hint_faults        22797   11838
numa_hint_faults_local  21539   11216
numa_hit                89308   90689
numa_huge_pte_updates   0       0
numa_interleave         865     1579
numa_local              88955   89634
numa_other              353     1055
numa_pages_migrated     149     92
numa_pte_updates        22930   12109
perf stats 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
Event                     Before     After
cs                        2,195,628  2,170,481
migrations                11,179     10,126
faults                    149,656    160,962
cache-misses              8,117,515  10,834,845
sched:sched_move_numa     49         10
sched:sched_stick_numa    0          0
sched:sched_swap_numa     0          0
migrate:mm_migrate_pages  5          2
vmstat 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
Event                   Before  After
numa_hint_faults        3577    403
numa_hint_faults_local  3476    358
numa_hit                26142   25898
numa_huge_pte_updates   0       0
numa_interleave         358     207
numa_local              26042   25860
numa_other              100     38
numa_pages_migrated     5       2
numa_pte_updates        3587    400
perf stats 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
Event                     Before           After
cs                        100,602,296      110,339,633
migrations                4,135,630        4,139,812
faults                    789,256          863,622
cache-misses              226,160,621,058  231,838,045,660
sched:sched_move_numa     1,366            2,196
sched:sched_stick_numa    16               33
sched:sched_swap_numa     374              544
migrate:mm_migrate_pages  1,350            2,469
vmstat 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
Event                   Before  After
numa_hint_faults        47857   85748
numa_hint_faults_local  39768   66831
numa_hit                240165  242213
numa_huge_pte_updates   0       0
numa_interleave         0       0
numa_local              240165  242211
numa_other              0       2
numa_pages_migrated     1224    2376
numa_pte_updates        48354   86233
perf stats 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
Event                     Before          After
cs                        58,515,496      59,331,057
migrations                564,845         552,019
faults                    245,807         266,586
cache-misses              73,603,757,976  73,796,312,990
sched:sched_move_numa     996             981
sched:sched_stick_numa    10              54
sched:sched_swap_numa     193             286
migrate:mm_migrate_pages  646             713
vmstat 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
Event                   Before  After
numa_hint_faults        13422   14807
numa_hint_faults_local  5619    5738
numa_hit                36118   36230
numa_huge_pte_updates   0       0
numa_interleave         0       0
numa_local              36116   36228
numa_other              2       2
numa_pages_migrated     616     703
numa_pte_updates        13374   14742";Srikar Dronamraju;2018-09-21;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;mm: soft-offline: close the race against page allocation;Naoya Horiguchi;2018-08-24;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;"A process can be killed with SIGBUS(BUS_MCEERR_AR) when it tries to
allocate a page that was just freed on the way of soft-offline";Naoya Horiguchi;2018-08-24;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;" This is
undesirable because soft-offline (which is about corrected error) is
less aggressive than hard-offline (which is about uncorrected error),
and we can make soft-offline fail and keep using the page for good
reason like ""system is busy.""
Two main changes of this patch are";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;- setting migrate type of the target page to MIGRATE_ISOLATE;Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;"As done
  in free_unref_page_commit(), this makes kernel bypass pcplist when
  freeing the page";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNGFlOTkxNmVhMjk0NzM0MTE4MGQyYjUzOGY0ODg3NWZmMzkzYTg2;"So we can assume that the page is in freelist just
  after put_page() returns,
- setting PG_hwpoison on free page under zone->lock which protects
  freelists, so this allows us to avoid setting PG_hwpoison on a page
  that is decided to be allocated soon.";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;mm: fix race on soft-offlining free huge pages;Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;"Patch series ""mm: soft-offline: fix race against page allocation""";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;"Xishi recently reported the issue about race on reusing the target pages
of soft offlining";Naoya Horiguchi;2018-08-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;" Discussion and analysis showed that we need make
sure that setting PG_hwpoison should be done in the right place under
zone->lock for soft offline";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;" 1/2 handles free hugepage's case, and 2/2
hanldes free buddy page's case";Naoya Horiguchi;2018-08-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;This patch (of 2);Naoya Horiguchi;2018-08-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;"There's a race condition between soft offline and hugetlb_fault which
causes unexpected process killing and/or hugetlb allocation failure";Naoya Horiguchi;2018-08-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;The process killing is caused by the following flow;Naoya Horiguchi;2018-08-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;"  CPU 0               CPU 1              CPU 2
  soft offline
    get_any_page
    // find the hugetlb is free
                      mmap a hugetlb file
                      page fault
                          hugetlb_fault
                            hugetlb_no_page
                              alloc_huge_page
                              // succeed
      soft_offline_free_page
      // set hwpoison flag
                                         mmap the hugetlb file
                                         page fault
                                             hugetlb_fault
                                               hugetlb_no_page
                                                 find_lock_page
                                                   return VM_FAULT_HWPOISON
                                           mm_fault_error
                                             do_sigbus
                                             // kill the process
The hugetlb allocation failure comes from the following flow";Naoya Horiguchi;2018-08-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;"  CPU 0                          CPU 1
                                 mmap a hugetlb file
                                 // reserve all free page but don't fault-in
  soft offline
    get_any_page
    // find the hugetlb is free
      soft_offline_free_page
      // set hwpoison flag
        dissolve_free_huge_page
        // fail because all free hugepages are reserved
                                 page fault
                                     hugetlb_fault
                                       hugetlb_no_page
                                         alloc_huge_page
                                             dequeue_huge_page_node_exact
                                             // ignore hwpoisoned hugepage
                                             // and finally fail due to no-mem
The root cause of this is that current soft-offline code is written based
on an assumption that PageHWPoison flag should be set at first to avoid
accessing the corrupted data";Naoya Horiguchi;2018-08-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;" This makes sense for memory_failure() or
hard offline, but does not for soft offline because soft offline is about
corrected (not uncorrected) error and is safe from data lost";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YmM5YjU2NDMzYjc2ZTQwZDExMDk5MzM4ZDI3ZmJjNWNkMjkzNWNh;" This patch
changes soft offline semantics where it sets PageHWPoison flag only after
containment of the error page completes successfully.";Naoya Horiguchi;2018-08-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;include/linux/compiler*.h: make compiler-*.h mutually exclusive;Nick Desaulniers;2018-08-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;"Commit cafa0010cd51 (""Raise the minimum required gcc version to 4.6"")
recently exposed a brittle part of the build for supporting non-gcc
compilers";Nick Desaulniers;2018-08-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;"Both Clang and ICC define __GNUC__, __GNUC_MINOR__, and
__GNUC_PATCHLEVEL__ for quick compatibility with code bases that haven't
added compiler specific checks for __clang__ or __INTEL_COMPILER";Nick Desaulniers;2018-08-22;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;"This is brittle, as they happened to get compatibility by posing as a
certain version of GCC";Nick Desaulniers;2018-08-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;" This broke when upgrading the minimal version
of GCC required to build the kernel, to a version above what ICC and
Clang claim to be";Nick Desaulniers;2018-08-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;"Rather than always including compiler-gcc.h then undefining or
redefining macros in compiler-intel.h or compiler-clang.h, let's
separate out the compiler specific macro definitions into mutually
exclusive headers, do more proper compiler detection, and keep shared
definitions in compiler_types.h";Nick Desaulniers;2018-08-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MTVmMGRkYjM0NmMxOTYwMThkNGQ4ZjhmNTVjMTJiODNkYTFkZTNm;"Fixes: cafa0010cd51 (""Raise the minimum required gcc version to 4.6"")";Nick Desaulniers;2018-08-22;0;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;dax: remove VM_MIXEDMAP for fsdax and device dax;Dave Jiang;2018-08-17;1;0
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;This patch is reworked from an earlier patch that Dan has posted;Dave Jiang;2018-08-17;1;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;"VM_MIXEDMAP is used by dax to direct mm paths like vm_normal_page() that
the memory page it is dealing with is not typical memory from the linear
map";Dave Jiang;2018-08-17;0;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;" The get_user_pages_fast() path, since it does not resolve the vma,
use that as a VM_MIXEDMAP replacement in some locations";Dave Jiang;2018-08-17;0;0
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;" In the cases
where there is no pte to consult we fallback to using vma_is_dax() to
detect the VM_MIXEDMAP special case";Dave Jiang;2018-08-17;0;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;"Now that we have explicit driver pfn_t-flag opt-in/opt-out for
get_user_pages() support for DAX we can stop setting VM_MIXEDMAP";Dave Jiang;2018-08-17;0;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;" This
also means we no longer need to worry about safely manipulating vm_flags
in a future where we support dynamically changing the dax mode of a
file";Dave Jiang;2018-08-17;1;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;"DAX should also now be supported with madvise_behavior(), vma_merge(),
and copy_page_range()";Dave Jiang;2018-08-17;1;1
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;This patch has been tested against ndctl unit test;Dave Jiang;2018-08-17;0;0
MDY6Q29tbWl0MjMyNTI5ODplMWZiNGEwODY0OTU4ZmFjMmZiMWIyM2Y5ZjQ1NjJhOWY5MGUzZThm;" It has also been
tested against xfstests commit: 625515d using fake pmem created by
memmap and no additional issues have been observed.";Dave Jiang;2018-08-17;0;0
MDY6Q29tbWl0MjMyNTI5ODowMTM1NjdiZTE5NzYxZTJkMTRmYzJhMjY3NmZlNzY4NmFjNTRjOWFj;mm: migrate: fix double call of radix_tree_replace_slot();Naoya Horiguchi;2018-05-11;1;1
MDY6Q29tbWl0MjMyNTI5ODowMTM1NjdiZTE5NzYxZTJkMTRmYzJhMjY3NmZlNzY4NmFjNTRjOWFj;"radix_tree_replace_slot() is called twice for head page, it's obviously
a bug";Naoya Horiguchi;2018-05-11;1;1
MDY6Q29tbWl0MjMyNTI5ODowMTM1NjdiZTE5NzYxZTJkMTRmYzJhMjY3NmZlNzY4NmFjNTRjOWFj; Let's fix it.;Naoya Horiguchi;2018-05-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;mm: enable thp migration for shmem thp;Naoya Horiguchi;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;"My testing for the latest kernel supporting thp migration showed an
infinite loop in offlining the memory block that is filled with shmem
thps";Naoya Horiguchi;2018-04-20;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;" We can get out of the loop with a signal, but kernel should return
with failure in this case";Naoya Horiguchi;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;"What happens in the loop is that scan_movable_pages() repeats returning
the same pfn without any progress";Naoya Horiguchi;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;" That's because page migration always
fails for shmem thps";Naoya Horiguchi;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;"In memory offline code, memory blocks containing unmovable pages should be
prevented from being offline targets by has_unmovable_pages() inside
start_isolate_page_range()";Naoya Horiguchi;2018-04-20;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;" So it's possible to change migratability for
non-anonymous thps to avoid the issue, but it introduces more complex and
thp-specific handling in migration code, so it might not good";Naoya Horiguchi;2018-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;"So this patch is suggesting to fix the issue by enabling thp migration for
shmem thp";Naoya Horiguchi;2018-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzE3NjlhZTUyNjA5ZWEwMDQ0YTk5MDE3MDkwNDJlNTYzNGMyMzA2;" Both of anon/shmem thp are migratable so we don't need
precheck about the type of thps.";Naoya Horiguchi;2018-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;mm: fix do_pages_move status handling;Michal Hocko;2018-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;"Li Wang has reported that LTP move_pages04 test fails with the current
tree";Michal Hocko;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;LTP move_pages04;Michal Hocko;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;"   TFAIL  :  move_pages04.c:143: status[1] is EPERM, expected EFAULT
The test allocates an array of two pages, one is present while the other
is not (resp";Michal Hocko;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;" backed by zero page) and it expects EFAULT for the second
page as the man page suggests";Michal Hocko;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;" We are reporting EPERM which doesn't make
any sense and this is a result of a bug from cf5f16b23ec9 (""mm: unclutter
THP migration"")";Michal Hocko;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;"do_pages_move tries to handle as many pages in one batch as possible so we
queue all pages with the same node target together and that corresponds to
[start, i] range which is then used to update status array";Michal Hocko;2018-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;add_page_for_migration will correctly notice the zero (resp;Michal Hocko;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;" !present)
page and returns with EFAULT which gets written to the status";Michal Hocko;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;" But if
this is the last page in the array we do not update start and so the last
store_status after the loop will overwrite the range of the last batch
with NUMA_NO_NODE (which corresponds to EPERM)";Michal Hocko;2018-04-20;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZjE3NWNmNWM5OWRjMGUzYWRkMmFhYzBlYTFjZDU0ZTBmOWNhODdk;"Fix this by simply bailing out from the last flush if the pagelist is
empty as there is clearly nothing more to do.";Michal Hocko;2018-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOTNiMDE2MzEzYjNiYTgwMDNjM2I4YmI3MWY1NjlhZjkxZjE5ZmM3;page cache: use xa_lock;Matthew Wilcox;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOTNiMDE2MzEzYjNiYTgwMDNjM2I4YmI3MWY1NjlhZjkxZjE5ZmM3;"Remove the address_space ->tree_lock and use the xa_lock newly added to
the radix_tree_root";Matthew Wilcox;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOTNiMDE2MzEzYjNiYTgwMDNjM2I4YmI3MWY1NjlhZjkxZjE5ZmM3;" Rename the address_space ->page_tree to ->i_pages,
since we don't really care that it's a tree.";Matthew Wilcox;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;mm: unclutter THP migration;Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;"THP migration is hacked into the generic migration with rather
surprising semantic";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" The migration allocation callback is supposed to
check whether the THP can be migrated at once and if that is not the
case then it allocates a simple page to migrate";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" unmap_and_move then
fixes that up by spliting the THP into small pages while moving the head
page to the newly allocated order-0 page";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" Remaning pages are moved to
the LRU list by split_huge_page";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" The same happens if the THP allocation
fails";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm; This is really ugly and error prone [1];Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;"I also believe that split_huge_page to the LRU lists is inherently wrong
because all tail pages are not migrated";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" Some callers will just work
around that by retrying (e.g";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm; memory hotplug);Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" There are other pfn
walkers which are simply broken though";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm; e.g;Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;"madvise_inject_error will
migrate head and then advances next pfn by the huge page size";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;"do_move_page_to_node_array, queue_pages_range (migrate_pages, mbind),
will simply split the THP before migration if the THP migration is not
supported then falls back to single page migration but it doesn't handle
tail pages if the THP migration path is not able to allocate a fresh THP
so we end up with ENOMEM and fail the whole migration which is a
questionable behavior";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" Page compaction doesn't try to migrate large
pages so it should be immune";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;"This patch tries to unclutter the situation by moving the special THP
handling up to the migrate_pages layer where it actually belongs";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" We
simply split the THP page into the existing list if unmap_and_move fails
with ENOMEM and retry";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NDcyM2FhZmI5ZTc2NDE0ZmFkYTdjMWMxOTg3MzNhODZmMDFlYThm;" So we will _always_ migrate all THP subpages and
specific migrate_pages users do not have to deal with this case in a
special way.";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;mm, migrate: remove reason argument from new_page_t;Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;No allocation callback is using this argument anymore;Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;" new_page_node
used to use this parameter to convey node_id resp";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;" migration error up
to move_pages code (do_move_page_to_node_array)";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;" The error status never
made it into the final status field and we have a better way to
communicate node id to the status field now";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo2NjZmZWIyMWEwMDgzZTViMjlkZGQ5NjU4ODU1M2ZmYTBjYzM1N2I2;" All other allocation
callbacks simply ignored the argument so we can drop it finally.";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;mm, numa: rework do_pages_move;Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"Patch series ""unclutter thp migration""
Motivation";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"THP migration is hacked into the generic migration with rather
surprising semantic";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" The migration allocation callback is supposed to
check whether the THP can be migrated at once and if that is not the
case then it allocates a simple page to migrate";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" unmap_and_move then
fixes that up by splitting the THP into small pages while moving the
head page to the newly allocated order-0 page";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Remaining pages are
moved to the LRU list by split_huge_page";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" The same happens if the THP
allocation fails";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi; This is really ugly and error prone [2];Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"I also believe that split_huge_page to the LRU lists is inherently wrong
because all tail pages are not migrated";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Some callers will just work
around that by retrying (e.g";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi; memory hotplug);Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" There are other pfn
walkers which are simply broken though";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi; e.g;Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"madvise_inject_error will
migrate head and then advances next pfn by the huge page size";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"do_move_page_to_node_array, queue_pages_range (migrate_pages, mbind),
will simply split the THP before migration if the THP migration is not
supported then falls back to single page migration but it doesn't handle
tail pages if the THP migration path is not able to allocate a fresh THP
so we end up with ENOMEM and fail the whole migration which is a
questionable behavior";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Page compaction doesn't try to migrate large
pages so it should be immune";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"The first patch reworks do_pages_move which relies on a very ugly
calling semantic when the return status is pushed to the migration path
via private pointer";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" It uses pre allocated fixed size batching to
achieve that";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" We simply cannot do the same if a THP is to be split
during the migration path which is done in the patch 3";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Patch 2 is
follow up cleanup which removes the mentioned return status calling
convention ugliness";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;On a side note;Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"There are some semantic issues I have encountered on the way when
working on patch 1 but I am not addressing them here";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi; E.g;Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"trying to
move THP tail pages will result in either success or EBUSY (the later
one more likely once we isolate head from the LRU list)";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Hugetlb
reports EACCESS on tail pages";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Some errors are reported via status
parameter but migration failures are not even though the original
`reason' argument suggests there was an intention to do so";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" From a
quick look into git history this never worked";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" I have tried to keep the
semantic unchanged";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"Then there is a relatively minor thing that the page isolation might
fail because of pages not being on the LRU - e.g";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"because they are
sitting on the per-cpu LRU caches";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi; Easily fixable;Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;This patch (of 3);Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"do_pages_move is supposed to move user defined memory (an array of
addresses) to the user defined numa nodes (an array of nodes one for
each address)";Michal Hocko;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" The user provided status array then contains resulting
numa node for each address or an error";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" The semantic of this function
is little bit confusing because only some errors are reported back";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;Notably migrate_pages error is only reported via the return value;Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" This
patch doesn't try to address these semantic nuances but rather change
the underlying implementation";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"Currently we are processing user input (which can be really large) in
batches which are stored to a temporarily allocated page";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Each address
is resolved to its struct page and stored to page_to_node structure
along with the requested target numa node";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" The array of these
structures is then conveyed down the page migration path via private
argument";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" new_page_node then finds the corresponding structure and
allocates the proper target page";Michal Hocko;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"What is the problem with the current implementation and why to change
it? Apart from being quite ugly it also doesn't cope with unexpected
pages showing up on the migration list inside migrate_pages path";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" That
doesn't happen currently but the follow up patch would like to make the
thp migration code more clear and that would need to split a THP into
the list for some cases";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;"How does the new implementation work? Well, instead of batching into a
fixed size array we simply batch all pages that should be migrated to
the same node and isolate all of them into a linked list which doesn't
require any additional storage";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" This should work reasonably well
because page migration usually migrates larger ranges of memory to a
specific node";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" So the common case should work equally well as the
current implementation";Michal Hocko;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" Even if somebody constructs an input where the
target numa nodes would be interleaved we shouldn't see a large
performance impact because page migration alone doesn't really benefit
from batching";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphNDliZDRkNzE2MzcwN2RlMzc3YWVlMDYyZjE3YmVmZWY2ZGE4OTFi;" mmap_sem batching for the lookup is quite questionable
and isolate_lru_page which would benefit from batching is not using it
even in the current implementation.";Michal Hocko;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;mm/migrate: properly preserve write attribute in special migrate entry;Ralph Campbell;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;"Use of pte_write(pte) is only valid for present pte, the common code
which set the migration entry can be reach for both valid present pte
and special swap entry (for device memory)";Ralph Campbell;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;" Fix the code to use the
mpfn value which properly handle both cases";Ralph Campbell;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;"On x86 this did not have any bad side effect because pte write bit is
below PAGE_BIT_GLOBAL and thus special swap entry have it set to 0 which
in turn means we were always creating read only special migration entry";Ralph Campbell;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;"So once migration did finish we always write protected the CPU page
table entry (moreover this is only an issue when migrating from device
memory to system memory)";Ralph Campbell;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;" End effect is that CPU write access would
fault again and restore write permission";Ralph Campbell;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;"This behaviour isn't too bad; it just burns CPU cycles by forcing CPU to
take a second fault on write access";Ralph Campbell;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;"ie, double faulting the same
address";Ralph Campbell;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowNzcwNzEyNWFlYzZhNzUyOTkwMDYxNmJhNDkxMjEwZWMzZDg1ZmM2;" There is no corruption or incorrect states (it behaves as a
COWed page from a fork with a mapcount of 1).";Ralph Campbell;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;sched/numa: avoid trapping faults and attempting migration of file-backed dirty pages;Mel Gorman;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"change_pte_range is called from task work context to mark PTEs for
receiving NUMA faulting hints";Mel Gorman;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" If the marked pages are dirty then
migration may fail";Mel Gorman;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" Some filesystems cannot migrate dirty pages without
blocking so are skipped in MIGRATE_ASYNC mode which just wastes CPU";Mel Gorman;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"Even when they can, it can be a waste of cycles when the pages are
shared forcing higher scan rates";Mel Gorman;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" This patch avoids marking shared
dirty pages for hinting faults but also will skip a migration if the
page was dirtied after the scanner updated a clean page";Mel Gorman;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"This is most noticeable running the NASA Parallel Benchmark when backed
by btrfs, the default root filesystem for some distributions, but also
noticeable when using XFS";Mel Gorman;2018-04-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"The following are results from a 4-socket machine running a 4.16-rc4
kernel with some scheduler patches that are pending for the next merge
window";Mel Gorman;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"                        4.16.0-rc4             4.16.0-rc4
                 schedtip-20180309          nodirty-v1
  Time cg.D      459.07 (   0.00%)      444.21 (   3.24%)
  Time ep.D       76.96 (   0.00%)       77.69 (  -0.95%)
  Time is.D       25.55 (   0.00%)       27.85 (  -9.00%)
  Time lu.D      601.58 (   0.00%)      596.87 (   0.78%)
  Time mg.D      107.73 (   0.00%)      108.22 (  -0.45%)
is.D regresses slightly in terms of absolute time but note that that
particular load varies quite a bit from run to run";Mel Gorman;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" The more relevant
observation is the total system CPU usage";Mel Gorman;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"            4.16.0-rc4  4.16.0-rc4
          schedtip-20180309 nodirty-v1
  User        71471.91    70627.04
  System      11078.96     8256.13
  Elapsed       661.66      632.74
That is a substantial drop in system CPU usage and overall the workload
completes faster";Mel Gorman;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" The NUMA balancing statistics are also interesting
  NUMA base PTE updates        111407972   139848884
  NUMA huge PMD updates           206506      264869
  NUMA page range updates      217139044   275461812
  NUMA hint faults               4300924     3719784
  NUMA hint local faults         3012539     3416618
  NUMA hint local percent             70          91
  NUMA pages migrated            1517487     1358420
While more PTEs are scanned due to changes in what faults are gathered,
it's clear that a far higher percentage of faults are local as the bulk
of the remote hits were dirty pages that, in this case with btrfs, had
no chance of migrating";Mel Gorman;2018-04-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;"The following is a comparison when using XFS as that is a more realistic
filesystem choice for a data partition
                        4.16.0-rc4             4.16.0-rc4
                 schedtip-20180309          nodirty-v1r47
  Time cg.D      485.28 (   0.00%)      442.62 (   8.79%)
  Time ep.D       77.68 (   0.00%)       77.54 (   0.18%)
  Time is.D       26.44 (   0.00%)       24.79 (   6.24%)
  Time lu.D      597.46 (   0.00%)      597.11 (   0.06%)
  Time mg.D      142.65 (   0.00%)      105.83 (  25.81%)
That is a reasonable gain on two relatively long-lived workloads";Mel Gorman;2018-04-10;0;0
MDY6Q29tbWl0MjMyNTI5ODowOWE5MTNhN2E5NDdmYjZmNjI0Mzc5ZTlkYTIyNjcwOTk0OTQyYjg1;" While
not presented, there is also a substantial drop in system CPu usage and
the NUMA balancing stats show similar improvements in locality as btrfs
did.";Mel Gorman;2018-04-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YWRkZjQ0Mzg4MjU1ZjZmYTk5YzgzZTNlMmFkNzljZWYwODEzNjk4;mm: add kernel_move_pages() helper, move compat syscall to mm/migrate.c;Dominik Brodowski;2018-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YWRkZjQ0Mzg4MjU1ZjZmYTk5YzgzZTNlMmFkNzljZWYwODEzNjk4;"Move compat_sys_move_pages() to mm/migrate.c and make it call a newly
introduced helper -- kernel_move_pages() -- instead of the syscall";Dominik Brodowski;2018-03-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YWRkZjQ0Mzg4MjU1ZjZmYTk5YzgzZTNlMmFkNzljZWYwODEzNjk4;This patch is part of a series which removes in-kernel calls to syscalls;Dominik Brodowski;2018-03-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YWRkZjQ0Mzg4MjU1ZjZmYTk5YzgzZTNlMmFkNzljZWYwODEzNjk4;On this basis, the syscall entry path can be streamlined;Dominik Brodowski;2018-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YWRkZjQ0Mzg4MjU1ZjZmYTk5YzgzZTNlMmFkNzljZWYwODEzNjk4;For details, see;Dominik Brodowski;2018-03-17;0;0
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;mm, hugetlb: do not rely on overcommit limit during migration;Michal Hocko;2018-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;hugepage migration relies on __alloc_buddy_huge_page to get a new page;Michal Hocko;2018-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;This has 2 main disadvantages;Michal Hocko;2018-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;"1) it doesn't allow to migrate any huge page if the pool is used
   completely which is not an exceptional case as the pool is static and
   unused memory is just wasted";Michal Hocko;2018-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;"2) it leads to a weird semantic when migration between two numa nodes
   might increase the pool size of the destination NUMA node while the
   page is in use";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" The issue is caused by per NUMA node surplus pages
   tracking (see free_huge_page)";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;"Address both issues by changing the way how we allocate and account
pages allocated for migration";Michal Hocko;2018-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk; Those should temporal by definition;Michal Hocko;2018-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" So
we mark them that way (we will abuse page flags in the 3rd page) and
update free_huge_page to free such pages to the page allocator";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" Page
migration path then just transfers the temporal status from the new page
to the old one which will be freed on the last reference";Michal Hocko;2018-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" The global
surplus count will never change during this path but we still have to be
careful when migrating a per-node suprlus page";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" This is now handled in
move_hugetlb_state which is called from the migration path and it copies
the hugetlb specific page state and fixes up the accounting when needed
Rename __alloc_buddy_huge_page to __alloc_surplus_huge_page to better
reflect its purpose";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;" The new allocation routine for the migration path
is __alloc_migrate_huge_page";Michal Hocko;2018-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;"The user visible effect of this patch is that migrated pages are really
temporal and they travel between NUMA nodes as per the migration
request";Michal Hocko;2018-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODphYjVhYzkwYWVjZjU2ODVlYjYzMGM0MmMzOTZmNWYxNDcyNmIwYWZk;"Before migration
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:1
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
After
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:0
  /sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:0
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:1
  /sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
with the previous implementation, both nodes would have nr_hugepages:1
until the page is freed.";Michal Hocko;2018-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNTVlMTAxNGY5ZTU2N2Q4MzBlYjNhN2Y1N2Q4NzlhMzQ4NzJhZjRi;"Revert ""mm, thp: Do not make pmd/pud dirty without a reason""";Linus Torvalds;2017-11-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNTVlMTAxNGY5ZTU2N2Q4MzBlYjNhN2Y1N2Q4NzlhMzQ4NzJhZjRi;This reverts commit 152e93af3cfe2d29d8136cc0a02a8612507136ee;Linus Torvalds;2017-11-29;1;0
MDY6Q29tbWl0MjMyNTI5ODpmNTVlMTAxNGY5ZTU2N2Q4MzBlYjNhN2Y1N2Q4NzlhMzQ4NzJhZjRi;"It was a nice cleanup in theory, but as Nicolai Stange points out, we do
need to make the page dirty for the copy-on-write case even when we
didn't end up making it writable, since the dirty bit is what we use to
check that we've gone through a COW cycle.";Linus Torvalds;2017-11-29;0;1
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;mm, thp: Do not make pmd/pud dirty without a reason;Kirill A. Shutemov;2017-11-27;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"Currently we make page table entries dirty all the time regardless of
access type and don't even consider if the mapping is write-protected";Kirill A. Shutemov;2017-11-27;1;0
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"The reasoning is that we don't really need dirty tracking on THP and
making the entry dirty upfront may save some time on first write to the
page";Kirill A. Shutemov;2017-11-27;0;1
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"Unfortunately, such approach may result in false-positive
can_follow_write_pmd() for huge zero page or read-only shmem file";Kirill A. Shutemov;2017-11-27;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"Let's only make page dirty only if we about to write to the page anyway
(as we do for small pages)";Kirill A. Shutemov;2017-11-27;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"I've restructured the code to make entry dirty inside
maybe_p[mu]d_mkwrite()";Kirill A. Shutemov;2017-11-27;0;1
MDY6Q29tbWl0MjMyNTI5ODoxNTJlOTNhZjNjZmUyZDI5ZDgxMzZjYzBhMDJhODYxMjUwNzEzNmVl;"It also takes into account if the vma is
write-protected.";Kirill A. Shutemov;2017-11-27;1;0
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;mm/mmu_notifier: avoid call to invalidate_range() in range_end();Jrme Glisse;2017-11-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;"This is an optimization patch that only affect mmu_notifier users which
rely on the invalidate_range() callback";Jrme Glisse;2017-11-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;" This patch avoids calling that
callback twice in a row from inside __mmu_notifier_invalidate_range_end
Existing pattern (before this patch)";Jrme Glisse;2017-11-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;New pattern (after this patch);Jrme Glisse;2017-11-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;"We call the invalidate_range callback after clearing the page table
under the page table lock and we skip the call to invalidate_range
inside the __mmu_notifier_invalidate_range_end() function";Jrme Glisse;2017-11-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NjQ1YjlmZTg0YmY0ODc4ZjA0Yzc5NTlhNzVkZjdjM2MyZDFiYmI5;Idea from Andrea Arcangeli;Jrme Glisse;2017-11-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;License cleanup: add SPDX GPL-2.0 license identifier to files with no license;Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Many source files in the tree are missing licensing information, which
makes it harder for compliance tools to determine the correct license";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"By default all files without license information are under the default
license of the kernel, which is GPL version 2";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Update the files which contain no license information with the 'GPL-2.0'
SPDX license identifier";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" The SPDX identifier is a legally binding
shorthand, which can be used instead of the full boiler plate text";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"This patch is based on work done by Thomas Gleixner and Kate Stewart and
Philippe Ombredanne";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;How this work was done;Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Patches were generated and checked against linux-4.14-rc6 for a subset of
the use cases";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk; - file had no licensing information it it;Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - file was a */uapi/* one with no licensing information in it,
 - file was a */uapi/* one with existing licensing information,
Further patches will be generated in subsequent months to fix up cases
where non-standard license headers were used, and references to license
had to be inferred by heuristics based on keywords";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"The analysis to determine which SPDX License Identifier to be applied to
a file was done in a spreadsheet of side by side results from of the
output of two independent scanners (ScanCode & Windriver) producing SPDX
tag:value files created by Philippe Ombredanne";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" Philippe prepared the
base worksheet, and did an initial spot review of a few 1000 files";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"The 4.13 kernel was the starting point of the analysis with 60,537 files
assessed";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" Kate Stewart did a file by file comparison of the scanner
to be applied to the file";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"She confirmed any determination that was not
immediately clear with lawyers working with the Linux Foundation";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;Criteria used to select files for SPDX license identifier tagging was;Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk; - Files considered eligible had to be source code files;Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - Make and config files were included as candidates if they contained >5
   lines of source
 - File already had some variant of a license header in it (even if <5
   lines)";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;All documentation files were explicitly excluded;Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"The following heuristics were used to determine which SPDX license
identifiers to apply";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - when both scanners couldn't find any license traces, file was
   considered to have no license information in it, and the top level
   COPYING file license applied";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;   For non */uapi/* files that summary was;Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"   SPDX license identifier                            # files
   GPL-2.0                                              11139
   and resulted in the first patch in this series";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"   If that file was a */uapi/* path one, it was ""GPL-2.0 WITH
   Linux-syscall-note"" otherwise it was ""GPL-2.0""";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk; Results of that was;Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"   SPDX license identifier                            # files
   GPL-2.0 WITH Linux-syscall-note                        930
   and resulted in the second patch in this series";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - if a file had some form of licensing information in it, and was one
   of the */uapi/* ones, it was denoted with the Linux-syscall-note if
   any GPL family license was found in the file or had no licensing in
   it (per prior point)";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk; Results summary;Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"   SPDX license identifier                            # files
   GPL-2.0 WITH Linux-syscall-note                       270
   GPL-2.0+ WITH Linux-syscall-note                      169
   ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
   ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
   LGPL-2.1+ WITH Linux-syscall-note                      15
   GPL-1.0+ WITH Linux-syscall-note                       14
   ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
   LGPL-2.0+ WITH Linux-syscall-note                       4
   LGPL-2.1 WITH Linux-syscall-note                        3
   ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
   ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
   and that resulted in the third patch in this series";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - when the two scanners agreed on the detected license(s), that became
   the concluded license(s)";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - when there was disagreement between the two scanners (one detected a
   license but the other didn't, or they both detected different
   licenses) a manual inspection of the file occurred";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - In most cases a manual inspection of the information in the file
   resulted in a clear resolution of the license that should apply (and
   which scanner probably needed to revisit its heuristics)";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - When it was not immediately clear, the license identifier was
   confirmed with lawyers working with the Linux Foundation";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - If there was any question as to the appropriate license identifier,
   the file was flagged for further research and to be revisited later
   in time";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"In total, over 70 hours of logged manual review was done on the
spreadsheet to determine the SPDX license identifiers to apply to the
source files by Kate, Philippe, Thomas and, in some cases, confirmation
by lawyers working with the Linux Foundation";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Kate also obtained a third independent scan of the 4.13 code base from
FOSSology, and compared selected files where the other two scanners
disagreed against that SPDX file, to see if there was new insights";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" The
Windriver scanner is based on an older version of FOSSology in part, so
they are related";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Thomas did random spot checks in about 500 files from the spreadsheets
for the uapi headers and agreed with SPDX license identifier in the
files he inspected";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"For the non-uapi files Thomas did random spot checks
in about 15000 files";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"In initial set of patches against 4.14-rc6, 3 files were found to have
copy/paste license identifier errors, and have been fixed to reflect the
correct identifier";Greg Kroah-Hartman;2017-11-01;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;"Additionally Philippe spent 10 hours this week doing a detailed manual
inspection and review of the 12,461 patched files from the initial patch
version early this week with";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" - a full scancode scan run, collecting the matched texts, detected
   license ids and scores
 - reviewing anything where there was a license detected (about 500+
   files) to ensure that the applied SPDX license was correct
 - reviewing anything where there was no detection but the patch license
   was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
   SPDX license was correct
This produced a worksheet with 20 files needing minor correction";Greg Kroah-Hartman;2017-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" This
worksheet was then exported into 3 different .csv files for the
different types of files to be modified";Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;These .csv files were then reviewed by Greg;Greg Kroah-Hartman;2017-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" Thomas wrote a script to
parse the csv files and add the proper SPDX tag to the file, in the
format that the file expected";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjQ0MTMxODBmNTYwMGJjYjNiYjcwZmJlZDVjZjE4NmI2MDg2NGJk;" This script was further refined by Greg
based on the output to detect more types of files automatically and to
distinguish between header and source .c files (which need different
comment types.)  Finally Greg ran the script using the .csv files to
generate the patches.";Greg Kroah-Hartman;2017-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjBkMTAzYjZjMzcwMzhjYTI3NDA5Zjc0NmYwYjMzNTFiY2QwYzQ0;mm/migrate: fix indexing bug (off by one) and avoid out of bound access;Mark Hairgrove;2017-10-13;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjBkMTAzYjZjMzcwMzhjYTI3NDA5Zjc0NmYwYjMzNTFiY2QwYzQ0;"Index was incremented before last use and thus the second array could
dereference to an invalid address (not mentioning the fact that it did
not properly clear the entry we intended to clear).";Mark Hairgrove;2017-10-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YjM2OGNkNGE0NGNlOTViMzNmMWQzMWYyZjkzMmU2YWU3MDdmMzE5;mm/hmm: avoid bloating arch that do not make use of HMM;Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YjM2OGNkNGE0NGNlOTViMzNmMWQzMWYyZjkzMmU2YWU3MDdmMzE5;"This moves all new code including new page migration helper behind kernel
Kconfig option so that there is no codee bloat for arch or user that do
not want to use HMM or any of its associated features";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YjM2OGNkNGE0NGNlOTViMzNmMWQzMWYyZjkzMmU2YWU3MDdmMzE5;arm allyesconfig (without all the patchset, then with and this patch);Jrme Glisse;2017-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YjM2OGNkNGE0NGNlOTViMzNmMWQzMWYyZjkzMmU2YWU3MDdmMzE5;"   text	   data	    bss	    dec	    hex	filename
83721896	46511131	27582964	157815991	96814b7	../without/vmlinux
83722364	46511131	27582964	157816459	968168b	vmlinux";Jrme Glisse;2017-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZjZhZDY5ODM4ZmM5ZGNkYmVlMGRjZjJmYzJjNmYxMTEzZjhkNjA5;mm/device-public-memory: device memory cache coherent with CPU;Jrme Glisse;2017-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZjZhZDY5ODM4ZmM5ZGNkYmVlMGRjZjJmYzJjNmYxMTEzZjhkNjA5;"Platform with advance system bus (like CAPI or CCIX) allow device memory
to be accessible from CPU in a cache coherent fashion";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZjZhZDY5ODM4ZmM5ZGNkYmVlMGRjZjJmYzJjNmYxMTEzZjhkNjA5;" Add a new type of
ZONE_DEVICE to represent such memory";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZjZhZDY5ODM4ZmM5ZGNkYmVlMGRjZjJmYzJjNmYxMTEzZjhkNjA5;" The use case are the same as for
the un-addressable device memory but without all the corners cases.";Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MzE1YWRhN2YwOTViZmEyY2FlMGNkMWU5MTViOTViZjYyMjY4OTdk;mm/migrate: allow migrate_vma() to alloc new page on empty entry;Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzE1YWRhN2YwOTViZmEyY2FlMGNkMWU5MTViOTViZjYyMjY4OTdk;"This allows callers of migrate_vma() to allocate new page for empty CPU
page table entry (pte_none or back by zero page)";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MzE1YWRhN2YwOTViZmEyY2FlMGNkMWU5MTViOTViZjYyMjY4OTdk;" This is only for
anonymous memory and it won't allow new page to be instanced if the
userfaultfd is armed";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzE1YWRhN2YwOTViZmEyY2FlMGNkMWU5MTViOTViZjYyMjY4OTdk;"This is useful to device driver that want to migrate a range of virtual
address and would rather allocate new memory than having to fault later";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODphNTQzMGRkYThhM2ExY2RkNTMyZTM3MjcwZTZmMzY0MzYyNDFiNmU3;mm/migrate: support un-addressable ZONE_DEVICE page in migration;Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODphNTQzMGRkYThhM2ExY2RkNTMyZTM3MjcwZTZmMzY0MzYyNDFiNmU3;"Allow to unmap and restore special swap entry of un-addressable
ZONE_DEVICE memory.";Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YzMzMjhmMWYzNmE1ZWZlODE3YWQ0ZTA2NDk3YWY2MDE5MzZhNDYw;mm/migrate: migrate_vma() unmap page from vma while collecting pages;Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YzMzMjhmMWYzNmE1ZWZlODE3YWQ0ZTA2NDk3YWY2MDE5MzZhNDYw;"Common case for migration of virtual address range is page are map only
once inside the vma in which migration is taking place";Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YzMzMjhmMWYzNmE1ZWZlODE3YWQ0ZTA2NDk3YWY2MDE5MzZhNDYw;" Because we
already walk the CPU page table for that range we can directly do the
unmap there and setup special migration swap entry.";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;mm/migrate: new memory migration helper for use with device memory;Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;"This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory (which
can be allocated through special allocator)";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;" It differs from numa
migration by working on a range of virtual address and thus by doing
migration in chunk that can be large enough to use DMA engine or special
copy offloading engine";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;"Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...)";Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;" As an
example IBM platform with CAPI bus can make use of this feature to migrate
between regular memory and CAPI device memory";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;" New CPU architecture with
a pool of high performance memory not manage as cache but presented as
regular memory (while being faster and with lower latency than DDR) will
also be prime user of this patch";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NzYzY2I0NWFiOTY3YTkyYTVlZTQ5ZTljNTQ0YzBmMGVhOTBlMmQ2;"Migration to private device memory will be useful for device that have
large pool of such like GPU, NVidia plans to use HMM for that.";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;mm/migrate: new migrate mode MIGRATE_SYNC_NO_COPY;Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;"Introduce a new migration mode that allow to offload the copy to a device
DMA engine";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;" This changes the workflow of migration and not all
address_space migratepage callback can support this";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;"This is intended to be use by migrate_vma() which itself is use for thing
like HMM (see include/linux/hmm.h)";Jrme Glisse;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;No additional per-filesystem migratepage testing is needed;Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;" I disables
MIGRATE_SYNC_NO_COPY in all problematic migratepage() callback and i
added comment in those to explain why (part of this patch)";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;" The commit
message is unclear it should say that any callback that wish to support
this new mode need to be aware of the difference in the migration flow
from other mode";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;"Some of these callbacks do extra locking while copying (aio, zsmalloc,
balloon, ...) and for DMA to be effective you want to copy multiple
pages in one DMA operations";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;" But in the problematic case you can not
easily hold the extra lock accross multiple call to this callback";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;Usual flow is;Jrme Glisse;2017-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;"For each page {
 1 - lock page
 2 - call migratepage() callback
 3 - (extra locking in some migratepage() callback)
 4 - migrate page state (freeze refcount, update page cache, buffer
     head, ...)
 5 - copy page
 6 - (unlock any extra lock of migratepage() callback)
 7 - return from migratepage() callback
 8 - unlock page
The new mode MIGRATE_SYNC_NO_COPY";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;" 1 - lock multiple pages
For each page {
 2 - call migratepage() callback
 3 - abort in all problematic migratepage() callback
 4 - migrate page state (freeze refcount, update page cache, buffer
     head, ...)
} // finished all calls to migratepage() callback
 5 - DMA copy multiple pages
 6 - unlock all the pages
To support MIGRATE_SYNC_NO_COPY in the problematic case we would need a
new callback migratepages() (for instance) that deals with multiple
pages in one transaction";Jrme Glisse;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTE2ZWNjMGY5ZDQzNWQ4NDljOThmNGRhNTBlNDUzMTI0Yzg3NTMx;"Because the problematic cases are not important for current usage I did
not wanted to complexify this patchset even more for no good reason.";Jrme Glisse;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODplOGRiNjdlYjBkZWQzNzk3MDg1ZjAzMmM4NGI1ZDgyNDhmNDEyZGUz;mm: migrate: move_pages() supports thp migration;Naoya Horiguchi;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODplOGRiNjdlYjBkZWQzNzk3MDg1ZjAzMmM4NGI1ZDgyNDhmNDEyZGUz;This patch enables thp migration for move_pages(2).;Naoya Horiguchi;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;mm: thp: enable thp migration in generic path;Zi Yan;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;"Add thp migration's core code, including conversions between a PMD entry
and a swap entry, setting PMD migration entry, removing PMD migration
entry, and waiting on PMD migration entries";Zi Yan;2017-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;This patch makes it possible to support thp migration;Zi Yan;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;" If you fail to
allocate a destination page as a thp, you just split the source thp as
we do now, and then enter the normal page migration";Zi Yan;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;" If you succeed to
allocate destination thp, you enter thp migration";Zi Yan;2017-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MTZiODM3MTUzOWE2YzQ4NzQwNGMzYjhmYjA0MDc4MDE2ZGFiNGJh;" Subsequent patches
actually enable thp migration for each caller of page migration by
allowing its get_new_page() callback to allocate thps.";Zi Yan;2017-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;Sanitize 'move_pages()' permission checks;Linus Torvalds;2017-08-20;0;0
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;"The 'move_paghes()' system call was introduced long long ago with the
same permission checks as for sending a signal (except using
CAP_SYS_NICE instead of CAP_SYS_KILL for the overriding capability)";Linus Torvalds;2017-08-20;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;"That turns out to not be a great choice - while the system call really
only moves physical page allocations around (and you need other
capabilities to do a lot of it), you can check the return value to map
out some the virtual address choices and defeat ASLR of a binary that
still shares your uid";Linus Torvalds;2017-08-20;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;"So change the access checks to the more common 'ptrace_may_access()'
model instead";Linus Torvalds;2017-08-20;0;0
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;"This tightens the access checks for the uid, and also effectively
changes the CAP_SYS_NICE check to CAP_SYS_PTRACE, but it's unlikely that
anybody really _uses_ this legacy system call any more (we hav ebetter
NUMA placement models these days), so I expect nobody to notice";Linus Torvalds;2017-08-20;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOTdlN2U1MjEzODRhMjNiOWU1ODUxNzhmM2YxMWM5ZmEwODI3NGI5;Famous last words.;Linus Torvalds;2017-08-20;0;0
MDY6Q29tbWl0MjMyNTI5ODphOWI4MDI1MDBlYmJmZjE1NDQ1MTlhMjk2OTMyM2I3MTlkYWMyMWYw;"Revert ""mm: numa: defer TLB flush for THP migration as long as possible""";Nadav Amit;2017-08-10;0;0
MDY6Q29tbWl0MjMyNTI5ODphOWI4MDI1MDBlYmJmZjE1NDQ1MTlhMjk2OTMyM2I3MTlkYWMyMWYw;"While deferring TLB flushes is a good practice, the reverted patch
caused pending TLB flushes to be checked while the page-table lock is
not taken";Nadav Amit;2017-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODphOWI4MDI1MDBlYmJmZjE1NDQ1MTlhMjk2OTMyM2I3MTlkYWMyMWYw;" As a result, in architectures with weak memory model (PPC),
Linux may miss a memory-barrier, miss the fact TLB flushes are pending,
and cause (in theory) a memory corruption";Nadav Amit;2017-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphOWI4MDI1MDBlYmJmZjE1NDQ1MTlhMjk2OTMyM2I3MTlkYWMyMWYw;"Since the alternative of using smp_mb__after_unlock_lock() was
considered a bit open-coded, and the performance impact is expected to
be small, the previous patch is reverted";Nadav Amit;2017-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphOWI4MDI1MDBlYmJmZjE1NDQ1MTlhMjk2OTMyM2I3MTlkYWMyMWYw;"This reverts b0943d61b8fa (""mm: numa: defer TLB flush for THP migration
as long as possible"").";Nadav Amit;2017-08-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNGUxNzdkMTI2ODZiZjk4YjVhMDQ3YjUxODcxMjFhNzFlZTBkZDhj;mm/migrate.c: stabilise page count when migrating transparent hugepages;Will Deacon;2017-07-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGUxNzdkMTI2ODZiZjk4YjVhMDQ3YjUxODcxMjFhNzFlZTBkZDhj;"When migrating a transparent hugepage, migrate_misplaced_transhuge_page
guards itself against a concurrent fastgup of the page by checking that
the page count is equal to 2 before and after installing the new pmd";Will Deacon;2017-07-10;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNGUxNzdkMTI2ODZiZjk4YjVhMDQ3YjUxODcxMjFhNzFlZTBkZDhj;"If the page count changes, then the pmd is reverted back to the original
entry, however there is a small window where the new (possibly writable)
pmd is installed and the underlying page could be written by userspace";Will Deacon;2017-07-10;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNGUxNzdkMTI2ODZiZjk4YjVhMDQ3YjUxODcxMjFhNzFlZTBkZDhj;Restoring the old pmd could therefore result in loss of data;Will Deacon;2017-07-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNGUxNzdkMTI2ODZiZjk4YjVhMDQ3YjUxODcxMjFhNzFlZTBkZDhj;"This patch fixes the problem by freezing the page count whilst updating
the page tables, which protects against a concurrent fastgup without the
need to restore the old pmd in the failure case (since the page count
can no longer change under our feet).";Will Deacon;2017-07-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;mm: hugetlb: soft-offline: dissolve source hugepage after successful migration;Anshuman Khandual;2017-07-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;Currently hugepage migrated by soft-offline (i.e;Anshuman Khandual;2017-07-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;" due to correctable
memory errors) is contained as a hugepage, which means many non-error
pages in it are unreusable, i.e";Anshuman Khandual;2017-07-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy; wasted;Anshuman Khandual;2017-07-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;This patch solves this issue by dissolving source hugepages into buddy;Anshuman Khandual;2017-07-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;"As done in previous patch, PageHWPoison is set only on a head page of
the error hugepage";Anshuman Khandual;2017-07-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMzExNGE4NGY3Zjk2YzlkNWM3M2M4YmZhN2UyMWZmNDJmZGE5N2Uy;" Then in dissoliving we move the PageHWPoison flag
to the raw error page so that all healthy subpages return back to buddy.";Anshuman Khandual;2017-07-10;0;1
MDY6Q29tbWl0MjMyNTI5ODozODMzMjFhYjg1NzhkZmUzYmJjYzBiYzU2MDRjMGY4YWUwOGE1Yzk4;mm/hugetlb/migration: use set_huge_pte_at instead of set_pte_at;Aneesh Kumar K.V;2017-07-06;1;0
MDY6Q29tbWl0MjMyNTI5ODozODMzMjFhYjg1NzhkZmUzYmJjYzBiYzU2MDRjMGY4YWUwOGE1Yzk4;"Patch series ""HugeTLB migration support for PPC64"", v2";Aneesh Kumar K.V;2017-07-06;0;1
MDY6Q29tbWl0MjMyNTI5ODozODMzMjFhYjg1NzhkZmUzYmJjYzBiYzU2MDRjMGY4YWUwOGE1Yzk4;This patch (of 9);Aneesh Kumar K.V;2017-07-06;1;0
MDY6Q29tbWl0MjMyNTI5ODozODMzMjFhYjg1NzhkZmUzYmJjYzBiYzU2MDRjMGY4YWUwOGE1Yzk4;The right interface to use to set a hugetlb pte entry is set_huge_pte_at;Aneesh Kumar K.V;2017-07-06;0;0
MDY6Q29tbWl0MjMyNTI5ODozODMzMjFhYjg1NzhkZmUzYmJjYzBiYzU2MDRjMGY4YWUwOGE1Yzk4;Use that instead of set_pte_at.;Aneesh Kumar K.V;2017-07-06;1;0
MDY6Q29tbWl0MjMyNTI5ODplNGI4MjIyMjcxMmVkMTU4MTNkMzUyMDRjOTE0Mjk4ODNkMjdkMWQ5;mm: make rmap_one boolean function;Minchan Kim;2017-05-03;1;0
MDY6Q29tbWl0MjMyNTI5ODplNGI4MjIyMjcxMmVkMTU4MTNkMzUyMDRjOTE0Mjk4ODNkMjdkMWQ5;"rmap_one's return value controls whether rmap_work should contine to
scan other ptes or not so it's target for changing to boolean";Minchan Kim;2017-05-03;1;0
MDY6Q29tbWl0MjMyNTI5ODplNGI4MjIyMjcxMmVkMTU4MTNkMzUyMDRjOTE0Mjk4ODNkMjdkMWQ5;" Return
true if the scan should be continued";Minchan Kim;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODplNGI4MjIyMjcxMmVkMTU4MTNkMzUyMDRjOTE0Mjk4ODNkMjdkMWQ5;" Otherwise, return false to stop
the scanning";Minchan Kim;2017-05-03;0;1
MDY6Q29tbWl0MjMyNTI5ODplNGI4MjIyMjcxMmVkMTU4MTNkMzUyMDRjOTE0Mjk4ODNkMjdkMWQ5;This patch makes rmap_one's return value to boolean.;Minchan Kim;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDRkMzYzZjY1NzgwZjJhYzJlYzY3MjE2NDU1NWFmNTQ4OTZkNDBk;mm: don't assume anonymous pages have SwapBacked flag;Shaohua Li;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDRkMzYzZjY1NzgwZjJhYzJlYzY3MjE2NDU1NWFmNTQ4OTZkNDBk;"There are a few places the code assumes anonymous pages should have
SwapBacked flag set";Shaohua Li;2017-05-03;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNDRkMzYzZjY1NzgwZjJhYzJlYzY3MjE2NDU1NWFmNTQ4OTZkNDBk;" MADV_FREE pages are anonymous pages but we are
going to add them to LRU_INACTIVE_FILE list and clear SwapBacked flag
for them";Shaohua Li;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDRkMzYzZjY1NzgwZjJhYzJlYzY3MjE2NDU1NWFmNTQ4OTZkNDBk; The assumption doesn't hold any more, so fix them.;Shaohua Li;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNTAzOGQwZGU5ZWNhMzNmNjZiYzFmZWQ0MjQzOTE0OTA2ZTA0ZGU0;mm: remove unnecessary reclaimability check from NUMA balancing target;Johannes Weiner;2017-05-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNTAzOGQwZGU5ZWNhMzNmNjZiYzFmZWQ0MjQzOTE0OTA2ZTA0ZGU0;"NUMA balancing already checks the watermarks of the target node to
decide whether it's a suitable balancing target";Johannes Weiner;2017-05-03;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNTAzOGQwZGU5ZWNhMzNmNjZiYzFmZWQ0MjQzOTE0OTA2ZTA0ZGU0;" Whether the node is
reclaimable or not is irrelevant when we don't intend to reclaim.";Johannes Weiner;2017-05-03;0;0
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1;mm: prevent NR_ISOLATE_* stats from going negative;Rabin Vincent;2017-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1;"Commit 6afcf8ef0ca0 (""mm, compaction: fix NR_ISOLATED_* stats for pfn
based migration"") moved the dec_node_page_state() call (along with the
page_is_file_cache() call) to after putback_lru_page()";Rabin Vincent;2017-04-20;0;1
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1;"But page_is_file_cache() can change after putback_lru_page() is called,
so it should be called before putback_lru_page(), as it was before that
patch, to prevent NR_ISOLATE_* stats from going negative";Rabin Vincent;2017-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1;"Without this fix, non-CONFIG_SMP kernels end up hanging in the
shrink_active_list() due to the negative stats";Rabin Vincent;2017-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1; Mem-Info;Rabin Vincent;2017-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYzI4MGZlODcxNDQ5ZWFkNGJkYmQxNjY1ZmE1MmM3YzAxYzY0NzY1;"  active_anon:32567 inactive_anon:121 isolated_anon:1
  active_file:6066 inactive_file:6639 isolated_file:4294967295
  unevictable:0 dirty:115 writeback:0 unstable:0
  slab_reclaimable:2086 slab_unreclaimable:3167
  mapped:3398 shmem:18366 pagetables:1145 bounce:0
  free:1798 free_pcp:13 free_cma:0
Fixes: 6afcf8ef0ca0 (""mm, compaction: fix NR_ISOLATED_* stats for pfn based migration"")";Rabin Vincent;2017-04-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;mm: migrate: fix remove_migration_pte() for ksm pages;Naoya Horiguchi;2017-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;"I found that calling page migration for ksm pages causes the following
bug";Naoya Horiguchi;2017-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;"    page:ffffea0004d51180 count:2 mapcount:2 mapping:ffff88013c785141 index:0x913
    flags: 0x57ffffc0040068(uptodate|lru|active|swapbacked)
    raw: 0057ffffc0040068 ffff88013c785141 0000000000000913 0000000200000001
    raw: ffffea0004d5f9e0 ffffea0004d53f60 0000000000000000 ffff88007d81b800
    page dumped because: VM_BUG_ON_PAGE(!PageLocked(page))
    page->mem_cgroup:ffff88007d81b800
    ------------[ cut here ]------------
    kernel BUG at /src/linux-dev/mm/rmap.c:1086!
    invalid opcode: 0000 [#1] SMP
    Modules linked in: ppdev parport_pc virtio_balloon i2c_piix4 pcspkr parport i2c_core acpi_cpufreq ip_tables xfs libcrc32c ata_generic pata_acpi ata_piix 8139too libata virtio_blk 8139cp crc32c_intel mii virtio_pci virtio_ring serio_raw virtio floppy dm_mirror dm_region_hash dm_log dm_mod
    CPU: 0 PID: 3162 Comm: bash Not tainted 4.11.0-rc2-mm1+ #1
    Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011
    RIP: 0010:do_page_add_anon_rmap+0x1ba/0x260
    RSP: 0018:ffffc90002473b30 EFLAGS: 00010282
    RAX: 0000000000000021 RBX: ffffea0004d51180 RCX: 0000000000000006
    RDX: 0000000000000000 RSI: 0000000000000082 RDI: ffff88007dc0dfe0
    RBP: ffffc90002473b58 R08: 00000000fffffffe R09: 00000000000001c1
    R10: 0000000000000005 R11: 00000000000001c0 R12: ffff880139ab3d80
    R13: 0000000000000000 R14: 0000700000000200 R15: 0000160000000000
    FS:  00007f5195f50740(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fd450287000 CR3: 000000007a08e000 CR4: 00000000001406f0
    The problem is in the following lines";Naoya Horiguchi;2017-03-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;"    new = page - pvmw.page->index +
The 'new' is calculated with 'page' which is given by the caller as a
destination page and some offset adjustment for thp";Naoya Horiguchi;2017-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;" But this doesn't
properly work for ksm pages because pvmw.page->index doesn't change for
each address but linear_page_index() changes, which means that 'new'
points to different pages for each addresses backed by the ksm page";Naoya Horiguchi;2017-03-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;" As
a result, we try to set totally unrelated pages as destination pages,
and that causes kernel crash";Naoya Horiguchi;2017-03-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;"This patch fixes the miscalculation and makes ksm page migration work
fine";Naoya Horiguchi;2017-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjBlY2U2ZmEwMTY3YjIyYzAwNGZmNjllMTM3ZGM5NGVlMmU0Njll;"Fixes: 3fe87967c536 (""mm: convert remove_migration_pte() to use page_vma_mapped_walk()"")";Naoya Horiguchi;2017-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTg0ZjMxNTIyZjkzMTAyN2JmNjk1NzUyMDg3ZWNlMjc4YzEwZDNm;sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>;Ingo Molnar;2017-02-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTg0ZjMxNTIyZjkzMTAyN2JmNjk1NzUyMDg3ZWNlMjc4YzEwZDNm;"We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
will have to be picked up from other headers and a couple of .c files";Ingo Molnar;2017-02-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTg0ZjMxNTIyZjkzMTAyN2JmNjk1NzUyMDg3ZWNlMjc4YzEwZDNm;"Create a trivial placeholder <linux/sched/mm.h> file that just
maps to <linux/sched.h> to make this patch obviously correct and
bisectable";Ingo Molnar;2017-02-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTg0ZjMxNTIyZjkzMTAyN2JmNjk1NzUyMDg3ZWNlMjc4YzEwZDNm;The APIs that are going to be moved first are;Ingo Molnar;2017-02-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTg0ZjMxNTIyZjkzMTAyN2JmNjk1NzUyMDg3ZWNlMjc4YzEwZDNm;"   mm_alloc()
   __mmdrop()
   mmdrop()
   mmdrop_async_fn()
   mmdrop_async()
   mmget_not_zero()
   mmput()
   mmput_async()
   get_task_mm()
   mm_access()
   mm_release()
Include the new header in the files that are going to need it.";Ingo Molnar;2017-02-08;1;1
MDY6Q29tbWl0MjMyNTI5ODozZmU4Nzk2N2M1MzZlODI4YmYxZWExNGIzZWMzODI3ZDExMDExNTJl;mm: convert remove_migration_pte() to use page_vma_mapped_walk();Kirill A. Shutemov;2017-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZmU4Nzk2N2M1MzZlODI4YmYxZWExNGIzZWMzODI3ZDExMDExNTJl;"remove_migration_pte() also can easily be converted to
page_vma_mapped_walk().";Kirill A. Shutemov;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;mm/migration: make isolate_movable_page() return int type;Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;"Patch series ""HWPOISON: soft offlining for non-lru movable page"", v6";Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;"After Minchan's commit bda807d44454 (""mm: migrate: support non-lru
movable page migration""), some type of non-lru page like zsmalloc and
virtio-balloon page also support migration";Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;Therefore, we can;Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;"1) soft offlining no-lru movable pages, which means when memory
   corrected errors occur on a non-lru movable page, we can stop to use
   it by migrating data onto another page and disable the original
   (maybe half-broken) one";Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;2) enable memory hotplug for non-lru movable pages, i.e;Yisheng Xie;2017-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;"we may offline
   blocks, which include such pages, by using non-lru page migration";Yisheng Xie;2017-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;This patchset is heavily dependent on non-lru movable page migration;Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;This patch (of 4);Yisheng Xie;2017-02-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;Change the return type of isolate_movable_page() from bool to int;Yisheng Xie;2017-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;" It
will return 0 when isolate movable page successfully, and return -EBUSY
when it isolates failed";Yisheng Xie;2017-02-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTViY2Q2MTBmZmNlZGY1ZTQ4NWU3OGE3Mjc2MjgxMGIyNWM3ZjI1;"There is no functional change within this patch but prepare for later
patch.";Yisheng Xie;2017-02-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MzI2ZmVjMTEyMmNkZTI1NmJkMmE4YzYzZjI2MDZlMDhlNDRjZTFk;mm: Use owner_priv bit for PageSwapCache, valid when PageSwapBacked;Nicholas Piggin;2016-12-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MzI2ZmVjMTEyMmNkZTI1NmJkMmE4YzYzZjI2MDZlMDhlNDRjZTFk;"A page is not added to the swap cache without being swap backed,
so PageSwapBacked mappings can use PG_owner_priv_1 for PageSwapCache.";Nicholas Piggin;2016-12-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZDc1ZjM2NmI5MjQyZjliMTdlZDdkMGIwNjA0ZDc0NjBmODE4ZjIx;lib: radix-tree: check accounting of existing slot replacement users;Johannes Weiner;2016-12-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDc1ZjM2NmI5MjQyZjliMTdlZDdkMGIwNjA0ZDc0NjBmODE4ZjIx;"The bug in khugepaged fixed earlier in this series shows that radix tree
slot replacement is fragile; and it will become more so when not only
NULL<->!NULL transitions need to be caught but transitions from and to
exceptional entries as well";Johannes Weiner;2016-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZDc1ZjM2NmI5MjQyZjliMTdlZDdkMGIwNjA0ZDc0NjBmODE4ZjIx; We need checks;Johannes Weiner;2016-12-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDc1ZjM2NmI5MjQyZjliMTdlZDdkMGIwNjA0ZDc0NjBmODE4ZjIx;"Re-implement radix_tree_replace_slot() on top of the sanity-checked
__radix_tree_replace()";Johannes Weiner;2016-12-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDc1ZjM2NmI5MjQyZjliMTdlZDdkMGIwNjA0ZDc0NjBmODE4ZjIx;" This requires existing callers to also pass the
radix tree root, but it'll warn us when somebody replaces slots with
contents that need proper accounting (transitions between NULL entries,
real entries, exceptional entries) and where a replacement through the
slot pointer would corrupt the radix tree node counts.";Johannes Weiner;2016-12-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;mm, compaction: fix NR_ISOLATED_* stats for pfn based migration;Ming Ling;2016-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;"Since commit bda807d44454 (""mm: migrate: support non-lru movable page
migration"") isolate_migratepages_block) can isolate !PageLRU pages which
would acct_isolated account as NR_ISOLATED_*";Ming Ling;2016-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" Accounting these non-lru
pages NR_ISOLATED_{ANON,FILE} doesn't make any sense and it can misguide
heuristics based on those counters such as pgdat_reclaimable_pages resp";Ming Ling;2016-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;"too_many_isolated which would lead to unexpected stalls during the
direct reclaim without any good reason";Ming Ling;2016-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" Note that
__alloc_contig_migrate_range can isolate a lot of pages at once";Ming Ling;2016-12-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;"On mobile devices such as 512M ram android Phone, it may use a big zram
swap";Ming Ling;2016-12-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" In some cases zram(zsmalloc) uses too many non-lru but
migratedable pages, such as";Ming Ling;2016-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;"      MemTotal: 468148 kB
      Normal free:5620kB
      Free swap:4736kB
      Total swap:409596kB
      ZRAM: 164616kB(zsmalloc non-lru pages)
      active_anon:60700kB
      inactive_anon:60744kB
      active_file:34420kB
      inactive_file:37532kB
Fix this by only accounting lru pages to NR_ISOLATED_* in
isolate_migratepages_block right after they were isolated and we still
know they were on LRU";Ming Ling;2016-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" Drop acct_isolated because it is called after
the fact and we've lost that information";Ming Ling;2016-12-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" Batching per-cpu counter
doesn't make much improvement anyway";Ming Ling;2016-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw;" Also make sure that we uncharge
only LRU pages when putting them back on the LRU in
putback_movable_pages resp";Ming Ling;2016-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YWZjZjhlZjBjYTBhNjlkMDE0ZjhlZGI2MTNkOTQ4MjFmMGFlNzAw; when unmap_and_move migrates the page.;Ming Ling;2016-12-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDIzMjlmODg3MmYyM2U0NmExOWQyNDA5MzA1NzE1MTBjZTUyNWVi;mm: vm_page_prot: update with WRITE_ONCE/READ_ONCE;Andrea Arcangeli;2016-10-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDIzMjlmODg3MmYyM2U0NmExOWQyNDA5MzA1NzE1MTBjZTUyNWVi;"vma->vm_page_prot is read lockless from the rmap_walk, it may be updated
concurrently and this prevents the risk of reading intermediate values.";Andrea Arcangeli;2016-10-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;mm, thp: remove __GFP_NORETRY from khugepaged and madvised allocations;Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"After the previous patch, we can distinguish costly allocations that
should be really lightweight, such as THP page faults, with
__GFP_NORETRY";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" This means we don't need to recognize khugepaged
allocations via PF_KTHREAD anymore";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" We can also change THP page faults
in areas where madvise(MADV_HUGEPAGE) was used to try as hard as
khugepaged, as the process has indicated that it benefits from THP's and
is willing to pay some initial latency costs";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"We can also make the flags handling less cryptic by distinguishing
GFP_TRANSHUGE_LIGHT (no reclaim at all, default mode in page fault) from
GFP_TRANSHUGE (only direct reclaim, khugepaged default)";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" Adding
__GFP_NORETRY or __GFP_KSWAPD_RECLAIM is done where needed";Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"The patch effectively changes the current GFP_TRANSHUGE users as
follows";Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"  long and it's shared by multiple users, so it's worth spending some
  effort on it";Vlastimil Babka;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj; We use GFP_TRANSHUGE, and __GFP_NORETRY is not added;Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"  This also restores direct reclaim to this allocation, which was
  unintentionally removed by commit e4a49efe4e7e (""mm: thp: set THP defrag
  by default to madvise and add a stall-free defrag option"")
  is not an issue";Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" So if khugepaged ""defrag"" is enabled (the default), do
  reclaim via GFP_TRANSHUGE without __GFP_NORETRY";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" We can remove the
  PF_KTHREAD check from page alloc";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"  As a side-effect, khugepaged will now no longer check if the initial
  compaction was deferred or contended";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" This is OK, as khugepaged sleep
  times between collapsion attempts are long enough to prevent noticeable
  disruption, so we should allow it to spend some effort";Vlastimil Babka;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;"  __GFP_RECLAIM, so just convert to GFP_TRANSHUGE_LIGHT which is
  equivalent";Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;  are now allocating without __GFP_NORETRY;Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" Other vma's keep using
  __GFP_NORETRY if direct reclaim/compaction is at all allowed (by default
  it's allowed only for madvised vma's)";Vlastimil Babka;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODoyNTE2MDM1NDk5Yjk1NTVmNmFjZDM3M2M5ZjEyZTQ0YmNiNTBkYmVj;" The rest is conversion to
  GFP_TRANSHUGE(_LIGHT).";Vlastimil Babka;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTFjODRiNDA0YTcxNzZiOGIzNmUyYTAwNDFiNmYwYWRiMzE1MWEz;mm: remove reclaim and compaction retry approximations;Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTFjODRiNDA0YTcxNzZiOGIzNmUyYTAwNDFiNmYwYWRiMzE1MWEz;"If per-zone LRU accounting is available then there is no point
approximating whether reclaim and compaction should retry based on pgdat
statistics";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTFjODRiNDA0YTcxNzZiOGIzNmUyYTAwNDFiNmYwYWRiMzE1MWEz;" This is effectively a revert of ""mm, vmstat: remove zone
and node double accounting by approximating retries"" with the difference
that inactive/active stats are still available";Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTFjODRiNDA0YTcxNzZiOGIzNmUyYTAwNDFiNmYwYWRiMzE1MWEz;" This preserves the
history of why the approximation was retried and why it had to be
reverted to handle OOM kills on 32-bit systems.";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;mm, vmstat: remove zone and node double accounting by approximating retries;Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"The number of LRU pages, dirty pages and writeback pages must be
accounted for on both zones and nodes because of the reclaim retry
logic, compaction retry logic and highmem calculations all depending on
per-zone stats";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"Many lowmem allocations are immune from OOM kill due to a check in
__alloc_pages_may_oom for (ac->high_zoneidx < ZONE_NORMAL) since commit
03668b3ceb0c (""oom: avoid oom killer for lowmem allocations"")";Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" The
exception is costly high-order allocations or allocations that cannot
fail";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" If the __alloc_pages_may_oom avoids OOM-kill for low-order lowmem
allocations then it would fall through to __alloc_pages_direct_compact";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"This patch will blindly retry reclaim for zone-constrained allocations
in should_reclaim_retry up to MAX_RECLAIM_RETRIES";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" This is not ideal
but without per-zone stats there are not many alternatives";Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" The impact
it that zone-constrained allocations may delay before considering the
OOM killer";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"As there is no guarantee enough memory can ever be freed to satisfy
compaction, this patch avoids retrying compaction for zone-contrained
allocations";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"In combination, that means that the per-node stats can be used when
deciding whether to continue reclaim using a rough approximation";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" While
it is possible this will make the wrong decision on occasion, it will
not infinite loop as the number of reclaim attempts is capped by
MAX_RECLAIM_RETRIES";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;The final step is calculating the number of dirtyable highmem pages;Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" As
those calculations only care about the global count of file pages in
highmem";Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;" This patch uses a global counter used instead of per-zone
stats as it is sufficient";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpiY2E2NzU5MjU4ZGJlZjM3OGJjZjViODcyMTc3YmNkMjI1OWNlYjY4;"In combination, this allows the per-zone LRU and dirty state counters to
be removed.";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMWZiOTk4OTg2YTcyYWE3ZTk5N2Q5NmQ2M2Q1MjU4MmEwMTIyOGM1;mm: move most file-based accounting to the node;Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMWZiOTk4OTg2YTcyYWE3ZTk5N2Q5NmQ2M2Q1MjU4MmEwMTIyOGM1;"There are now a number of accounting oddities such as mapped file pages
being accounted for on the node while the total number of file pages are
accounted on the zone";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWZiOTk4OTg2YTcyYWE3ZTk5N2Q5NmQ2M2Q1MjU4MmEwMTIyOGM1;" This can be coped with to some extent but it's
confusing so this patch moves the relevant file-based accounted";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWZiOTk4OTg2YTcyYWE3ZTk5N2Q5NmQ2M2Q1MjU4MmEwMTIyOGM1;" Due to
throttling logic in the page allocator for reliable OOM detection, it is
still necessary to track dirty and writeback pages on a per-zone basis.";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;mm: rename NR_ANON_PAGES to NR_ANON_MAPPED;Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;NR_FILE_PAGES  is the number of        file pages;Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;NR_FILE_MAPPED is the number of mapped file pages;Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;NR_ANON_PAGES  is the number of mapped anon pages;Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;"This is unhelpful naming as it's easy to confuse NR_FILE_MAPPED and
NR_ANON_PAGES for mapped pages";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;" This patch renames NR_ANON_PAGES so we
have
NR_FILE_PAGES  is the number of        file pages";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;NR_FILE_MAPPED is the number of mapped file pages;Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjlkMGZhYjcxNjZjOTMyM2YwNmQ3MDg1MThhMzVjZjNhOTA0MjZj;NR_ANON_MAPPED is the number of mapped anon pages.;Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;mm, vmscan: move LRU lists to node;Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"This moves the LRU lists from the zone to the node and related data such
as counters, tracing, congestion tracking and writeback tracking";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"Unfortunately, due to reclaim and compaction retry logic, it is
necessary to account for the number of LRU pages on both zone and node
logic";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;" Most reclaim logic is based on the node counters but the retry
logic uses the zone counters which do not distinguish inactive and
active sizes";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;" It would be possible to leave the LRU counters on a
per-zone basis but it's a heavier calculation across multiple cache
lines that is much more frequent than the retry checks";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"Other than the LRU counters, this is mostly a mechanical patch but note
that it introduces a number of anomalies";Mel Gorman;2016-07-28;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;" For example, the scans are
per-zone but using per-node counters";Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;" We also mark a node as congested
when a zone is congested";Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;" This causes weird problems that are fixed
later but is easier to review";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"In the event that there is excessive overhead on 32-bit systems due to
the nodes being on LRU then there are two potential solutions
1";Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"Long-term isolation of highmem pages when reclaim is lowmem
   When pages are skipped, they are immediately added back onto the LRU
   list";Mel Gorman;2016-07-28;0;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"If lowmem reclaim persisted for long periods of time, the same
   highmem pages get continually scanned";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"The idea would be that lowmem
   keeps those pages on a separate list until a reclaim for highmem pages
   arrives that splices the highmem pages back onto the LRU";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"It potentially
   could be implemented similar to the UNEVICTABLE list";Mel Gorman;2016-07-28;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"   That would reduce the skip rate with the potential corner case is that
   highmem pages have to be scanned and reclaimed to free lowmem slab pages";Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;2;Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODo1OTlkMGM5NTRmOTFkMDY4OWM5YmI0MjFiNWJjMDRlYTAyNDM3YTQx;"Linear scan lowmem pages if the initial LRU shrink fails
   This will break LRU ordering but may be preferable and faster during
   memory pressure than skipping LRU pages.";Mel Gorman;2016-07-28;1;0
MDY6Q29tbWl0MjMyNTI5ODoxMDEwMjQ1OTY0NDE1YmI3NDAzNDYzMTE1YmFiMmNkMjYyNDRiNDQ1;mm: introduce do_set_pmd();Kirill A. Shutemov;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODoxMDEwMjQ1OTY0NDE1YmI3NDAzNDYzMTE1YmFiMmNkMjYyNDRiNDQ1;With postponed page table allocation we have chance to setup huge pages;Kirill A. Shutemov;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMDEwMjQ1OTY0NDE1YmI3NDAzNDYzMTE1YmFiMmNkMjYyNDRiNDQ1;do_set_pte() calls do_set_pmd() if following criteria met:;Kirill A. Shutemov;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkZDc4ZmVkZGU0Yjk5YjMyMmYyZGM4NDlkNDY3ZDM2NWE4MmUyM2Nh;rmap: support file thp;Kirill A. Shutemov;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDc4ZmVkZGU0Yjk5YjMyMmYyZGM4NDlkNDY3ZDM2NWE4MmUyM2Nh;"Naive approach: on mapping/unmapping the page as compound we update
->_mapcount on each 4k page";Kirill A. Shutemov;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkZDc4ZmVkZGU0Yjk5YjMyMmYyZGM4NDlkNDY3ZDM2NWE4MmUyM2Nh;" That's not efficient, but it's not obvious
how we can optimize this";Kirill A. Shutemov;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZDc4ZmVkZGU0Yjk5YjMyMmYyZGM4NDlkNDY3ZDM2NWE4MmUyM2Nh; We can look into optimization later;Kirill A. Shutemov;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZDc4ZmVkZGU0Yjk5YjMyMmYyZGM4NDlkNDY3ZDM2NWE4MmUyM2Nh;"PG_double_map optimization doesn't work for file pages since lifecycle
of file pages is different comparing to anon pages: file page can be
mapped again at any time.";Kirill A. Shutemov;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMTEyM2VhNmQzYjNkYTI1YWY1YzhhOWQ4NDNiZDA3YWI2MzIxM2Y0;mm: balloon: use general non-lru movable page feature;Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMTEyM2VhNmQzYjNkYTI1YWY1YzhhOWQ4NDNiZDA3YWI2MzIxM2Y0;"Now, VM has a feature to migrate non-lru movable pages so balloon
doesn't need custom migration hooks in migrate.c and compaction.c";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMTEyM2VhNmQzYjNkYTI1YWY1YzhhOWQ4NDNiZDA3YWI2MzIxM2Y0;"Instead, this patch implements the page->mapping->a_ops->
{isolate|migrate|putback} functions";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMTEyM2VhNmQzYjNkYTI1YWY1YzhhOWQ4NDNiZDA3YWI2MzIxM2Y0;"With that, we could remove hooks for ballooning in general migration
functions and make balloon compaction simple.";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;mm: migrate: support non-lru movable page migration;Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"We have allowed migration for only LRU pages until now and it was enough
to make high-order pages";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" But recently, embedded system(e.g., webOS,
android) uses lots of non-movable pages(e.g., zram, GPU memory) so we
have seen several reports about troubles of small high-order allocation";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;For fixing the problem, there were several efforts (e,g,;Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" enhance
compaction algorithm, SLUB fallback to 0-order page, reserved memory,
vmalloc and so on) but if there are lots of non-movable pages in system,
their solutions are void in the long run";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"So, this patch is to support facility to change non-movable pages with
movable";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" For the feature, this patch introduces functions related to
migration to address_space_operations as well as some page flags";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"If a driver want to make own pages movable, it should define three
functions which are function pointers of struct
address_space_operations";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"What VM expects on isolate_page function of driver is to return *true*
if driver isolates page successfully";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" On returing true, VM marks the
page as PG_isolated so concurrent isolation in several CPUs skip the
page for isolation";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" If a driver cannot isolate the page, it should
return *false*";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"Once page is successfully isolated, VM uses page.lru fields so driver
shouldn't expect to preserve values in that fields";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;2;Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"int (*migratepage) (struct address_space *mapping,
After isolation, VM calls migratepage of driver with isolated page";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" The
function of migratepage is to move content of the old page to new page
and set up fields of struct page newpage";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Keep in mind that you should
indicate to the VM the oldpage is no longer movable via
__ClearPageMovable() under page_lock if you migrated the oldpage
successfully and returns 0";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" If driver cannot migrate the page at the
moment, driver can return -EAGAIN";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" On -EAGAIN, VM will retry page
migration in a short time because VM interprets -EAGAIN as ""temporal
migration failure""";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" On returning any error except -EAGAIN, VM will give
up the page migration without retrying in this time";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;Driver shouldn't touch page.lru field VM using in the functions;Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"If migration fails on isolated page, VM should return the isolated page
to the driver so VM calls driver's putback_page with migration failed
page";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" In this function, driver should put the isolated page back to the
own data structure";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;4;Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"non-lru movable page flags
There are two page flags for supporting non-lru movable page";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"Driver should use the below function to make page movable under
page_lock";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"	void __SetPageMovable(struct page *page, struct address_space *mapping)
It needs argument of address_space for registering migration family
functions which will be called by VM";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Exactly speaking, PG_movable is
not a real flag of struct page";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Rather than, VM reuses page->mapping's
lower bits to represent it";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;so driver shouldn't access page->mapping directly;Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Instead, driver
should use page_mapping which mask off the low two bits of page->mapping
so it can get right struct address_space";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;For testing of non-lru movable page, VM supports __PageMovable function;Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"However, it doesn't guarantee to identify non-lru movable page because
page->mapping field is unified with other variables in struct page";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" As
well, if driver releases the page after isolation by VM, page->mapping
doesn't have stable value although it has PAGE_MAPPING_MOVABLE (Look at
__ClearPageMovable)";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" But __PageMovable is cheap to catch whether page
is LRU or non-lru movable once the page has been isolated";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Because LRU
pages never can have PAGE_MAPPING_MOVABLE in page->mapping";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" It is also
good for just peeking to test non-lru movable pages before more
expensive checking with lock_page in pfn scanning to select victim";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;For guaranteeing non-lru movable page, VM provides PageMovable function;Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"Unlike __PageMovable, PageMovable functions validates page->mapping and
mapping->a_ops->isolate_page under lock_page";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" The lock_page prevents
sudden destroying of page->mapping";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"Driver using __SetPageMovable should clear the flag via
__ClearMovablePage under page_lock before the releasing the page";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;"To prevent concurrent isolation among several CPUs, VM marks isolated
page as PG_isolated under lock_page";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" So if a CPU encounters PG_isolated
non-lru movable page, it can skip it";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Driver doesn't need to manipulate
the flag because VM will set/clear it automatically";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" Keep in mind that
if driver sees PG_isolated page, it means the page have been isolated by
VM so it shouldn't touch page.lru field";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4MDdkNDQ0NTQxNGU4ZTc3ZGE3MDRmMTE2YmIwODgwZmUwYzc2;" PG_isolated is alias with
PG_reclaim flag so driver shouldn't use the flag for own purpose.";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;mm: use put_page() to free page instead of putback_lru_page();Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"Recently, I got many reports about perfermance degradation in embedded
system(Android mobile phone, webOS TV and so on) and easy fork fail";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;The problem was fragmentation caused by zram and GPU driver mainly;Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"With memory pressure, their pages were spread out all of pageblock and
it cannot be migrated with current compaction algorithm which supports
only LRU pages";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" In the end, compaction cannot work well so reclaimer
shrinks all of working set pages";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" It made system very slow and even to
fail to fork easily which requires order-[2 or 3] allocations";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"Other pain point is that they cannot use CMA memory space so when OOM
kill happens, I can see many free pages in CMA area, which is not memory
efficient";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" In our product which has big CMA memory, it reclaims zones
too exccessively to allocate GPU and zram page although there are lots
of free space in CMA so system becomes very slow easily";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"To solve these problem, this patch tries to add facility to migrate
non-lru pages via introducing new functions and page flags to help
migration";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"struct address_space_operations {
new page flags
	PG_movable
	PG_isolated
For details, please read description in ""mm: migrate: support non-lru
movable page migration""";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"Originally, Gioh Kim had tried to support this feature but he moved so I
took over the work";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" I took many code from his work and changed a little
bit and Konstantin Khlebnikov helped Gioh a lot so he should deserve to
have many credit, too";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"And I should mention Chulmin who have tested this patchset heavily so I
can find many bugs from him";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" :)
Thanks, Gioh, Konstantin and Chulmin!
This patchset consists of five parts";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;1;Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"clean up migration
  mm: use put_page to free page instead of putback_lru_page
2";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"add non-lru page migration feature
  mm: migrate: support non-lru movable page migration
3";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"rework KVM memory-ballooning
  mm: balloon: use general non-lru movable page feature
4";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"zsmalloc refactoring for preparing page migration
  zsmalloc: keep max_object in size_class
  zsmalloc: use bit_spin_lock
  zsmalloc: use accessor
  zsmalloc: factor page chain functionality out
  zsmalloc: introduce zspage structure
  zsmalloc: separate free_zspage from putback_zspage
  zsmalloc: use freeobj for index
5";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"zsmalloc page migration
  zsmalloc: page migration support
  zram: use __GFP_MOVABLE for memory allocation
This patch (of 12)";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;Procedure of page migration is as follows;Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"First of all, it should isolate a page from LRU and try to migrate the
page";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5; If it is successful, it releases the page for freeing;Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;Otherwise, it should put the page back to LRU list;Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"For LRU pages, we have used putback_lru_page for both freeing and
putback to LRU list";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" It's okay because put_page is aware of LRU list so
if it releases last refcount of the page, it removes the page from LRU
list";Minchan Kim;2016-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" However, It makes unnecessary operations (e.g., lru_cache_add,
pagevec and flags operations";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" It would be not significant but no worth
to do) and harder to support new non-lru page migration because put_page
isn't aware of non-lru page's data structure";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"To solve the problem, we can add new hook in put_page with PageMovable
flags check but it can increase overhead in hot path and needs new
locking scheme to stabilize the flag check with put_page";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;So, this patch cleans it up to divide two semantic(ie, put and putback);Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"If migration is successful, use put_page instead of putback_lru_page and
use putback_lru_page only on failure";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" That makes code more readable and
doesn't add overhead in put_page";Minchan Kim;2016-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;"Comment from Vlastimil
 ""Yeah, and compaction (perhaps also other migration users) has to drain
  the lru pvec..";Minchan Kim;2016-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNmM5MTllYjkwZTAyMWZiY2ZjYmZhOWRkM2Q1NTkzMGNkYmI2N2Y5;" Getting rid of this stuff is worth even by itself.""";Minchan Kim;2016-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMTE4ZGNlNzczZDg0ZjM5ZWJkNTFhOWZlNzI2MWY5MTY5Y2IwNTZl;mm: Export migrate_page_move_mapping and migrate_page_copy;Richard Weinberger;2016-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODoxMTE4ZGNlNzczZDg0ZjM5ZWJkNTFhOWZlNzI2MWY5MTY5Y2IwNTZl;"Export these symbols such that UBIFS can implement
->migratepage.";Richard Weinberger;2016-06-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZmVmMmVmNDAyN2IxMzA0MTQ5YTY1ZGMzMzc5NGVhYjY1ZThhM2Jm;mm, migrate: increment fail count on ENOMEM;David Rientjes;2016-05-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZmVmMmVmNDAyN2IxMzA0MTQ5YTY1ZGMzMzc5NGVhYjY1ZThhM2Jm;"If page migration fails due to -ENOMEM, nr_failed should still be
incremented for proper statistics";David Rientjes;2016-05-20;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZmVmMmVmNDAyN2IxMzA0MTQ5YTY1ZGMzMzc5NGVhYjY1ZThhM2Jm;"This was encountered recently when all page migration vmstats showed 0,
and inferred that migrate_pages() was never called, although in reality
the first page migration failed because compaction_alloc() failed to
find a migration target";David Rientjes;2016-05-20;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZmVmMmVmNDAyN2IxMzA0MTQ5YTY1ZGMzMzc5NGVhYjY1ZThhM2Jm;"This patch increments nr_failed so the vmstat is properly accounted on
ENOMEM.";David Rientjes;2016-05-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;mm: use __SetPageSwapBacked and dont ClearPageSwapBacked;Hugh Dickins;2016-05-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;"v3.16 commit 07a427884348 (""mm: shmem: avoid atomic operation during
shmem_getpage_gfp"") rightly replaced one instance of SetPageSwapBacked
by __SetPageSwapBacked, pointing out that the newly allocated page is
not yet visible to other users (except speculative get_page_unless_zero-
ers, who may not update page flags before their further checks)";Hugh Dickins;2016-05-20;0;0
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;That was part of a series in which Mel was focused on tmpfs profiles;Hugh Dickins;2016-05-20;0;0
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;"but almost all SetPageSwapBacked uses can be so optimized, with the same
justification";Hugh Dickins;2016-05-20;0;1
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;Remove ClearPageSwapBacked from __read_swap_cache_async() error path;Hugh Dickins;2016-05-20;1;0
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;it's not an error to free a page with PG_swapbacked set;Hugh Dickins;2016-05-20;0;0
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;"Follow a convention of __SetPageLocked, __SetPageSwapBacked instead of
doing it differently in different places; but that's for tidiness - if
the ordering actually mattered, we should not be using the __variants";Hugh Dickins;2016-05-20;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYTk5NDlkYTU5YTE1MDE3YTAyYzg2YjA4N2M3NDk5ZDdiNTcwMmJl;"There's probably scope for further __SetPageFlags in other places, but
SwapBacked is the one I'm interested in at the moment.";Hugh Dickins;2016-05-20;0;0
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5;mm/hwpoison: fix wrong num_poisoned_pages accounting;Minchan Kim;2016-04-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5;"Currently, migration code increses num_poisoned_pages on *failed*
migration page as well as successfully migrated one at the trial of
memory-failure";Minchan Kim;2016-04-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5; It will make the stat wrong;Minchan Kim;2016-04-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5;" As well, it marks the
page as PG_HWPoison even if the migration trial failed";Minchan Kim;2016-04-28;1;1
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5;" It would mean
we cannot recover the corrupted page using memory-failure facility";Minchan Kim;2016-04-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpkN2U2OTQ4OGJkMDRkZTE2NTY2N2Y2YmM3NDFjMWMwZWM2MDQyYWI5;This patches fixes it.;Minchan Kim;2016-04-28;1;1
MDY6Q29tbWl0MjMyNTI5ODplMzg4NDY2ZGU0YTJhMWE1MGM0M2JmYWVhY2MwYzgyNTRkOWU3Y2Iy;mm: make remove_migration_ptes() beyond mm/migration.c;Kirill A. Shutemov;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODplMzg4NDY2ZGU0YTJhMWE1MGM0M2JmYWVhY2MwYzgyNTRkOWU3Y2Iy;Make remove_migration_ptes() available to be used in split_huge_page();Kirill A. Shutemov;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODplMzg4NDY2ZGU0YTJhMWE1MGM0M2JmYWVhY2MwYzgyNTRkOWU3Y2Iy;"New parameter 'locked' added: as with try_to_umap() we need a way to
indicate that caller holds rmap lock";Kirill A. Shutemov;2016-03-17;1;1
MDY6Q29tbWl0MjMyNTI5ODplMzg4NDY2ZGU0YTJhMWE1MGM0M2JmYWVhY2MwYzgyNTRkOWU3Y2Iy;"We also shouldn't try to mlock() pte-mapped huge pages: pte-mapeed THP
pages are never mlocked.";Kirill A. Shutemov;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;mm: introduce page reference manipulation functions;Joonsoo Kim;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;"The success of CMA allocation largely depends on the success of
migration and key factor of it is page reference count";Joonsoo Kim;2016-03-17;0;0
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" Until now, page
reference is manipulated by direct calling atomic functions so we cannot
follow up who and where manipulate it";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" Then, it is hard to find actual
reason of CMA allocation failure";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" CMA allocation should be guaranteed
to succeed so finding offending place is really important";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;"In this patch, call sites where page reference is manipulated are
converted to introduced wrapper function";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" This is preparation step to
add tracepoint to each page reference manipulation function";Joonsoo Kim;2016-03-17;0;0
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" With this
facility, we can easily find reason of CMA allocation failure";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" There is
no functional change in this patch";Joonsoo Kim;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;In addition, this patch also converts reference read sites;Joonsoo Kim;2016-03-17;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZTg5NmQxODc4OTQ5ZWE5MmJhNTQ3NTg3YmMzMDc1Y2M2ODhmYjhm;" It will
help a second step that renames page._count to something else and
prevents later attempt to direct access to it (Suggested by Andrew).";Joonsoo Kim;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NThhYTc2ZDEzMmRjMWMzYzYwYmUwZjBkYjk5YmNjMGNlMTc2N2Zj;mm/thp/migration: switch from flush_tlb_range to flush_pmd_tlb_range;Aneesh Kumar K.V;2016-03-17;0;0
MDY6Q29tbWl0MjMyNTI5ODo0NThhYTc2ZDEzMmRjMWMzYzYwYmUwZjBkYjk5YmNjMGNlMTc2N2Zj;We remove one instace of flush_tlb_range here;Aneesh Kumar K.V;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo0NThhYTc2ZDEzMmRjMWMzYzYwYmUwZjBkYjk5YmNjMGNlMTc2N2Zj;" That was added by commit
f714f4f20e59 (""mm: numa: call MMU notifiers on THP migration"")";Aneesh Kumar K.V;2016-03-17;0;0
MDY6Q29tbWl0MjMyNTI5ODo0NThhYTc2ZDEzMmRjMWMzYzYwYmUwZjBkYjk5YmNjMGNlMTc2N2Zj;" But the
pmdp_huge_clear_flush_notify should have done the require flush for us";Aneesh Kumar K.V;2016-03-17;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NThhYTc2ZDEzMmRjMWMzYzYwYmUwZjBkYjk5YmNjMGNlMTc2N2Zj;Hence remove the extra flush.;Aneesh Kumar K.V;2016-03-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDQ4NWNmMmJjODVkMmExMGMzNjUzZmZmNGZlOTU2ZGI2N2NlMmE5;mm: migrate: consolidate mem_cgroup_migrate() calls;Johannes Weiner;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDQ4NWNmMmJjODVkMmExMGMzNjUzZmZmNGZlOTU2ZGI2N2NlMmE5;"Rather than scattering mem_cgroup_migrate() calls all over the place,
have a single call from a safe place where every migration operation
eventually ends up in - migrate_page_copy().";Johannes Weiner;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;mm: migrate: do not touch page->mem_cgroup of live pages;Johannes Weiner;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;"Changing a page's memcg association complicates dealing with the page,
so we want to limit this as much as possible";Johannes Weiner;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj; Page migration e.g;Johannes Weiner;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" does
not have to do that";Johannes Weiner;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" Just like page cache replacement, it can forcibly
charge a replacement page, and then uncharge the old page when it gets
freed";Johannes Weiner;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" Temporarily overcharging the cgroup by a single page is not an
issue in practice, and charging is so cheap nowadays that this is much
preferrable to the headache of messing with live pages";Johannes Weiner;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;"The only place that still changes the page->mem_cgroup binding of live
pages is when pages move along with a task to another cgroup";Johannes Weiner;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" But that
path isolates the page from the LRU, takes the page lock, and the move
lock (lock_page_memcg())";Johannes Weiner;2016-03-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" That means page->mem_cgroup is always stable
in callers that have the page isolated from the LRU or locked";Johannes Weiner;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YTkzY2E4ZmRlM2NmY2UwZjAwZjAyMjgxMTM5YTM3N2M4M2U4ZDhj;" Lighter
unlocked paths, like writeback accounting, can use lock_page_memcg().";Johannes Weiner;2016-03-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;mm, page_owner: track and print last migrate reason;Vlastimil Babka;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;"During migration, page_owner info is now copied with the rest of the
page, so the stacktrace leading to free page allocation during migration
is overwritten";Vlastimil Babka;2016-03-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;" For debugging purposes, it might be however useful to
know that the page has been migrated since its initial allocation";Vlastimil Babka;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;" This
might happen many times during the lifetime for different reasons and
fully tracking this, especially with stacktraces would incur extra
memory costs";Vlastimil Babka;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;" As a compromise, store and print the migrate_reason of
the last migration that occurred to the page";Vlastimil Babka;2016-03-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;" This is enough to
distinguish compaction, numa balancing etc";Vlastimil Babka;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;Example page_owner entry after the patch;Vlastimil Babka;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2QxMmI0YWJmZDJmOGY0MjQxNGM1MjBiYmQwNTFhNWI3ZGM3YThj;"  Page allocated via order 0, mask 0x24200ca(GFP_HIGHUSER_MOVABLE)
  PFN 628753 type Movable Block 1228 type Movable Flags 0x1fffff80040030(dirty|lru|swapbacked)
   [<ffffffff811682c4>] __alloc_pages_nodemask+0x134/0x230
   [<ffffffff811b6325>] alloc_pages_vma+0xb5/0x250
   [<ffffffff81177491>] shmem_alloc_page+0x61/0x90
   [<ffffffff8117a438>] shmem_getpage_gfp+0x678/0x960
   [<ffffffff8117c2b9>] shmem_fallocate+0x329/0x440
   [<ffffffff811de600>] vfs_fallocate+0x140/0x230
   [<ffffffff811df434>] SyS_fallocate+0x44/0x70
   [<ffffffff8158cc2e>] entry_SYSCALL_64_fastpath+0x12/0x71
  Page has been migrated, last migrate reason: compaction";Vlastimil Babka;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;mm, page_owner: copy page owner info during migration;Vlastimil Babka;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;"The page_owner mechanism stores gfp_flags of an allocation and stack
trace that lead to it";Vlastimil Babka;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;" During page migration, the original information
is practically replaced by the allocation of free page as the migration
target";Vlastimil Babka;2016-03-15;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;" Arguably this is less useful and might lead to all the
page_owner info for migratable pages gradually converge towards
compaction or numa balancing migrations";Vlastimil Babka;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;" It has also lead to
inaccuracies such as one fixed by commit e2cfc91120fa (""mm/page_owner";Vlastimil Babka;2016-03-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;"set correct gfp_mask on page_owner"")";Vlastimil Babka;2016-03-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;This patch thus introduces copying the page_owner info during migration;Vlastimil Babka;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNDM1ZWRjYTkyODgwNTA3NGRhZTAwNWFiOWE0MmQ5ZmE2MGZjNzAy;"However, since the fact that the page has been migrated from its
original place might be useful for debugging, the next patch will
introduce a way to track that information as well.";Vlastimil Babka;2016-03-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;mm: numa: quickly fail allocations for NUMA balancing on full nodes;Mel Gorman;2016-02-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"Commit 4167e9b2cf10 (""mm: remove GFP_THISNODE"") removed the GFP_THISNODE
flag combination due to confusing semantics";Mel Gorman;2016-02-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" It noted that
alloc_misplaced_dst_page() was one such user after changes made by
commit e97ca8e5b864 (""mm: fix GFP_THISNODE callers and clarify"")";Mel Gorman;2016-02-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"Unfortunately when GFP_THISNODE was removed, users of
alloc_misplaced_dst_page() started waking kswapd and entering direct
reclaim because the wrong GFP flags are cleared";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" The consequence is
that workloads that used to fit into memory now get reclaimed which is
addressed by this patch";Mel Gorman;2016-02-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"The problem can be demonstrated with ""mutilate"" that exercises memcached
which is software dedicated to memory object caching";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" The configuration
uses 80% of memory and is run 3 times for varying numbers of clients";Mel Gorman;2016-02-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"The results on a 4-socket NUMA box are
mutilate
                          vanilla           numaswap-v1
Hmean    1      8394.71 (  0.00%)     8395.32 (  0.01%)
Hmean    4     30024.62 (  0.00%)    34513.54 ( 14.95%)
Hmean    7     32821.08 (  0.00%)    70542.96 (114.93%)
Hmean    12    55229.67 (  0.00%)    93866.34 ( 69.96%)
Hmean    21    39438.96 (  0.00%)    85749.21 (117.42%)
Hmean    30    37796.10 (  0.00%)    50231.49 ( 32.90%)
Hmean    47    18070.91 (  0.00%)    38530.13 (113.22%)
The metric is queries/second with the more the better";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" The results are
way outside of the noise and the reason for the improvement is obvious
from some of the vmstats
                               vanillanumaswap-v1r1
Minor Faults                1929399272  2146148218
Major Faults                  19746529        3567
Swap Ins                      57307366        9913
Swap Outs                     50623229       17094
Allocation stalls                35909         443
DMA allocs                           0           0
DMA32 allocs                  72976349   170567396
Normal allocs               5306640898  5310651252
Movable allocs                       0           0
Direct pages scanned         404130893      799577
Kswapd pages scanned         160230174           0
Kswapd pages reclaimed        55928786           0
Direct pages reclaimed         1843936       41921
Page writes file                  2391           0
Page writes anon              50623229       17094
The vanilla kernel is swapping like crazy with large amounts of direct
reclaim and kswapd activity";Mel Gorman;2016-02-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" The figures are aggregate but it's known
that the bad activity is throughout the entire test";Mel Gorman;2016-02-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"Note that simple streaming anon/file memory consumers also see this
problem but it's not as obvious";Mel Gorman;2016-02-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" In those cases, kswapd is awake when
it should not be";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;"As there are at least two reclaim-related bugs out there, it's worth
spelling out the user-visible impact";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" This patch only addresses bugs
related to excessive reclaim on NUMA hardware when the working set is
larger than a NUMA node";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4NDc5ZWJhNzc4MWZhOWZmYjI4MjY4ODQwZGU2ZmFjZmMxMmMzNWE3;" There is a bug related to high kswapd CPU
usage but the reports are against laptops and other UMA hardware and is
not addressed by this patch.";Mel Gorman;2016-02-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;thp: introduce deferred_split_huge_page();Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;Currently we don't split huge page on partial unmap;Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;" It's not an ideal
situation";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4; It can lead to memory overhead;Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;Furtunately, we can detect partial unmap on page_remove_rmap();Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;" But we
cannot call split_huge_page() from there due to locking context";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;"It's also counterproductive to do directly from munmap() codepath: in
many cases we will hit this from exit(2) and splitting the huge page
just to free it up in small pages is not what we really want";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;"The patch introduce deferred_split_huge_page() which put the huge page
into queue for splitting";Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;" The splitting itself will happen when we get
memory pressure via shrinker interface";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YTk4MjI1MGY3NzNjYzhjNzZmMWVlZTY4YTc3MGI3Y2JmMmZhZjc4;" The page will be dropped from
list on freeing through compound page destructor.";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZDJmYTk2NTQ4M2Y0YzM5YmQwOTdmZjliYmYzZWZlNjJkNGNmMzY3;thp, mm: split_huge_page(): caller need to lock page;Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ZDJmYTk2NTQ4M2Y0YzM5YmQwOTdmZjliYmYzZWZlNjJkNGNmMzY3;"We're going to use migration entries instead of compound_lock() to
stabilize page refcounts";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZDJmYTk2NTQ4M2Y0YzM5YmQwOTdmZjliYmYzZWZlNjJkNGNmMzY3;" Setup and remove migration entries require
page to be locked";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZDJmYTk2NTQ4M2Y0YzM5YmQwOTdmZjliYmYzZWZlNjJkNGNmMzY3;Some of split_huge_page() callers already have the page locked;Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZDJmYTk2NTQ4M2Y0YzM5YmQwOTdmZjliYmYzZWZlNjJkNGNmMzY3;" Let's
require everybody to lock the page before calling split_huge_page().";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;mm: rework mapcount accounting to enable 4k mapping of THPs;Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;We're going to allow mapping of individual 4k pages of THP compound;Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" It
means we need to track mapcount on per small page basis";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;"Straight-forward approach is to use ->_mapcount in all subpages to track
how many time this subpage is mapped with PMDs or PTEs combined";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" But
this is rather expensive: mapping or unmapping of a THP page with PMD
would require HPAGE_PMD_NR atomic operations instead of single we have
now";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;"The idea is to store separately how many times the page was mapped as
whole -- compound_mapcount";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" This frees up ->_mapcount in subpages to
track PTE mapcount";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;"We use the same approach as with compound page destructor and compound
order to store compound_mapcount: use space in first tail page,
->mapping this time";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;"Any time we map/unmap whole compound page (THP or hugetlb) -- we
increment/decrement compound_mapcount";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" When we map part of compound
page with PTE we operate on ->_mapcount of the subpage";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;page_mapcount() counts both: PTE and PMD mappings of the page;Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;Basically, we have mapcount for a subpage spread over two counters;Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" It
makes tricky to detect when last mapcount for a page goes away";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;We introduced PageDoubleMap() for this;Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;" When we split THP PMD for the
first time and there's other PMD mapping left we offset up ->_mapcount
in all subpages by one and set PG_double_map on the compound page";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;These additional references go away with last compound_mapcount;Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1M2Y5MjYzYmFiYTY5ZmMxNjMwZTNjNzgwYzRkMTFiNzI2NDNmOTYy;"This approach provides a way to detect when last mapcount goes away on
per small page basis without introducing new overhead for most common
cases.";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMjgxZWU2MTQ1MTgzNTk0Nzg4YWI2ZDViNTVmOGQxNDRlNjllYWNl;rmap: add argument to charge compound page;Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMjgxZWU2MTQ1MTgzNTk0Nzg4YWI2ZDViNTVmOGQxNDRlNjllYWNl;"We're going to allow mapping of individual 4k pages of THP compound
page";Kirill A. Shutemov;2016-01-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMjgxZWU2MTQ1MTgzNTk0Nzg4YWI2ZDViNTVmOGQxNDRlNjllYWNl;" It means we cannot rely on PageTransHuge() check to decide if
map/unmap small page or THP";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMjgxZWU2MTQ1MTgzNTk0Nzg4YWI2ZDViNTVmOGQxNDRlNjllYWNl;"The patch adds new argument to rmap functions to indicate whether we
want to operate on whole compound page or only the small page.";Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;page-flags: define PG_locked behavior on compound pages;Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;lock_page() must operate on the whole compound page;Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" It doesn't make
much sense to lock part of compound page";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" Change code to use head
page's PG_locked, if tail page is passed";Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;"This patch also gets rid of custom helper functions --
__set_page_locked() and __clear_page_locked()";Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" They are replaced with
helpers generated by __SETPAGEFLAG/__CLEARPAGEFLAG";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" Tail pages to these
helper would trigger VM_BUG_ON()";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;SLUB uses PG_locked as a bit spin locked;Kirill A. Shutemov;2016-01-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" IIUC, tail pages should never
appear there";Kirill A. Shutemov;2016-01-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo0OGM5MzVhZDg4ZjViZTIwZWI1NDQ1YTc3YzE3MTM1MWIxZWI1MTEx;" VM_BUG_ON() is added to make sure that this assumption is
correct.";Kirill A. Shutemov;2016-01-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;mm, page_alloc: rename __GFP_WAIT to __GFP_RECLAIM;Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;"__GFP_WAIT was used to signal that the caller was in atomic context and
could not sleep";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;" Now it is possible to distinguish between true atomic
context and callers that are not willing to sleep";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;" The latter should
clear __GFP_DIRECT_RECLAIM so kswapd will still wake";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;" As clearing
__GFP_WAIT behaves differently, there is a risk that people will clear the
wrong flags";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3MWJhYmE0YjkyZGMxZmExYmM0NjE3NDJjNmFiMTk0MmVjNjAzNGU5;" This patch renames __GFP_WAIT to __GFP_RECLAIM to clearly
indicate what it does -- setting it allows all reclaim activity, clearing
them prevents it.";Mel Gorman;2015-11-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;mm, page_alloc: distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd;Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"__GFP_WAIT has been used to identify atomic context in callers that hold
spinlocks or are in interrupts";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" They are expected to be high priority and
have access one of two watermarks lower than ""min"" which can be referred
to as the ""atomic reserve""";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" __GFP_HIGH users get access to the first
lower watermark and can be called the ""high priority reserve""";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"Over time, callers had a requirement to not block when fallback options
were available";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" Some have abused __GFP_WAIT leading to a situation where
an optimisitic allocation with a fallback option can access atomic
reserves";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
cannot sleep and have no alternative";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" High priority users continue to use
__GFP_HIGH";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" __GFP_DIRECT_RECLAIM identifies callers that can sleep and
are willing to enter direct reclaim";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" __GFP_KSWAPD_RECLAIM to identify
callers that want to wake kswapd for background reclaim";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" __GFP_WAIT is
redefined as a caller that is willing to enter direct reclaim and wake
kswapd for background reclaim";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"This patch then converts a number of sites
o __GFP_ATOMIC is used by callers that are high priority and have memory
  pools for those requests";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;GFP_ATOMIC uses this flag;Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"o Callers that have a limited mempool to guarantee forward progress clear
  __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"bio allocations fall
  into this category where kswapd will still be woken but atomic reserves
  are not used as there is a one-entry mempool to guarantee progress";Mel Gorman;2015-11-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"o Callers that are checking if they are non-blocking should use the
  helper gfpflags_allow_blocking() where possible";Mel Gorman;2015-11-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"This is because
  checking for __GFP_WAIT as was done historically now can trigger false
  positives";Mel Gorman;2015-11-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"Some exceptions like dm-crypt.c exist where the code intent
  is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
  flag manipulations";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"o Callers that built their own GFP flags instead of starting with GFP_KERNEL
  and friends now also need to specify __GFP_KSWAPD_RECLAIM";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"The first key hazard to watch out for is callers that removed __GFP_WAIT
and was depending on access to atomic reserves for inconspicuous reasons";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;In some cases it may be appropriate for them to use __GFP_HIGH;Mel Gorman;2015-11-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;"The second key hazard is callers that assembled their own combination of
GFP flags instead of starting with something like GFP_KERNEL";Mel Gorman;2015-11-07;1;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" They may
now wish to specify __GFP_KSWAPD_RECLAIM";Mel Gorman;2015-11-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpkMDE2NGFkYzg5ZjZiYjM3NGQzMDRmZmNjMzc1YzZkMjY1MmZlNjdk;" It's almost certainly harmless
if it's missed in most cases as other activity will wake kswapd.";Mel Gorman;2015-11-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;mm: migrate dirty page without clear_page_dirty_for_io etc;Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"clear_page_dirty_for_io() has accumulated writeback and memcg subtleties
since v2.6.16 first introduced page migration; and the set_page_dirty()
which completed its migration of PageDirty, later had to be moderated to
__set_page_dirty_nobuffers(); then PageSwapBacked had to skip that too";Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"No actual problems seen with this procedure recently, but if you look into
what the clear_page_dirty_for_io(page)+set_page_dirty(newpage) is actually
achieving, it turns out to be nothing more than moving the PageDirty flag,
and its NR_FILE_DIRTY stat from one zone to another";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"It would be good to avoid a pile of irrelevant decrementations and
incrementations, and improper event counting, and unnecessary descent of
the radix_tree under tree_lock (to set the PAGECACHE_TAG_DIRTY which
radix_tree_replace_slot() left in place anyway)";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"Do the NR_FILE_DIRTY movement, like the other stats movements, while
interrupts still disabled in migrate_page_move_mapping(); and don't even
bother if the zone is the same";Hugh Dickins;2015-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;" Do the PageDirty movement there under
tree_lock too, where old page is frozen and newpage not yet visible";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"bearing in mind that as soon as newpage becomes visible in radix_tree, an
un-page-locked set_page_dirty() might interfere (or perhaps that's just
not possible: anything doing so should already hold an additional
reference to the old page, preventing its migration; but play safe)";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0MmNiMTRiMTEwYTU2OThjY2YyNmNlNTljNDQ0MTcyMjYwNWEzNzQz;"But we do still need to transfer PageDirty in migrate_page_copy(), for
those who don't go the mapping route through migrate_page_move_mapping().";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;mm: page migration avoid touching newpage until no going back;Hugh Dickins;2015-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;"We have had trouble in the past from the way in which page migration's
newpage is initialized in dribs and drabs - see commit 8bdd63809160 (""mm";Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;"fix direct reclaim writeback regression"") which proposed a cleanup";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;"We have no actual problem now, but I think the procedure would be clearer
(and alternative get_new_page pools safer to implement) if we assert that
newpage is not touched until we are sure that it's going to be used -
except for taking the trylock on it in __unmap_and_move()";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;"So shift the early initializations from move_to_new_page() into
migrate_page_move_mapping(), mapping and NULL-mapping paths";Hugh Dickins;2015-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;" Similarly
migrate_huge_page_move_mapping(), but its NULL-mapping path can just be
deleted: you cannot reach hugetlbfs_migrate_page() with a NULL mapping";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZjRiNzY5YWJiOGFlZjAxZjg4NzU0M2NiODMwOGMwZDg2NzEzNjdj;Adjust stages 3 to 8 in the Documentation file accordingly.;Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw;mm: simplify page migration's anon_vma comment and flow;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw;"__unmap_and_move() contains a long stale comment on page_get_anon_vma()
and PageSwapCache(), with an odd control flow that's hard to follow";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw;"Mostly this reflects our confusion about the lifetime of an anon_vma, in
the early days of page migration, before we could take a reference to one";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw; Nowadays this seems quite straightforward: cut it all down to essentials;Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw;"I cannot see the relevance of swapcache here at all, so don't treat it any
differently: I believe the old comment reflects in part our anon_vma
confusions, and in part the original v2.6.16 page migration technique,
which used actual swap to migrate anon instead of swap-like migration
entries";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2YxNWM4NmM4ZDFiOWQ4MWU2ZDIxNTcxNWUxMTBhZWY4ZjkzNmUw;" Why should a swapcache page not be migrated with the aid of
migration entry ptes like everything else?  So lose that comment now, and
enable migration entries for swapcache in the next patch.";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzNmOWE2NzM3MTY0M2I2ZmFhOTg3NjIyYmMxYjY3NjY3YmFiODQ4;mm: page migration remove_migration_ptes at lock+unlock level;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzNmOWE2NzM3MTY0M2I2ZmFhOTg3NjIyYmMxYjY3NjY3YmFiODQ4;"Clean up page migration a little more by calling remove_migration_ptes()
from the same level, on success or on failure, from __unmap_and_move() or
from unmap_and_move_huge_page()";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YzNmOWE2NzM3MTY0M2I2ZmFhOTg3NjIyYmMxYjY3NjY3YmFiODQ4;"Don't reset page->mapping of a PageAnon old page in move_to_new_page(),
leave that to when the page is freed";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YzNmOWE2NzM3MTY0M2I2ZmFhOTg3NjIyYmMxYjY3NjY3YmFiODQ4;" Except for here in page migration,
it has been an invariant that a PageAnon (bit set in page->mapping) page
stays PageAnon until it is freed, and I think we're safer to keep to that";Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YzNmOWE2NzM3MTY0M2I2ZmFhOTg3NjIyYmMxYjY3NjY3YmFiODQ4;"And with the above rearrangement, it's necessary because zap_pte_range()
wants to identify whether a migration entry represents a file or an anon
page, to update the appropriate rss stats without waiting on it.";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;mm: page migration trylock newpage at same level as oldpage;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;"Clean up page migration a little by moving the trylock of newpage from
move_to_new_page() into __unmap_and_move(), where the old page has been
locked";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;" Adjust unmap_and_move_huge_page() and balloon_page_migrate()
accordingly";Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;"But make one kind-of-functional change on the way: whereas trylock of
newpage used to BUG() if it failed, now simply return -EAGAIN if so";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;"Cutting out BUG()s is good, right?  But, to be honest, this is really to
extend the usefulness of the custom put_new_page feature, allowing a pool
of new pages to be shared perhaps with racing uses";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZGI3NjcxZjgzNWNjYWQ2NmRiMjAxNTRhYzEyNzQxNDA5MzdkOWI3;"Use an ""else"" instead of that ""skip_unmap"" label.";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZGVmNzQyNGM5YmUwMDY5ODMxMzgwODIzZmRiNWNmNzIxMDNiOTE5;mm: page migration use the put_new_page whenever necessary;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZGVmNzQyNGM5YmUwMDY5ODMxMzgwODIzZmRiNWNmNzIxMDNiOTE5;"I don't know of any problem from the way it's used in our current tree,
but there is one defect in page migration's custom put_new_page feature";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZGVmNzQyNGM5YmUwMDY5ODMxMzgwODIzZmRiNWNmNzIxMDNiOTE5;"An unused newpage is expected to be released with the put_new_page(), but
there was one MIGRATEPAGE_SUCCESS (0) path which released it with
putback_lru_page(): which can be very wrong for a custom pool";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODoyZGVmNzQyNGM5YmUwMDY5ODMxMzgwODIzZmRiNWNmNzIxMDNiOTE5;"Fixed more easily by resetting put_new_page once it won't be needed, than
by adding a further flag to modify the rc test.";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNGUwZjliY2M5NWYxYWVmMjZhOWY4NjBjY2VkYTM1ZmFlZTc5YjM0;mm: correct a couple of page migration comments;Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNGUwZjliY2M5NWYxYWVmMjZhOWY4NjBjY2VkYTM1ZmFlZTc5YjM0;"It's migrate.c not migration,c, and nowadays putback_movable_pages() not
putback_lru_pages().";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NTYzN2JhYjMwZDZlNzY1MTczN2Y1MWFhOTk0MTdiYWVmNGQxMTRh;mm: rename mem_cgroup_migrate to mem_cgroup_replace_page;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NTYzN2JhYjMwZDZlNzY1MTczN2Y1MWFhOTk0MTdiYWVmNGQxMTRh;"After v4.3's commit 0610c25daa3e (""memcg: fix dirty page migration"")
mem_cgroup_migrate() doesn't have much to offer in page migration: convert
migrate_misplaced_transhuge_page() to set_page_memcg() instead";Hugh Dickins;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NTYzN2JhYjMwZDZlNzY1MTczN2Y1MWFhOTk0MTdiYWVmNGQxMTRh;"Then rename mem_cgroup_migrate() to mem_cgroup_replace_page(), since its
remaining callers are replace_page_cache_page() and shmem_replace_page()";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NTYzN2JhYjMwZDZlNzY1MTczN2Y1MWFhOTk0MTdiYWVmNGQxMTRh;both of whom passed lrucare true, so just eliminate that argument.;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MWFmYjEyYmE4MDlkYjY2NDY4MmEzMTE1NGMxMWU3MjBlMmMzNjNj;mm: page migration fix PageMlocked on migrated pages;Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MWFmYjEyYmE4MDlkYjY2NDY4MmEzMTE1NGMxMWU3MjBlMmMzNjNj;"Commit e6c509f85455 (""mm: use clear_page_mlock() in page_remove_rmap()"")
in v3.7 inadvertently made mlock_migrate_page() impotent: page migration
unmaps the page from userspace before migrating, and that commit clears
PageMlocked on the final unmap, leaving mlock_migrate_page() with
nothing to do";Hugh Dickins;2015-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo1MWFmYjEyYmE4MDlkYjY2NDY4MmEzMTE1NGMxMWU3MjBlMmMzNjNj;" Not a serious bug, the next attempt at reclaiming the
page would fix it up; but a betrayal of page migration's intent - the
new page ought to emerge as PageMlocked";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MWFmYjEyYmE4MDlkYjY2NDY4MmEzMTE1NGMxMWU3MjBlMmMzNjNj;"I don't see how to fix it for mlock_migrate_page() itself; but easily
fixed in remove_migration_pte(), by calling mlock_vma_page() when the vma
is VM_LOCKED - under pte lock as in try_to_unmap_one()";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MWFmYjEyYmE4MDlkYjY2NDY4MmEzMTE1NGMxMWU3MjBlMmMzNjNj;"Delete mlock_migrate_page()?  Not quite, it does still serve a purpose for
migrate_misplaced_transhuge_page(): where we could replace it by a test,
clear_page_mlock(), mlock_vma_page() sequence; but would that be an
improvement?  mlock_migrate_page() is fairly lean, and let's make it
leaner by skipping the irq save/restore now clearly not needed.";Hugh Dickins;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmMmY4MWZiMmI3MmI4M2I2NjFiMTFkYTZmMWIwYmQzNTI2NzA2Mjc4;mm, migrate: count pages failing all retries in vmstat and tracepoint;Vlastimil Babka;2015-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODpmMmY4MWZiMmI3MmI4M2I2NjFiMTFkYTZmMWIwYmQzNTI2NzA2Mjc4;"Migration tries up to 10 times to migrate pages that return -EAGAIN until
it gives up";Vlastimil Babka;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODpmMmY4MWZiMmI3MmI4M2I2NjFiMTFkYTZmMWIwYmQzNTI2NzA2Mjc4;" If some pages fail all retries, they are counted towards the
number of failed pages that migrate_pages() returns";Vlastimil Babka;2015-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODpmMmY4MWZiMmI3MmI4M2I2NjFiMTFkYTZmMWIwYmQzNTI2NzA2Mjc4;" They should also be
counted in the /proc/vmstat pgmigrate_fail and in the mm_migrate_pages
tracepoint.";Vlastimil Babka;2015-11-06;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;memcg: fix dirty page migration;Greg Thelen;2015-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;"The problem starts with a file backed dirty page which is charged to a
memcg";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4; Then page migration is used to move oldpage to newpage;Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;Migration;Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" - copies the oldpage's data to newpage
 - clears oldpage.PG_dirty
 - sets newpage.PG_dirty
 - uncharges oldpage from memcg
 - charges newpage to memcg
Clearing oldpage.PG_dirty decrements the charged memcg's dirty page
count";Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;"However, because newpage is not yet charged, setting newpage.PG_dirty
does not increment the memcg's dirty page count";Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" After migration
completes newpage.PG_dirty is eventually cleared, often in
account_page_cleaned()";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" At this time newpage is charged to a memcg so
the memcg's dirty page count is decremented which causes underflow
because the count was not previously incremented by migration";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" This
underflow causes balance_dirty_pages() to see a very large unsigned
number of dirty memcg pages which leads to aggressive throttling of
buffered writes by processes in non root memcg";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;This issue;Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4; - can harm performance of non root memcg buffered writes;Greg Thelen;2015-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" - can report too small (even negative) values in
   memory.stat[(total_)dirty] counters of all memcg, including the root";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;"To avoid polluting migrate.c with #ifdef CONFIG_MEMCG checks, introduce
page_memcg() and set_page_memcg() helpers";Greg Thelen;2015-10-01;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;Test;Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;"    0) setup and enter limited memcg
    mkdir /sys/fs/cgroup/test
    echo 1G > /sys/fs/cgroup/test/memory.limit_in_bytes
    echo $$ > /sys/fs/cgroup/test/cgroup.procs
    1) buffered writes baseline
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
    2) buffered writes with compaction antagonist to induce migration
    yes 1 > /proc/sys/vm/compact_memory &
    rm -rf /data/tmp/foo
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    kill %
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
    3) buffered writes without antagonist, should match baseline
    rm -rf /data/tmp/foo
    dd if=/dev/zero of=/data/tmp/foo bs=1M count=1k
    sync
    grep ^dirty /sys/fs/cgroup/test/memory.stat
                       (speed, dirty residue)
             unpatched                       patched
    1) 841 MB/s 0 dirty pages          886 MB/s 0 dirty pages
    2) 611 MB/s -33427456 dirty pages  793 MB/s 0 dirty pages
    3) 114 MB/s -33427456 dirty pages  891 MB/s 0 dirty pages
    Notice that unpatched baseline performance (1) fell after
    migration (3): 841 -> 114 MB/s";Greg Thelen;2015-10-01;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;" In the patched kernel, post
    migration performance matches baseline";Greg Thelen;2015-10-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjEwYzI1ZGFhM2U3NmUzOGFkNWE4ZmFlNjgzYTg5ZmY5ZjcxNzk4;"Fixes: c4843a7593a9 (""memcg: add per cgroup dirty page accounting"")";Greg Thelen;2015-10-01;1;1
MDY6Q29tbWl0MjMyNTI5ODozYWFhNzZlMTI1YzFkZDU4YzliNTk5YmFhOGM2MDIxODk2ODc0YzEy;mm: migrate: hugetlb: putback destination hugepage to active list;Naoya Horiguchi;2015-09-22;1;0
MDY6Q29tbWl0MjMyNTI5ODozYWFhNzZlMTI1YzFkZDU4YzliNTk5YmFhOGM2MDIxODk2ODc0YzEy;"Since commit bcc54222309c (""mm: hugetlb: introduce page_huge_active"")
each hugetlb page maintains its active flag to avoid a race condition
betwe= en multiple calls of isolate_huge_page(), but current kernel
doesn't set the f= lag on a hugepage allocated by migration because the
proper putback routine isn= 't called";Naoya Horiguchi;2015-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODozYWFhNzZlMTI1YzFkZDU4YzliNTk5YmFhOGM2MDIxODk2ODc0YzEy;" This means that users could
still encounter the race referred to by bcc54222309c in this special
case, so this patch fixes it";Naoya Horiguchi;2015-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODozYWFhNzZlMTI1YzFkZDU4YzliNTk5YmFhOGM2MDIxODk2ODc0YzEy;"Fixes: bcc54222309c (""mm: hugetlb: introduce page_huge_active"")";Naoya Horiguchi;2015-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;mm: introduce idle page tracking;Vladimir Davydov;2015-09-09;1;0
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"Knowing the portion of memory that is not used by a certain application or
memory cgroup (idle memory) can be useful for partitioning the system
efficiently, e.g";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1; by setting memory cgroup limits appropriately;Vladimir Davydov;2015-09-09;1;0
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"Currently, the only means to estimate the amount of idle memory provided
by the kernel is /proc/PID/{clear_refs,smaps}: the user can clear the
access bit for all pages mapped to a particular process by writing 1 to
clear_refs, wait for some time, and then count smaps:Referenced";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;" However,
this method has two serious shortcomings";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;" - it does not count unmapped file pages
 - it affects the reclaimer logic
To overcome these drawbacks, this patch introduces two new page flags,
Idle and Young, and a new sysfs file, /sys/kernel/mm/page_idle/bitmap";Vladimir Davydov;2015-09-09;1;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"A page's Idle flag can only be set from userspace by setting bit in
/sys/kernel/mm/page_idle/bitmap at the offset corresponding to the page,
and it is cleared whenever the page is accessed either through page tables
(it is cleared in page_referenced() in this case) or using the read(2)
system call (mark_page_accessed())";Vladimir Davydov;2015-09-09;1;0
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"Thus by setting the Idle flag for
pages of a particular workload, which can be found e.g";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;" by reading
/proc/PID/pagemap, waiting for some time to let the workload access its
working set, and then reading the bitmap file, one can estimate the amount
of pages that are not used by the workload";Vladimir Davydov;2015-09-09;1;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"The Young page flag is used to avoid interference with the memory
reclaimer";Vladimir Davydov;2015-09-09;1;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;" A page's Young flag is set whenever the Access bit of a page
table entry pointing to the page is cleared by writing to the bitmap file";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"If page_referenced() is called on a Young page, it will add 1 to its
return value, therefore concealing the fact that the Access bit was
cleared";Vladimir Davydov;2015-09-09;1;1
MDY6Q29tbWl0MjMyNTI5ODozM2MzZmM3MWM4Y2ZhM2NjM2E5OGJlYWE5MDFjMDY5YzE3N2RjMjk1;"Note, since there is no room for extra page flags on 32 bit, this feature
uses extended page flags when compiled on 32 bit.";Vladimir Davydov;2015-09-09;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;mm: rename alloc_pages_exact_node() to __alloc_pages_node();Vlastimil Babka;2015-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"alloc_pages_exact_node() was introduced in commit 6484eb3e2a81 (""page
allocator: do not check NUMA node ID when the caller knows the node is
valid"") as an optimized variant of alloc_pages_node(), that doesn't
fallback to current node for nid == NUMA_NO_NODE";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;" Unfortunately the
name of the function can easily suggest that the allocation is
restricted to the given node and fails otherwise";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;" In truth, the node is
only preferred, unless __GFP_THISNODE is passed among the gfp flags";Vlastimil Babka;2015-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"The misleading name has lead to mistakes in the past, see for example
commits 5265047ac301 (""mm, thp: really limit transparent hugepage
allocation to local node"") and b360edb43f8e (""mm, mempolicy";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"migrate_to_node should only migrate to node"")";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"Another issue with the name is that there's a family of
alloc_pages_exact*() functions where 'exact' means exact size (instead
of page order), which leads to more confusion";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"To prevent further mistakes, this patch effectively renames
alloc_pages_exact_node() to __alloc_pages_node() to better convey that
it's an optimized variant of alloc_pages_node() not intended for general
usage";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy; Both functions get described in comments;Vlastimil Babka;2015-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"It has been also considered to really provide a convenience function for
allocations restricted to a node, but the major opinion seems to be that
__GFP_THISNODE already provides that functionality and we shouldn't
duplicate the API needlessly";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;" The number of users would be small
anyway";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"Existing callers of alloc_pages_exact_node() are simply converted to
call __alloc_pages_node(), with the exception of sba_alloc_coherent()
which open-codes the check for NUMA_NO_NODE, so it is converted to use
alloc_pages_node() instead";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;" This means it no longer performs some
VM_BUG_ON checks, and since the current check for nid in
alloc_pages_node() uses a 'nid < 0' comparison (which includes
NUMA_NO_NODE), it may hide wrong values which would be previously
exposed";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;Both differences will be rectified by the next patch;Vlastimil Babka;2015-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;"To sum up, this patch makes no functional changes, except temporarily
hiding potentially buggy callers";Vlastimil Babka;2015-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NmRiODAwZjVkNzNjZDVjNDk0NjEyNTNkNDU3NjZlMDk0ZjBmOGMy;" Restricting the checks in
alloc_pages_node() is left for the next patch which can in turn expose
more existing buggy callers.";Vlastimil Babka;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;mm/hwpoison: fix race between soft_offline_page and unpoison_memory;Wanpeng Li;2015-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"Wanpeng Li reported a race between soft_offline_page() and
unpoison_memory(), which causes the following kernel panic";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"   BUG: Bad page state in process bash  pfn:97000
   page:ffffea00025c0000 count:0 mapcount:1 mapping:          (null) index:0x7f4fdbe00
   flags: 0x1fffff80080048(uptodate|active|swapbacked)
   page dumped because: PAGE_FLAGS_CHECK_AT_FREE flag(s) set
   bad because of flags";Wanpeng Li;2015-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"   flags: 0x40(active)
   Modules linked in: snd_hda_codec_hdmi i915 rpcsec_gss_krb5 nfsv4 dns_resolver bnep rfcomm nfsd bluetooth auth_rpcgss nfs_acl nfs rfkill lockd grace sunrpc i2c_algo_bit drm_kms_helper snd_hda_codec_realtek snd_hda_codec_generic drm snd_hda_intel fscache snd_hda_codec x86_pkg_temp_thermal coretemp kvm_intel snd_hda_core snd_hwdep kvm snd_pcm snd_seq_dummy snd_seq_oss crct10dif_pclmul snd_seq_midi crc32_pclmul snd_seq_midi_event ghash_clmulni_intel snd_rawmidi aesni_intel lrw gf128mul snd_seq glue_helper ablk_helper snd_seq_device cryptd fuse snd_timer dcdbas serio_raw mei_me parport_pc snd mei ppdev i2c_core video lp soundcore parport lpc_ich shpchp mfd_core ext4 mbcache jbd2 sd_mod e1000e ahci ptp libahci crc32c_intel libata pps_core
   CPU: 3 PID: 2211 Comm: bash Not tainted 4.2.0-rc5-mm1+ #45
   Hardware name: Dell Inc";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"OptiPlex 7020/0F5C5X, BIOS A03 01/08/2015
   This race is explained like below";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"  CPU0                    CPU1
  soft_offline_page
  __soft_offline_page
  TestSetPageHWPoison
                        unpoison_memory
                        PageHWPoison check (true)
                        TestClearPageHWPoison
                        put_page    -> release refcount held by get_hwpoison_page in unpoison_memory
                        put_page    -> release refcount held by isolate_lru_page in __soft_offline_page
  migrate_pages
The second put_page() releases refcount held by isolate_lru_page() which
will lead to unmap_and_move() releases the last refcount of page and w/
mapcount still 1 since try_to_unmap() is not called if there is only one
user map the page";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;" Anyway, the page refcount and mapcount will still
mess if the page is mapped by multiple users";Wanpeng Li;2015-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"This race was introduced by commit 4491f71260 (""mm/memory-failure: set
PageHWPoison before migrate_pages()""), which focuses on preventing the
reuse of successfully migrated page";Wanpeng Li;2015-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;" Before this commit we prevent the
reuse by changing the migratetype to MIGRATE_ISOLATE during soft
offlining, which has the following problems, so simply reverting the
commit is not a best option";Wanpeng Li;2015-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"  1) it doesn't eliminate the reuse completely, because
     set_migratetype_isolate() can fail to set MIGRATE_ISOLATE to the
     target page if the pageblock of the page contains one or more
     unmovable pages (i.e";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1; has_unmovable_pages() returns true);Wanpeng Li;2015-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"  2) the original code changes migratetype to MIGRATE_ISOLATE
     forcibly, and sets it to MIGRATE_MOVABLE forcibly after soft offline,
     regardless of the original migratetype state, which could impact
     other subsystems like memory hotplug or compaction";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"This patch moves PageSetHWPoison just after put_page() in
unmap_and_move(), which closes up the reported race window and minimizes
another race window b/w SetPageHWPoison and reallocation (which causes
the reuse of soft-offlined page.) The latter race window still exists
but it's acceptable, because it's rare and effectively the same as
ordinary ""containment failure"" case even if it happens, so keep the
window open is acceptable";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYTFiMTNjY2ZiZWJlMGI5ZDY5YjVkNjFlZmYwYTY3NWUxOWU2OWE1;"Fixes: 4491f71260 (""mm/memory-failure: set PageHWPoison before migrate_pages()"")";Wanpeng Li;2015-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpkODk5ODQ0ZTljOThjOWM3NGI0ZDk5MjZmZDNiZDY2YTIyNWY2OTc4;mm: fix status code which move_pages() returns for zero page;Kirill A. Shutemov;2015-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODpkODk5ODQ0ZTljOThjOWM3NGI0ZDk5MjZmZDNiZDY2YTIyNWY2OTc4;"The manpage for move_pages(2) specifies that status code for zero page is
supposed to be -EFAULT";Kirill A. Shutemov;2015-09-04;0;0
MDY6Q29tbWl0MjMyNTI5ODpkODk5ODQ0ZTljOThjOWM3NGI0ZDk5MjZmZDNiZDY2YTIyNWY2OTc4; Currently kernel return -ENOENT in this case;Kirill A. Shutemov;2015-09-04;0;1
MDY6Q29tbWl0MjMyNTI5ODpkODk5ODQ0ZTljOThjOWM3NGI0ZDk5MjZmZDNiZDY2YTIyNWY2OTc4;follow_page() can do it for us, if we would ask for FOLL_DUMP;Kirill A. Shutemov;2015-09-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpkODk5ODQ0ZTljOThjOWM3NGI0ZDk5MjZmZDNiZDY2YTIyNWY2OTc4;" The use of
FOLL_DUMP also means that the upper layer page tables pages are no longer
allocated.";Kirill A. Shutemov;2015-09-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo0NDkxZjcxMjYwNjNlZjUxMDgxZjU2NjJiZDRmY2FlMzE2MjFhMzMz;mm/memory-failure: set PageHWPoison before migrate_pages();Naoya Horiguchi;2015-08-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0NDkxZjcxMjYwNjNlZjUxMDgxZjU2NjJiZDRmY2FlMzE2MjFhMzMz;"Now page freeing code doesn't consider PageHWPoison as a bad page, so by
setting it before completing the page containment, we can prevent the
error page from being reused just after successful page migration";Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NDkxZjcxMjYwNjNlZjUxMDgxZjU2NjJiZDRmY2FlMzE2MjFhMzMz;"I added TTU_IGNORE_HWPOISON for try_to_unmap() to make sure that the
page table entry is transformed into migration entry, not to hwpoison
entry.";Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;mm: check __PG_HWPOISON separately from PAGE_FLAGS_CHECK_AT_*;Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;"The race condition addressed in commit add05cecef80 (""mm: soft-offline";Naoya Horiguchi;2015-08-06;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;"don't free target page in successful page migration"") was not closed
completely, because that can happen not only for soft-offline, but also
for hard-offline";Naoya Horiguchi;2015-08-06;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;" Consider that a slab page is about to be freed into
buddy pool, and then an uncorrected memory error hits the page just
after entering __free_one_page(), then VM_BUG_ON_PAGE(page->flags &
PAGE_FLAGS_CHECK_AT_PREP) is triggered, despite the fact that it's not
necessary because the data on the affected page is not consumed";Naoya Horiguchi;2015-08-06;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;"To solve it, this patch drops __PG_HWPOISON from page flag checks at
allocation/free time";Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;" I think it's justified because __PG_HWPOISON
flags is defined to prevent the page from being reused, and setting it
outside the page's alloc-free cycle is a designed behavior (not a bug.)
For recent months, I was annoyed about BUG_ON when soft-offlined page
remains on lru cache list for a while, which is avoided by calling
put_page() instead of putback_lru_page() in page migration's success
path";Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5;" This means that this patch reverts a major change from commit
add05cecef80 about the new refcounting rule of soft-offlined pages, so
""reuse window"" revives";Naoya Horiguchi;2015-08-06;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNGMxOGU2ZjdiNWJiYjViNTI4YjMzMzQxMTU4MDZiMGQ3NmY1MGY5; This will be closed by a subsequent patch.;Naoya Horiguchi;2015-08-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODA5YWEyZDI4ZDc0MTExZmYyZjE5MjhlZGFhNGU5ODQ1Yzk3YTdk;mm: clarify that the function operates on hugepage pte;Aneesh Kumar K.V;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODA5YWEyZDI4ZDc0MTExZmYyZjE5MjhlZGFhNGU5ODQ1Yzk3YTdk;We have confusing functions to clear pmd, pmd_clear_* and pmd_clear;Aneesh Kumar K.V;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODA5YWEyZDI4ZDc0MTExZmYyZjE5MjhlZGFhNGU5ODQ1Yzk3YTdk;" Add
_huge_ to pmdp_clear functions so that we are clear that they operate on
hugepage pte";Aneesh Kumar K.V;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ODA5YWEyZDI4ZDc0MTExZmYyZjE5MjhlZGFhNGU5ODQ1Yzk3YTdk;"We don't bother about other functions like pmdp_set_wrprotect,
pmdp_clear_flush_young, because they operate on PTE bits and hence
indicate they are operating on hugepage ptes";Aneesh Kumar K.V;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;mm: soft-offline: don't free target page in successful page migration;Naoya Horiguchi;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"Stress testing showed that soft offline events for a process iterating
""mmap-pagefault-munmap"" loop can trigger
VM_BUG_ON(PAGE_FLAGS_CHECK_AT_PREP) in __free_one_page()";Naoya Horiguchi;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"  Soft offlining page 0x70fe1 at 0x70100008d000
  Soft offlining page 0x705fb at 0x70300008d000
  page:ffffea0001c3f840 count:0 mapcount:0 mapping:          (null) index:0x2
  flags: 0x1fffff80800000(hwpoison)
  page dumped because: VM_BUG_ON_PAGE(page->flags & ((1 << 25) - 1))
  ------------[ cut here ]------------
  kernel BUG at /src/linux-dev/mm/page_alloc.c:585!
  invalid opcode: 0000 [#1] SMP DEBUG_PAGEALLOC
  Modules linked in: cfg80211 rfkill crc32c_intel microcode ppdev parport_pc pcspkr serio_raw virtio_balloon parport i2c_piix4 virtio_blk virtio_net ata_generic pata_acpi floppy
  CPU: 3 PID: 1779 Comm: test_base_madv_ Not tainted 4.0.0-v4.0-150511-1451-00009-g82360a3730e6 #139
  RIP: free_pcppages_bulk+0x52a/0x6f0
  When soft offline successfully migrates page, the source page is supposed
to be freed";Naoya Horiguchi;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" But there is a race condition where a source page looks
isolated (i.e";Naoya Horiguchi;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" the refcount is 0 and the PageHWPoison is set) but
somewhat linked to pcplist";Naoya Horiguchi;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" Then another soft offline event calls
drain_all_pages() and tries to free such hwpoisoned page, which is
forbidden";Naoya Horiguchi;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"This odd page state seems to happen due to the race between put_page() in
putback_lru_page() and __pagevec_lru_add_fn()";Naoya Horiguchi;2015-06-24;1;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" But I don't want to play
with tweaking drain code as done in commit 9ab3b598d2df ""mm: hwpoison";Naoya Horiguchi;2015-06-24;1;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"drop lru_add_drain_all() in __soft_offline_page()"", or to change page
freeing code for this soft offline's purpose";Naoya Horiguchi;2015-06-24;1;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"Instead, let's think about the difference between hard offline and soft
offline";Naoya Horiguchi;2015-06-24;1;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" There is an interesting difference in how to isolate the in-use
page between these, that is, hard offline marks PageHWPoison of the target
page at first, and doesn't free it by keeping its refcount 1";Naoya Horiguchi;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" OTOH, soft
offline tries to free the target page then marks PageHWPoison";Naoya Horiguchi;2015-06-24;0;0
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" This
difference might be the source of complexity and result in bugs like the
above";Naoya Horiguchi;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" So making soft offline isolate with keeping refcount can be a
solution for this problem";Naoya Horiguchi;2015-06-24;0;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;"We can pass to page migration code the ""reason"" which shows the caller, so
let's use this more to avoid calling putback_lru_page() when called from
soft offline, which effectively does the isolation for soft offline";Naoya Horiguchi;2015-06-24;1;1
MDY6Q29tbWl0MjMyNTI5ODphZGQwNWNlY2VmODAzZjMzNzJjNWZjMWQyYTk2NDE3MTg3MmRhZjlm;" With
this change, target pages of soft offline never be reused without changing
migratetype, so this patch also removes the related code.";Naoya Horiguchi;2015-06-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpiM2IzYTk5YzUzNzFlMmU5NmEyYzY4MGU2YWMyMDIxOGJkZGJkNDIy;mm/migrate: check-before-clear PageSwapCache;Naoya Horiguchi;2015-04-15;1;1
MDY6Q29tbWl0MjMyNTI5ODpiM2IzYTk5YzUzNzFlMmU5NmEyYzY4MGU2YWMyMDIxOGJkZGJkNDIy;"With the page flag sanitization patchset, an invalid usage of
ClearPageSwapCache() is detected in migration_page_copy()";Naoya Horiguchi;2015-04-15;0;1
MDY6Q29tbWl0MjMyNTI5ODpiM2IzYTk5YzUzNzFlMmU5NmEyYzY4MGU2YWMyMDIxOGJkZGJkNDIy;"migrate_page_copy() is shared by both normal and hugepage (both thp and
hugetlb) code path, so let's check PageSwapCache() and clear it if it's
set to avoid misuse of the invalid clear operation.";Naoya Horiguchi;2015-04-15;1;1
MDY6Q29tbWl0MjMyNTI5ODoyYThlNzAwMjY0MzVhZDk3NTcwYTFlMGEwYzRjOTQxZTBmNzAwYTNl;mm: numa: remove migrate_ratelimited;Mel Gorman;2015-04-14;1;1
MDY6Q29tbWl0MjMyNTI5ODoyYThlNzAwMjY0MzVhZDk3NTcwYTFlMGEwYzRjOTQxZTBmNzAwYTNl;"This code is dead since commit 9e645ab6d089 (""sched/numa: Continue PTE
scanning even if migrate rate limited"") so remove it.";Mel Gorman;2015-04-14;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;"mm/migrate: mark unmap_and_move() ""noinline"" to avoid ICE in gcc 4.7.3";Geert Uytterhoeven;2015-04-14;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;With gcc version 4.7.3 (Ubuntu/Linaro 4.7.3-12ubuntu1) ;Geert Uytterhoeven;2015-04-14;0;0
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;    mm/migrate.c: In function `migrate_pages';Geert Uytterhoeven;2015-04-14;0;1
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;"    mm/migrate.c:1148:1: internal compiler error: in push_minipool_fix, at config/arm/arm.c:13500
    Please submit a full bug report,
    with preprocessed source if appropriate";Geert Uytterhoeven;2015-04-14;1;1
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;    See <file:///usr/share/doc/gcc-4.7/README.Bugs> for instructions;Geert Uytterhoeven;2015-04-14;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;    Preprocessed source stored into /tmp/ccPoM1tr.out file, please attach this to your bugreport;Geert Uytterhoeven;2015-04-14;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjJhNTE1M2I0ZDJjNDhjMDViOTI4MDQ5MWNiNTU5MmE0NmRmMzg1;"    make[1]: *** [mm/migrate.o] Error 1
    make: *** [mm/migrate.o] Error 2
Mark unmap_and_move() (which is used in a single place only) ""noinline""
to work around this compiler bug.";Geert Uytterhoeven;2015-04-14;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZDk0MjQ2Njk5NDY1MzJiZTc1NGE2ZTExNjYxOGRjYjU4NDMwY2I0;mm: convert p[te|md]_mknonnuma and remaining page table manipulations;Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZDk0MjQ2Njk5NDY1MzJiZTc1NGE2ZTExNjYxOGRjYjU4NDMwY2I0;"With PROT_NONE, the traditional page table manipulation functions are
sufficient.";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;mm: numa: do not dereference pmd outside of the lock during NUMA hinting fault;Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"Automatic NUMA balancing depends on being able to protect PTEs to trap a
fault and gather reference locality information";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" Very broadly speaking
it would mark PTEs as not present and use another bit to distinguish
between NUMA hinting faults and other types of faults";Mel Gorman;2015-02-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" It was
universally loved by everybody and caused no problems whatsoever";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" That
last sentence might be a lie";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"This series is very heavily based on patches from Linus and Aneesh to
replace the existing PTE/PMD NUMA helper functions with normal change
protections";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" I did alter and add parts of it but I consider them
relatively minor contributions";Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" At their suggestion, acked-bys are in
there but I've no problem converting them to Signed-off-by if requested";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"AFAIK, this has received no testing on ppc64 and I'm depending on Aneesh
for that";Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" I tested trinity under kvm-tool and passed and ran a few
other basic tests";Mel Gorman;2015-02-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" At the time of writing, only the short-lived tests
have completed but testing of V2 indicated that long-term testing had no
surprises";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" In most cases I'm leaving out detail as it's not that
interesting";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"specjbb single JVM: There was negligible performance difference in the
	benchmark itself for short runs";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"However, system activity is
	higher and interrupts are much higher over time -- possibly TLB
	flushes";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;Migrations are also higher;Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"Overall, this is more overhead
	but considering the problems faced with the old approach I think
	we just have to suck it up and find another way of reducing the
	overhead";Mel Gorman;2015-02-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"specjbb multi JVM: Negligible performance difference to the actual benchmark
	but like the single JVM case, the system overhead is noticeably
	higher";Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4; Again, interrupts are a major factor;Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"autonumabench: This was all over the place and about all that can be
	reasonably concluded is that it's different but not necessarily
	better or worse";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"autonumabench
                                     3.18.0-rc5            3.18.0-rc5
                                 mmotm-20141119         protnone-v3r3
User    NUMA01               32380.24 (  0.00%)    21642.92 ( 33.16%)
User    NUMA01_THEADLOCAL    22481.02 (  0.00%)    22283.22 (  0.88%)
User    NUMA02                3137.00 (  0.00%)     3116.54 (  0.65%)
User    NUMA02_SMT            1614.03 (  0.00%)     1543.53 (  4.37%)
System  NUMA01                 322.97 (  0.00%)     1465.89 (-353.88%)
System  NUMA01_THEADLOCAL       91.87 (  0.00%)       49.32 ( 46.32%)
System  NUMA02                  37.83 (  0.00%)       14.61 ( 61.38%)
System  NUMA02_SMT               7.36 (  0.00%)        7.45 ( -1.22%)
Elapsed NUMA01                 716.63 (  0.00%)      599.29 ( 16.37%)
Elapsed NUMA01_THEADLOCAL      553.98 (  0.00%)      539.94 (  2.53%)
Elapsed NUMA02                  83.85 (  0.00%)       83.04 (  0.97%)
Elapsed NUMA02_SMT              86.57 (  0.00%)       79.15 (  8.57%)
CPU     NUMA01                4563.00 (  0.00%)     3855.00 ( 15.52%)
CPU     NUMA01_THEADLOCAL     4074.00 (  0.00%)     4136.00 ( -1.52%)
CPU     NUMA02                3785.00 (  0.00%)     3770.00 (  0.40%)
CPU     NUMA02_SMT            1872.00 (  0.00%)     1959.00 ( -4.65%)
System CPU usage of NUMA01 is worse but it's an adverse workload on this
machine so I'm reluctant to conclude that it's a problem that matters";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" On
the other workloads that are sensible on this machine, system CPU usage is
great";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" Overall time to complete the benchmark is comparable
          3.18.0-rc5  3.18.0-rc5
        mmotm-20141119protnone-v3r3
User        59612.50    48586.44
System        460.22     1537.45
Elapsed      1442.20     1304.29
NUMA alloc hit                 5075182     5743353
NUMA alloc miss                      0           0
NUMA interleave hit                  0           0
NUMA alloc local               5075174     5743339
NUMA base PTE updates        637061448   443106883
NUMA huge PMD updates          1243434      864747
NUMA page range updates     1273699656   885857347
NUMA hint faults               1658116     1214277
NUMA hint local faults          959487      754113
NUMA hint local percent             57          62
NUMA pages migrated            5467056    61676398
The NUMA pages migrated look terrible but when I looked at a graph of the
activity over time I see that the massive spike in migration activity was
during NUMA01";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" This correlates with high system CPU usage and could be
simply down to bad luck but any modifications that affect that workload
would be related to scan rates and migrations, not the protection
mechanism";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4; For all other workloads, migration activity was comparable;Mel Gorman;2015-02-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"Overall, headline performance figures are comparable but the overhead is
higher, mostly in interrupts";Mel Gorman;2015-02-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" To some extent, higher overhead from this
approach was anticipated but not to this degree";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" It's going to be
necessary to reduce this again with a separate series in the future";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" It's
still worth going ahead with this series though as it's likely to avoid
constant headaches with Xen and is probably easier to maintain";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;This patch (of 10);Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;"A transhuge NUMA hinting fault may find the page is migrating and should
wait until migration completes";Mel Gorman;2015-02-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;" The check is race-prone because the pmd
is deferenced outside of the page lock and while the race is tiny, it'll
be larger if the PMD is cleared while marking PMDs for hinting fault";Mel Gorman;2015-02-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo1ZDgzMzA2MjEzOWQyOTBhZGI4YjYyYzA5M2I2NTRhMDFhMzUzNDQ4;This patch closes the race.;Mel Gorman;2015-02-12;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;mm/hugetlb: take page table lock in follow_huge_pmd();Naoya Horiguchi;2015-02-11;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;"We have a race condition between move_pages() and freeing hugepages, where
move_pages() calls follow_page(FOLL_GET) for hugepages internally and
tries to get its refcount without preventing concurrent freeing";Naoya Horiguchi;2015-02-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;" This
race crashes the kernel, so this patch fixes it by moving FOLL_GET code
for hugepages into follow_huge_pmd() with taking the page table lock";Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;This patch intentionally removes page==NULL check after pte_page;Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;"This is justified because pte_page() never returns NULL for any
architectures or configurations";Naoya Horiguchi;2015-02-11;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;"This patch changes the behavior of follow_huge_pmd() for tail pages and
then tail pages can be pinned/returned";Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;" So the caller must be changed to
properly handle the returned tail pages";Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;"We could have a choice to add the similar locking to
follow_huge_(addr|pud) for consistency, but it's not necessary because
currently these functions don't support FOLL_GET flag, so let's leave it
for future development";Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;Here is the reproducer;Naoya Horiguchi;2015-02-11;1;0
MDY6Q29tbWl0MjMyNTI5ODplNjZmMTdmZjcxNzcyYjIwOWVlZDM5ZGUzNWFhYTk5YmE4MTljOTNk;"  #include <stdio.h>
  #include <stdlib.h>
  #include <numaif.h>
  #define ADDR_INPUT      0x700000000000UL
  #define HPS             0x200000
  #define PS              0x1000
                  ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
                  ret = numa_move_pages(pid, nr_p, addrs, nodes, status,
  #include <stdio.h>
  #include <sys/mman.h>
  #include <string.h>
  #define ADDR_INPUT      0x700000000000UL
  #define HPS             0x200000
                  p = mmap((void *)ADDR_INPUT, nr_hp * HPS, PROT_READ | PROT_WRITE,
  $ sysctl vm.nr_hugepages=40
  $ ./hugepage 10 &
  $ ./movepages 10 $(pgrep -f hugepage)
Fixes: e632a938d914 (""mm: migrate: add hugepage migration code to move_pages()"")";Naoya Horiguchi;2015-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODoyN2JhMDY0NGVhOWRmZTZlNzY5M2FiYzg1ODM3YjYwZTQwNTgzYjk2;rmap: drop support of non-linear mappings;Kirill A. Shutemov;2015-02-10;1;0
MDY6Q29tbWl0MjMyNTI5ODoyN2JhMDY0NGVhOWRmZTZlNzY5M2FiYzg1ODM3YjYwZTQwNTgzYjk2;We don't create non-linear mappings anymore;Kirill A. Shutemov;2015-02-10;1;1
MDY6Q29tbWl0MjMyNTI5ODoyN2JhMDY0NGVhOWRmZTZlNzY5M2FiYzg1ODM3YjYwZTQwNTgzYjk2;" Let's drop code which
handles them in rmap.";Kirill A. Shutemov;2015-02-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MDA2MjE3NWZmYzg0NGI4ZmY5NjY0MDI0YzY0MTZhMzdhZDYzYzc3;vm_area_operations: kill ->migrate();Al Viro;2014-05-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MDA2MjE3NWZmYzg0NGI4ZmY5NjY0MDI0YzY0MTZhMzdhZDYzYzc3;"the only instance this method has ever grown was one in kernfs -
one that call ->migrate() of another vm_ops if it exists.";Al Viro;2014-05-15;1;0
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;mm: unmapped page migration avoid unmap+remap overhead;Hugh Dickins;2014-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;"Page migration's __unmap_and_move(), and rmap's try_to_unmap(), were
created for use on pages almost certainly mapped into userspace";Hugh Dickins;2014-12-13;0;1
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;" But
nowadays compaction often applies them to unmapped page cache pages: which
may exacerbate contention on i_mmap_rwsem quite unnecessarily, since
try_to_unmap_file() makes no preliminary page_mapped() check";Hugh Dickins;2014-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;"Now check page_mapped() in __unmap_and_move(); and avoid repeating the
same overhead in rmap_walk_file() - don't remove_migration_ptes() when we
never inserted any";Hugh Dickins;2014-12-13;1;0
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;"(The PageAnon(page) comment blocks now look even sillier than before, but
clean that up on some other occasion";Hugh Dickins;2014-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZWJiYTZiN2UxZDk4NzI0ZDI2NmFlMDQ4ZDhhZjRmN2NhOTVjYWZk;" And note in passing that
try_to_unmap_one() does not use a migration entry when PageSwapCache, so
remove_migration_ptes() will then not update that swap entry to newpage
pte: not a big deal, but something else to clean up later.)
Davidlohr remarked in ""mm,fs: introduce helpers around the i_mmap_mutex""
conversion to i_mmap_rwsem, that ""The biggest winner of these changes is
migration"": a part of the reason might be all of that unnecessary taking
of i_mmap_mutex in page migration; and it's rather a shame that I didn't
get around to sending this patch in before his - this one is much less
useful after Davidlohr's conversion to rwsem, but still good.";Hugh Dickins;2014-12-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;mm/balloon_compaction: redesign ballooned pages management;Konstantin Khlebnikov;2014-10-09;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;Sasha Levin reported KASAN splash inside isolate_migratepages_range();Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;"Problem is in the function __is_movable_balloon_page() which tests
AS_BALLOON_MAP in page->mapping->flags";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" This function has no protection
against anonymous pages";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" As result it tried to check address space flags
inside struct anon_vma";Konstantin Khlebnikov;2014-10-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;Further investigation shows more problems in current implementation;Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;  balloon_page_movable() checks page flags and page_count;Konstantin Khlebnikov;2014-10-09;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" In
  __unmap_and_move() page is locked, reference counter is elevated, thus
  balloon_page_movable() always fails";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" As a result execution goes to the
  normal migration path";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" virtballoon_migratepage() returns
  MIGRATEPAGE_BALLOON_SUCCESS instead of MIGRATEPAGE_SUCCESS,
  move_to_new_page() thinks this is an error code and assigns
  newpage->mapping to NULL";Konstantin Khlebnikov;2014-10-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" Newly migrated page lose connectivity with
  balloon an all ability for further migration";Konstantin Khlebnikov;2014-10-09;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;  isolation ballooned page;Konstantin Khlebnikov;2014-10-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" This function releases lru_lock periodically,
  this makes migration mostly impossible for some pages";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;"  balloon_page_isolate could be executed in parallel with dequeue between
  picking page from list and locking page_lock";Konstantin Khlebnikov;2014-10-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" Race is rare because they
  use trylock_page() for locking";Konstantin Khlebnikov;2014-10-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;This patch fixes all of them;Konstantin Khlebnikov;2014-10-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;"Instead of fake mapping with special flag this patch uses special state of
page->_mapcount: PAGE_BALLOON_MAPCOUNT_VALUE = -256";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" Buddy allocator uses
PAGE_BUDDY_MAPCOUNT_VALUE = -128 for similar purpose";Konstantin Khlebnikov;2014-10-09;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" Storing mark
directly in struct page makes everything safer and easier";Konstantin Khlebnikov;2014-10-09;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;PagePrivate is used to mark pages present in page list (i.e;Konstantin Khlebnikov;2014-10-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" not
isolated, like PageLRU for normal pages)";Konstantin Khlebnikov;2014-10-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" It replaces special rules for
reference counter and makes balloon migration similar to migration of
normal pages";Konstantin Khlebnikov;2014-10-09;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNmQ4NmMwYTdmOGRkYzViMzhjZjA4OTIyMmNiMWQ5NTQwNzYyZGMy;" This flag is protected by page_lock together with link to
the balloon device.";Konstantin Khlebnikov;2014-10-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;mm: migrate: Close race between migration completion and mprotect;Mel Gorman;2014-10-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"A migration entry is marked as write if pte_write was true at the time the
entry was created";Mel Gorman;2014-10-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"The VMA protections are not double checked when migration
entries are being removed as mprotect marks write-migration-entries as
read";Mel Gorman;2014-10-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"It means that potentially we take a spurious fault to mark PTEs write
again but it's straight-forward";Mel Gorman;2014-10-02;0;0
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"However, there is a race between write
migrations being marked read and migrations finishing";Mel Gorman;2014-10-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"This potentially
allows a PTE to be write that should have been read";Mel Gorman;2014-10-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpkM2NiOGJmNjA4MWI4YjdhMmRhYmIxMjY0ZmU5NjhmZDg3MGZhNTk1;"Close this race by
double checking the VMA permissions using maybe_mkwrite when migration
completes.";Mel Gorman;2014-10-02;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;mm: memcontrol: rewrite uncharge API;Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"The memcg uncharging code that is involved towards the end of a page's
lifetime - truncation, reclaim, swapout, migration - is impressively
complicated and fragile";Johannes Weiner;2014-08-08;0;0
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"Because anonymous and file pages were always charged before they had their
page->mapping established, uncharges had to happen when the page type
could still be known from the context; as in unmap for anonymous, page
cache removal for file and shmem pages, and swap cache truncation for swap
pages";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" However, these operations happen well before the page is actually
freed, and so a lot of synchronization is necessary";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"- Charging, uncharging, page migration, and charge migration all need
  to take a per-page bit spinlock as they could race with uncharging";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"- Swap cache truncation happens during both swap-in and swap-out, and
  possibly repeatedly before the page is actually freed";Johannes Weiner;2014-08-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" This means
  that the memcg swapout code is called from many contexts that make
  no sense and it has to figure out the direction from page state to
  make sure memory and memory+swap are always correctly charged";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"- On page migration, the old page might be unmapped but then reused,
  so memcg code has to prevent untimely uncharging in that case";Johannes Weiner;2014-08-08;0;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"  Because this code - which should be a simple charge transfer - is so
  special-cased, it is not reusable for replace_page_cache()";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"But now that charged pages always have a page->mapping, introduce
mem_cgroup_uncharge(), which is called after the final put_page(), when we
know for sure that nobody is looking at the page anymore";Johannes Weiner;2014-08-08;0;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"For page migration, introduce mem_cgroup_migrate(), which is called after
the migration is successful and the new page is fully rmapped";Johannes Weiner;2014-08-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" Because
the old page is no longer uncharged after migration, prevent double
charges by decoupling the page's memcg association (PCG_USED and
pc->mem_cgroup) from the page holding an actual charge";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" The new bits
PCG_MEM and PCG_MEMSW represent the respective charges and are transferred
to the new page during migration";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"mem_cgroup_migrate() is suitable for replace_page_cache() as well,
which gets rid of mem_cgroup_replace_page_cache()";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" However, care
needs to be taken because both the source and the target page can
already be charged and on the LRU when fuse is splicing: grab the page
lock on the charge moving side to prevent changing pc->mem_cgroup of a
page under migration";Johannes Weiner;2014-08-08;0;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" Also, the lruvecs of both pages change as we
uncharge the old and charge the new during migration, and putback may
race with us, so grab the lru lock and isolate the pages iff on LRU to
prevent races and ensure the pages are on the right lruvec afterward";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"Swap accounting is massively simplified: because the page is no longer
uncharged as early as swap cache deletion, a new mem_cgroup_swapout() can
transfer the page's memory+swap charge (PCG_MEMSW) to the swap entry
before the final put_page() in page reclaim";Johannes Weiner;2014-08-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"Finally, page_cgroup changes are now protected by whatever protection the
page itself offers: anonymous pages are charged under the page table lock,
whereas page cache insertions, swapin, and migration hold the page lock";Johannes Weiner;2014-08-08;0;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;Uncharging happens under full exclusion with no outstanding references;Johannes Weiner;2014-08-08;0;0
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;"Charging and uncharging also ensure that the page is off-LRU, which
serializes against charge migration";Johannes Weiner;2014-08-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowYTMxYmM5N2M4MGMzZmE4N2IzMmMwOTFkOWE5MzBhYzE5Y2QwYzQw;" Remove the very costly page_cgroup
lock and set pc->flags non-atomically.";Johannes Weiner;2014-08-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;mm: fix direct reclaim writeback regression;Hugh Dickins;2014-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;Shortly before 3.16-rc1, Dave Jones reported;Hugh Dickins;2014-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;"  WARNING: CPU: 3 PID: 19721 at fs/xfs/xfs_aops.c:971
           xfs_vm_writepage+0x5ce/0x630 [xfs]()
  CPU: 3 PID: 19721 Comm: trinity-c61 Not tainted 3.15.0+ #3
   970   if (WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) ==
 971                   PF_MEMALLOC))
I did not respond at the time, because a glance at the PageDirty block
in shrink_page_list() quickly shows that this is impossible: we don't do
writeback on file pages (other than tmpfs) from direct reclaim nowadays";Hugh Dickins;2014-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;Dave was hallucinating, but it would have been disrespectful to say so;Hugh Dickins;2014-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;"However, my own /var/log/messages now shows similar complaints
  WARNING: CPU: 1 PID: 28814 at fs/ext4/inode.c:1881 ext4_writepage+0xa7/0x38b()
  WARNING: CPU: 0 PID: 27347 at fs/ext4/inode.c:1764 ext4_writepage+0xa7/0x38b()
from stressing some mmotm trees during July";Hugh Dickins;2014-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;"Could a dirty xfs or ext4 file page somehow get marked PageSwapBacked,
so fail shrink_page_list()'s page_is_file_cache() test, and so proceed
to mapping->a_ops->writepage()?
Yes, 3.16-rc1's commit 68711a746345 (""mm, migration: add destination
page freeing callback"") has provided such a way to compaction: if
migrating a SwapBacked page fails, its newpage may be put back on the
list for later use with PageSwapBacked still set, and nothing will clear
Whether that can do anything worse than issue WARN_ON_ONCEs, and get
some statistics wrong, is unclear: easier to fix than to think through
the consequences";Hugh Dickins;2014-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;"Fixing it here, before the put_new_page(), addresses the bug directly,
but is probably the worst place to fix it";Hugh Dickins;2014-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YmRkNjM4MDkxNjA1ZGM2NmQ5MmM1N2M0YjgwZWI4N2ZmZmMxNWY3;" Page migration is doing too
many parts of the job on too many levels: fixing it in
move_to_new_page() to complement its SetPageSwapBacked would be
preferable, except why is it (and newpage->mapping and newpage->index)
done there, rather than down in migrate_page_move_mapping(), once we are
sure of success? Not a cleanup to get into right now, especially not
with memcg cleanups coming in 3.17.";Hugh Dickins;2014-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;mm: let mm_find_pmd fix buggy race with THP fault;Hugh Dickins;2014-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;Trinity has reported;Hugh Dickins;2014-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;"    BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
    IP: __lock_acquire (kernel/locking/lockdep.c:3070 (discriminator 1))
    CPU: 6 PID: 16173 Comm: trinity-c364 Tainted: G        W
                            3.15.0-rc1-next-20140415-sasha-00020-gaa90d09 #398
    lock_acquire (arch/x86/include/asm/current.h:14
                  kernel/locking/lockdep.c:3602)
    _raw_spin_lock (include/linux/spinlock_api_smp.h:143
                    kernel/locking/spinlock.c:151)
    remove_migration_pte (mm/migrate.c:137)
    rmap_walk (mm/rmap.c:1628 mm/rmap.c:1699)
    remove_migration_ptes (mm/migrate.c:224)
    migrate_pages (mm/migrate.c:922 mm/migrate.c:960 mm/migrate.c:1126)
    migrate_misplaced_page (mm/migrate.c:1733)
    __handle_mm_fault (mm/memory.c:3762 mm/memory.c:3812 mm/memory.c:3925)
    handle_mm_fault (mm/memory.c:3948)
    __get_user_pages (mm/memory.c:1851)
    __mlock_vma_pages_range (mm/mlock.c:255)
    __mm_populate (mm/mlock.c:711)
    SyS_mlockall (include/linux/mm.h:1799 mm/mlock.c:817 mm/mlock.c:791)
I believe this comes about because, whereas collapsing and splitting THP
functions take anon_vma lock in write mode (which excludes concurrent
rmap walks), faulting THP functions (write protection and misplaced
NUMA) do not - and mostly they do not need to";Hugh Dickins;2014-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;"But they do use a pmdp_clear_flush(), set_pmd_at() sequence which, for
an instant (indeed, for a long instant, given the inter-CPU TLB flush in
there), leaves *pmd neither present not trans_huge";Hugh Dickins;2014-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;"Which can confuse a concurrent rmap walk, as when removing migration
ptes, seen in the dumped trace";Hugh Dickins;2014-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;" Although that rmap walk has a 4k page
to insert, anon_vmas containing THPs are in no way segregated from
4k-page anon_vmas, so the 4k-intent mm_find_pmd() does need to cope with
that instant when a trans_huge pmd is temporarily absent";Hugh Dickins;2014-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;"I don't think we need strengthen the locking at the THP end: it's easily
handled with an ACCESS_ONCE() before testing both conditions";Hugh Dickins;2014-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpmNzJlN2RjZGQyNTIyOTQ0NmIxMDJlNTg3ZWYyZjgyNmY3NmJmZjI4;"And since mm_find_pmd() had only one caller who wanted a THP rather than
a pmd, let's slightly repurpose it to fail when it hits a THP or
non-present pmd, and open code split_huge_page_address() again.";Hugh Dickins;2014-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMDA4NzNkN2E3NzdiNjdhZDM1MTk3YzVhOTk4YjVlNzc4ZjhiZjNm;hugetlb: rename hugepage_migration_support() to ..._supported();Naoya Horiguchi;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMDA4NzNkN2E3NzdiNjdhZDM1MTk3YzVhOTk4YjVlNzc4ZjhiZjNm;"We already have a function named hugepages_supported(), and the similar
name hugepage_migration_support() is a bit unconfortable, so let's rename
it hugepage_migration_supported().";Naoya Horiguchi;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;mm, migration: add destination page freeing callback;David Rientjes;2014-06-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;"Memory migration uses a callback defined by the caller to determine how to
allocate destination pages";David Rientjes;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;" When migration fails for a source page,
however, it frees the destination page back to the system";David Rientjes;2014-06-04;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;"This patch adds a memory migration callback defined by the caller to
determine how to free destination pages";David Rientjes;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;" If a caller, such as memory
compaction, builds its own freelist for migration targets, this can reuse
already freed memory instead of scanning additional memory";David Rientjes;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;"If the caller provides a function to handle freeing of destination pages,
it is called when page migration fails";David Rientjes;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;" If the caller passes NULL then
freeing back to the system will be handled as usual";David Rientjes;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ODcxMWE3NDYzNDVjNDRhZTAwYzY0ZDhkYmFjNmE5Y2UxM2FjNTRh;" This patch
introduces no functional change.";David Rientjes;2014-06-04;1;0
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;mm: numa: add migrated transhuge pages to LRU the same way as base pages;Mel Gorman;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;"Migration of misplaced transhuge pages uses page_add_new_anon_rmap() when
putting the page back as it avoided an atomic operations and added the new
page to the correct LRU";Mel Gorman;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;" A side-effect is that the page gets marked
activated as part of the migration meaning that transhuge and base pages
are treated differently from an aging perspective than base page
migration";Mel Gorman;2014-06-04;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;"This patch uses page_add_anon_rmap() and putback_lru_page() on completion
of a transhuge migration similar to base page migration";Mel Gorman;2014-06-04;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;" It would require
fewer atomic operations to use lru_cache_add without taking an additional
reference to the page";Mel Gorman;2014-06-04;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;" The downside would be that it's still different to
base page migration and unevictable pages may be added to the wrong LRU
for cleaning up later";Mel Gorman;2014-06-04;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWRlOTkyN2Y5ZGQzY2IwYTBmMTgwNjRmYTRiNjk3NmZjMzdlNzlj;" Testing of the usual workloads did not show any
adverse impact to the change.";Mel Gorman;2014-06-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;mm: fix swapops.h:131 bug if remap_file_pages raced migration;Hugh Dickins;2014-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;"Add remove_linear_migration_ptes_from_nonlinear(), to fix an interesting
little include/linux/swapops.h:131 BUG_ON(!PageLocked) found by trinity";Hugh Dickins;2014-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;"indicating that remove_migration_ptes() failed to find one of the
migration entries that was temporarily inserted";Hugh Dickins;2014-03-21;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;"The problem comes from remap_file_pages()'s switch from vma_interval_tree
(good for inserting the migration entry) to i_mmap_nonlinear list (no good
range does not cover the whole of the vma (zap_pte() clears the range)";Hugh Dickins;2014-03-21;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;"remove_migration_ptes() needs a file_nonlinear method to go down the
i_mmap_nonlinear list, applying linear location to look for migration
entries in those vmas too, just in case there was this race";Hugh Dickins;2014-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;but it never needed vma passed in - vma comes from its own iteration;Hugh Dickins;2014-03-21;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZTA5ZTczOGFmZDIxZWY5OWYwNDc0MjVmYzBiMGM5YmU4YjAzMjU0;"Reported-and-tested-by: Dave Jones <davej@redhat.com>
Reported-and-tested-by: Sasha Levin <sasha.levin@oracle.com>";Hugh Dickins;2014-03-21;1;0
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;mm: fix GFP_THISNODE callers and clarify;Johannes Weiner;2014-03-10;1;1
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;"GFP_THISNODE is for callers that implement their own clever fallback to
remote nodes";Johannes Weiner;2014-03-10;0;0
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;" It restricts the allocation to the specified node and
does not invoke reclaim, assuming that the caller will take care of it
when the fallback fails, e.g";Johannes Weiner;2014-03-10;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;" through a subsequent allocation request
without GFP_THISNODE set";Johannes Weiner;2014-03-10;0;0
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;"However, many current GFP_THISNODE users only want the node exclusive
aspect of the flag, without actually implementing their own fallback or
triggering reclaim if necessary";Johannes Weiner;2014-03-10;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;" This results in things like page
migration failing prematurely even when there is easily reclaimable
memory available, unless kswapd happens to be running already or a
concurrent allocation attempt triggers the necessary reclaim";Johannes Weiner;2014-03-10;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;"Convert all callsites that don't implement their own fallback strategy
to __GFP_THISNODE";Johannes Weiner;2014-03-10;1;0
MDY6Q29tbWl0MjMyNTI5ODplOTdjYThlNWI4NjRmODhiMDI4YzE3NTliYTg1MzZmYTgyN2Q2ZDk2;" This restricts the allocation a single node too, but
at the same time allows the allocator to enter the slowpath, wake
kswapd, and invoke direct reclaim if necessary, to make the allocation
happen when memory is full.";Johannes Weiner;2014-03-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphMzk3OGE1MTk0NjFiMDk1Yjc3NmY0NGE4NjA3OWY1NDQ4Yzk2OTYz;mm/migrate.c: fix setting of cpupid on page migration twice against normal page;Wanpeng Li;2014-01-28;1;1
MDY6Q29tbWl0MjMyNTI5ODphMzk3OGE1MTk0NjFiMDk1Yjc3NmY0NGE4NjA3OWY1NDQ4Yzk2OTYz;"Commit 7851a45cd3f6 (""mm: numa: Copy cpupid on page migration"") copies
over the cpupid at page migration time";Wanpeng Li;2014-01-28;0;0
MDY6Q29tbWl0MjMyNTI5ODphMzk3OGE1MTk0NjFiMDk1Yjc3NmY0NGE4NjA3OWY1NDQ4Yzk2OTYz;" It is unnecessary to set it
again in alloc_misplaced_dst_page().";Wanpeng Li;2014-01-28;0;1
MDY6Q29tbWl0MjMyNTI5ODpiYWFlOTExYjI3YjhkYmVlNjgzMGY0ZTNlZjBmY2Y0ZGM4ZTljMDdi;sched/numa: fix setting of cpupid on page migration twice;Wanpeng Li;2014-01-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpiYWFlOTExYjI3YjhkYmVlNjgzMGY0ZTNlZjBmY2Y0ZGM4ZTljMDdi;"Commit 7851a45cd3f6 (""mm: numa: Copy cpupid on page migration"") copiess
over the cpupid at page migration time";Wanpeng Li;2014-01-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpiYWFlOTExYjI3YjhkYmVlNjgzMGY0ZTNlZjBmY2Y0ZGM4ZTljMDdi;" It is unnecessary to set it
again in migrate_misplaced_transhuge_page().";Wanpeng Li;2014-01-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozMDkzODFmZWFlZTU2NDI4MWMzZDllOTBmYmNhODk2M2JiNzQyOGFk;mm: dump page when hitting a VM_BUG_ON using VM_BUG_ON_PAGE;Sasha Levin;2014-01-23;0;0
MDY6Q29tbWl0MjMyNTI5ODozMDkzODFmZWFlZTU2NDI4MWMzZDllOTBmYmNhODk2M2JiNzQyOGFk;Most of the VM_BUG_ON assertions are performed on a page;Sasha Levin;2014-01-23;0;0
MDY6Q29tbWl0MjMyNTI5ODozMDkzODFmZWFlZTU2NDI4MWMzZDllOTBmYmNhODk2M2JiNzQyOGFk;" Usually, when
one of these assertions fails we'll get a BUG_ON with a call stack and
the registers";Sasha Levin;2014-01-23;0;0
MDY6Q29tbWl0MjMyNTI5ODozMDkzODFmZWFlZTU2NDI4MWMzZDllOTBmYmNhODk2M2JiNzQyOGFk;"I've recently noticed based on the requests to add a small piece of code
that dumps the page to various VM_BUG_ON sites that the page dump is
quite useful to people debugging issues in mm";Sasha Levin;2014-01-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozMDkzODFmZWFlZTU2NDI4MWMzZDllOTBmYmNhODk2M2JiNzQyOGFk;"This patch adds a VM_BUG_ON_PAGE(cond, page) which beyond doing what
VM_BUG_ON() does, also dumps the page before executing the actual
BUG_ON.";Sasha Levin;2014-01-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OGQ1NTA2ZTgyYjIxYTFhMWRlNjhjMjQxODJkYjJjMmZlNTIxNDIy;mm/migrate: remove unused function, fail_migrate_page();Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3OGQ1NTA2ZTgyYjIxYTFhMWRlNjhjMjQxODJkYjJjMmZlNTIxNDIy;fail_migrate_page() isn't used anywhere, so remove it.;Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OWM4MmI3MGRjZDljYzI3M2MyMWZhZTVhYmMyOWU0MWZjNzMyYTE3;mm/migrate: remove putback_lru_pages, fix comment on putback_movable_pages;Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OWM4MmI3MGRjZDljYzI3M2MyMWZhZTVhYmMyOWU0MWZjNzMyYTE3;"Some part of putback_lru_pages() and putback_movable_pages() is
duplicated, so it could confuse us what we should use";Joonsoo Kim;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODo1OWM4MmI3MGRjZDljYzI3M2MyMWZhZTVhYmMyOWU0MWZjNzMyYTE3;" We can remove
putback_lru_pages() since it is not really needed now";Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OWM4MmI3MGRjZDljYzI3M2MyMWZhZTVhYmMyOWU0MWZjNzMyYTE3;" This makes us
undestand and maintain the code more easily";Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo1OWM4MmI3MGRjZDljYzI3M2MyMWZhZTVhYmMyOWU0MWZjNzMyYTE3;And comment on putback_movable_pages() is stale now, so fix it.;Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY2NWYyYmJmZWQyZTMyNWQzNzIzNmQ5YjAwNzFhMTFhNjkxMjRl;mm/migrate: correct failure handling if !hugepage_migration_support();Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY2NWYyYmJmZWQyZTMyNWQzNzIzNmQ5YjAwNzFhMTFhNjkxMjRl;"We should remove the page from the list if we fail with ENOSYS, since
migrate_pages() consider error cases except -ENOMEM and -EAGAIN as
permanent failure and it assumes that the page would be removed from the
list";Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY2NWYyYmJmZWQyZTMyNWQzNzIzNmQ5YjAwNzFhMTFhNjkxMjRl; Without this patch, we could overcount number of failure;Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY2NWYyYmJmZWQyZTMyNWQzNzIzNmQ5YjAwNzFhMTFhNjkxMjRl;"In addition, we should put back the new hugepage if
!hugepage_migration_support()";Joonsoo Kim;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY2NWYyYmJmZWQyZTMyNWQzNzIzNmQ5YjAwNzFhMTFhNjkxMjRl; If not, we would leak hugepage memory.;Joonsoo Kim;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozNTRhMzM2MzM2MzcyNGMyMWVhMmU0YjI4MzcwZTI3OTgzYzI0NTJl;mm/migrate: add comment about permanent failure path;Naoya Horiguchi;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozNTRhMzM2MzM2MzcyNGMyMWVhMmU0YjI4MzcwZTI3OTgzYzI0NTJl;"Let's add a comment about where the failed page goes to, which makes
code more readable.";Naoya Horiguchi;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODphZjE4MzlkNzIyYzk4NmZmZWFhZTFlNzBhNmVmMWM3NWZmMzhkY2Q1;mm: numa: trace tasks that fail migration due to rate limiting;Mel Gorman;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjE4MzlkNzIyYzk4NmZmZWFhZTFlNzBhNmVmMWM3NWZmMzhkY2Q1;"A low local/remote numa hinting fault ratio is potentially explained by
failed migrations";Mel Gorman;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODphZjE4MzlkNzIyYzk4NmZmZWFhZTFlNzBhNmVmMWM3NWZmMzhkY2Q1;" This patch adds a tracepoint that fires when
migration fails due to migration rate limitation.";Mel Gorman;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYzVlOWMyN2NiZDk2NmM3ZjAwMzg2OThkNWRjZDVhZGEzNTc0ZjQ3;mm: numa: limit scope of lock for NUMA migrate rate limiting;Mel Gorman;2014-01-21;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYzVlOWMyN2NiZDk2NmM3ZjAwMzg2OThkNWRjZDVhZGEzNTc0ZjQ3;"NUMA migrate rate limiting protects a migration counter and window using
a lock but in some cases this can be a contended lock";Mel Gorman;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYzVlOWMyN2NiZDk2NmM3ZjAwMzg2OThkNWRjZDVhZGEzNTc0ZjQ3;" It is not
critical that the number of pages be perfect, lost updates are
acceptable";Mel Gorman;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYzVlOWMyN2NiZDk2NmM3ZjAwMzg2OThkNWRjZDVhZGEzNTc0ZjQ3; Reduce the importance of this lock.;Mel Gorman;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYzMwZTAxNzdlNGY0MWExMWNiODhiMGYxZjA1NmNjZWJmZTBmZmY0;mm: numa: make NUMA-migrate related functions static;Mel Gorman;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYzMwZTAxNzdlNGY0MWExMWNiODhiMGYxZjA1NmNjZWJmZTBmZmY0;"numamigrate_update_ratelimit and numamigrate_isolate_page only have
callers in mm/migrate.c";Mel Gorman;2014-01-21;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYzMwZTAxNzdlNGY0MWExMWNiODhiMGYxZjA1NmNjZWJmZTBmZmY0; This patch makes them static.;Mel Gorman;2014-01-21;1;0
MDY6Q29tbWl0MjMyNTI5ODowNTFhYzgzYWRmNjllZWE0ZjU3YTk3MzU2ZTQyODJlMzk1YTVmYTZk;mm/rmap: make rmap_walk to get the rmap_walk_control argument;Joonsoo Kim;2014-01-21;1;0
MDY6Q29tbWl0MjMyNTI5ODowNTFhYzgzYWRmNjllZWE0ZjU3YTk3MzU2ZTQyODJlMzk1YTVmYTZk;"In each rmap traverse case, there is some difference so that we need
function pointers and arguments to them in order to handle these
For this purpose, struct rmap_walk_control is introduced in this patch,
and will be extended in following patch";Joonsoo Kim;2014-01-21;0;1
MDY6Q29tbWl0MjMyNTI5ODowNTFhYzgzYWRmNjllZWE0ZjU3YTk3MzU2ZTQyODJlMzk1YTVmYTZk;" Introducing and extending are
separate, because it clarify changes.";Joonsoo Kim;2014-01-21;1;0
MDY6Q29tbWl0MjMyNTI5ODozNGVlNjQ1ZTgzYjYwYWUzZDU5NTVmNzBhYjlhYjlhMTU5MTM2Njcz;mmu_notifier: call mmu_notifier_invalidate_range() from VMM;Joerg Roedel;2014-11-13;0;0
MDY6Q29tbWl0MjMyNTI5ODozNGVlNjQ1ZTgzYjYwYWUzZDU5NTVmNzBhYjlhYjlhMTU5MTM2Njcz;"Add calls to the new mmu_notifier_invalidate_range() function to all
places in the VMM that need it.";Joerg Roedel;2014-11-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZTMyMWZlZmIwZTYwYmFlNGUyYTI4ZDIwZmM0ZmEzMDc1OGQyN2M2;aio/migratepages: make aio migrate pages sane;Benjamin LaHaise;2013-12-21;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ZTMyMWZlZmIwZTYwYmFlNGUyYTI4ZDIwZmM0ZmEzMDc1OGQyN2M2;"The arbitrary restriction on page counts offered by the core
migrate_page_move_mapping() code results in rather suspicious looking
fiddling with page reference counts in the aio_migratepage() operation";Benjamin LaHaise;2013-12-21;0;1
MDY6Q29tbWl0MjMyNTI5ODo4ZTMyMWZlZmIwZTYwYmFlNGUyYTI4ZDIwZmM0ZmEzMDc1OGQyN2M2;"To fix this, make migrate_page_move_mapping() take an extra_count parameter
that allows aio to tell the code about its own reference count on the page
being migrated";Benjamin LaHaise;2013-12-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ZTMyMWZlZmIwZTYwYmFlNGUyYTI4ZDIwZmM0ZmEzMDc1OGQyN2M2;"While cleaning up aio_migratepage(), make it validate that the old page
being passed in is actually what aio_migratepage() expects to prevent
misbehaviour in the case of races.";Benjamin LaHaise;2013-12-21;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMDk0M2Q2MWI4ZmE0MjAxODBmOTJmNjRlZjY3NjYyYjRmNmNjNDkz;mm: numa: defer TLB flush for THP migration as long as possible;Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMDk0M2Q2MWI4ZmE0MjAxODBmOTJmNjRlZjY3NjYyYjRmNmNjNDkz;THP migration can fail for a variety of reasons;Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMDk0M2Q2MWI4ZmE0MjAxODBmOTJmNjRlZjY3NjYyYjRmNmNjNDkz;" Avoid flushing the TLB
to deal with THP migration races until the copy is ready to start.";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpkZTQ2NmJkNjI4ZThkNjYzZmRmM2Y3OTFiYzhkYjMxOGVlODVjNzE0;mm: numa: avoid unnecessary disruption of NUMA hinting during migration;Mel Gorman;2013-12-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpkZTQ2NmJkNjI4ZThkNjYzZmRmM2Y3OTFiYzhkYjMxOGVlODVjNzE0;"do_huge_pmd_numa_page() handles the case where there is parallel THP
migration";Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZTQ2NmJkNjI4ZThkNjYzZmRmM2Y3OTFiYzhkYjMxOGVlODVjNzE0;" However, by the time it is checked the NUMA hinting
information has already been disrupted";Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpkZTQ2NmJkNjI4ZThkNjYzZmRmM2Y3OTFiYzhkYjMxOGVlODVjNzE0;" This patch adds an earlier
check with some helpers.";Mel Gorman;2013-12-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplYjQ0ODlmNjlmMjI0MzU2MTkzMzY0ZGMyNzYyYWEwMDk3MzhjYTdm;mm: numa: avoid unnecessary work on the failure path;Mel Gorman;2013-12-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplYjQ0ODlmNjlmMjI0MzU2MTkzMzY0ZGMyNzYyYWEwMDk3MzhjYTdm;"If a PMD changes during a THP migration then migration aborts but the
failure path is doing more work than is necessary.";Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNzE0ZjRmMjBlNTllYTZlZWEyNjRhODZiOWE1MWZkNTFiODhmYzU0;mm: numa: call MMU notifiers on THP migration;Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpmNzE0ZjRmMjBlNTllYTZlZWEyNjRhODZiOWE1MWZkNTFiODhmYzU0;"MMU notifiers must be called on THP page migration or secondary MMUs
will get very confused.";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;mm: numa: serialise parallel get_user_page against THP migration;Mel Gorman;2013-12-19;1;0
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;"Base pages are unmapped and flushed from cache and TLB during normal
page migration and replaced with a migration entry that causes any
parallel NUMA hinting fault or gup to block until migration completes";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;"THP does not unmap pages due to a lack of support for migration entries
at a PMD level";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;" This allows races with get_user_pages and
get_user_pages_fast which commit 3f926ab945b6 (""mm: Close races between
THP migration and PMD numa clearing"") made worse by introducing a
pmd_clear_flush()";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;"This patch forces get_user_page (fast and normal) on a pmd_numa page to
go through the slow get_user_page path where it will serialise against
THP migration and properly account for the NUMA hinting fault";Mel Gorman;2013-12-19;0;1
MDY6Q29tbWl0MjMyNTI5ODoyYjQ4NDdlNzMwMDRjMTBhZTY2NjZjMmUyN2I1YzU0MzBhZWQ4Njk4;" On the
migration side the page table lock is taken for each PTE update.";Mel Gorman;2013-12-19;0;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;mm: thp: give transparent hugepage code a separate copy_page;Dave Hansen;2013-11-21;1;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;"Right now, the migration code in migrate_page_copy() uses copy_huge_page()
for hugetlbfs and thp pages";Dave Hansen;2013-11-21;0;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;So, yay for code reuse;Dave Hansen;2013-11-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4; But;Dave Hansen;2013-11-21;0;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;"  void copy_huge_page(struct page *dst, struct page *src)
and a non-hugetlbfs page has no page_hstate()";Dave Hansen;2013-11-21;1;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;" This works 99% of the
time because page_hstate() determines the hstate from the page order
alone";Dave Hansen;2013-11-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;" Since the page order of a THP page matches the default hugetlbfs
page order, it works";Dave Hansen;2013-11-21;0;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;"But, if you change the default huge page size on the boot command-line
(say default_hugepagesz=1G), then we might not even *have* a 2MB hstate
so page_hstate() returns null and copy_huge_page() oopses pretty fast
since copy_huge_page() dereferences the hstate";Dave Hansen;2013-11-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;"  void copy_huge_page(struct page *dst, struct page *src)
Mel noticed that the migration code is really the only user of these
functions";Dave Hansen;2013-11-21;0;0
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;" This moves all the copy code over to migrate.c and makes
copy_huge_page() work for THP by checking for it explicitly";Dave Hansen;2013-11-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGIwYTEwNWQ5ZjcxNDFlNGNiZjcyYWU1NTExODMyNDU3ZDg5Nzg4;"I believe the bug was introduced in commit b32967ff101a (""mm: numa: Add
THP migration for the NUMA working set scanning fault case"")";Dave Hansen;2013-11-21;0;0
MDY6Q29tbWl0MjMyNTI5ODpjNDA4OGViZGNhNjRjOWEyZTM0YTM4MTc3ZDIyNDk4MDVlZGUxZjRi;mm: convert the rest to new page table lock api;Kirill A. Shutemov;2013-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNDA4OGViZGNhNjRjOWEyZTM0YTM4MTc3ZDIyNDk4MDVlZGUxZjRi;Only trivial cases left;Kirill A. Shutemov;2013-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNDA4OGViZGNhNjRjOWEyZTM0YTM4MTc3ZDIyNDk4MDVlZGUxZjRi;Let's convert them altogether.;Kirill A. Shutemov;2013-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjkwMGY0MTIxNTQ0NzQzM2NiYzQ1NmQxYzQyOTRlODU4YTg0ZDdj;mm, hugetlb: convert hugetlbfs to use split pmd lock;Kirill A. Shutemov;2013-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODpjYjkwMGY0MTIxNTQ0NzQzM2NiYzQ1NmQxYzQyOTRlODU4YTg0ZDdj;Hugetlb supports multiple page sizes;Kirill A. Shutemov;2013-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYjkwMGY0MTIxNTQ0NzQzM2NiYzQ1NmQxYzQyOTRlODU4YTg0ZDdj;"We use split lock only for PMD
level, but not for PUD.";Kirill A. Shutemov;2013-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjkyNmFiOTQ1YjYwYTU4MjQzNjlkMjFhZGQ3NzEwNjIyYTJlYWMw;mm: Close races between THP migration and PMD numa clearing;Mel Gorman;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjkyNmFiOTQ1YjYwYTU4MjQzNjlkMjFhZGQ3NzEwNjIyYTJlYWMw;"THP migration uses the page lock to guard against parallel allocations
but there are cases like this still open
  Task A					Task B
  do_huge_pmd_numa_page				do_huge_pmd_numa_page
  lock_page
  mpol_misplaced == -1
  unlock_page
  goto clear_pmdnuma
						lock_page
						mpol_misplaced == 2
						migrate_misplaced_transhuge
  pmd = pmd_mknonnuma
  set_pmd_at
During hours of testing, one crashed with weird errors and while I have
no direct evidence, I suspect something like the race above happened";Mel Gorman;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjkyNmFiOTQ1YjYwYTU4MjQzNjlkMjFhZGQ3NzEwNjIyYTJlYWMw;"This patch extends the page lock to being held until the pmd_numa is
cleared to prevent migration starting in parallel while the pmd_numa is
being cleared";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjkyNmFiOTQ1YjYwYTU4MjQzNjlkMjFhZGQ3NzEwNjIyYTJlYWMw;"It also flushes the old pmd entry and orders pagetable
insertion before rmap insertion.";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpjM2QxNmUxNjUyMmZlM2ZlODc1OTczNTg1MGEwNjc2ZGExOGY0YjFk;mm: migration: do not lose soft dirty bit if page is in migration state;Cyrill Gorcunov;2013-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpjM2QxNmUxNjUyMmZlM2ZlODc1OTczNTg1MGEwNjc2ZGExOGY0YjFk;"If page migration is turned on in config and the page is migrating, we
may lose the soft dirty bit";Cyrill Gorcunov;2013-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpjM2QxNmUxNjUyMmZlM2ZlODc1OTczNTg1MGEwNjc2ZGExOGY0YjFk;" If fork and mprotect are called on
migrating pages (once migration is complete) pages do not obtain the
soft dirty bit in the correspond pte entries";Cyrill Gorcunov;2013-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpjM2QxNmUxNjUyMmZlM2ZlODc1OTczNTg1MGEwNjc2ZGExOGY0YjFk;" Fix it adding an
appropriate test on swap entries.";Cyrill Gorcunov;2013-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ODUxYTQ1Y2QzZjYxOThiZjU0MmMzMGUyN2IzMzBlOGVlYjM3MzZj;mm: numa: Copy cpupid on page migration;Rik van Riel;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3ODUxYTQ1Y2QzZjYxOThiZjU0MmMzMGUyN2IzMzBlOGVlYjM3MzZj;After page migration, the new page has the nidpid unset;Rik van Riel;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ODUxYTQ1Y2QzZjYxOThiZjU0MmMzMGUyN2IzMzBlOGVlYjM3MzZj;"This makes
every fault on a recently migrated page look like a first numa fault,
leading to another page migration";Rik van Riel;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ODUxYTQ1Y2QzZjYxOThiZjU0MmMzMGUyN2IzMzBlOGVlYjM3MzZj;"Copying over the nidpid at page migration time should prevent erroneous
migrations of recently migrated pages.";Rik van Riel;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MDU3Mjg5MGQyMDI1MjdjMzY2YWE5NDg5YjMyNDA0ZTg4YTdjMDIw;mm: numa: Change page last {nid,pid} into {cpu,pid};Peter Zijlstra;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MDU3Mjg5MGQyMDI1MjdjMzY2YWE5NDg5YjMyNDA0ZTg4YTdjMDIw;"Change the per page last fault tracking to use cpu,pid instead of
nid,pid";Peter Zijlstra;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MDU3Mjg5MGQyMDI1MjdjMzY2YWE5NDg5YjMyNDA0ZTg4YTdjMDIw;"This will allow us to try and lookup the alternate task more
easily";Peter Zijlstra;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MDU3Mjg5MGQyMDI1MjdjMzY2YWE5NDg5YjMyNDA0ZTg4YTdjMDIw;"Note that even though it is the cpu that is store in the page
flags that the mpol_misplaced decision is still based on the node.";Peter Zijlstra;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;sched/numa: Set preferred NUMA node based on number of private faults;Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Ideally it would be possible to distinguish between NUMA hinting faults that
are private to a task and those that are shared";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"If treated identically
there is a risk that shared pages bounce between nodes depending on
the order they are referenced by tasks";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Ultimately what is desirable is
that task private pages remain local to the task while shared pages are
interleaved between sharing tasks running on different nodes to give good
average performance";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"This is further complicated by THP as even
applications that partition their data may not be partitioning on a huge
page boundary";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"To start with, this patch assumes that multi-threaded or multi-process
applications partition their data and that in general the private accesses
are more important for cpu->memory locality in the general case";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Also,
no new infrastructure is required to treat private pages properly but
interleaving for shared pages requires additional infrastructure";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"To detect private accesses the pid of the last accessing task is required
but the storage requirements are a high";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"This patch borrows heavily from
Ingo Molnar's patch ""numa, mm, sched: Implement last-CPU+PID hash tracking""
to encode some bits from the last accessing task in the page flags as
well as the node information";Mel Gorman;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Collisions will occur but it is better than
just depending on the node information";Mel Gorman;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Node information is then used to
determine if a page needs to migrate";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The PID information is used to detect
private/shared accesses";Mel Gorman;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The preferred NUMA node is selected based on where
the maximum number of approximately private faults were measured";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Shared
faults are not taken into consideration for a few reasons";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"First, if there are many tasks sharing the page then they'll all move
towards the same node";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The node will be compute overloaded and then
scheduled away later only to bounce back again";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Alternatively the shared
tasks would just bounce around nodes because the fault information is
effectively noise";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"Either way accounting for shared faults the same as
private faults can result in lower performance overall";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The second reason is based on a hypothetical workload that has a small
number of very important, heavily accessed private pages but a large shared
array";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The shared array would dominate the number of faults and be selected
as a preferred node even though it's the wrong decision";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODpiNzk1ODU0YjFmYTcwZjZhZWU5MjNhZTVkZjc0ZmY3YWZlYWRkY2Fh;"The third reason is that multiple threads in a process will race each
other to fault the shared page making the fault information unreliable.";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;mm: numa: Scan pages with elevated page_mapcount;Mel Gorman;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"Currently automatic NUMA balancing is unable to distinguish between false
shared versus private pages except by ignoring pages with an elevated
page_mapcount entirely";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"This avoids shared pages bouncing between the
nodes whose task is using them but that is ignored quite a lot of data";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"This patch kicks away the training wheels in preparation for adding support
for identifying shared/private pages is now in place";Mel Gorman;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"The ordering is so
that the impact of the shared/private detection can be easily measured";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"Note
that the patch does not migrate shared, file-backed within vmas marked
VM_EXEC as these are generally shared library pages";Mel Gorman;2013-10-07;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYmMxMTVkODdkZmZkMWM0M2JkYzNjOWM5ZDFlM2E1MWMxOTVkMThl;"Migrating such pages
is not beneficial as there is an expectation they are read-shared between
caches and iTLB and iCache pressure is generally low.";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODphNTRhNDA3ZmJmNzczNWZkOGY3ODQxMzc1NTc0ZjVkOWIwMzc1Zjkz;mm: Close races between THP migration and PMD numa clearing;Mel Gorman;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphNTRhNDA3ZmJmNzczNWZkOGY3ODQxMzc1NTc0ZjVkOWIwMzc1Zjkz;"THP migration uses the page lock to guard against parallel allocations
but there are cases like this still open
  Task A					Task B
  do_huge_pmd_numa_page				do_huge_pmd_numa_page
  lock_page
  mpol_misplaced == -1
  unlock_page
  goto clear_pmdnuma
						lock_page
						mpol_misplaced == 2
						migrate_misplaced_transhuge
  pmd = pmd_mknonnuma
  set_pmd_at
During hours of testing, one crashed with weird errors and while I have
no direct evidence, I suspect something like the race above happened";Mel Gorman;2013-10-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphNTRhNDA3ZmJmNzczNWZkOGY3ODQxMzc1NTc0ZjVkOWIwMzc1Zjkz;"This patch extends the page lock to being held until the pmd_numa is
cleared to prevent migration starting in parallel while the pmd_numa is
being cleared";Mel Gorman;2013-10-07;0;1
MDY6Q29tbWl0MjMyNTI5ODphNTRhNDA3ZmJmNzczNWZkOGY3ODQxMzc1NTc0ZjVkOWIwMzc1Zjkz;"It also flushes the old pmd entry and orders pagetable
insertion before rmap insertion.";Mel Gorman;2013-10-07;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMTdhYWQxZTllNGQ5NzQ0OGQxZGYzZjg0YjA4YmQ2NTgxMWU2ZDZh;mm: avoid reinserting isolated balloon pages into LRU lists;Rafael Aquini;2013-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMTdhYWQxZTllNGQ5NzQ0OGQxZGYzZjg0YjA4YmQ2NTgxMWU2ZDZh;"Isolated balloon pages can wrongly end up in LRU lists when
migrate_pages() finishes its round without draining all the isolated
page list";Rafael Aquini;2013-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMTdhYWQxZTllNGQ5NzQ0OGQxZGYzZjg0YjA4YmQ2NTgxMWU2ZDZh;"The same issue can happen when reclaim_clean_pages_from_list() tries to
reclaim pages from an isolated page list, before migration, in the CMA
path";Rafael Aquini;2013-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMTdhYWQxZTllNGQ5NzQ0OGQxZGYzZjg0YjA4YmQ2NTgxMWU2ZDZh;" Such balloon page leak opens a race window against LRU lists
shrinkers that leads us to the following kernel panic";Rafael Aquini;2013-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMTdhYWQxZTllNGQ5NzQ0OGQxZGYzZjg0YjA4YmQ2NTgxMWU2ZDZh;"  BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
  IP: [<ffffffff810c2625>] shrink_page_list+0x24e/0x897
  PGD 3cda2067 PUD 3d713067 PMD 0
  Oops: 0000 [#1] SMP
  CPU: 0 PID: 340 Comm: kswapd0 Not tainted 3.12.0-rc1-22626-g4367597 #87
  Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
  RIP: shrink_page_list+0x24e/0x897
  RSP: 0000:ffff88003da499b8  EFLAGS: 00010286
  RAX: 0000000000000000 RBX: ffff88003e82bd60 RCX: 00000000000657d5
  RDX: 0000000000000000 RSI: 000000000000031f RDI: ffff88003e82bd40
  RBP: ffff88003da49ab0 R08: 0000000000000001 R09: 0000000081121a45
  R10: ffffffff81121a45 R11: ffff88003c4a9a28 R12: ffff88003e82bd40
  R13: ffff88003da0e800 R14: 0000000000000001 R15: ffff88003da49d58
  FS:  0000000000000000(0000) GS:ffff88003fc00000(0000) knlGS:0000000000000000
  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
  CR2: 00000000067d9000 CR3: 000000003ace5000 CR4: 00000000000407b0
  This patch fixes the issue, by assuring the proper tests are made at
putback_movable_pages() & reclaim_clean_pages_from_list() to avoid
isolated balloon pages being wrongly reinserted in LRU lists.";Rafael Aquini;2013-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;Merge git://git.kvack.org/~bcrl/aio-next;Linus Torvalds;2013-09-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;Pull aio changes from Ben LaHaise;Linus Torvalds;2013-09-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;" ""First off, sorry for this pull request being late in the merge window";Linus Torvalds;2013-09-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;  Al had raised a couple of concerns about 2 items in the series below;Linus Torvalds;2013-09-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;"  I addressed the first issue (the race introduced by Gu's use of
  mm_populate()), but he has not provided any further details on how he
  wants to rework the anon_inode.c changes (which were sent out months
  ago but have yet to be commented on)";Linus Torvalds;2013-09-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo5YmYxMmRmMzFmMjgyZTg0NWIzZGZhYWMxZTVkNTM3NmEwNDFkYTIy;"  The bulk of the changes have been sitting in the -next tree for a few
  months, with all the issues raised being addressed""
  aio: rcu_read_lock protection for new rcu_dereference calls
  aio: fix race in ring buffer page lookup introduced by page migration support
  aio: fix rcu sparse warnings introduced by ioctx table lookup patch
  aio: remove unnecessary debugging from aio_free_ring()
  aio: table lookup: verify ctx pointer
  staging/lustre: kiocb->ki_left is removed
  aio: fix error handling and rcu usage in ""convert the ioctx list to table lookup v3""
  aio: be defensive to ensure request batching is non-zero instead of BUG_ON()
  aio: convert the ioctx list to table lookup v3
  aio: double aio_max_nr in calculations
  aio: Kill ki_dtor
  aio: Kill ki_users
  aio: Kill unneeded kiocb members
  aio: Kill aio_rw_vect_retry()
  aio: Don't use ctx->tail unnecessarily
  aio: io_cancel() no longer returns the io_event
  aio: percpu ioctx refcount
  aio: percpu reqs_available
  aio: reqs_active -> reqs_available
  aio: fix build when migration is disabled";Linus Torvalds;2013-09-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;mm: vmscan: fix do_try_to_free_pages() livelock;Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"This patch is based on KOSAKI's work and I add a little more description,
please refer 
Currently, I found system can enter a state that there are lots of free
pages in a zone but only order-0 and order-1 pages which means the zone is
heavily fragmented, then high order allocation could make direct reclaim
path's long stall(ex, 60 seconds) especially in no swap and no compaciton
enviroment";Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" This problem happened on v3.4, but it seems issue still lives
in current tree, the reason is do_try_to_free_pages enter live lock";Lisa Du;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"kswapd will go to sleep if the zones have been fully scanned and are still
not balanced";Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" As kswapd thinks there's little point trying all over again
to avoid infinite loop";Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" Instead it changes order from high-order to
0-order because kswapd think order-0 is the most important";Lisa Du;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" Look at
73ce02e9 in detail";Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" If watermarks are ok, kswapd will go back to sleep
and may leave zone->all_unreclaimable =3D 0";Lisa Du;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" It assume high-order users
can still perform direct reclaim if they wish";Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"Direct reclaim continue to reclaim for a high order which is not a
COSTLY_ORDER without oom-killer until kswapd turn on
zone->all_unreclaimble= ";Lisa Du;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk; This is because to avoid too early oom-kill;Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;So it means direct_reclaim depends on kswapd to break this loop;Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"In worst case, direct-reclaim may continue to page reclaim forever when
kswapd sleeps forever until someone like watchdog detect and finally kill
the process";Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk; As described in;Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"We can't turn on zone->all_unreclaimable from direct reclaim path because
direct reclaim path don't take any lock and this way is racy";Lisa Du;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" Thus this
patch removes zone->all_unreclaimable field completely and recalculates
zone reclaimable state every time";Lisa Du;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;"Note: we can't take the idea that direct-reclaim see zone->pages_scanned
directly and kswapd continue to use zone->all_unreclaimable";Lisa Du;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" Because, it
is racy";Lisa Du;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZTU0M2Q1NzgwZTM2ZmY1ZWU1NmM0NGQ3ZTJlMzBkYjM0NTdhN2Vk;" commit 929bea7c71 (vmscan: all_unreclaimable() use
zone->all_unreclaimable as a name) describes the detail.";Lisa Du;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;mm: migrate: check movability of hugepage in unmap_and_move_huge_page();Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;"Currently hugepage migration works well only for pmd-based hugepages
(mainly due to lack of testing,) so we had better not enable migration of
other levels of hugepages until we are ready for it";Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;"Some users of hugepage migration (mbind, move_pages, and migrate_pages) do
page table walk and check pud/pmd_huge() there, so they are safe";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;" But the
other users (softoffline and memory hotremove) don't do this, so without
this patch they can try to migrate unexpected types of hugepages";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;"To prevent this, we introduce hugepage_migration_support() as an
architecture dependent check of whether hugepage are implemented on a pmd
basis or not";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzQ2N2VmYmRiNzk0ODE0NjU4MWE1NmNiZDY4M2EyMmEwNjg0YmJi;" And on some architecture multiple sizes of hugepages are
available, so hugepage_migration_support() also checks hugepage size.";Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODplNjMyYTkzOGQ5MTRkMjcxYmVjMjZlNTcwZDM2Yzc1NWExZTM1ZTRj;mm: migrate: add hugepage migration code to move_pages();Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODplNjMyYTkzOGQ5MTRkMjcxYmVjMjZlNTcwZDM2Yzc1NWExZTM1ZTRj;Extend move_pages() to handle vma with VM_HUGETLB set;Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODplNjMyYTkzOGQ5MTRkMjcxYmVjMjZlNTcwZDM2Yzc1NWExZTM1ZTRj;" We will be able to
migrate hugepage with move_pages(2) after applying the enablement patch
which comes later in this series";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplNjMyYTkzOGQ5MTRkMjcxYmVjMjZlNTcwZDM2Yzc1NWExZTM1ZTRj;"We avoid getting refcount on tail pages of hugepage, because unlike thp,
hugepage is not split and we need not care about races with splitting";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplNjMyYTkzOGQ5MTRkMjcxYmVjMjZlNTcwZDM2Yzc1NWExZTM1ZTRj;And migration of larger (1GB for x86_64) hugepage are not enabled.;Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;mm: soft-offline: use migrate_pages() instead of migrate_huge_page();Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;"Currently migrate_huge_page() takes a pointer to a hugepage to be migrated
as an argument, instead of taking a pointer to the list of hugepages to be
migrated";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;" This behavior was introduced in commit 189ebff28 (""hugetlb";Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;"simplify migrate_huge_page()""), and was OK because until now hugepage
migration is enabled only for soft-offlining which migrates only one
hugepage in a single call";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;"But the situation will change in the later patches in this series which
enable other users of page migration to support hugepage migration";Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;" They
can kick migration for both of normal pages and hugepages in a single
call, so we need to go back to original implementation which uses linked
lists to collect the hugepages to be migrated";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2;"With this patch, soft_offline_huge_page() switches to use migrate_pages(),
and migrate_huge_page() is not used any more";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOGVjMWNlZTVhNDM3NWMxMjQ0Yjg1NzA5MTM4YTJlYWMyZDg5Y2I2; So let's remove it.;Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;mm: migrate: make core migration code aware of hugepage;Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"Currently hugepage migration is available only for soft offlining, but
it's also useful for some other users of page migration (clearly because
users of hugepage can enjoy the benefit of mempolicy and memory hotplug.)
So this patchset tries to extend such users to support hugepage migration";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"The target of this patchset is to enable hugepage migration for NUMA
related system calls (migrate_pages(2), move_pages(2), and mbind(2)), and
memory hotplug";Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"This patchset does not add hugepage migration for memory compaction,
because users of memory compaction mainly expect to construct thp by
arranging raw pages, and there's little or no need to compact hugepages";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"CMA, another user of page migration, can have benefit from hugepage
migration, but is not enabled to support it for now (just because of lack
of testing and expertise in CMA.)
Hugepage migration of non pmd-based hugepage (for example 1GB hugepage in
x86_64, or hugepages in architectures like ia64) is not enabled for now
(again, because of lack of testing.)
As for how these are achived, I extended the API (migrate_pages()) to
handle hugepage (with patch 1 and 2) and adjusted code of each caller to
check and collect movable hugepages (with patch 3-7)";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;" Remaining 2 patches
are kind of miscellaneous ones to avoid unexpected behavior";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;" Patch 8 is
about making sure that we only migrate pmd-based hugepages";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;" And patch 9
is about choosing appropriate zone for hugepage allocation";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"My test is mainly functional one, simply kicking hugepage migration via
each entry point and confirm that migration is done correctly";Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;" Test code
is available here";Naoya Horiguchi;2013-09-11;0;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"  git://github.com/Naoya-Horiguchi/test_hugepage_migration_extension.git
And I always run libhugetlbfs test when changing hugetlbfs's code";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;" With
this patchset, no regression was found in the test";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;This patch (of 9);Naoya Horiguchi;2013-09-11;1;0
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"Before enabling each user of page migration to support hugepage,
this patch enables the list of pages for migration to link not only
LRU pages, but also hugepages";Naoya Horiguchi;2013-09-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozMWNhZjY2NWU2NjZiNTFmZTM2ZWZkMWU1NDAzMWVkMjllODZjMGI0;"As a result, putback_movable_pages()
and migrate_pages() can handle both of LRU pages and hugepages.";Naoya Horiguchi;2013-09-11;0;1
MDY6Q29tbWl0MjMyNTI5ODozNmJjMDhjYzAxNzA5YjRhOWJiNTYzYjM1YWE1MzAyNDFkZGM2M2Uz;fs/aio: Add support to aio ring pages migration;Gu Zheng;2013-07-16;1;0
MDY6Q29tbWl0MjMyNTI5ODozNmJjMDhjYzAxNzA5YjRhOWJiNTYzYjM1YWE1MzAyNDFkZGM2M2Uz;"As the aio job will pin the ring pages, that will lead to mem migrated
failed";Gu Zheng;2013-07-16;0;1
MDY6Q29tbWl0MjMyNTI5ODozNmJjMDhjYzAxNzA5YjRhOWJiNTYzYjM1YWE1MzAyNDFkZGM2M2Uz;"In order to fix this problem we use an anon inode to manage the aio ring
pages, and  setup the migratepage callback in the anon inode's address space, so
that when mem migrating the aio ring pages will be moved to other mem node safely.";Gu Zheng;2013-07-16;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGRhZDMwOTIyY2NjNzMzY2ZkYmZlMjMyMDkwY2Y2NzRkYzM3NGRj;mm: migration: add migrate_entry_wait_huge();Naoya Horiguchi;2013-06-12;1;1
MDY6Q29tbWl0MjMyNTI5ODozMGRhZDMwOTIyY2NjNzMzY2ZkYmZlMjMyMDkwY2Y2NzRkYzM3NGRj;"When we have a page fault for the address which is backed by a hugepage
under migration, the kernel can't wait correctly and do busy looping on
hugepage fault until the migration finishes";Naoya Horiguchi;2013-06-12;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGRhZDMwOTIyY2NjNzMzY2ZkYmZlMjMyMDkwY2Y2NzRkYzM3NGRj;" As a result, users who try
to kick hugepage migration (via soft offlining, for example) occasionally
experience long delay or soft lockup";Naoya Horiguchi;2013-06-12;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGRhZDMwOTIyY2NjNzMzY2ZkYmZlMjMyMDkwY2Y2NzRkYzM3NGRj;"This is because pte_offset_map_lock() can't get a correct migration entry
or a correct page table lock for hugepage";Naoya Horiguchi;2013-06-12;0;1
MDY6Q29tbWl0MjMyNTI5ODozMGRhZDMwOTIyY2NjNzMzY2ZkYmZlMjMyMDkwY2Y2NzRkYzM3NGRj;" This patch introduces
migration_entry_wait_huge() to solve this.";Naoya Horiguchi;2013-06-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;mm compaction: fix of improper cache flush in migration code;Leonid Yegoshin;2013-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;Page 'new' during MIGRATION can't be flushed with flush_cache_page();Leonid Yegoshin;2013-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;"Using flush_cache_page(vma, addr, pfn) is justified only if the page is
already placed in process page table, and that is done right after
flush_cache_page()";Leonid Yegoshin;2013-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;" But without it the arch function has no knowledge
of process PTE and does nothing";Leonid Yegoshin;2013-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;"Besides that, flush_cache_page() flushes an application cache page, but
the kernel has a different page virtual address and dirtied it";Leonid Yegoshin;2013-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;Replace it with flush_dcache_page(new) which is the proper usage;Leonid Yegoshin;2013-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;The old page is flushed in try_to_unmap_one() before migration;Leonid Yegoshin;2013-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;"This bug takes place in Sead3 board with M14Kc MIPS CPU without cache
aliasing (but Harvard arch - separate I and D cache) in tight memory
environment (128MB) each 1-3days on SOAK test";Leonid Yegoshin;2013-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMmNjNDk5YzViY2Y5MDQwYTczOGY0OWU4MDUxYjQyMDc4MjA1NzQ4;" It fails in cc1 during
kernel build (SIGILL, SIGBUS, SIGSEG) if CONFIG_COMPACTION is switched";Leonid Yegoshin;2013-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNzNlNWM5YzU5YTBmN2JhMzBiM2U1ZjdiZDJkODA5N2Q0Yzg5YzZk;mm: rewrite the comment over migrate_pages() more comprehensibly;Srivatsa S. Bhat;2013-04-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpjNzNlNWM5YzU5YTBmN2JhMzBiM2U1ZjdiZDJkODA5N2Q0Yzg5YzZk;"The comment over migrate_pages() looks quite weird, and makes it hard to
grasp what it is trying to say";Srivatsa S. Bhat;2013-04-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpjNzNlNWM5YzU5YTBmN2JhMzBiM2U1ZjdiZDJkODA5N2Q0Yzg5YzZk; Rewrite it more comprehensibly.;Srivatsa S. Bhat;2013-04-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZWQ1YjY0YTk1MzI2Njk3Zjk0MmY1MDAzYzEzOGM3ZmYzMDQzZWY1;mm/migrate: fix comment typo syncronous->synchronous;Jianguo Wu;2013-04-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZWQ1YjY0YTk1MzI2Njk3Zjk0MmY1MDAzYzEzOGM3ZmYzMDQzZWY1;;Jianguo Wu;2013-04-29;0;0
MDY6Q29tbWl0MjMyNTI5ODo5YzYyMGUyYmM1YWE0MjU2YzEwMmFkYTM0ZTZjNzYyMDRlZDU4OThi;mm: remove offlining arg to migrate_pages;Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo5YzYyMGUyYmM1YWE0MjU2YzEwMmFkYTM0ZTZjNzYyMDRlZDU4OThi;"No functional change, but the only purpose of the offlining argument to
migrate_pages() etc, was to ensure that __unmap_and_move() could migrate a
KSM page for memory hotremove (which took ksm_thread_mutex) but not for
other callers";Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo5YzYyMGUyYmM1YWE0MjU2YzEwMmFkYTM0ZTZjNzYyMDRlZDU4OThi; Now all cases are safe, remove the arg.;Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNzliYzBhMGM3OWUwNmNjODdlMTc1MzBlOWMxYzU2YzZmMjk3ZTE3;ksm: enable KSM page migration;Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNzliYzBhMGM3OWUwNmNjODdlMTc1MzBlOWMxYzU2YzZmMjk3ZTE3;"Migration of KSM pages is now safe: remove the PageKsm restrictions from
mempolicy.c and migrate.c";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNzliYzBhMGM3OWUwNmNjODdlMTc1MzBlOWMxYzU2YzZmMjk3ZTE3;"But keep PageKsm out of __unmap_and_move()'s anon_vma contortions, which
are irrelevant to KSM: it looks as if that code was preventing hotremove
migration of KSM pages, unless they happened to be in swapcache";Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNzliYzBhMGM3OWUwNmNjODdlMTc1MzBlOWMxYzU2YzZmMjk3ZTE3;"There is some question as to whether enforcing a NUMA mempolicy migration
ought to migrate KSM pages, mapped into entirely unrelated processes; but
moving page_mapcount > 1 is only permitted with MPOL_MF_MOVE_ALL anyway,
and it seems reasonable to assume that you wouldn't set MADV_MERGEABLE on
any area where this is a worry.";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;ksm: make KSM page migration possible;Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"KSM page migration is already supported in the case of memory hotremove,
which takes the ksm_thread_mutex across all its migrations to keep life
simple";Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"But the new KSM NUMA merge_across_nodes knob introduces a problem, when
it's set to non-default 0: if a KSM page is migrated to a different NUMA
node, how do we migrate its stable node to the right tree?  And what if
that collides with an existing stable node?
So far there's no provision for that, and this patch does not attempt to
deal with it either";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;" But how will I test a solution, when I don't know
how to hotremove memory?  The best answer is to enable KSM page migration
in all cases now, and test more common cases";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;" With THP and compaction
added since KSM came in, page migration is now mainstream, and it's a
shame that a KSM page can frustrate freeing a page block";Hugh Dickins;2013-02-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"Without worrying about merge_across_nodes 0 for now, this patch gets KSM
page migration working reliably for default merge_across_nodes 1 (but
leave the patch enabling it until near the end of the series)";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"It's much simpler than I'd originally imagined, and does not require an
additional tier of locking: page migration relies on the page lock, KSM
page reclaim relies on the page lock, the page lock is enough for KSM page
migration too";Hugh Dickins;2013-02-23;0;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"Almost all the care has to be in get_ksm_page(): that's the function which
worries about when a stable node is stale and should be freed, now it also
has to worry about the KSM page being migrated";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"The only new overhead is an additional put/get/lock/unlock_page when
stable_tree_search() arrives at a matching node: to make sure migration
respects the raised page count, and so does not migrate the page while
we're busy with it here";Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;" That's probably avoidable, either by changing
internal interfaces from using kpage to stable_node, or by moving the
ksm_migrate_page() callsite into a page_freeze_refs() section (even if not
swapcache); but this works well, I've no urge to pull it apart now";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjOGQ2NTUzYjk1ODAxODhhMTMyNDQ4NjE3M2Q3OWMwZjg2NDJlODcw;"(Descents of the stable tree may pass through nodes whose KSM pages are
under migration: being unlocked, the raised page count does not prevent
that, nor need it: it's safe to memcmp against either old or new page.)
You might worry about mremap, and whether page migration's rmap_walk to
remove migration entries will find all the KSM locations where it inserted
earlier: that should already be handled, by the satisfyingly heavy hammer
of move_vma()'s call to ksm_madvise(,,,MADV_UNMERGEABLE,).";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMmI3NTFjM2QwMzc2ZTg2YTM3N2UzYTBhYTJkZGJiZTlkMmVlZmMx;mm: rename page struct field helpers;Mel Gorman;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODoyMmI3NTFjM2QwMzc2ZTg2YTM3N2UzYTBhYTJkZGJiZTlkMmVlZmMx;"The function names page_xchg_last_nid(), page_last_nid() and
reset_page_last_nid() were judged to be inconsistent so rename them to a
struct_field_op style pattern";Mel Gorman;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMmI3NTFjM2QwMzc2ZTg2YTM3N2UzYTBhYTJkZGJiZTlkMmVlZmMx;" As it looked jarring to have
reset_page_mapcount() and page_nid_reset_last() beside each other in
memmap_init_zone(), this patch also renames reset_page_mapcount() to
page_mapcount_reset()";Mel Gorman;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMmI3NTFjM2QwMzc2ZTg2YTM3N2UzYTBhYTJkZGJiZTlkMmVlZmMx;" There are others like init_page_count() but as
it is used throughout the arch code a rename would likely cause more
conflicts than it is worth.";Mel Gorman;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;mm: numa: cleanup flow of transhuge page migration;Hugh Dickins;2013-02-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;"When correcting commit 04fa5d6a6547 (""mm: migrate: check page_count of
THP before migrating"") Hugh Dickins noted that the control flow for
transhuge migration was difficult to follow";Hugh Dickins;2013-02-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;" Unconditionally calling
put_page() in numamigrate_isolate_page() made the failure paths of both
migrate_misplaced_transhuge_page() and migrate_misplaced_page() more
complex that they should be";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;" Further, he was extremely wary that an
unlock_page() should ever happen after a put_page() even if the
put_page() should never be the final put_page";Hugh Dickins;2013-02-23;1;0
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;"Hugh implemented the following cleanup to simplify the path by calling
putback_lru_page() inside numamigrate_isolate_page() if it failed to
isolate and always calling unlock_page() within
migrate_misplaced_transhuge_page()";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODozNDBlZjM5MDJjZjIwY2VjNDNjZGNkMWU3MmFlNWNiNTE4YmU3MzI4;"There is no functional change after this patch is applied but the code
is easier to follow and unlock_page() always happens before put_page().";Hugh Dickins;2013-02-23;1;1
MDY6Q29tbWl0MjMyNTI5ODozYWJlZjRlNmMyM2ZlZWY0YWE5YWIxNjFhZTEzOGQ2ZDM5YWU2OWYz;mm: numa: take THP into account when migrating pages for NUMA balancing;Mel Gorman;2013-02-23;0;0
MDY6Q29tbWl0MjMyNTI5ODozYWJlZjRlNmMyM2ZlZWY0YWE5YWIxNjFhZTEzOGQ2ZDM5YWU2OWYz;"Wanpeng Li pointed out that numamigrate_isolate_page() assumes that only
one base page is being migrated when in fact it can also be checking
THP";Mel Gorman;2013-02-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozYWJlZjRlNmMyM2ZlZWY0YWE5YWIxNjFhZTEzOGQ2ZDM5YWU2OWYz;"The consequences are that a migration will be attempted when a target
node is nearly full and fail later";Mel Gorman;2013-02-23;0;0
MDY6Q29tbWl0MjMyNTI5ODozYWJlZjRlNmMyM2ZlZWY0YWE5YWIxNjFhZTEzOGQ2ZDM5YWU2OWYz;" It's unlikely to be user-visible
but it should be fixed";Mel Gorman;2013-02-23;0;1
MDY6Q29tbWl0MjMyNTI5ODozYWJlZjRlNmMyM2ZlZWY0YWE5YWIxNjFhZTEzOGQ2ZDM5YWU2OWYz;" While we are there, migrate_balanced_pgdat()
should treat nr_migrate_pages as an unsigned long as it is treated as a
watermark.";Mel Gorman;2013-02-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpiZTc1MTdkNmFiOTcyMmYwYWJhZDZiYTVmZmQzOWNmY2VkOTU1NDlj;mm/hugetlb: set PTE as huge in hugetlb_change_protection and remove_migration_pte;Tony Lu;2013-02-04;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZTc1MTdkNmFiOTcyMmYwYWJhZDZiYTVmZmQzOWNmY2VkOTU1NDlj;"When setting a huge PTE, besides calling pte_mkhuge(), we also need to
call arch_make_huge_pte(), which we indeed do in make_huge_pte(), but we
forget to do in hugetlb_change_protection() and remove_migration_pte().";Tony Lu;2013-02-04;1;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;mm: migrate: check page_count of THP before migrating;Mel Gorman;2013-01-11;1;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;"Hugh Dickins pointed out that migrate_misplaced_transhuge_page() does
not check page_count before migrating like base page migration and
khugepage";Mel Gorman;2013-01-11;1;0
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi; He could not see why this was safe and he is right;Mel Gorman;2013-01-11;0;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;"The potential impact of the bug is avoided due to the limitations of
NUMA balancing";Mel Gorman;2013-01-11;0;0
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" The page_mapcount() check ensures that only a single
address space is using this page and as THPs are typically private it
should not be possible for another address space to fault it in
parallel";Mel Gorman;2013-01-11;0;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" If the address space has one associated task then it's
difficult to have both a GUP pin and be referencing the page at the same
time";Mel Gorman;2013-01-11;1;0
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" If there are multiple tasks then a buggy scenario requires that
another thread be accessing the page while the direct IO is in flight";Mel Gorman;2013-01-11;0;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;"This is dodgy behaviour as there is a possibility of corruption with or
without THP migration";Mel Gorman;2013-01-11;0;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" It would be
While we happen to be safe for the most part it is shoddy to depend on
such ""safety"" so this patch checks the page count similar to anonymous
pages";Mel Gorman;2013-01-11;1;1
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" Note that this does not mean that the page_mapcount() check can
go away";Mel Gorman;2013-01-11;0;0
MDY6Q29tbWl0MjMyNTI5ODowNGZhNWQ2YTY1NDdmYmZjZjYxM2VmZDAwNjM3NjY2ZmUxOWIyNGFi;" If we were to remove the page_mapcount() check the the THP
would have to be unmapped from all referencing PTEs, replaced with
migration PTEs and restored properly afterwards.";Mel Gorman;2013-01-11;1;0
MDY6Q29tbWl0MjMyNTI5ODpjZTRhOWNjNTc5MzgxYmM3MGIxMmViYjkxYzU3ZGEzMWJhZjhlM2I3;mm,numa: fix update_mmu_cache_pmd call;Stephen Rothwell;2012-12-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZTRhOWNjNTc5MzgxYmM3MGIxMmViYjkxYzU3ZGEzMWJhZjhlM2I3;"This build error is currently hidden by the fact that the x86
implementation of 'update_mmu_cache_pmd()' is a macro that doesn't use
its last argument, but commit b32967ff101a (""mm: numa: Add THP migration
for the NUMA working set scanning fault case"") introduced a call with
the wrong third argument";Stephen Rothwell;2012-12-10;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZTRhOWNjNTc5MzgxYmM3MGIxMmViYjkxYzU3ZGEzMWJhZjhlM2I3;In the akpm tree, it causes this build error;Stephen Rothwell;2012-12-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZTRhOWNjNTc5MzgxYmM3MGIxMmViYjkxYzU3ZGEzMWJhZjhlM2I3;  mm/migrate.c: In function 'migrate_misplaced_transhuge_page_put';Stephen Rothwell;2012-12-10;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZTRhOWNjNTc5MzgxYmM3MGIxMmViYjkxYzU3ZGEzMWJhZjhlM2I3;"  mm/migrate.c:1666:2: error: incompatible type for argument 3 of 'update_mmu_cache_pmd'
  arch/x86/include/asm/pgtable.h:792:20: note: expected 'struct pmd_t *' but argument is of type 'pmd_t'
Fix it.";Stephen Rothwell;2012-12-10;1;1
MDY6Q29tbWl0MjMyNTI5ODozODkxNjJjMjJkYzM3MzkzMjA4OWNlMTZjYmFiNDNlODBhODhiMDM1;mm,migrate: use N_MEMORY instead N_HIGH_MEMORY;Lai Jiangshan;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODozODkxNjJjMjJkYzM3MzkzMjA4OWNlMTZjYmFiNDNlODBhODhiMDM1;N_HIGH_MEMORY stands for the nodes that has normal or high memory;Lai Jiangshan;2012-12-12;0;0
MDY6Q29tbWl0MjMyNTI5ODozODkxNjJjMjJkYzM3MzkzMjA4OWNlMTZjYmFiNDNlODBhODhiMDM1;N_MEMORY stands for the nodes that has any memory;Lai Jiangshan;2012-12-12;0;0
MDY6Q29tbWl0MjMyNTI5ODozODkxNjJjMjJkYzM3MzkzMjA4OWNlMTZjYmFiNDNlODBhODhiMDM1;"The code here need to handle with the nodes which have memory, we should
use N_MEMORY instead.";Lai Jiangshan;2012-12-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NzMzYzdkMTFkZmY0NGU5OGQyY2ExNjYxNzg4NmE3ODA4NmIzNTRm;mm: introduce putback_movable_pages();Rafael Aquini;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1NzMzYzdkMTFkZmY0NGU5OGQyY2ExNjYxNzg4NmE3ODA4NmIzNTRm;"The PATCH ""mm: introduce compaction and migration for virtio ballooned pages""
hacks around putback_lru_pages() in order to allow ballooned pages to be
re-inserted on balloon page list as if a ballooned page was like a LRU page";Rafael Aquini;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo1NzMzYzdkMTFkZmY0NGU5OGQyY2ExNjYxNzg4NmE3ODA4NmIzNTRm;"As ballooned pages are not legitimate LRU pages, this patch introduces
putback_movable_pages() to properly cope with cases where the isolated
pageset contains ballooned pages and LRU pages, thus fixing the mentioned
inelegant hack around putback_lru_pages().";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZjZiZGRmMTkyNGVhZWJmMmJlYjg1ZTQyNDlhODlkZDE2ZDRlZWQ2;mm: introduce compaction and migration for ballooned pages;Rafael Aquini;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiZjZiZGRmMTkyNGVhZWJmMmJlYjg1ZTQyNDlhODlkZDE2ZDRlZWQ2;"Memory fragmentation introduced by ballooning might reduce significantly
the number of 2MB contiguous memory blocks that can be used within a guest,
thus imposing performance penalties associated with the reduced number of
transparent huge pages that could be used by the guest workload";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZjZiZGRmMTkyNGVhZWJmMmJlYjg1ZTQyNDlhODlkZDE2ZDRlZWQ2;"This patch introduces the helper functions as well as the necessary changes
to teach compaction and migration bits how to cope with pages which are
part of a guest memory balloon, in order to make them movable by memory
compaction procedures.";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;mm: adjust address_space_operations.migratepage() return code;Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;"Memory fragmentation introduced by ballooning might reduce significantly
the number of 2MB contiguous memory blocks that can be used within a
guest, thus imposing performance penalties associated with the reduced
number of transparent huge pages that could be used by the guest workload";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;This patch-set follows the main idea discussed at 2012 LSFMMS session;Rafael Aquini;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;"""Ballooning for transparent huge pages"" -- to introduce the required changes to the virtio_balloon driver, as well as
the changes to the core compaction & migration bits, in order to make
those subsystems aware of ballooned pages and allow memory balloon pages
become movable within a guest, thus avoiding the aforementioned
fragmentation issue
Following are numbers that prove this patch benefits on allowing
compaction to be more effective at memory ballooned guests";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;"Results for STRESS-HIGHALLOC benchmark, from Mel Gorman's mmtests suite,
running on a 4gB RAM KVM guest which was ballooning 512mB RAM in 64mB
chunks, at every minute (inflating/deflating), while test was running";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;"===BEGIN stress-highalloc
STRESS-HIGHALLOC
                 highalloc-3.7     highalloc-3.7
                     rc4-clean         rc4-patch
Pass 1          55.00 ( 0.00%)    62.00 ( 7.00%)
Pass 2          54.00 ( 0.00%)    62.00 ( 8.00%)
while Rested    75.00 ( 0.00%)    80.00 ( 5.00%)
MMTests Statistics: duration
           rc4-clean   rc4-patch
User         1207.59     1207.46
System       1300.55     1299.61
Elapsed      2273.72     2157.06
MMTests Statistics: vmstat
                          rc4-clean   rc4-patch
Page Ins                    3581516     2374368
Page Outs                  11148692    10410332
Swap Ins                         80          47
Swap Outs                      3641         476
Direct pages scanned          37978       33826
Kswapd pages scanned        1828245     1342869
Kswapd pages reclaimed      1710236     1304099
Direct pages reclaimed        32207       31005
Kswapd efficiency               93%         97%
Kswapd velocity             804.077     622.546
Direct efficiency               84%         91%
Direct velocity              16.703      15.682
Percentage direct scans          2%          2%
Page writes by reclaim        79252        9704
Page writes file              75611        9228
Page writes anon               3641         476
Page reclaim immediate        16764       11014
Page rescued immediate            0           0
Slabs scanned               2171904     2152448
Direct inode steals             385        2261
Kswapd inode steals          659137      609670
Kswapd skipped wait               1          69
THP fault alloc                 546         631
THP collapse alloc              361         339
THP splits                      259         263
THP fault fallback               98          50
THP collapse fail                20          17
Compaction stalls               747         499
Compaction success              244         145
Compaction failures             503         354
Compaction pages moved       370888      474837
Compaction move failure       77378       65259
===END stress-highalloc
This patch";Rafael Aquini;2012-12-12;0;1
MDY6Q29tbWl0MjMyNTI5ODo3OGJkNTIwOTdkMDQyMDVhMzNhODAxNGExYjhhYzAxY2YxYWU5ZDA2;"Introduce MIGRATEPAGE_SUCCESS as the default return code for
address_space_operations.migratepage() method and documents the expected
return code for the same method in failure cases.";Rafael Aquini;2012-12-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjE5MDQ5YWUxY2UzMmI4OTIzNjY0NmNjY2FlYzJhNWZjNmM0ZmQy;mm: introduce mm_find_pmd();Bob Liu;2012-12-12;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MjE5MDQ5YWUxY2UzMmI4OTIzNjY0NmNjY2FlYzJhNWZjNmM0ZmQy;"Several place need to find the pmd by(mm_struct, address), so introduce a
function to simplify it.";Bob Liu;2012-12-12;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZmMzZjFkNjZiMWVmMGQ3YjhkYzExZjRmZjFjYzUxMGY3OGIzN2Q2;mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable;Ingo Molnar;2012-12-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZmMzZjFkNjZiMWVmMGQ3YjhkYzExZjRmZjFjYzUxMGY3OGIzN2Q2;"rmap_walk_anon() and try_to_unmap_anon() appears to be too
careful about locking the anon vma: while it needs protection
against anon vma list modifications, it does not need exclusive
access to the list itself";Ingo Molnar;2012-12-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZmMzZjFkNjZiMWVmMGQ3YjhkYzExZjRmZjFjYzUxMGY3OGIzN2Q2;"Transforming this exclusive lock to a read-locked rwsem removes
a global lock from the hot path of page-migration intense
threaded workloads which can cause pathological performance like
this";Ingo Molnar;2012-12-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ZmMzZjFkNjZiMWVmMGQ3YjhkYzExZjRmZjFjYzUxMGY3OGIzN2Q2;"    96.43%        process 0  [kernel.kallsyms]  [k] perf_trace_sched_switch
                  --- perf_trace_sched_switch
                      __schedule
                      schedule
                      schedule_preempt_disabled
                      __mutex_lock_common.isra.6
                      __mutex_lock_slowpath
                      mutex_lock
                     |--50.61%-- rmap_walk
                     |          move_to_new_page
                     |          migrate_pages
                     |          migrate_misplaced_page
                     |          __do_numa_page.isra.69
                     |          handle_pte_fault
                     |          handle_mm_fault
                     |          __do_page_fault
                     |          do_page_fault
                     |          page_fault
                     |          __memset_sse2
                     |           --100.00%-- worker_thread
                     |                      --100.00%-- start_thread
                      --49.39%-- page_lock_anon_vma
                                try_to_unmap_anon
                                try_to_unmap
                                migrate_pages
                                migrate_misplaced_page
                                __do_numa_page.isra.69
                                handle_pte_fault
                                handle_mm_fault
                                __do_page_fault
                                do_page_fault
                                page_fault
                                __memset_sse2
                                 --100.00%-- worker_thread
                                           start_thread
With this change applied the profile is now nicely flat
and there's no anon-vma related scheduling/blocking";Ingo Molnar;2012-12-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZmMzZjFkNjZiMWVmMGQ3YjhkYzExZjRmZjFjYzUxMGY3OGIzN2Q2;"Rename anon_vma_[un]lock() => anon_vma_[un]lock_write(),
to make it clearer that it's an exclusive write-lock in
that case - suggested by Rik van Riel.";Ingo Molnar;2012-12-02;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMjhkNDMzNTEyZjRmMzg3ZTI1NjNjMTRkYjQ1YTdiYjhhMzM4YjFh;mm: migrate: Account a transhuge page properly when rate limiting;Mel Gorman;2012-11-29;1;1
MDY6Q29tbWl0MjMyNTI5ODpkMjhkNDMzNTEyZjRmMzg3ZTI1NjNjMTRkYjQ1YTdiYjhhMzM4YjFh;"If there is excessive migration due to NUMA balancing it gets rate
limited";Mel Gorman;2012-11-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMjhkNDMzNTEyZjRmMzg3ZTI1NjNjMTRkYjQ1YTdiYjhhMzM4YjFh;"It does this by counting the number of pages it has migrated
recently but counts a transhuge page as 1 page";Mel Gorman;2012-11-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpkMjhkNDMzNTEyZjRmMzg3ZTI1NjNjMTRkYjQ1YTdiYjhhMzM4YjFh;Account for it properly.;Mel Gorman;2012-11-29;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTQ4MzQxYjI4OTU2Y2NkMzVhNjNhYjEyZjAxZDg1NDEwNDFhYTcw;mm: numa: Account for failed allocations and isolations as migration failures;Mel Gorman;2012-11-27;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTQ4MzQxYjI4OTU2Y2NkMzVhNjNhYjEyZjAxZDg1NDEwNDFhYTcw;Subject says it all;Mel Gorman;2012-11-27;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTQ4MzQxYjI4OTU2Y2NkMzVhNjNhYjEyZjAxZDg1NDEwNDFhYTcw;"Allocation failures and a failure to isolate should
be accounted as a migration failure";Mel Gorman;2012-11-27;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTQ4MzQxYjI4OTU2Y2NkMzVhNjNhYjEyZjAxZDg1NDEwNDFhYTcw;"This is partially another
difference between base page and transhuge page migration";Mel Gorman;2012-11-27;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTQ4MzQxYjI4OTU2Y2NkMzVhNjNhYjEyZjAxZDg1NDEwNDFhYTcw;"A base page
migration makes multiple attempts for these conditions before it would
be accounted for as a failure.";Mel Gorman;2012-11-27;0;0
MDY6Q29tbWl0MjMyNTI5ODoyMjAwMThkMzg4YjhhYjFmY2ExYzVmMGM2NDc0YmFiNDdhZDJjOWMw;mm: numa: Add THP migration for the NUMA working set scanning fault case build fix;Mel Gorman;2012-12-05;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMjAwMThkMzg4YjhhYjFmY2ExYzVmMGM2NDc0YmFiNDdhZDJjOWMw;"Commit ""Add THP migration for the NUMA working set scanning fault case""
breaks the build because HPAGE_PMD_SHIFT and HPAGE_PMD_MASK defined to
explode without CONFIG_TRANSPARENT_HUGEPAGE";Mel Gorman;2012-12-05;1;0
MDY6Q29tbWl0MjMyNTI5ODoyMjAwMThkMzg4YjhhYjFmY2ExYzVmMGM2NDc0YmFiNDdhZDJjOWMw;mm/migrate.c: In function 'migrate_misplaced_transhuge_page_put';Mel Gorman;2012-12-05;0;1
MDY6Q29tbWl0MjMyNTI5ODoyMjAwMThkMzg4YjhhYjFmY2ExYzVmMGM2NDc0YmFiNDdhZDJjOWMw;"mm/migrate.c:1549: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1564: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1566: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1573: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1606: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
mm/migrate.c:1648: error: call to '__build_bug_failed' declared with attribute error: BUILD_BUG failed
CONFIG_NUMA_BALANCING allows compilation without enabling transparent
hugepages, so define the dummy function for such a configuration and only
define migrate_misplaced_transhuge_page_put() when transparent hugepages
are enabled.";Mel Gorman;2012-12-05;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;mm: numa: Add THP migration for the NUMA working set scanning fault case.;Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;"Note: This is very heavily based on a patch from Peter Zijlstra with
	fixes from Ingo Molnar, Hugh Dickins and Johannes Weiner";Mel Gorman;2012-11-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;" That patch
	put a lot of migration logic into mm/huge_memory.c where it does
	not belong";Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;"This version puts tries to share some of the migration
	logic with migrate_misplaced_page";Mel Gorman;2012-11-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;" However, it should be noted
	that now migrate.c is doing more with the pagetable manipulation
	than is preferred";Mel Gorman;2012-11-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;"The end result is barely recognisable so as
	before, the signed-offs had to be removed but will be re-added if
	the original authors are ok with it";Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;Add THP migration for the NUMA working set scanning fault case;Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;It uses the page lock to serialize;Mel Gorman;2012-11-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMzI5NjdmZjEwMWE3NTA4ZjcwYmU4ZGU1OWIyNzhkNGRmOTJmYTAw;"No migration pte dance is
necessary because the pte is already unmapped when we decide
to migrate.";Mel Gorman;2012-11-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiYWMwMzgyYzZhZDc2NDE1NjAyNTk3ODg0NTE0N2U1YTZlY2NjYTA5;mm: numa: migrate: Set last_nid on newly allocated page;Hillf Danton;2012-11-27;0;0
MDY6Q29tbWl0MjMyNTI5ODpiYWMwMzgyYzZhZDc2NDE1NjAyNTk3ODg0NTE0N2U1YTZlY2NjYTA5;Pass last_nid from misplaced page to newly allocated migration target page.;Hillf Danton;2012-11-27;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTQ4MDhiNDlmNTVlMGUxMTM1ZGE1ZTRhMTU0YTU0MGRkOWYzNjYy;mm: numa: Rate limit setting of pte_numa if node is saturated;Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTQ4MDhiNDlmNTVlMGUxMTM1ZGE1ZTRhMTU0YTU0MGRkOWYzNjYy;"If there are a large number of NUMA hinting faults and all of them
are resulting in migrations it may indicate that memory is just
bouncing uselessly around";Mel Gorman;2012-11-19;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTQ4MDhiNDlmNTVlMGUxMTM1ZGE1ZTRhMTU0YTU0MGRkOWYzNjYy;"NUMA balancing cost is likely exceeding
any benefit from locality";Mel Gorman;2012-11-19;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTQ4MDhiNDlmNTVlMGUxMTM1ZGE1ZTRhMTU0YTU0MGRkOWYzNjYy;"Rate limit the PTE updates if the node
is migration rate-limited";Mel Gorman;2012-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODplMTQ4MDhiNDlmNTVlMGUxMTM1ZGE1ZTRhMTU0YTU0MGRkOWYzNjYy;"As noted in the comments, this distorts
the NUMA faulting statistics.";Mel Gorman;2012-11-19;1;1
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;mm: numa: Rate limit the amount of memory that is migrated between nodes;Mel Gorman;2012-11-14;0;0
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;NOTE: This is very heavily based on similar logic in autonuma;Mel Gorman;2012-11-14;0;0
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;"It should
	be signed off by Andrea but because there was no standalone
	patch and it's sufficiently different from what he did that
	the signed-off is omitted";Mel Gorman;2012-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;Will be added back if requested;Mel Gorman;2012-11-14;1;0
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;"If a large number of pages are misplaced then the memory bus can be
saturated just migrating pages between nodes";Mel Gorman;2012-11-14;0;1
MDY6Q29tbWl0MjMyNTI5ODphOGY2MDc3MjEzZDI4NWNhMDhkYmY2ZDRhNjc0NzA3ODczODgxMzhi;"This patch rate-limits
the amount of memory that can be migrating between nodes.";Mel Gorman;2012-11-14;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;mm: numa: Add pte updates, hinting and migration stats;Mel Gorman;2012-11-02;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"It is tricky to quantify the basic cost of automatic NUMA placement in a
meaningful manner";Mel Gorman;2012-11-02;0;0
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"This patch adds some vmstats that can be used as part
of a basic costing model";Mel Gorman;2012-11-02;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"u    = basic unit = sizeof(void *)
Ca   = cost of struct page access = sizeof(struct page) / u
Cpte = Cost PTE access = Ca
Cupdate = Cost PTE update = (2 * Cpte) + (2 * Wlock)
	where Cpte is incurred twice for a read and a write and Wlock
	is a constant representing the cost of taking or releasing a
	lock
Cnumahint = Cost of a minor page fault = some high constant e.g";Mel Gorman;2012-11-02;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"1000
Cpagerw = Cost to read or write a full page = Ca + PAGE_SIZE/u
Ci = Cost of page isolation = Ca + Wi
	where Wi is a constant that should reflect the approximate cost
	of the locking operation
Cpagecopy = Cpagerw + (Cpagerw * Wnuma) + Ci + (Ci * Wnuma)
	where Wnuma is the approximate NUMA factor";Mel Gorman;2012-11-02;1;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;1 is local;Mel Gorman;2012-11-02;0;0
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"1.2
	would imply that remote accesses are 20% more expensive
Balancing cost = Cpte * numa_pte_updates +
		Cnumahint * numa_hint_faults +
		Ci * numa_pages_migrated +
		Cpagecopy * numa_pages_migrated
Note that numa_pages_migrated is used as a measure of how many pages
were isolated even though it would miss pages that failed to migrate";Mel Gorman;2012-11-02;0;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"A
vmstat counter could have been added for it but the isolation cost is
pretty marginal in comparison to the overall cost so it seemed overkill";Mel Gorman;2012-11-02;0;0
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"The ideal way to measure automatic placement benefit would be to count
the number of remote accesses versus local accesses and do something like
	benefit = (remote_accesses_before - remove_access_after) * Wnuma
but the information is not readily available";Mel Gorman;2012-11-02;1;0
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"As a workload converges, the
expection would be that the number of remote numa hints would reduce to 0";Mel Gorman;2012-11-02;0;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"	convergence = numa_hint_faults_local / numa_hint_faults
		where this is measured for the last N number of
		numa hints recorded";Mel Gorman;2012-11-02;0;1
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"When the workload is fully
		converged the value is 1";Mel Gorman;2012-11-02;0;0
MDY6Q29tbWl0MjMyNTI5ODowM2M1YTZlMTYzMjJjOTk3YmY4ZjI2NDg1MWJmYTNmNTMyYWQ1MTVm;"This can measure if the placement policy is converging and how fast it is
doing it.";Mel Gorman;2012-11-02;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNDljMzNlMWM5OGY4MzA1MDg3MDUxNGYzODA5MDJkYzZkNjE3YmQ1;mm: migrate: Drop the misplaced pages reference count if the target node is full;Mel Gorman;2012-11-27;1;0
MDY6Q29tbWl0MjMyNTI5ODoxNDljMzNlMWM5OGY4MzA1MDg3MDUxNGYzODA5MDJkYzZkNjE3YmQ1;"If we have to avoid migrating to a node that is nearly full, put page
and return zero.";Mel Gorman;2012-11-27;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;mm: migrate: Introduce migrate_misplaced_page();Peter Zijlstra;2012-10-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;"Note: This was originally based on Peter's patch ""mm/migrate: Introduce
	migrate_misplaced_page()"" but borrows extremely heavily from Andrea's
	""autonuma: memory follows CPU algorithm and task/mm_autonuma stats
	collection""";Peter Zijlstra;2012-10-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;"The end result is barely recognisable so signed-offs
	had to be dropped";Peter Zijlstra;2012-10-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;"If original authors are ok with it, I'll
	re-add the signed-off-bys";Peter Zijlstra;2012-10-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;"Add migrate_misplaced_page() which deals with migrating pages from
faults";Peter Zijlstra;2012-10-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3MDM5ZTFkYmVjNmVlYWE4ZWNhYjQzYTgyZDY1ODllZWNlZDk5NWMz;"Based-on-work-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
Based-on-work-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
Based-on-work-by: Andrea Arcangeli <aarcange@redhat.com>";Peter Zijlstra;2012-10-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YjJhMmQ0YTE4ZmZmYWMzYzQ4NzIwMjE1MjliMDY1Nzg5NmRiNzg4;mm: migrate: Add a tracepoint for migrate_pages;Mel Gorman;2012-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YjJhMmQ0YTE4ZmZmYWMzYzQ4NzIwMjE1MjliMDY1Nzg5NmRiNzg4;"The pgmigrate_success and pgmigrate_fail vmstat counters tells the user
about migration activity but not the type or the reason";Mel Gorman;2012-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YjJhMmQ0YTE4ZmZmYWMzYzQ4NzIwMjE1MjliMDY1Nzg5NmRiNzg4;"This patch adds
a tracepoint to identify the type of page migration and why the page is
being migrated.";Mel Gorman;2012-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NjQ3YmMyOTNhYjE1ZjY2YTdiMWNkYTg1MGM1ZTlkMTYyYTZjN2My;mm: compaction: Move migration fail/success stats to migrate.c;Mel Gorman;2012-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjQ3YmMyOTNhYjE1ZjY2YTdiMWNkYTg1MGM1ZTlkMTYyYTZjN2My;"The compact_pages_moved and compact_pagemigrate_failed events are
convenient for determining if compaction is active and to what
degree migration is succeeding but it's at the wrong level";Mel Gorman;2012-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NjQ3YmMyOTNhYjE1ZjY2YTdiMWNkYTg1MGM1ZTlkMTYyYTZjN2My;"Other
users of migration may also want to know if migration is working
properly and this will be particularly true for any automated
NUMA migration";Mel Gorman;2012-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjQ3YmMyOTNhYjE1ZjY2YTdiMWNkYTg1MGM1ZTlkMTYyYTZjN2My;"This patch moves the counters down to migration
with the new events called pgmigrate_success and pgmigrate_fail";Mel Gorman;2012-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NjQ3YmMyOTNhYjE1ZjY2YTdiMWNkYTg1MGM1ZTlkMTYyYTZjN2My;"The compact_blocks_moved counter is removed because while it was
useful for debugging initially, it's worthless now as no meaningful
conclusions can be drawn from its value.";Mel Gorman;2012-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;mm: memcg: fix compaction/migration failing due to memcg limits;Johannes Weiner;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;"Compaction (and page migration in general) can currently be hindered
through pages being owned by memory cgroups that are at their limits and
unreclaimable";Johannes Weiner;2012-07-31;0;0
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;"The reason is that the replacement page is being charged against the limit
while the page being replaced is also still charged";Johannes Weiner;2012-07-31;0;0
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;" But this seems
unnecessary, given that only one of the two pages will still be in use
after migration finishes";Johannes Weiner;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;"This patch changes the memcg migration sequence so that the replacement
page is not charged";Johannes Weiner;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;" Whatever page is still in use after successful or
failed migration gets to keep the charge of the page that was going to be
replaced";Johannes Weiner;2012-07-31;0;0
MDY6Q29tbWl0MjMyNTI5ODowMDMwZjUzNWE1Y2Y5YjE4NDFkMjA4OGMxMGEwYjJmOGYyOTg3NDYw;"The replacement page will still show up temporarily in the rss/cache
statistics, this can be fixed in a later patch as it's less urgent.";Johannes Weiner;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo4ZTZhYzdmYWIzNzQ4MTZkZTlhOGIwYThmYmIwMmVmNzYxYTMwZmY0;hugetlb/cgroup: migrate hugetlb cgroup info from oldpage to new page during migration;Aneesh Kumar K.V;2012-07-31;1;0
MDY6Q29tbWl0MjMyNTI5ODo4ZTZhYzdmYWIzNzQ4MTZkZTlhOGIwYThmYmIwMmVmNzYxYTMwZmY0;"With HugeTLB pages, hugetlb cgroup is uncharged in compound page
destructor";Aneesh Kumar K.V;2012-07-31;0;0
MDY6Q29tbWl0MjMyNTI5ODo4ZTZhYzdmYWIzNzQ4MTZkZTlhOGIwYThmYmIwMmVmNzYxYTMwZmY0;" Since we are holding a hugepage reference, we can be sure
that old page won't get uncharged till the last put_page().";Aneesh Kumar K.V;2012-07-31;0;1
MDY6Q29tbWl0MjMyNTI5ODoxODllYmZmMjg5NGE5ZDBmNGUyNTBkZDFlMTU0ZDI4MmVmMGE2Nzc5;hugetlb: simplify migrate_huge_page();Aneesh Kumar K.V;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODllYmZmMjg5NGE5ZDBmNGUyNTBkZDFlMTU0ZDI4MmVmMGE2Nzc5;"Since we migrate only one hugepage, don't use linked list for passing the
page around";Aneesh Kumar K.V;2012-07-31;1;0
MDY6Q29tbWl0MjMyNTI5ODoxODllYmZmMjg5NGE5ZDBmNGUyNTBkZDFlMTU0ZDI4MmVmMGE2Nzc5; Directly pass the page that need to be migrated as argument;Aneesh Kumar K.V;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODoxODllYmZmMjg5NGE5ZDBmNGUyNTBkZDFlMTU0ZDI4MmVmMGE2Nzc5;This also removes the usage of page->lru in the migrate path.;Aneesh Kumar K.V;2012-07-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;mm: fix warning in __set_page_dirty_nobuffers;Hugh Dickins;2012-06-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;"New tmpfs use of !PageUptodate pages for fallocate() is triggering the
WARNING: at mm/page-writeback.c:1990 when __set_page_dirty_nobuffers()
is called from migrate_page_copy() for compaction";Hugh Dickins;2012-06-02;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;"It is anomalous that migration should use __set_page_dirty_nobuffers()
on an address_space that does not participate in dirty and writeback
accounting; and this has also been observed to insert surprising dirty
tags into a tmpfs radix_tree, despite tmpfs not using tags at all";Hugh Dickins;2012-06-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;"We should probably give migrate_page_copy() a better way to preserve the
tag and migrate accounting info, when mapping_cap_account_dirty()";Hugh Dickins;2012-06-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;" But
that needs some more work: so in the interim, avoid the warning by using
a simple SetPageDirty on PageSwapBacked pages";Hugh Dickins;2012-06-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTJkYzE4NWRhY2JhMWVkY2JhNDI1ZTY3ZmM2ZGYzYzc3OTNhNWMz;Reported-and-tested-by: Dave Jones <davej@redhat.com>;Hugh Dickins;2012-06-02;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzhhODZlYjE5NmQ5NDAyMjk5OTE5NDU2ZmUzZjI4ZTQ5MGM3NmZh;userns: Convert the move_pages, and migrate_pages permission checks to use uid_eq;Eric W. Biederman;2012-03-12;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMzhhODZlYjE5NmQ5NDAyMjk5OTE5NDU2ZmUzZjI4ZTQ5MGM3NmZh;;Eric W. Biederman;2012-03-12;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZThiMDllYWYyNjhiY2VhYzBjNjJlMzg5YjRiYzBjYjgzZGZiOGU1;mm: fix NULL ptr dereference in move_pages;Sasha Levin;2012-04-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZThiMDllYWYyNjhiY2VhYzBjNjJlMzg5YjRiYzBjYjgzZGZiOGU1;"Commit 3268c63 (""mm: fix move/migrate_pages() race on task struct"") has
added an odd construct where 'mm' is checked for being NULL, and if it is,
it would get dereferenced anyways by mput()ing it.";Sasha Levin;2012-04-25;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;mm: fix move/migrate_pages() race on task struct;Christoph Lameter;2012-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;Migration functions perform the rcu_read_unlock too early;Christoph Lameter;2012-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;" As a result
the task pointed to may change from under us";Christoph Lameter;2012-03-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;" This can result in an oops,
as reported by Dave Hansen in 
The following patch extend the period of the rcu_read_lock until after the
permissions checks are done";Christoph Lameter;2012-03-21;1;0
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;" We also take a refcount so that the task
reference is stable when calling security check functions and performing
cpuset node validation (which takes a mutex)";Christoph Lameter;2012-03-21;1;0
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;"The refcount is dropped before actual page migration occurs so there is no
change to the refcounts held during page migration";Christoph Lameter;2012-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;"Also move the determination of the mm of the task struct to immediately
before the do_migrate*() calls so that it is clear that we switch from
handling the task during permission checks to the mm for the actual
migration";Christoph Lameter;2012-03-21;0;1
MDY6Q29tbWl0MjMyNTI5ODozMjY4YzYzZWRlZDQ2MTJhM2QwN2I1NmQxZTAyY2U3NzMxZTY2MDhl;" Since the determination is only done once and we then no
longer use the task_struct we can be sure that we operate on a specific
address space that will not change from under us.";Christoph Lameter;2012-03-21;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;memcg: fix GPF when cgroup removal races with last exit;Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"When moving tasks from old memcg (with move_charge_at_immigrate on new
memcg), followed by removal of old memcg, hit General Protection Fault in
mem_cgroup_lru_del_list() (called from release_pages called from
free_pages_and_swap_cache from tlb_flush_mmu from tlb_finish_mmu from
exit_mmap from mmput from exit_mm from do_exit)";Hugh Dickins;2012-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"Somewhat reproducible, takes a few hours: the old struct mem_cgroup has
been freed and poisoned by SLAB_DEBUG, but mem_cgroup_lru_del_list() is
still trying to update its stats, and take page off lru before freeing";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"A task, or a charge, or a page on lru: each secures a memcg against
removal";Hugh Dickins;2012-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;" In this case, the last task has been moved out of the old memcg,
and it is exiting: anonymous pages are uncharged one by one from the
memcg, as they are zapped from its pagetables, so the charge gets down to
0; but the pages themselves are queued in an mmu_gather for freeing";Hugh Dickins;2012-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"Most of those pages will be on lru (and force_empty is careful to
lru_add_drain_all, to add pages from pagevec to lru first), but not
necessarily all: perhaps some have been isolated for page reclaim, perhaps
some isolated for other reasons";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;" So, force_empty may find no task, no
charge and no page on lru, and let the removal proceed";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"There would still be no problem if these pages were immediately freed; but
typically (and the put_page_testzero protocol demands it) they have to be
added back to lru before they are found freeable, then removed from lru
and freed";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;" We don't see the issue when adding, because the
mem_cgroup_iter() loops keep their own reference to the memcg being
scanned; but when it comes to mem_cgroup_lru_del_list()";Hugh Dickins;2012-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"I believe this was not an issue in v3.2: there, PageCgroupAcctLRU and
PageCgroupUsed flags were used (like a trick with mirrors) to deflect view
of pc->mem_cgroup to the stable root_mem_cgroup when neither set";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"38c5d72f3ebe (""memcg: simplify LRU handling by new rule"") mercifully
removed those convolutions, but left this General Protection Fault";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"But it's surprisingly easy to restore the old behaviour: just check
PageCgroupUsed in mem_cgroup_lru_add_list() (which decides on which lruvec
to add), and reset pc to root_mem_cgroup if page is uncharged";Hugh Dickins;2012-03-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;" A risky
change?  just going back to how it worked before; testing, and an audit of
uses of pc->mem_cgroup, show no problem";Hugh Dickins;2012-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"And there's a nice bonus: with mem_cgroup_lru_add_list() itself making
sure that an uncharged page goes to root lru, mem_cgroup_reset_owner() no
longer has any purpose, and we can safely revert 4e5f01c2b9b9 (""memcg";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"clear pc->mem_cgroup if necessary"")";Hugh Dickins;2012-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NTEyMTAyY2Y2NGQzNmUzYzc0NDQ0ODAyNzM2MjNjN2FhYjM1NjNm;"Calling update_page_reclaim_stat() after add_page_to_lru_list() in swap.c
is not strictly necessary: the lru_lock there, with RCU before memcg
structures are freed, makes mem_cgroup_get_reclaim_stat_from_page safe
without that; but it seems cleaner to rely on one dependency less.";Hugh Dickins;2012-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODozNTUxMmVjYWVmMDMyNTBmZTUwYWQ4MTQzMGRkNDY3ZjAxZDlhOTZi;mm: postpone migrated page mapping reset;Konstantin Khlebnikov;2012-02-03;1;1
MDY6Q29tbWl0MjMyNTI5ODozNTUxMmVjYWVmMDMyNTBmZTUwYWQ4MTQzMGRkNDY3ZjAxZDlhOTZi;Postpone resetting page->mapping until the final remove_migration_ptes();Konstantin Khlebnikov;2012-02-03;1;0
MDY6Q29tbWl0MjMyNTI5ODozNTUxMmVjYWVmMDMyNTBmZTUwYWQ4MTQzMGRkNDY3ZjAxZDlhOTZi;"Otherwise the expression PageAnon(migration_entry_to_page(entry)) does not
work.";Konstantin Khlebnikov;2012-02-03;0;0
MDY6Q29tbWl0MjMyNTI5ODphNmJjMzJiODk5MjIzYTg3N2Y1OTVlZjlkZGMxZTg5ZWFkNTA3MmI4;mm: compaction: introduce sync-light migration for use by compaction;Mel Gorman;2012-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODphNmJjMzJiODk5MjIzYTg3N2Y1OTVlZjlkZGMxZTg5ZWFkNTA3MmI4;"This patch adds a lightweight sync migrate operation MIGRATE_SYNC_LIGHT
mode that avoids writing back pages to backing storage";Mel Gorman;2012-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODphNmJjMzJiODk5MjIzYTg3N2Y1OTVlZjlkZGMxZTg5ZWFkNTA3MmI4;" Async compaction
maps to MIGRATE_ASYNC while sync compaction maps to MIGRATE_SYNC_LIGHT";Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODphNmJjMzJiODk5MjIzYTg3N2Y1OTVlZjlkZGMxZTg5ZWFkNTA3MmI4;"For other migrate_pages users such as memory hotplug, MIGRATE_SYNC is
used";Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODphNmJjMzJiODk5MjIzYTg3N2Y1OTVlZjlkZGMxZTg5ZWFkNTA3MmI4;"This avoids sync compaction stalling for an excessive length of time,
particularly when copying files to a USB stick where there might be a
large number of dirty pages backed by a filesystem that does not support
->writepages.";Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;mm: compaction: determine if dirty pages can be migrated without blocking within ->migratepage;Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;"Asynchronous compaction is used when allocating transparent hugepages to
avoid blocking for long periods of time";Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;" Due to reports of stalling,
there was a debate on disabling synchronous compaction but this severely
impacted allocation success rates";Mel Gorman;2012-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;" Part of the reason was that many dirty
	if (PageDirty(page) && !sync &&
		mapping->a_ops->migratepage != migrate_page)
This skips over all mapping aops using buffer_migrate_page() even though
it is possible to migrate some of these pages without blocking";Mel Gorman;2012-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;" This
patch updates the ->migratepage callback with a ""sync"" parameter";Mel Gorman;2012-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOTY5YzRhYjlmMTgyYTZlMWIyYTA4NDhiZTM0OWY5OTcxNDk0N2Iw;" It is
the responsibility of the callback to fail gracefully if migration would
block.";Mel Gorman;2012-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;memcg: clear pc->mem_cgroup if necessary.;KAMEZAWA Hiroyuki;2012-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;"This is a preparation before removing a flag PCG_ACCT_LRU in page_cgroup
and reducing atomic ops/complexity in memcg LRU handling";KAMEZAWA Hiroyuki;2012-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;"In some cases, pages are added to lru before charge to memcg and pages
are not classfied to memory cgroup at lru addtion";KAMEZAWA Hiroyuki;2012-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;" Now, the lru where
the page should be added is determined a bit in page_cgroup->flags and
pc->mem_cgroup";KAMEZAWA Hiroyuki;2012-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0; I'd like to remove the check of flag;KAMEZAWA Hiroyuki;2012-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;"To handle the case pc->mem_cgroup may contain stale pointers if pages
are added to LRU before classification";KAMEZAWA Hiroyuki;2012-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ZTVmMDFjMmI5Yjk0MzIxOTkyYWNiMDljMzVkMzRmNWVlNWJiMjc0;" This patch resets
pc->mem_cgroup to root_mem_cgroup before lru additions.";KAMEZAWA Hiroyuki;2012-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYWVkODM2YTIzNzFhOTY5MDEwNTdmMzEwZTQzNmEwOWVkZWQ5NGZk;mm/migrate.c: remove the unused macro lru_to_page;Wang Sheng-Hui;2012-01-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpmYWVkODM2YTIzNzFhOTY5MDEwNTdmMzEwZTQzNmEwOWVkZWQ5NGZk;lru_to_page is not used in mm/migrate.c.;Wang Sheng-Hui;2012-01-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjRjODFkYjE5ZjM2MzBmNTNhMTRiYmNlYjdiODVlYjk2NjBkZWQz;mm/migrate.c: cleanup comment for migration_entry_wait();Wang Sheng-Hui;2012-01-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NjRjODFkYjE5ZjM2MzBmNTNhMTRiYmNlYjdiODVlYjk2NjBkZWQz;migration_entry_wait() can also be called from hugetlb_fault() now;Wang Sheng-Hui;2012-01-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo1NjRjODFkYjE5ZjM2MzBmNTNhMTRiYmNlYjdiODVlYjk2NjBkZWQz;Remove the incorrect comment.;Wang Sheng-Hui;2012-01-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzdhOTRjOWRiMzBhODE4YmFhNWUyYzA5ZGJmNDU4OTI1MTM1NWMz;mm: migrate: one less atomic operation;Jacobo Giralt;2012-01-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzdhOTRjOWRiMzBhODE4YmFhNWUyYzA5ZGJmNDU4OTI1MTM1NWMz;"migrate_page_move_mapping() drops a reference from the old page after
unfreezing its counter";Jacobo Giralt;2012-01-10;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzdhOTRjOWRiMzBhODE4YmFhNWUyYzA5ZGJmNDU4OTI1MTM1NWMz;" Both operations can be merged into a single
atomic operation by directly unfreezing to one less reference";Jacobo Giralt;2012-01-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzdhOTRjOWRiMzBhODE4YmFhNWUyYzA5ZGJmNDU4OTI1MTM1NWMz;The same applies to migrate_huge_page_move_mapping().;Jacobo Giralt;2012-01-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowOTc2MTMzM2VkNDdlODk5Y2MxNDgyYzEzMDkwYjk1ZjNmNzExOTcx;mm/migrate.c: pair unlock_page() and lock_page() when migrating huge pages;Hillf Danton;2011-12-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowOTc2MTMzM2VkNDdlODk5Y2MxNDgyYzEzMDkwYjk1ZjNmNzExOTcx;Avoid unlocking and unlocked page if we failed to lock it.;Hillf Danton;2011-12-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowZGFiZWM5M2RlNjMzYTg3YWRmYmJlMWQ4MDBhNGM1NmNkMTlkNzNi;mm: migration: clean up unmap_and_move();Minchan Kim;2011-11-01;1;1
MDY6Q29tbWl0MjMyNTI5ODowZGFiZWM5M2RlNjMzYTg3YWRmYmJlMWQ4MDBhNGM1NmNkMTlkNzNi;unmap_and_move() is one a big messy function;Minchan Kim;2011-11-01;0;0
MDY6Q29tbWl0MjMyNTI5ODowZGFiZWM5M2RlNjMzYTg3YWRmYmJlMWQ4MDBhNGM1NmNkMTlkNzNi; Clean it up.;Minchan Kim;2011-11-01;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOTVmMWIzMWI3NTU4ODMwNmUzMmIyYWZkMzIxNjZjYWQ0OGY2NzBi;mm: Map most files to use export.h instead of module.h;Paul Gortmaker;2011-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOTVmMWIzMWI3NTU4ODMwNmUzMmIyYWZkMzIxNjZjYWQ0OGY2NzBi;"The files changed within are only using the EXPORT_SYMBOL
macro variants";Paul Gortmaker;2011-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpiOTVmMWIzMWI3NTU4ODMwNmUzMmIyYWZkMzIxNjZjYWQ0OGY2NzBi;" They are not using core modular infrastructure
and hence don't need module.h but only the export.h header.";Paul Gortmaker;2011-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;mm: fix race between mremap and removing migration entry;Hugh Dickins;2011-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;"I don't usually pay much attention to the stale ""? "" addresses in
stack backtraces, but this lucky report from Pawel Sikora hints that
mremap's move_ptes() has inadequate locking against page migration";Hugh Dickins;2011-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll; 3.0 BUG_ON(!PageLocked(p)) in migration_entry_to_page();Hugh Dickins;2011-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;" kernel BUG at include/linux/swapops.h:105!
 RIP: 0010:[<ffffffff81127b76>]  [<ffffffff81127b76>]
                       migration_entry_wait+0x156/0x160
  [<ffffffff811016a1>] handle_pte_fault+0xae1/0xaf0
  [<ffffffff810feee2>] ? __pte_alloc+0x42/0x120
  [<ffffffff8112c26b>] ? do_huge_pmd_anonymous_page+0xab/0x310
  [<ffffffff81102a31>] handle_mm_fault+0x181/0x310
  [<ffffffff81106097>] ? vma_adjust+0x537/0x570
  [<ffffffff81424bed>] do_page_fault+0x11d/0x4e0
  [<ffffffff81109a05>] ? do_mremap+0x2d5/0x570
  [<ffffffff81421d5f>] page_fault+0x1f/0x30
mremap's down_write of mmap_sem, together with i_mmap_mutex or lock,
and pagetable locks, were good enough before page migration (with its
requirement that every migration entry be found) came in, and enough
while migration always held mmap_sem; but not enough nowadays, when
there's memory hotremove and compaction";Hugh Dickins;2011-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;"The danger is that move_ptes() lets a migration entry dodge around
behind remove_migration_pte()'s back, so it's in the old location when
looking at the new, then in the new location when looking at the old";Hugh Dickins;2011-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;"Either mremap's move_ptes() must additionally take anon_vma lock(), or
migration's remove_migration_pte() must stop peeking for is_swap_entry()
before it takes pagetable lock";Hugh Dickins;2011-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;"Consensus chooses the latter: we prefer to add overhead to migration
than to mremapping, which gets used by JVMs and by exec stack setup";Hugh Dickins;2011-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo0ODZjZjQ2ZjNmOWJlNWYyYTk2NjAxNmMxYThmZTAxZTMyY2RlMDll;Reported-and-tested-by: Pawe Sikora <pluto@agmk.net>;Hugh Dickins;2011-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo5OWExNWUyMWQ5NmY2ODU3ZGFmYWIxZTUxNjdlNWU4MTgzMjE1Yzlj;migrate: don't account swapcache as shmem;Andrea Arcangeli;2011-06-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5OWExNWUyMWQ5NmY2ODU3ZGFmYWIxZTUxNjdlNWU4MTgzMjE1Yzlj;"swapcache will reach the below code path in migrate_page_move_mapping,
and swapcache is accounted as NR_FILE_PAGES but it's not accounted as
NR_SHMEM";Andrea Arcangeli;2011-06-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5OWExNWUyMWQ5NmY2ODU3ZGFmYWIxZTUxNjdlNWU4MTgzMjE1Yzlj;"Hugh pointed out we must use PageSwapCache instead of comparing
mapping to &swapper_space, to avoid build failure with CONFIG_SWAP=n.";Andrea Arcangeli;2011-06-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDZiMThkNDIxZGE3ZjI3ZTk0OGU4YWYxYWQ4MmI2ZDAzMDkzMjRk;mm: use refcounts for page_lock_anon_vma();Peter Zijlstra;2011-05-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDZiMThkNDIxZGE3ZjI3ZTk0OGU4YWYxYWQ4MmI2ZDAzMDkzMjRk;Convert page_lock_anon_vma() over to use refcounts;Peter Zijlstra;2011-05-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDZiMThkNDIxZGE3ZjI3ZTk0OGU4YWYxYWQ4MmI2ZDAzMDkzMjRk;" This is done to
prepare for the conversion of anon_vma from spinlock to mutex";Peter Zijlstra;2011-05-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDZiMThkNDIxZGE3ZjI3ZTk0OGU4YWYxYWQ4MmI2ZDAzMDkzMjRk;"Sadly this inceases the cost of page_lock_anon_vma() from one to two
atomics, a follow up patch addresses this, lets keep that simple for now.";Peter Zijlstra;2011-05-25;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTk4NWVkY2VkZWE2Mzk2Mjc3MDAzODU0NjU3YjVmM2NiMzFhNjI4;Fix common misspellings;Lucas De Marchi;2011-03-31;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNTk4NWVkY2VkZWE2Mzk2Mjc3MDAzODU0NjU3YjVmM2NiMzFhNjI4;Fixes generated by 'codespell' and manually reviewed.;Lucas De Marchi;2011-03-31;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;memcg: fix ugly initialization of return value is in caller;KAMEZAWA Hiroyuki;2011-03-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;Remove initialization of vaiable in caller of memory cgroup function;KAMEZAWA Hiroyuki;2011-03-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;"Actually, it's return value of memcg function but it's initialized in
caller";KAMEZAWA Hiroyuki;2011-03-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;"Some memory cgroup uses following style to bring the result of start
function to the end function for avoiding races";KAMEZAWA Hiroyuki;2011-03-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;"   mem_cgroup_start_A(&(*ptr))
   mem_cgroup_end_A(*ptr)
In some calls, *ptr should be initialized to NULL be caller";KAMEZAWA Hiroyuki;2011-03-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl;" But it's
ugly";KAMEZAWA Hiroyuki;2011-03-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NjAzOWVmYTE4ZjI1MzBmYzIzZThlZjE5ZTcxNmI2NWVlMmExZDFl; This patch fixes that *ptr is initialized by _start function.;KAMEZAWA Hiroyuki;2011-03-23;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;mm: compaction: Use async migration for __GFP_NO_KSWAPD and enforce no writeback;Andrea Arcangeli;2011-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;"__GFP_NO_KSWAPD allocations are usually very expensive and not mandatory
to succeed as they have graceful fallback";Andrea Arcangeli;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;" Waiting for I/O in those,
tends to be overkill in terms of latencies, so we can reduce their latency
by disabling sync migrate";Andrea Arcangeli;2011-03-22;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;"Unfortunately, even with async migration it's still possible for the
process to be blocked waiting for a request slot (e.g";Andrea Arcangeli;2011-03-22;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;" get_request_wait
in the block layer) when ->writepage is called";Andrea Arcangeli;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;" To prevent
__GFP_NO_KSWAPD blocking, this patch prevents ->writepage being called on
dirty page cache for asynchronous migration";Andrea Arcangeli;2011-03-22;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMWJjODJkNjdkMTE1MDc2NzkwMWJjYTU0YTI0NDY2NjIxZDc2M2Q3;Addresses ;Andrea Arcangeli;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTYwMTA5ZjEyNTAxM2I2YzU3MWYzOTlhMTVhOGIwZmUxZmZhNGU2;mm: rename drop_anon_vma() to put_anon_vma();Peter Zijlstra;2011-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ZTYwMTA5ZjEyNTAxM2I2YzU3MWYzOTlhMTVhOGIwZmUxZmZhNGU2;The normal code pattern used in the kernel is: get/put.;Peter Zijlstra;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODplZjZhM2M2MzExMmU4NjVkNjMyZmY3YzQ3OGJhN2M3MTYwY2FkMGQx;mm: add replace_page_cache_page() function;Miklos Szeredi;2011-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjZhM2M2MzExMmU4NjVkNjMyZmY3YzQ3OGJhN2M3MTYwY2FkMGQx;This function basically does;Miklos Szeredi;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODplZjZhM2M2MzExMmU4NjVkNjMyZmY3YzQ3OGJhN2M3MTYwY2FkMGQx;"Except it does this atomically, so there's no possibility for the ""add"" to
fail because of a race";Miklos Szeredi;2011-03-22;1;1
MDY6Q29tbWl0MjMyNTI5ODplZjZhM2M2MzExMmU4NjVkNjMyZmY3YzQ3OGJhN2M3MTYwY2FkMGQx;"If memory cgroups are enabled, then the memory cgroup charge is also moved
from the old page to the new";Miklos Szeredi;2011-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODplZjZhM2M2MzExMmU4NjVkNjMyZmY3YzQ3OGJhN2M3MTYwY2FkMGQx;"This function is currently used by fuse to move pages into the page cache
on read, instead of copying the page contents.";Miklos Szeredi;2011-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODphODc5YmY1ODJkZmIzYTc5ZDMwZDc2Y2EzYWYyYWU4YTBmMzkwMTBj;mm: grab rcu read lock in move_pages();Greg Thelen;2011-02-25;1;0
MDY6Q29tbWl0MjMyNTI5ODphODc5YmY1ODJkZmIzYTc5ZDMwZDc2Y2EzYWYyYWU4YTBmMzkwMTBj;"The move_pages() usage of find_task_by_vpid() requires rcu_read_lock() to
prevent free_pid() from reclaiming the pid";Greg Thelen;2011-02-25;0;1
MDY6Q29tbWl0MjMyNTI5ODphODc5YmY1ODJkZmIzYTc5ZDMwZDc2Y2EzYWYyYWU4YTBmMzkwMTBj;"Without this patch, RCU warnings are printed in v2.6.38-rc4 move_pages()
with";Greg Thelen;2011-02-25;1;1
MDY6Q29tbWl0MjMyNTI5ODphODc5YmY1ODJkZmIzYTc5ZDMwZDc2Y2EzYWYyYWU4YTBmMzkwMTBj;"  CONFIG_LOCKUP_DETECTOR=y
  CONFIG_PREEMPT=y
  CONFIG_LOCKDEP=y
  CONFIG_PROVE_LOCKING=y
  CONFIG_PROVE_RCU=y
Previously, migrate_pages() went through a similar transformation
replacing usage of tasklist_lock with rcu read lock";Greg Thelen;2011-02-25;1;1
MDY6Q29tbWl0MjMyNTI5ODphODc5YmY1ODJkZmIzYTc5ZDMwZDc2Y2EzYWYyYWU4YTBmMzkwMTBj;"  commit 55cfaa3cbdd29c4919ecb5fb8965c310f357e48c
  Author: Zeng Zhaoming <zengzm.kernel@gmail.com>
  Date:   Thu Dec 2 14:31:13 2010 -0800
      mm/mempolicy.c: add rcu read lock to protect pid structure
  commit 1e50df39f6e2c3a4a3394df62baa8a213df16c54
  Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
  Date:   Thu Jan 13 15:46:14 2011 -0800
      mempolicy: remove tasklist_lock from migrate_pages";Greg Thelen;2011-02-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo0OGRiNTRlZTJmNDFlOGFlMmZhZjMzMGI1NWRiMzRhOWZmZmI1YjNj;mm/migration: fix page corruption during hugepage migration;Minchan Kim;2011-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGRiNTRlZTJmNDFlOGFlMmZhZjMzMGI1NWRiMzRhOWZmZmI1YjNj;"If migrate_huge_page by memory-failure fails , it calls put_page in itself
to decrease page reference and caller of migrate_huge_page also calls
putback_lru_pages";Minchan Kim;2011-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo0OGRiNTRlZTJmNDFlOGFlMmZhZjMzMGI1NWRiMzRhOWZmZmI1YjNj;" It can do double free of page so it can make page
corruption on page holder";Minchan Kim;2011-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0OGRiNTRlZTJmNDFlOGFlMmZhZjMzMGI1NWRiMzRhOWZmZmI1YjNj;"In addtion, clean of pages on caller is consistent behavior with
migrate_pages by cf608ac19c (""mm: compaction: fix COMPACTPAGEFAILED
counting"").";Minchan Kim;2011-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo1N2ZjNGE1ZWUzMjJjZGU5NmMzM2YxMDFkM2MyZDNiNzkwMTFjMDVj;mm: when migrate_pages returns 0, all pages must have been released;Andrea Arcangeli;2011-02-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo1N2ZjNGE1ZWUzMjJjZGU5NmMzM2YxMDFkM2MyZDNiNzkwMTFjMDVj;"In some cases migrate_pages could return zero while still leaving a few
pages in the pagelist (and some caller wouldn't notice it has to call
putback_lru_pages after commit cf608ac19c9 (""mm: compaction: fix
COMPACTPAGEFAILED counting""))";Andrea Arcangeli;2011-02-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo1N2ZjNGE1ZWUzMjJjZGU5NmMzM2YxMDFkM2MyZDNiNzkwMTFjMDVj;"Add one missing putback_lru_pages not added by commit cf608ac19c95 (""mm";Andrea Arcangeli;2011-02-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo1N2ZjNGE1ZWUzMjJjZGU5NmMzM2YxMDFkM2MyZDNiNzkwMTFjMDVj;"compaction: fix COMPACTPAGEFAILED counting"").";Andrea Arcangeli;2011-02-01;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOGJkNjU3ODFjODQ4ZDk1YmE2YTdmNThiNWM0YjgyNjVhODA0ZWM2;mm: migration: clarify migrate_pages() comment;Minchan Kim;2011-01-25;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOGJkNjU3ODFjODQ4ZDk1YmE2YTdmNThiNWM0YjgyNjVhODA0ZWM2;"Callers of migrate_pages should putback_lru_pages to return pages
isolated to LRU or free list";Minchan Kim;2011-01-25;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOGJkNjU3ODFjODQ4ZDk1YmE2YTdmNThiNWM0YjgyNjVhODA0ZWM2; Now comment is rather confusing;Minchan Kim;2011-01-25;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOGJkNjU3ODFjODQ4ZDk1YmE2YTdmNThiNWM0YjgyNjVhODA0ZWM2;" It says
caller always have to call it";Minchan Kim;2011-01-25;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOGJkNjU3ODFjODQ4ZDk1YmE2YTdmNThiNWM0YjgyNjVhODA0ZWM2;"It is more clear to point out that the caller has to call it if
migrate_pages's return value isn't zero.";Minchan Kim;2011-01-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo1MGRlMWRkOTY3ZDRiYTNiOGE5MGViZTdhNGY1ZmVjYTI0MTkxMzE3;memcg: fix memory migration of shmem swapcache;Daisuke Nishimura;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MGRlMWRkOTY3ZDRiYTNiOGE5MGViZTdhNGY1ZmVjYTI0MTkxMzE3;"In the current implementation mem_cgroup_end_migration() decides whether
the page migration has succeeded or not by checking ""oldpage->mapping""";Daisuke Nishimura;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MGRlMWRkOTY3ZDRiYTNiOGE5MGViZTdhNGY1ZmVjYTI0MTkxMzE3;"But if we are tring to migrate a shmem swapcache, the page->mapping of it
is NULL from the begining, so the check would be invalid";Daisuke Nishimura;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo1MGRlMWRkOTY3ZDRiYTNiOGE5MGViZTdhNGY1ZmVjYTI0MTkxMzE3;" As a result,
mem_cgroup_end_migration() assumes the migration has succeeded even if
it's not, so ""newpage"" would be freed while it's not uncharged";Daisuke Nishimura;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo1MGRlMWRkOTY3ZDRiYTNiOGE5MGViZTdhNGY1ZmVjYTI0MTkxMzE3;"This patch fixes it by passing mem_cgroup_end_migration() the result of
the page migration.";Daisuke Nishimura;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZDRhNDY2M2RiMjkzYmZkNWRjMjBmYjQxMTM5NzdmNjI4OTVlNTUw;mm: fix hugepage migration;Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZDRhNDY2M2RiMjkzYmZkNWRjMjBmYjQxMTM5NzdmNjI4OTVlNTUw;"2.6.37 added an unmap_and_move_huge_page() for memory failure recovery,
but its anon_vma handling was still based around the 2.6.35 conventions";Hugh Dickins;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpmZDRhNDY2M2RiMjkzYmZkNWRjMjBmYjQxMTM5NzdmNjI4OTVlNTUw;"Update it to use page_lock_anon_vma, get_anon_vma, page_unlock_anon_vma,
drop_anon_vma in the same way as we're now changing unmap_and_move()";Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpmZDRhNDY2M2RiMjkzYmZkNWRjMjBmYjQxMTM5NzdmNjI4OTVlNTUw;"I don't particularly like to propose this for stable when I've not seen
its problems in practice nor tested the solution: but it's clearly out of
synch at present.";Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoxY2U4MmI2OWU5NmM4MzhkMDA3ZjMxNmI4MzQ3YjkxMWZkZmE5ODQy;mm: fix migration hangs on anon_vma lock;Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoxY2U4MmI2OWU5NmM4MzhkMDA3ZjMxNmI4MzQ3YjkxMWZkZmE5ODQy;"Increased usage of page migration in mmotm reveals that the anon_vma
locking in unmap_and_move() has been deficient since 2.6.36 (or even
earlier)";Hugh Dickins;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODoxY2U4MmI2OWU5NmM4MzhkMDA3ZjMxNmI4MzQ3YjkxMWZkZmE5ODQy;" Review at the time of f18194275c39835cb84563500995e0d503a32d9a
(""mm: fix hang on anon_vma->root->lock"") missed the issue here: the
anon_vma to which we get a reference may already have been freed back to
its slab (it is in use when we check page_mapped, but that can change),
and so its anon_vma->root may be switched at any moment by reuse in
anon_vma_prepare";Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoxY2U4MmI2OWU5NmM4MzhkMDA3ZjMxNmI4MzQ3YjkxMWZkZmE5ODQy;"Perhaps we could fix that with a get_anon_vma_unless_zero(), but let's
not: just rely on page_lock_anon_vma() to do all the hard thinking for us,
then we don't need any rcu read locking over here";Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoxY2U4MmI2OWU5NmM4MzhkMDA3ZjMxNmI4MzQ3YjkxMWZkZmE5ODQy;"In removing the rcu_unlock label: since PageAnon is a bit in
page->mapping, it's impossible for a !page->mapping page to be anon; but
insert VM_BUG_ON in case the implementation ever changes.";Hugh Dickins;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;mm: migration: use rcu_dereference_protected when dereferencing the radix tree slot during file page migration;Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;"migrate_pages() -> unmap_and_move() only calls rcu_read_lock() for
anonymous pages, as introduced by git commit
989f89c57e6361e7d16fbd9572b5da7d313b073d (""fix rcu_read_lock() in page
migraton"")";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;" The point of the RCU protection there is part of getting a
stable reference to anon_vma and is only held for anon pages as file pages
are locked which is sufficient protection against freeing";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;"However, while a file page's mapping is being migrated, the radix tree is
double checked to ensure it is the expected page";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;" This uses
radix_tree_deref_slot() -> rcu_dereference() without the RCU lock held
triggering the following warning";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3; stack backtrace;Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;"This patch introduces radix_tree_deref_slot_protected() which calls
rcu_dereference_protected()";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;" Users of it must pass in the
mapping->tree_lock that is protecting this dereference";Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODoyOWMxZjY3N2Q0MjRlOGM1NjgzYTgzN2ZjNGYwM2ZjOWYxOTIwMWQ3;" Holding the tree
lock protects against parallel updaters of the radix tree meaning that
rcu_dereference_protected is allowable.";Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MDBkNjVkNDcxMDE4ZDlhMTNiMGQ1MWI3ZTE0MWVkMmEzNTU1YzFk;thp: pmd_trans_huge migrate bugcheck;Andrea Arcangeli;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MDBkNjVkNDcxMDE4ZDlhMTNiMGQ1MWI3ZTE0MWVkMmEzNTU1YzFk;"No pmd_trans_huge should ever materialize in migration ptes areas, because
we split the hugepage before migration ptes are instantiated.";Andrea Arcangeli;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZjBmMjQ5NjdiMDM0OTc5ODgwMzI2MGIyZTRiZjM0N2NmZmExOTkw;mm: migration: cleanup migrate_pages API by matching types for offlining and sync;Mel Gorman;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZjBmMjQ5NjdiMDM0OTc5ODgwMzI2MGIyZTRiZjM0N2NmZmExOTkw;"With the introduction of the boolean sync parameter, the API looks a
little inconsistent as offlining is still an int";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZjBmMjQ5NjdiMDM0OTc5ODgwMzI2MGIyZTRiZjM0N2NmZmExOTkw;" Convert offlining to a
bool for the sake of being tidy.";Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo3N2YxZmU2YjA4YjEzYTg3MzkxNTQ5YzhhODIwZGRjODE3YjZmNTBl;mm: migration: allow migration to operate asynchronously and avoid synchronous compaction in the faster path;Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo3N2YxZmU2YjA4YjEzYTg3MzkxNTQ5YzhhODIwZGRjODE3YjZmNTBl;Migration synchronously waits for writeback if the initial passes fails;Mel Gorman;2011-01-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo3N2YxZmU2YjA4YjEzYTg3MzkxNTQ5YzhhODIwZGRjODE3YjZmNTBl;"Callers of memory compaction do not necessarily want this behaviour if the
caller is latency sensitive or expects that synchronous migration is not
going to have a significantly better success rate";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo3N2YxZmU2YjA4YjEzYTg3MzkxNTQ5YzhhODIwZGRjODE3YjZmNTBl;"This patch adds a sync parameter to migrate_pages() allowing the caller to
indicate if wait_on_page_writeback() is allowed within migration or not";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo3N2YxZmU2YjA4YjEzYTg3MzkxNTQ5YzhhODIwZGRjODE3YjZmNTBl;"For reclaim/compaction, try_to_compact_pages() is first called
asynchronously, direct reclaim runs and then try_to_compact_pages() is
called synchronously as there is a greater expectation that it'll succeed.";Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;mm: vmscan: reclaim order-0 and use compaction instead of lumpy reclaim;Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;Lumpy reclaim is disruptive;Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;" It reclaims a large number of pages and
ignores the age of the pages it reclaims";Mel Gorman;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;" This can incur significant
stalls and potentially increase the number of major faults";Mel Gorman;2011-01-13;0;1
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;"Compaction has reached the point where it is considered reasonably stable
(meaning it has passed a lot of testing) and is a potential candidate for
displacing lumpy reclaim";Mel Gorman;2011-01-13;1;1
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;" This patch introduces an alternative to lumpy
reclaim whe compaction is available called reclaim/compaction";Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODozZTdkMzQ0OTcwNjczYzUzMzRjZjdiNWJiMjdjOGMwOTQyYjA2MTI2;" The basic
operation is very simple - instead of selecting a contiguous range of
pages to reclaim, a number of order-0 pages are reclaimed and then
compaction is later by either kswapd (compact_zone_order()) or direct
compaction (__alloc_pages_direct_compact()).";Mel Gorman;2011-01-13;0;0
MDY6Q29tbWl0MjMyNTI5ODowZDE4MzZjMzY2MTU3OTk0NDc0YWZkMjk2MzI5OTIzNzVhM2RkMjBj;mm/migrate.c: fix compilation error;Michal Nazarewicz;2010-12-22;1;1
MDY6Q29tbWl0MjMyNTI5ODowZDE4MzZjMzY2MTU3OTk0NDc0YWZkMjk2MzI5OTIzNzVhM2RkMjBj;GCC complained about update_mmu_cache() not being defined in migrate.c;Michal Nazarewicz;2010-12-22;0;1
MDY6Q29tbWl0MjMyNTI5ODowZDE4MzZjMzY2MTU3OTk0NDc0YWZkMjk2MzI5OTIzNzVhM2RkMjBj;Including <asm/tlbflush.h> seems to solve the problem.;Michal Nazarewicz;2010-12-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM4NGRjNmRjYzZhYTc2NzYyMjAwMjYyODIwYmRiOGI3MjRlY2Q1;mm: fix error reporting in move_pages() syscall;Gleb Natapov;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM4NGRjNmRjYzZhYTc2NzYyMjAwMjYyODIwYmRiOGI3MjRlY2Q1;"The vma returned by find_vma does not necessarily include the target
address";Gleb Natapov;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo3MDM4NGRjNmRjYzZhYTc2NzYyMjAwMjYyODIwYmRiOGI3MjRlY2Q1;" If this happens the code tries to follow a page outside of any
vma and returns ENOENT instead of EFAULT.";Gleb Natapov;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;mm: compaction: fix COMPACTPAGEFAILED counting;Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;Presently update_nr_listpages() doesn't have a role;Minchan Kim;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" That's because lists
passed is always empty just after calling migrate_pages";Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" The
migrate_pages cleans up page list which have failed to migrate before
returning by aaa994b3";Minchan Kim;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" [PATCH] page migration: handle freeing of pages in migrate_pages()
 Do not leave pages on the lists passed to migrate_pages()";Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" Seems that we will
 not need any postprocessing of pages";Minchan Kim;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" This will simplify the handling of
 pages by the callers of migrate_pages()";Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;At that time, we thought we don't need any postprocessing of pages;Minchan Kim;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" But
the situation is changed";Minchan Kim;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" The compaction need to know the number of
failed to migrate for COMPACTPAGEFAILED stat
This patch makes new rule for caller of migrate_pages to call
putback_lru_pages";Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi;" So caller need to clean up the lists so it has a
chance to postprocess the pages";Minchan Kim;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZjYwOGFjMTljOTU4MDRkYzJkZjQzYjFmNGY5ZTA2OGFhOTAzNGFi; [suggested by Christoph Lameter];Minchan Kim;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;writeback: remove nonblocking/encountered_congestion references;Wu Fengguang;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;"This removes more dead code that was somehow missed by commit 0d99519efef
(writeback: remove unused nonblocking and congestion checks)";Wu Fengguang;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;" There are
no behavior change except for the removal of two entries from one of the
ext4 tracing interface";Wu Fengguang;2010-10-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;"The nonblocking checks in ->writepages are no longer used because the
flusher now prefer to block on get_request_wait() than to skip inodes on
IO congestion";Wu Fengguang;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2; The latter will lead to more seeky IO;Wu Fengguang;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;"The nonblocking checks in ->writepage are no longer used because it's
redundant with the WB_SYNC_NONE check";Wu Fengguang;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;"We no long set ->nonblocking in VM page out and page migration, because
a) it's effectively redundant with WB_SYNC_NONE in current code
b) it's old semantic of ""Don't get stuck on request queues"" is mis-behavior";Wu Fengguang;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;"   that would skip some dirty inodes on congestion and page out others, which
   is unfair in terms of LRU age";Wu Fengguang;2010-10-26;0;1
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;Inspired by Christoph Hellwig;Wu Fengguang;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODoxYjQzMGJlZWU1ZTM4ODYwNWRmYjA5MmIyMTRlZjAzMjBmNzUyY2Y2;Thanks!;Wu Fengguang;2010-10-26;0;0
MDY6Q29tbWl0MjMyNTI5ODozZWY4ZmQ3ZjcyMGZjNGY0NjJmY2RjYWUyZmNkZTZmMWMwNTM2YmZl;Fix migration.c compilation on s390;Andi Kleen;2010-10-11;1;1
MDY6Q29tbWl0MjMyNTI5ODozZWY4ZmQ3ZjcyMGZjNGY0NjJmY2RjYWUyZmNkZTZmMWMwNTM2YmZl;31bit s390 doesn't have huge pages and failed with;Andi Kleen;2010-10-11;0;0
MDY6Q29tbWl0MjMyNTI5ODozZWY4ZmQ3ZjcyMGZjNGY0NjJmY2RjYWUyZmNkZTZmMWMwNTM2YmZl;> mm/migrate.c: In function 'remove_migration_pte';Andi Kleen;2010-10-11;0;1
MDY6Q29tbWl0MjMyNTI5ODozZWY4ZmQ3ZjcyMGZjNGY0NjJmY2RjYWUyZmNkZTZmMWMwNTM2YmZl;"> mm/migrate.c:143:3: error: implicit declaration of function 'pte_mkhuge'
> mm/migrate.c:143:7: error: incompatible types when assigning to type 'pte_t' from type 'int'
Put that code into a ifdef";Andi Kleen;2010-10-11;0;0
MDY6Q29tbWl0MjMyNTI5ODozZWY4ZmQ3ZjcyMGZjNGY0NjJmY2RjYWUyZmNkZTZmMWMwNTM2YmZl;Reported by Heiko Carstens;Andi Kleen;2010-10-11;0;0
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;hugetlb: hugepage migration core;Naoya Horiguchi;2010-09-08;1;0
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;This patch extends page migration code to support hugepage migration;Naoya Horiguchi;2010-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;"One of the potential users of this feature is soft offlining which
is triggered by memory corrected errors (added by the next patch.)
Todo";Naoya Horiguchi;2010-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;"- there are other users of page migration such as memory policy,
  memory hotplug and memocy compaction";Naoya Horiguchi;2010-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;  They are not ready for hugepage support for now;Naoya Horiguchi;2010-09-08;0;0
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;ChangeLog since v4;Naoya Horiguchi;2010-09-08;0;1
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;"- define migrate_huge_pages()
- remove changes on isolation/putback_lru_page()
ChangeLog since v2";Naoya Horiguchi;2010-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;"- refactor isolate/putback_lru_page() to handle hugepage
- add comment about race on unmap_and_move_huge_page()
ChangeLog since v1";Naoya Horiguchi;2010-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODoyOTA0MDhkNGEyNTAwMmYwOTllZmVlZTdiNmE1Nzc4ZDQzMTE1NGQ2;"- divide migration code path for hugepage
- define routine checking migration swap entry for hugetlb";Naoya Horiguchi;2010-09-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;mm: extend KSM refcounts to the anon_vma root;Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;"KSM reference counts can cause an anon_vma to exist after the processe it
belongs to have already exited";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;" Because the anon_vma lock now lives in
the root anon_vma, we need to ensure that the root anon_vma stays around
until after all the ""child"" anon_vmas have been freed";Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;"The obvious way to do this is to have a ""child"" anon_vma take a reference
to the root in anon_vma_fork";Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;" When the anon_vma is freed at munmap or
process exit, we drop the refcount in anon_vma_unlink and possibly free
the root anon_vma";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;"The KSM anon_vma reference count function also needs to be modified to
deal with the possibility of freeing 2 levels of anon_vma";Rik van Riel;2010-08-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;" The easiest
way to do this is to break out the KSM magic and make it generic";Rik van Riel;2010-08-10;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NjU0NTA2NmM4NTIxZjNlMzJjODQ5NzQ0NzQ0ODQyYjRkZjI1Yjc5;When compiling without CONFIG_KSM, this code is compiled out.;Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;mm: always lock the root (oldest) anon_vma;Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;"Always (and only) lock the root (oldest) anon_vma whenever we do something
in an anon_vma";Rik van Riel;2010-08-10;1;0
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;" The recently introduced anon_vma scalability is due to
the rmap code scanning only the VMAs that need to be scanned";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;" Many common
operations still took the anon_vma lock on the root anon_vma, so always
taking that lock is not expected to introduce any scalability issues";Rik van Riel;2010-08-10;1;1
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;"However, always taking the same lock does mean we only need to take one
lock, which means rmap_walk on pages from any anon_vma in the vma is
excluded from occurring during an munmap, expand_stack or other operation
that needs to exclude rmap_walk and similar functions";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODowMTJmMTgwMDRkYTMzYmE2NzJlM2M2MDgzOGNjNDg5ODEyNjE3NGQz;Also add the proper locking to vma_adjust.;Rik van Riel;2010-08-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYmE0OGI5OGYyMzQ4YzgxNDMxNmM0YjRmNDExYTA3YTBlNGEyYmY5;mm: change direct call of spin_lock(anon_vma->lock) to inline function;Rik van Riel;2010-08-10;0;0
MDY6Q29tbWl0MjMyNTI5ODpjYmE0OGI5OGYyMzQ4YzgxNDMxNmM0YjRmNDExYTA3YTBlNGEyYmY5;"Subsitute a direct call of spin_lock(anon_vma->lock) with an inline
function doing exactly the same";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODpjYmE0OGI5OGYyMzQ4YzgxNDMxNmM0YjRmNDExYTA3YTBlNGEyYmY5;"This makes it easier to do the substitution to the root anon_vma lock in a
following patch";Rik van Riel;2010-08-10;1;1
MDY6Q29tbWl0MjMyNTI5ODpjYmE0OGI5OGYyMzQ4YzgxNDMxNmM0YjRmNDExYTA3YTBlNGEyYmY5;"We will deal with the handful of special locks (nested, dec_and_lock, etc)
separately.";Rik van Riel;2010-08-10;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;memcg: fix mis-accounting of file mapped racy with migration;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"FILE_MAPPED per memcg of migrated file cache is not properly updated,
because our hook in page_add_file_rmap() can't know to which memcg
FILE_MAPPED should be counted";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"Basically, this patch is for fixing the bug but includes some big changes
to fix up other messes";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;Now, at migrating mapped file, events happen in following sequence;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 1;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;allocate a new page;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 2;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;get memcg of an old page;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 3;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;charge ageinst a new page before migration;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"But at this point,
    no changes to new page's page_cgroup, no commit for the charge";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"    (IOW, PCG_USED bit is not set.)
 4";akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;page migration replaces radix-tree, old-page and new-page;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 5;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;page migration remaps the new page if the old page was mapped;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 6;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;Here, the new page is unlocked;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 7;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"memcg commits the charge for newpage, Mark the new page's page_cgroup
    as PCG_USED";akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"Because ""commit"" happens after page-remap, we can count FILE_MAPPED
at ""5"", because we should avoid to trust page_cgroup->mem_cgroup";akpm@linux-foundation.org;2010-05-26;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;if PCG_USED bit is unset;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"(Note: memcg's LRU removal code does that but LRU-isolation logic is used
 for helping it";akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"When we overwrite page_cgroup->mem_cgroup, page_cgroup is
 not on LRU or page_cgroup->mem_cgroup is NULL.)
We can lose file_mapped accounting information at 5 because FILE_MAPPED
is updated only when mapcount changes 0->1";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;So we should catch it;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"BTW, historically, above implemntation comes from migration-failure
of anonymous page";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"Because we charge both of old page and new page
with mapcount=0, we can't catch
  - the page is really freed before remap";akpm@linux-foundation.org;2010-05-26;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"  - migration fails but it's freed before remap
or .....corner cases";akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;New migration sequence with memcg is;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 1;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;allocate a new page;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 2;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;mark PageCgroupMigration to the old page;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 3;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;charge against a new page onto the old page's memcg;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"(here, new page's pc
    is marked as PageCgroupUsed.)
 4";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;page migration replaces radix-tree, page table, etc..;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 5;akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"At remapping, new page's page_cgroup is now makrked as ""USED""
    We can catch 0->1 event and FILE_MAPPED will be properly updated";akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"    And we can catch SWAPOUT event after unlock this and freeing this
    page by unmap() can be caught";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1; 7;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;Clear PageCgroupMigration of the old page;akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;So, FILE_MAPPED will be correctly updated;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"Then, for what MIGRATION flag is ?
  Without it, at migration failure, we may have to charge old page again
  because it may be fully unmapped";akpm@linux-foundation.org;2010-05-26;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"""charge"" means that we have to dive into
  memory reclaim or something complated";akpm@linux-foundation.org;2010-05-26;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"So, it's better to avoid
  charge it again";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"Before this patch, __commit_charge() was working for
  both of the old/new page and fixed up all";akpm@linux-foundation.org;2010-05-26;0;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"But this technique has some
  racy condtion around FILE_MAPPED and SWAPOUT etc..";akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;"  Now, the kernel use MIGRATION flag and don't uncharge old page until
  the end of migration";akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;I hope this change will make memcg's page migration much simpler;akpm@linux-foundation.org;2010-05-26;1;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;" This
page migration has caused several troubles";akpm@linux-foundation.org;2010-05-26;0;1
MDY6Q29tbWl0MjMyNTI5ODphYzM5Y2Y4Y2I4NmM0NWVlYWM2YTU5MmNlMGQ1OGY5MDIxYTk3MjM1;" Worth to add a flag for
simplification.";akpm@linux-foundation.org;2010-05-26;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;mm: compaction: memory compaction core;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;"This patch is the core of a mechanism which compacts memory in a zone by
relocating movable pages towards the end of the zone";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;A single compaction run involves a migration scanner and a free scanner;Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;Both scanners operate on pageblock-sized areas in the zone;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;" The migration
scanner starts at the bottom of the zone and searches for all movable
pages within each area, isolating them onto a private list called
migratelist";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;" The free scanner starts at the top of the zone and searches
for suitable areas and consumes the free pages within making them
available for the migration scanner";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDg0NDZiYjZiNWE5MzkwYjU0NmFmMzhlYzg5OWM4NjhhOWRiY2Yw;" The pages isolated for migration are
then migrated to the newly isolated free pages.";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZmUyMDExZmY1MWU5MjUwMDAxMGE0OTVkZjRiZTg2NzQ1ZmJiZGE5;mm: migration: allow the migration of PageSwapCache pages;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZmUyMDExZmY1MWU5MjUwMDAxMGE0OTVkZjRiZTg2NzQ1ZmJiZGE5;"PageAnon pages that are unmapped may or may not have an anon_vma so are
not currently migrated";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZmUyMDExZmY1MWU5MjUwMDAxMGE0OTVkZjRiZTg2NzQ1ZmJiZGE5;" However, a swap cache page can be migrated and
fits this description";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZmUyMDExZmY1MWU5MjUwMDAxMGE0OTVkZjRiZTg2NzQ1ZmJiZGE5;" This patch identifies page swap caches and allows
them to be migrated but ensures that no attempt to made to remap the pages
would would potentially try to access an already freed anon_vma.";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;mm: migration: do not try to migrate unmapped anonymous pages;Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"rmap_walk_anon() was triggering errors in memory compaction that look like
use-after-free errors";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;" The problem is that between the page being
isolated from the LRU and rcu_read_lock() being taken, the mapcount of the
page dropped to 0 and the anon_vma gets freed";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;" This can happen during
memory compaction if pages being migrated belong to a process that exits
before migration completes";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;" Hence, the use-after-free race looks like
 1";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"Page isolated for migration
 2";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"Process exits
 3";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"page_mapcount(page) drops to zero so anon_vma was no longer reliable
 4";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"unmap_and_move() takes the rcu_lock but the anon_vma is already garbage
 4";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;"call try_to_unmap, looks up tha anon_vma and ""locks"" it but the lock
    is garbage";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;This patch checks the mapcount after the rcu lock is taken;Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo2N2I5NTA5YjJjNjhhZTM4Y2VjYjgzYTIzOTg4MWNiMGRkZjA4N2Rj;" If the
mapcount is zero, the anon_vma is assumed to be freed and no further
action is taken.";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo3ZjYwYzIxNGZkM2EzNjA0NjFmMzI4NmM2OTA4MDg0ZjdmOGIxOTUw;mm: migration: share the anon_vma ref counts between KSM and page migration;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZjYwYzIxNGZkM2EzNjA0NjFmMzI4NmM2OTA4MDg0ZjdmOGIxOTUw;"For clarity of review, KSM and page migration have separate refcounts on
the anon_vma";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo3ZjYwYzIxNGZkM2EzNjA0NjFmMzI4NmM2OTA4MDg0ZjdmOGIxOTUw; While clear, this is a waste of memory;Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo3ZjYwYzIxNGZkM2EzNjA0NjFmMzI4NmM2OTA4MDg0ZjdmOGIxOTUw;" This patch gets
KSM and page migration to share their toys in a spirit of harmony.";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;mm: migration: take a reference to the anon_vma before migrating;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"This patchset is a memory compaction mechanism that reduces external
fragmentation memory by moving GFP_MOVABLE pages to a fewer number of
pageblocks";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" The term ""compaction"" was chosen as there are is a number of
mechanisms that are not mutually exclusive that can be used to defragment
memory";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" For example, lumpy reclaim is a form of defragmentation as was
slub ""defragmentation"" (really a form of targeted reclaim)";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" Hence, this
is called ""compaction"" to distinguish it from other forms of
defragmentation";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"In this implementation, a full compaction run involves two scanners
operating within a zone - a migration and a free scanner";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" The migration
scanner starts at the beginning of a zone and finds all movable pages
within one pageblock_nr_pages-sized area and isolates them on a
migratepages list";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" The free scanner begins at the end of the zone and
searches on a per-area basis for enough free pages to migrate all the
pages on the migratepages list";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" As each area is respectively migrated or
exhausted of free pages, the scanners are advanced one area";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" A compaction
run completes within a zone when the two scanners meet";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"This method is a bit primitive but is easy to understand and greater
sophistication would require maintenance of counters on a per-pageblock
basis";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" This would have a big impact on allocator fast-paths to improve
compaction which is a poor trade-off";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"It also does not try relocate virtually contiguous pages to be physically
contiguous";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" However, assuming transparent hugepages were in use, a
hypothetical khugepaged might reuse compaction code to isolate free pages,
split them and relocate userspace pages for promotion";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;Memory compaction can be triggered in one of three ways;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" It may be
triggered explicitly by writing any value to /proc/sys/vm/compact_memory
and compacting all of memory";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" It can be triggered on a per-node basis by
writing any value to /sys/devices/system/node/nodeN/compact where N is the
node ID to be compacted";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" When a process fails to allocate a high-order
page, it may compact memory in an attempt to satisfy the allocation
instead of entering direct reclaim";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" Explicit compaction does not finish
until the two scanners meet and direct compaction ends if a suitable page
becomes available that would meet watermarks";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;The series is in 14 patches;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" The first three are not ""core"" to the series
but are important pre-requisites";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;Patch 1 reference counts anon_vma for rmap_walk_anon();Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Without this
	patch, it's possible to use anon_vma after free if the caller is
	not holding a VMA or mmap_sem for the pages in question";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"While
	there should be no existing user that causes this problem,
	it's a requirement for memory compaction to be stable";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"The patch
	is at the start of the series for bisection reasons";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;Patch 2 merges the KSM and migrate counts;Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"It could be merged with patch 1
	but would be slightly harder to review";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Patch 3 skips over unmapped anon pages during migration as there are no
	guarantees about the anon_vma existing";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"There is a window between
	when a page was isolated and migration started during which anon_vma
	could disappear";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Patch 4 notes that PageSwapCache pages can still be migrated even if they
	are unmapped";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Patch 5 allows CONFIG_MIGRATION to be set without CONFIG_NUMA
Patch 6 exports a ""unusable free space index"" via debugfs";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"It's
	a measure of external fragmentation that takes the size of the
	allocation request into account";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"It can also be calculated from
	userspace so can be dropped if requested
Patch 7 exports a ""fragmentation index"" which only has meaning when an
	allocation request fails";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"It determines if an allocation failure
	would be due to a lack of memory or external fragmentation";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Patch 8 moves the definition for LRU isolation modes for use by compaction
Patch 9 is the compaction mechanism although it's unreachable at this point
Patch 10 adds a means of compacting all of memory with a proc trgger
Patch 11 adds a means of compacting a specific node with a sysfs trigger
Patch 12 adds ""direct compaction"" before ""direct reclaim"" if it is
	determined there is a good chance of success";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Patch 13 adds a sysctl that allows tuning of the threshold at which the
	kernel will compact or direct reclaim
Patch 14 temporarily disables compaction if an allocation failure occurs
	after compaction";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;Testing of compaction was in three stages;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" For the test, debugging,
preempt, the sleep watchdog and lockdep were all enabled but nothing nasty
popped out";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" min_free_kbytes was tuned as recommended by hugeadm to help
fragmentation avoidance and high-order allocations";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" It was tested on X86,
X86-64 and PPC64";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Ths first test represents one of the easiest cases that can be faced for
lumpy reclaim or memory compaction";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;1;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Machine freshly booted and configured for hugepage usage with
	a) hugeadm --create-global-mounts
	b) hugeadm --pool-pages-max DEFAULT:8G
	c) hugeadm --set-recommended-min_free_kbytes
	d) hugeadm --set-recommended-shmmax
	The min_free_kbytes here is important";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Anti-fragmentation works best
	when pageblocks don't mix";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"hugeadm knows how to calculate a value that
	will significantly reduce the worst of external-fragmentation-related
	events as reported by the mm_page_alloc_extfrag tracepoint";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;2;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Load up memory
	a) Start updatedb
	b) Create in parallel a X files of pagesize*128 in size";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Wait
	   until files are created";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"By parallel, I mean that 4096 instances
	   of dd were launched, one after the other using &";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"The crude
	   objective being to mix filesystem metadata allocations with
	   the buffer cache";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"	c) Delete every second file so that pageblocks are likely to
	   have holes
	d) kill updatedb if it's still running
	At this point, the system is quiet, memory is full but it's full with
	clean filesystem metadata and clean buffer cache that is unmapped";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"	This is readily migrated or discarded so you'd expect lumpy reclaim
	to have no significant advantage over compaction but this is at
	the POC stage";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;3;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;In increments, attempt to allocate 5% of memory as hugepages;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"	   Measure how long it took, how successful it was, how many
	   direct reclaims took place and how how many compactions";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"Note
	   the compaction figures might not fully add up as compactions
	   can take place for orders other than the hugepage size
X86				vanilla		compaction
Final page count                    913                916 (attempted 1002)
pages reclaimed                   68296               9791
X86-64				vanilla		compaction
Final page count:                   901                902 (attempted 1002)
Total pages reclaimed:           112599              53234
PPC64				vanilla		compaction
Final page count:                    93                 94 (attempted 110)
Total pages reclaimed:           103216              61838
There was not a dramatic improvement in success rates but it wouldn't be
expected in this case either";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" What was important is that fewer pages were
reclaimed in all cases reducing the amount of IO required to satisfy a
huge page allocation";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"The second tests were all performance related - kernbench, netperf, iozone
and sysbench";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz; None showed anything too remarkable;Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;The last test was a high-order allocation stress test;Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" Many kernel
compiles are started to fill memory with a pressured mix of unmovable and
movable allocations";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" During this, an attempt is made to allocate 90% of
memory as huge pages - one at a time with small delays between attempts to
avoid flooding the IO queue";Mel Gorman;2010-05-24;0;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"                                             vanilla   compaction
Percentage of request allocated X86               98           99
Percentage of request allocated X86-64            95           98
Percentage of request allocated PPC64             55           70
This patch";Mel Gorman;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"rmap_walk_anon() does not use page_lock_anon_vma() for looking up and
locking an anon_vma and it does not appear to have sufficient locking to
ensure the anon_vma does not disappear from under it";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;"This patch copies an approach used by KSM to take a reference on the
anon_vma while pages are being migrated";Mel Gorman;2010-05-24;1;0
MDY6Q29tbWl0MjMyNTI5ODozZjZjODI3MjhmNGUzMWE5N2MzYTFiMzJhYmNjYjUxMmZlZDBiNTcz;" This should prevent rmap_walk()
running into nasty surprises later because anon_vma has been freed.";Mel Gorman;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTM4NjFkODIyZjhmNDQzY2EwYzAyMGVhOGZjMmRjMDEwMzljZDYz;mm: remove return value of putback_lru_pages();Minchan Kim;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTM4NjFkODIyZjhmNDQzY2EwYzAyMGVhOGZjMmRjMDEwMzljZDYz;putback_lru_page() never can fail;Minchan Kim;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTM4NjFkODIyZjhmNDQzY2EwYzAyMGVhOGZjMmRjMDEwMzljZDYz;" So it doesn't matter count of ""the
number of pages put back""";Minchan Kim;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODplMTM4NjFkODIyZjhmNDQzY2EwYzAyMGVhOGZjMmRjMDEwMzljZDYz;In addition, users of this functions don't use return value;Minchan Kim;2010-05-24;0;1
MDY6Q29tbWl0MjMyNTI5ODplMTM4NjFkODIyZjhmNDQzY2EwYzAyMGVhOGZjMmRjMDEwMzljZDYz;Let's remove unnecessary code.;Minchan Kim;2010-05-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h;Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"percpu.h is included by sched.h and module.h and thus ends up being
included when building most .c files";Tejun Heo;2010-03-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" percpu.h includes slab.h which
in turn includes gfp.h making everything defined by the two files
universally available and complicating inclusion dependencies";Tejun Heo;2010-03-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;percpu.h -> slab.h dependency is about to be removed;Tejun Heo;2010-03-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" Prepare for
this change by updating users of gfp and slab facilities include those
headers directly instead of assuming availability";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" As this conversion
needs to touch large number of source files, the following script is
used as the basis of conversion";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;The script does the followings;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;  only the necessary includes are there;Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1; ie;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"if only gfp is used,
  gfp.h, if slab is used, slab.h";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"  blocks and try to put the new include such that its order conforms
  to its surrounding";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" It's put in the include block which contains
  core kernel includes, in the same order that the rest are ordered -
  alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
  doesn't seem to be any matching order";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"  because the file doesn't have fitting include block), it prints out
  an error message indicating which .h file needs to be added to the
  file";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;The conversion was done in the following steps;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;1;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"The initial automatic conversion of all .c files updated slightly
   over 4000 files, deleting around 700 includes and adding ~480 gfp.h
   and ~3000 slab.h inclusions";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" The script emitted errors for ~400
   files";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;2;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;Each error was manually checked;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" Some didn't need the inclusion,
   some needed manual addition while adding it to implementation .h or
   embedding .c file was more appropriate for others";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" This step added
   inclusions to around 150 files";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;3;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"The script was run again and the output was compared to the edits
   from #2 to make sure no file was left behind";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;4;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;Several build tests were done and a couple of problems were fixed;Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;   e.g;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"lib/decompress_*.c used malloc/free() wrappers around slab
   APIs requiring slab.h to be added manually";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;5;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"The script was run on all .h files but without automatically
   editing them as sprinkling gfp.h and slab.h inclusions around .h
   files could easily lead to inclusion dependency hell";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" Most gfp.h
   inclusion directives were ignored as stuff from gfp.h was usually
   wildly available and often used in preprocessor macros";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" Each
   slab.h inclusion directive was examined and added manually as
   necessary";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;6;Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;percpu.h was updated not to include slab.h;Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;7;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"Build test were done on the following configurations and failures
   were fixed";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;" CONFIG_GCOV_KERNEL was turned off for all tests (as my
   distributed build env didn't work with gcov compiles) and a few
   more options had to be turned off depending on archs to make things
   build (like ipr on powerpc/64 which failed due to missing writeq)";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;8;Tejun Heo;2010-03-24;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"percpu.h modifications were reverted so that it could be applied as
   a separate patch and serve as bisection point";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"Given the fact that I had only a couple of failures from tests on step
6, I'm fairly confident about the coverage of this conversion patch";Tejun Heo;2010-03-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YTBlM2FkNmFmODY2MGJlMjFjYTk4YTk3MWNkMDBmMzMxMzE4YzA1;"If there is a breakage, it's likely to be something in one of the arch
headers which should be easily discoverable easily on most builds of
the specific arch.";Tejun Heo;2010-03-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NWYxZmI3MmZhNzZlYWJjNDQ4MWRjNzlmNDJkMmIwMTFkZjU0NzYy;mm/migrate.c: kill anon local variable from migrate_page_copy;KOSAKI Motohiro;2010-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NWYxZmI3MmZhNzZlYWJjNDQ4MWRjNzlmNDJkMmIwMTFkZjU0NzYy;"commit 01b1ae63c2 (""memcg: simple migration handling"") removed
mem_cgroup_uncharge_cache_page() call from migrate_page_copy";KOSAKI Motohiro;2010-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo4NWYxZmI3MmZhNzZlYWJjNDQ4MWRjNzlmNDJkMmIwMTFkZjU0NzYy;" Local
variable `anon' is now unused.";KOSAKI Motohiro;2010-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;mm: Make copy_from_user() in migrate.c statically predictable;H. Peter Anvin;2010-02-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;x86-32 has had a static test for copy_on_user() overflow for a while;H. Peter Anvin;2010-02-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;"This test currently fails in mm/migrate.c resulting in an
allyesconfig/allmodconfig build failure on x86-32";H. Peter Anvin;2010-02-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;"In function copy_from_user,
    inlined from do_pages_stat at
    /home/hpa/kernel/git/mm/migrate.c:1012";H. Peter Anvin;2010-02-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;/home/hpa/kernel/git/arch/x86/include/asm/uaccess_32.h:212: error;H. Peter Anvin;2010-02-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;"    call to copy_from_user_overflow declared
Make the logic more explicit and therefore easier for gcc to
understand";H. Peter Anvin;2010-02-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo4N2I4ZDFhZGVmYTE1NDhiNTkxY2JmMGQ2Mzk2NTk4N2UyY2Y4OTNk;"v2: rewrite the loop entirely using a more normal structure for a
    chunked-data loop (Linus Torvalds)";H. Peter Anvin;2010-02-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself;Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;"On VIVT ARM, when we have multiple shared mappings of the same file
in the same MM, we need to ensure that we have coherency across all
copies";Russell King;2009-12-18;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;" We do this via make_coherent() by making the pages
uncacheable";Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;"This used to work fine, until we allowed highmem with highpte - we
now have a page table which is mapped as required, and is not available
for modification via update_mmu_cache()";Russell King;2009-12-18;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;"Ralf Beache suggested getting rid of the PTE value passed to
update_mmu_cache()";Russell King;2009-12-18;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;"  On MIPS update_mmu_cache() calls __update_tlb() which walks pagetables
  to construct a pointer to the pte again";Russell King;2009-12-18;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;" Passing a pte_t * is much
  more elegant";Russell King;2009-12-18;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;" Maybe we might even replace the pte argument with the
  pte_t?
Ben Herrenschmidt would also like the pte pointer for PowerPC";Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;  Passing the ptep in there is exactly what I want;Russell King;2009-12-18;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;" I want that
  -instead- of the PTE value, because I have issue on some ppc cases,
  for I$/D$ coherency, where set_pte_at() may decide to mask out the
  _PAGE_EXEC";Russell King;2009-12-18;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;"So, pass in the mapped page table pointer into update_mmu_cache(), and
remove the PTE value, updating all implementations and call sites to
suit";Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;Includes a fix from Stephen Rothwell;Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjMwNzNlMWM1M2EyNTYyNzVmMTA3OWMwZmJmYmU4NTg4M2Q5Mjc1;  sparc: fix fallout from update_mmu_cache API change;Russell King;2009-12-18;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjVhNTVmMWE2YzVhYmVlMTVhMGU4NzhlNWM3NGQ5ZjE1NjliOGIw;Fix potential crash with sys_move_pages;Linus Torvalds;2010-02-06;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZjVhNTVmMWE2YzVhYmVlMTVhMGU4NzhlNWM3NGQ5ZjE1NjliOGIw;"We incorrectly depended on the 'node_state/node_isset()' functions
testing the node range, rather than checking it explicitly";Linus Torvalds;2010-02-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZjVhNTVmMWE2YzVhYmVlMTVhMGU4NzhlNWM3NGQ5ZjE1NjliOGIw;" That's not
reliable, even if it might often happen to work";Linus Torvalds;2010-02-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZjVhNTVmMWE2YzVhYmVlMTVhMGU4NzhlNWM3NGQ5ZjE1NjliOGIw;" So do the proper
explicit test.";Linus Torvalds;2010-02-06;0;1
MDY6Q29tbWl0MjMyNTI5ODo0MThiMjdlZjUwZTdlOWIwYzJmYmQ4OGRiODA0YmYwNjVlNWViMWE2;mm: remove unevictable_migrate_page function;Lee Schermerhorn;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo0MThiMjdlZjUwZTdlOWIwYzJmYmQ4OGRiODA0YmYwNjVlNWViMWE2;"unevictable_migrate_page() in mm/internal.h is a relic of the since
removed UNEVICTABLE_LRU Kconfig option";Lee Schermerhorn;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo0MThiMjdlZjUwZTdlOWIwYzJmYmQ4OGRiODA0YmYwNjVlNWViMWE2;" This patch removes the function
and open codes the test in migrate_page_copy().";Lee Schermerhorn;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;ksm: memory hotremove migration only;Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;"The previous patch enables page migration of ksm pages, but that soon gets
into trouble: not surprising, since we're using the ksm page lock to lock
operations on its stable_node, but page migration switches the page whose
lock is to be used for that";Hugh Dickins;2009-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;" Another layer of locking would fix it, but
do we need that yet?
Do we actually need page migration of ksm pages?  Yes, memory hotremove
needs to offline sections of memory: and since we stopped allocating ksm
pages with GFP_HIGHUSER, they will tend to be GFP_HIGHUSER_MOVABLE
candidates for migration";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;"But KSM is currently unconscious of NUMA issues, happily merging pages
from different NUMA nodes: at present the rule must be, not to use
MADV_MERGEABLE where you care about NUMA";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;" So no, NUMA page migration of
ksm pages does not make sense yet";Hugh Dickins;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;So, to complete support for ksm swapping we need to make hotremove safe;Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;"ksm_memory_callback() take ksm_thread_mutex when MEM_GOING_OFFLINE and
release it when MEM_OFFLINE or MEM_CANCEL_OFFLINE";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;" But if mapped pages
are freed before migration reaches them, stable_nodes may be left still
pointing to struct pages which have been removed from the system: the
stable_node needs to identify a page by pfn rather than page pointer, then
it can safely prune them when MEM_OFFLINE";Hugh Dickins;2009-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;And make NUMA migration skip PageKsm pages where it skips PageReserved;Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmI2MWY2MTFlYjVlMjBmN2U5Zjg2MTliZmQwM2JkZmU4YWY2MzQ4;"But it's only when we reach unmap_and_move() that the page lock is taken
and we can be sure that raised pagecount has prevented a PageAnon from
being upgraded: so add offlining arg to migrate_pages(), to migrate ksm
page when offlining (has sufficient locking) but reject it otherwise.";Hugh Dickins;2009-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;ksm: rmap_walk to remove_migation_ptes;Hugh Dickins;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;"A side-effect of making ksm pages swappable is that they have to be placed
on the LRUs: which then exposes them to isolate_lru_page() and hence to
page migration";Hugh Dickins;2009-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;"Add rmap_walk() for remove_migration_ptes() to use: rmap_walk_anon() and
rmap_walk_file() in rmap.c, but rmap_walk_ksm() in ksm.c";Hugh Dickins;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;" Perhaps some
consolidation with existing code is possible, but don't attempt that yet
(try_to_unmap needs to handle nonlinears, but migration pte removal does
not)";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;"rmap_walk() is sadly less general than it appears: rmap_walk_anon(), like
remove_anon_migration_ptes() which it replaces, avoids calling
page_lock_anon_vma(), because that includes a page_mapped() test which
fails when all migration ptes are in place";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;" That was valid when NUMA page
migration was introduced (holding mmap_sem provided the missing guarantee
that anon_vma's slab had not already been destroyed), but I believe not
valid in the memory hotremove case added since";Hugh Dickins;2009-12-15;0;0
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;"For now do the same as before, and consider the best way to fix that
unlikely race later on";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODplOTk5NWVmOTc4YTdkNTI5NmZlMDRhOWEyYzVjYTZlNjZkOGJiNGU1;" When fixed, we can probably use rmap_walk() on
hwpoisoned ksm pages too: for now, they remain among hwpoison's various
exceptions (its PageKsm test comes before the page is locked, but its
page_lock_anon_vma fails safely if an anon gets upgraded).";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODozY2E3YjNjNWI2NGQzNWZlMDJjMzViNWQ0NGMyYzU4YjQ5NDk5ZmVl;mm: define PAGE_MAPPING_FLAGS;Hugh Dickins;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODozY2E3YjNjNWI2NGQzNWZlMDJjMzViNWQ0NGMyYzU4YjQ5NDk5ZmVl;"At present we define PageAnon(page) by the low PAGE_MAPPING_ANON bit set
in page->mapping, with the higher bits a pointer to the anon_vma; and have
defined PageKsm(page) as that with NULL anon_vma";Hugh Dickins;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODozY2E3YjNjNWI2NGQzNWZlMDJjMzViNWQ0NGMyYzU4YjQ5NDk5ZmVl;"But KSM swapping will need to store a pointer there: so in preparation for
that, now define PAGE_MAPPING_FLAGS as the low two bits, including
PAGE_MAPPING_KSM (always set along with PAGE_MAPPING_ANON, until some
other use for the bit emerges)";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODozY2E3YjNjNWI2NGQzNWZlMDJjMzViNWQ0NGMyYzU4YjQ5NDk5ZmVl;"Declare page_rmapping(page) to return the pointer part of page->mapping,
and page_anon_vma(page) to return the anon_vma pointer when that's what it
is";Hugh Dickins;2009-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODozY2E3YjNjNWI2NGQzNWZlMDJjMzViNWQ0NGMyYzU4YjQ5NDk5ZmVl;" Use these in a few appropriate places: notably, unuse_vma() has been
testing page->mapping, but is better to be testing page_anon_vma() (cases
may be added in which flag bits are set without any pointer).";Hugh Dickins;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZDljMjg1YTYzMmIzOWFiODNjNmFlMTRjYmZmMGU2MDZkNDA0MmVl;mm: move inc_zone_page_state(NR_ISOLATED) to just isolated place;KOSAKI Motohiro;2009-12-15;1;1
MDY6Q29tbWl0MjMyNTI5ODo2ZDljMjg1YTYzMmIzOWFiODNjNmFlMTRjYmZmMGU2MDZkNDA0MmVl;"Christoph pointed out inc_zone_page_state(NR_ISOLATED) should be placed
in right after isolate_page()";KOSAKI Motohiro;2009-12-15;0;1
MDY6Q29tbWl0MjMyNTI5ODo2ZDljMjg1YTYzMmIzOWFiODNjNmFlMTRjYmZmMGU2MDZkNDA0MmVl;This patch does it.;KOSAKI Motohiro;2009-12-15;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOTI1NTg1MDM5Y2YzOTI3NWMyZTBlNTc1MTJlNWRmMjdmYTczYWFk;mm: Adjust do_pages_stat() so gcc can see copy_from_user() is safe;H. Peter Anvin;2009-12-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTI1NTg1MDM5Y2YzOTI3NWMyZTBlNTc1MTJlNWRmMjdmYTczYWFk;"Slightly adjust the logic for determining the size of the
copy_form_user() in do_pages_stat(); with this change, gcc can see
that the copying is safe";H. Peter Anvin;2009-12-08;0;1
MDY6Q29tbWl0MjMyNTI5ODpiOTI1NTg1MDM5Y2YzOTI3NWMyZTBlNTc1MTJlNWRmMjdmYTczYWFk;Without this, we get a build error for i386 allyesconfig;H. Peter Anvin;2009-12-08;1;1
MDY6Q29tbWl0MjMyNTI5ODpiOTI1NTg1MDM5Y2YzOTI3NWMyZTBlNTc1MTJlNWRmMjdmYTczYWFk;/home/hpa/kernel/linux-2.6-tip.urgent/arch/x86/include/asm/uaccess_32.h:213;H. Peter Anvin;2009-12-08;1;0
MDY6Q29tbWl0MjMyNTI5ODpiOTI1NTg1MDM5Y2YzOTI3NWMyZTBlNTc1MTJlNWRmMjdmYTczYWFk;"error: call to copy_from_user_overflow declared with attribute
error: copy_from_user() buffer size is not provably correct
Unlike an earlier patch from Arjan, this doesn't introduce new
variables; merely reshuffles the compare so that gcc can see that an
overflow cannot happen.";H. Peter Anvin;2009-12-08;1;1
MDY6Q29tbWl0MjMyNTI5ODplMDBlNDMxNjEyYzNhNmU0MzdhMDFmMjEyOWZkMzg0M2RhMGM5ODJh;memcg: fix wrong pointer initialization at page migration when memcg is disabled.;KAMEZAWA Hiroyuki;2009-11-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplMDBlNDMxNjEyYzNhNmU0MzdhMDFmMjEyOWZkMzg0M2RhMGM5ODJh;"Lee Schermerhorn reported that he saw bad pointer dereference in
mem_cgroup_end_migration() when he disabled memcg by boot option";KAMEZAWA Hiroyuki;2009-11-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplMDBlNDMxNjEyYzNhNmU0MzdhMDFmMjEyOWZkMzg0M2RhMGM5ODJh;"memcg's page migration logic works as
	do page migration
Now, ptr is not initialized in prepare_migration when memcg is disabled
by boot option";KAMEZAWA Hiroyuki;2009-11-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplMDBlNDMxNjEyYzNhNmU0MzdhMDFmMjEyOWZkMzg0M2RhMGM5ODJh;This causes panic in end_migration;KAMEZAWA Hiroyuki;2009-11-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplMDBlNDMxNjEyYzNhNmU0MzdhMDFmMjEyOWZkMzg0M2RhMGM5ODJh;This patch fixes it.;KAMEZAWA Hiroyuki;2009-11-11;1;1
MDY6Q29tbWl0MjMyNTI5ODplZGNmNDc0OGNkNTZhZGNkZjA4NTZjYzk5ZWYxMDhhNGVhM2FjN2Zl;mm: return boolean from page_has_private();Johannes Weiner;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODplZGNmNDc0OGNkNTZhZGNkZjA4NTZjYzk5ZWYxMDhhNGVhM2FjN2Zl;"Make page_has_private() return a true boolean value and remove the double
negations from the two callsites using it for arithmetic.";Johannes Weiner;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo2YzBiMTM1MTlkMWM3NTVkODc0ZTgyYzhmYjhhNmRjZWYwZWU0MDJj;mm: return boolean from page_is_file_cache();Johannes Weiner;2009-09-22;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YzBiMTM1MTlkMWM3NTVkODc0ZTgyYzhmYjhhNmRjZWYwZWU0MDJj;"page_is_file_cache() has been used for both boolean checks and LRU
arithmetic, which was always a bit weird";Johannes Weiner;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YzBiMTM1MTlkMWM3NTVkODc0ZTgyYzhmYjhhNmRjZWYwZWU0MDJj;"Now that page_lru_base_type() exists for LRU arithmetic, make
page_is_file_cache() a real predicate function and adjust the
boolean-using callsites to drop those pesky double negations.";Johannes Weiner;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;mm: vmstat: add isolate pages;KOSAKI Motohiro;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;"If the system is running a heavy load of processes then concurrent reclaim
can isolate a large number of pages from the LRU";KOSAKI Motohiro;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;"/proc/vmstat and the
output generated for an OOM do not show how many pages were isolated";KOSAKI Motohiro;2009-09-22;1;0
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;This has been observed during process fork bomb testing (mstctl11 in LTP);KOSAKI Motohiro;2009-09-22;0;0
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;This patch shows the information about isolated pages;KOSAKI Motohiro;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;Reproduced via;KOSAKI Motohiro;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODphNzMxMjg2ZGU2MjI5NGI2M2Q4Y2ViM2M1OTE0YWM1MmNjMTdlNjkw;"% ./hackbench 140 process 1000
   => OOM occur
active_anon:146 inactive_anon:0 isolated_anon:49245
 active_file:79 inactive_file:18 isolated_file:113
 unevictable:0 dirty:0 writeback:0 unstable:0 buffer:39
 free:370 slab_reclaimable:309 slab_unreclaimable:5492
 mapped:53 shmem:15 pagetables:28140 bounce:0";KOSAKI Motohiro;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;mm: oom analysis: add shmem vmstat;KOSAKI Motohiro;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;Recently we encountered OOM problems due to memory use of the GEM cache;KOSAKI Motohiro;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;"Generally a large amuont of Shmem/Tmpfs pages tend to create a memory
shortage problem";KOSAKI Motohiro;2009-09-22;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;"We often use the following calculation to determine the amount of shmem
pages";KOSAKI Motohiro;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;"shmem = NR_ACTIVE_ANON + NR_INACTIVE_ANON - NR_ANON_PAGES
however the expression does not consider isolated and mlocked pages";KOSAKI Motohiro;2009-09-22;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YjAyMTA4YWMxYjMzNTRhMjJiMGQ4M2M2ODQ3OTc2OTJlZmRjMzk1;This patch adds explicit accounting for pages used by shmem and tmpfs.;KOSAKI Motohiro;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODphYmZjMzQ4ODExOGQ0OGEyYjhjY2U1YTIzNDU5MDFhYWM2YjAzZmVl;memory hotplug: migrate swap cache page;Shaohua Li;2009-09-22;1;0
MDY6Q29tbWl0MjMyNTI5ODphYmZjMzQ4ODExOGQ0OGEyYjhjY2U1YTIzNDU5MDFhYWM2YjAzZmVl;In test, some pages in swap-cache can't be migrated, as they aren't rmap;Shaohua Li;2009-09-22;0;1
MDY6Q29tbWl0MjMyNTI5ODphYmZjMzQ4ODExOGQ0OGEyYjhjY2U1YTIzNDU5MDFhYWM2YjAzZmVl;"unmap_and_move() ignores swap-cache page which is just read in and hasn't
rmap (see the comments in the code), but swap_aops provides .migratepage";Shaohua Li;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODphYmZjMzQ4ODExOGQ0OGEyYjhjY2U1YTIzNDU5MDFhYWM2YjAzZmVl;Better to migrate such pages instead of ignore them.;Shaohua Li;2009-09-22;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;HWPOISON: Use bitmask/action code for try_to_unmap behaviour;Andi Kleen;2009-09-16;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"try_to_unmap currently has multiple modi (migration, munlock, normal unmap)
which are selected by magic flag variables";Andi Kleen;2009-09-16;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"The logic is not very straight
forward, because each of these flag change multiple behaviours (e.g";Andi Kleen;2009-09-16;0;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"migration turns off aging, not only sets up migration ptes etc.)
Also the different flags interact in magic ways";Andi Kleen;2009-09-16;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"A later patch in this series adds another mode to try_to_unmap, so
this becomes quickly unmanageable";Andi Kleen;2009-09-16;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;and some additional flags as modifiers (ignore mlock, ignore aging);Andi Kleen;2009-09-16;0;0
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"This makes the logic more straight forward and allows easier extension
to new behaviours";Andi Kleen;2009-09-16;0;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"Change all the caller to declare what they want to
This patch is supposed to be a nop in behaviour";Andi Kleen;2009-09-16;1;1
MDY6Q29tbWl0MjMyNTI5ODoxNGZhMzFiODljNWFlNzllNDEzMWRhNDE3NjEzNzhhNmRmNjc0MzUy;"If anyone can prove
it is not that would be a bug.";Andi Kleen;2009-09-16;0;1
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;migration: only migrate_prep() once per move_pages();Brice Goglin;2009-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;migrate_prep() is fairly expensive (72us on 16-core barcelona 1.9GHz);Brice Goglin;2009-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;"Commit 3140a2273009c01c27d316f35ab76a37e105fdd8 improved move_pages()
throughput by breaking it into chunks, but it also made migrate_prep() be
called once per chunk (every 128pages or so) instead of once per
move_pages()";Brice Goglin;2009-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;"This patch reverts to calling migrate_prep() only once per chunk as we did
before 2.6.29";Brice Goglin;2009-06-16;1;0
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;" It is also a followup to commit
0aedadf91a70a11c4a3e7c7d99b21e5528af8d5d (""mm: move migrate_prep out from
under mmap_sem"")";Brice Goglin;2009-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODozNTI4MmEyZGU0ZTVlNGUxNzNhYjYxYWE5ZDcwMTU4ODYwMjFhODIx;"This improves migration throughput on the above machine from 600MB/s to
750MB/s.";Brice Goglin;2009-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo2NDg0ZWIzZTJhODE4MDc3MjJjNWYyOGVmZWY5NGQ4MzM4YjdiOTk2;page allocator: do not check NUMA node ID when the caller knows the node is valid;Mel Gorman;2009-06-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo2NDg0ZWIzZTJhODE4MDc3MjJjNWYyOGVmZWY5NGQ4MzM4YjdiOTk2;"Callers of alloc_pages_node() can optionally specify -1 as a node to mean
""allocate from the current node""";Mel Gorman;2009-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo2NDg0ZWIzZTJhODE4MDc3MjJjNWYyOGVmZWY5NGQ4MzM4YjdiOTk2;" However, a number of the callers in
fast paths know for a fact their node is valid";Mel Gorman;2009-06-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo2NDg0ZWIzZTJhODE4MDc3MjJjNWYyOGVmZWY5NGQ4MzM4YjdiOTk2;" To avoid a comparison and
branch, this patch adds alloc_pages_exact_node() that only checks the nid
with VM_BUG_ON()";Mel Gorman;2009-06-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo2NDg0ZWIzZTJhODE4MDc3MjJjNWYyOGVmZWY5NGQ4MzM4YjdiOTk2;" Callers that know their node is valid are then
converted.";Mel Gorman;2009-06-16;0;0
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;FS-Cache: Recruit a page flags for cache management;David Howells;2009-04-03;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;Recruit a page flag to aid in cache management;David Howells;2009-04-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;" The following extra flag is
defined";David Howells;2009-04-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;" (1) PG_fscache (PG_private_2)
     The marked page is backed by a local cache and is pinning resources in the
     cache driver";David Howells;2009-04-03;0;1
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;"If PG_fscache is set, then things that checked for PG_private will now also
check for that";David Howells;2009-04-03;1;1
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi; This includes things like truncation and page invalidation;David Howells;2009-04-03;1;0
MDY6Q29tbWl0MjMyNTI5ODoyNjZjZjY1OGVmY2Y2YWMzMzU0MWE0Njc0MGY3NGY1MGM3OWQyYjZi;"The function page_has_private() had been added to make the checks for both
PG_private and PG_private_2 at the same time.";David Howells;2009-04-03;0;1
MDY6Q29tbWl0MjMyNTI5ODoxMDAxYzlmYjg3MjFhYjM5NWUyMWY1NzFlZDJhYWE1MjNjZGQxZTI5;"migration: migrate_vmas should check ""vma""";Daisuke Nishimura;2009-02-11;1;1
MDY6Q29tbWl0MjMyNTI5ODoxMDAxYzlmYjg3MjFhYjM5NWUyMWY1NzFlZDJhYWE1MjNjZGQxZTI5;"migrate_vmas() should check ""vma"" not ""vma->vm_next"" for for-loop condition.";Daisuke Nishimura;2009-02-11;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzhiYjlmNWU4NDBlZGRiZjU0ZTRmNjJmNmM1YmE5YjNhZTEyYzlk;[CVE-2009-0029] System call wrappers part 28;Heiko Carstens;2009-01-14;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzhiYjlmNWU4NDBlZGRiZjU0ZTRmNjJmNmM1YmE5YjNhZTEyYzlk;;Heiko Carstens;2009-01-14;0;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;memcg: simple migration handling;KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;"Now, management of ""charge"" under page migration is done under following
manner";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;"(Assume migrate page contents from oldpage to newpage)
 before
  - ""newpage"" is charged before migration";KAMEZAWA Hiroyuki;2009-01-08;0;1
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4; at success;KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;"  - ""oldpage"" is uncharged at somewhere(unmap, radix-tree-replace)
 at failure
  - ""newpage"" is uncharged";KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;But (*1) is not reliable....because of GFP_ATOMIC;KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;This patch tries to change behavior as following by charge/commit/cancel ops;KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;" before
  - charge PAGE_SIZE (no target page)
 success
  - commit charge against ""newpage""";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;" failure
  - commit charge against ""oldpage""";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODowMWIxYWU2M2MyMjcwY2JhY2ZkNDNmZWE5NDU3OGMxNzk1MGViNTQ4;"    (PCG_USED bit works effectively to avoid double-counting)
  - if ""oldpage"" is obsolete, cancel charge of PAGE_SIZE.";KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;memcg: introduce charge-commit-cancel style of functions;KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;There is a small race in do_swap_page();KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;" When the page swapped-in is
charged, the mapcount can be greater than 0";KAMEZAWA Hiroyuki;2009-01-08;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;" But, at the same time some
process (shares it ) call unmap and make mapcount 1->0 and the page is
uncharged";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"      CPUA 			CPUB
       mapcount == 1";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;                                (2) mapcount 1 => 0;KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;			        (3) uncharge();KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"(success)
   (4) set page's rmap()
       mapcount 0=>1
Then, this swap page's account is leaked";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;For fixing this, I added a new interface;KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - charge
   account to res_counter by PAGE_SIZE and try to free pages if necessary";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - commit
   register page_cgroup and add to LRU if necessary";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - cancel
   uncharge PAGE_SIZE because of do_swap_page failure";KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"     CPUA
  (1) charge (always)
  (2) set page's rmap (mapcount > 0)
  (3) commit charge was necessary or not after set_pte()";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;This protocol uses PCG_USED bit on page_cgroup for avoiding over accounting;KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;Usual mem_cgroup_charge_common() does charge -> commit at a time;KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;And this patch also adds following function to clarify all charges;KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - mem_cgroup_newpage_charge() ....replacement for mem_cgroup_charge()
	called against newly allocated anon pages";KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - mem_cgroup_charge_migrate_fixup()
        called only from remove_migration_ptes()";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"	we'll have to rewrite this later.(this patch just keeps old behavior)
	This function will be removed by additional patch to make migration
	clearer";KAMEZAWA Hiroyuki;2009-01-08;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"Good for clarifying ""what we do""
Then, we have 4 following charge points";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;"  - newpage
  - swap-in
  - add-to-cache";KAMEZAWA Hiroyuki;2009-01-08;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YTgxYjg4Y2I1M2UzMzVmZjdkMDE5ZTYzOThjOTU3OTJjODE3ZDkz;  - migration.;KAMEZAWA Hiroyuki;2009-01-08;0;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDkxYWRkMDlmNGJhZDVmNGQ0MjMzYjEzZmFhMzkyZjBjNGIxNmJl;mm: add Set,ClearPageSwapCache stubs;Hugh Dickins;2009-01-06;1;0
MDY6Q29tbWl0MjMyNTI5ODo2ZDkxYWRkMDlmNGJhZDVmNGQ0MjMzYjEzZmFhMzkyZjBjNGIxNmJl;"If we add NOOP stubs for SetPageSwapCache() and ClearPageSwapCache(), then
we can remove the #ifdef CONFIG_SWAPs from mm/migrate.c.";Hugh Dickins;2009-01-06;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YmQxNDU1YzIzOTY3MjA4MWQwZTdmMDg2ZTg5OWI4Y2JjN2E5ODQ0;mm: move_pages: no need to set pp->page to ZERO_PAGE(0) by default;Brice Goglin;2009-01-06;0;0
MDY6Q29tbWl0MjMyNTI5ODo1YmQxNDU1YzIzOTY3MjA4MWQwZTdmMDg2ZTg5OWI4Y2JjN2E5ODQ0;"pp->page is never used when not set to the right page, so there is no need
to set it to ZERO_PAGE(0) by default.";Brice Goglin;2009-01-06;0;0
MDY6Q29tbWl0MjMyNTI5ODozMTQwYTIyNzMwMDljMDFjMjdkMzE2ZjM1YWI3NmEzN2UxMDVmZGQ4;mm: rework do_pages_move() to work on page_sized chunks;Brice Goglin;2009-01-06;1;0
MDY6Q29tbWl0MjMyNTI5ODozMTQwYTIyNzMwMDljMDFjMjdkMzE2ZjM1YWI3NmEzN2UxMDVmZGQ4;"Rework do_pages_move() to work by page-sized chunks of struct page_to_node
that are passed to do_move_page_to_node_array()";Brice Goglin;2009-01-06;0;1
MDY6Q29tbWl0MjMyNTI5ODozMTQwYTIyNzMwMDljMDFjMjdkMzE2ZjM1YWI3NmEzN2UxMDVmZGQ4;" We now only have to
allocate a single page instead a possibly very large vmalloc area to store
all page_to_node entries";Brice Goglin;2009-01-06;0;1
MDY6Q29tbWl0MjMyNTI5ODozMTQwYTIyNzMwMDljMDFjMjdkMzE2ZjM1YWI3NmEzN2UxMDVmZGQ4;"As a result, new_page_node() will now have a very small lookup, hidding
much of the overall sys_move_pages() overhead.";Brice Goglin;2009-01-06;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMDk1YWRiYzIxMWY5ZjRlOTkwZWFjN2Q2Y2I0NDBkZTM1ZTRmMDVm;mm: Don't touch uninitialized variable in do_pages_stat_array();KOSAKI Motohiro;2008-12-16;0;1
MDY6Q29tbWl0MjMyNTI5ODpjMDk1YWRiYzIxMWY5ZjRlOTkwZWFjN2Q2Y2I0NDBkZTM1ZTRmMDVm;"Commit 80bba1290ab5122c60cdb73332b26d288dc8aedd removed one necessary
variable initialization";KOSAKI Motohiro;2008-12-16;1;1
MDY6Q29tbWl0MjMyNTI5ODpjMDk1YWRiYzIxMWY5ZjRlOTkwZWFjN2Q2Y2I0NDBkZTM1ZTRmMDVm; As a result following warning happened;KOSAKI Motohiro;2008-12-16;1;0
MDY6Q29tbWl0MjMyNTI5ODpjMDk1YWRiYzIxMWY5ZjRlOTkwZWFjN2Q2Y2I0NDBkZTM1ZTRmMDVm;"    CC      mm/migrate.o
  mm/migrate.c: In function 'sys_move_pages'";KOSAKI Motohiro;2008-12-16;0;0
MDY6Q29tbWl0MjMyNTI5ODpjMDk1YWRiYzIxMWY5ZjRlOTkwZWFjN2Q2Y2I0NDBkZTM1ZTRmMDVm;"  mm/migrate.c:1001: warning: 'err' may be used uninitialized in this function
More unfortunately, if find_vma() failed, kernel read uninitialized
memory.";KOSAKI Motohiro;2008-12-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MGJiYTEyOTBhYjUxMjJjNjBjZGI3MzMzMmIyNmQyODhkYzhhZWRk;mm: no get_user/put_user while holding mmap_sem in do_pages_stat?;Brice Goglin;2008-12-09;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MGJiYTEyOTBhYjUxMjJjNjBjZGI3MzMzMmIyNmQyODhkYzhhZWRk;"Since commit 2f007e74bb85b9fc4eab28524052161703300f1a, do_pages_stat()
gets the page address from user-space and puts the corresponding status
back while holding the mmap_sem for read";Brice Goglin;2008-12-09;0;0
MDY6Q29tbWl0MjMyNTI5ODo4MGJiYTEyOTBhYjUxMjJjNjBjZGI3MzMzMmIyNmQyODhkYzhhZWRk;" There is no need to hold
mmap_sem there while some page-faults may occur";Brice Goglin;2008-12-09;0;1
MDY6Q29tbWl0MjMyNTI5ODo4MGJiYTEyOTBhYjUxMjJjNjBjZGI3MzMzMmIyNmQyODhkYzhhZWRk;"This patch adds a temporary address and status buffer so as to only
hold mmap_sem while working on these kernel buffers";Brice Goglin;2008-12-09;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MGJiYTEyOTBhYjUxMjJjNjBjZGI3MzMzMmIyNmQyODhkYzhhZWRk;" This is
implemented by extracting do_pages_stat_array() out of do_pages_stat().";Brice Goglin;2008-12-09;0;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4NTUwZGVlZDk2Njg3ZjI5OTkyZDcxMWE4OGVhMjFjZmY0ZDI2;migration: fix writepage error;Hugh Dickins;2008-11-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiZGE4NTUwZGVlZDk2Njg3ZjI5OTkyZDcxMWE4OGVhMjFjZmY0ZDI2;"Page migration's writeout() has got understandably confused by the nasty
AOP_WRITEPAGE_ACTIVATE case: as in normal success, a writepage() error has
unlocked the page, so writeout() then needs to relock it.";Hugh Dickins;2008-11-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNjllOGQ5YzAxZGIyYWRjNTAzNDY0OTkzYzM1ODkwMWM5YWY5ZGU0;CRED: Use RCU to access another task's creds and to release a task's own creds;David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNjllOGQ5YzAxZGIyYWRjNTAzNDY0OTkzYzM1ODkwMWM5YWY5ZGU0;Use RCU to access another task's creds and to release a task's own creds;David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpjNjllOGQ5YzAxZGIyYWRjNTAzNDY0OTkzYzM1ODkwMWM5YWY5ZGU0;"This means that it will be possible for the credentials of a task to be
replaced without another task (a) requiring a full lock to read them, and (b)
seeing deallocated memory.";David Howells;2008-11-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNmRmZjNlYzVlMTE2ZTNhZjZmNTM3ZDRjYWVkY2FkNmI5ZTUwODJh;CRED: Separate task security context from task_struct;David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNmRmZjNlYzVlMTE2ZTNhZjZmNTM3ZDRjYWVkY2FkNmI5ZTUwODJh;Separate the task security context from task_struct;David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNmRmZjNlYzVlMTE2ZTNhZjZmNTM3ZDRjYWVkY2FkNmI5ZTUwODJh;" At this point, the
security data is temporarily embedded in the task_struct with two pointers
pointing to it";David Howells;2008-11-13;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNmRmZjNlYzVlMTE2ZTNhZjZmNTM3ZDRjYWVkY2FkNmI5ZTUwODJh;"Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
entry.S via asm-offsets";David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNmRmZjNlYzVlMTE2ZTNhZjZmNTM3ZDRjYWVkY2FkNmI5ZTUwODJh;With comment fixes ;David Howells;2008-11-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NmFhYzBlOWExNzc0MmU2MGQ0MDhiZTFhNzA2ZTlhYWFkMzcwODkx;CRED: Wrap task credential accesses in the core kernel;David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NmFhYzBlOWExNzc0MmU2MGQ0MDhiZTFhNzA2ZTlhYWFkMzcwODkx;"Wrap access to task credentials so that they can be separated more easily from
the task_struct during the introduction of COW creds";David Howells;2008-11-13;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NmFhYzBlOWExNzc0MmU2MGQ0MDhiZTFhNzA2ZTlhYWFkMzcwODkx;Change most current->(|e|s|fs)[ug]id to current_(|e|s|fs)[ug]id();David Howells;2008-11-13;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NmFhYzBlOWExNzc0MmU2MGQ0MDhiZTFhNzA2ZTlhYWFkMzcwODkx;Change some task->e?[ug]id to task_e?[ug]id();David Howells;2008-11-13;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NmFhYzBlOWExNzc0MmU2MGQ0MDhiZTFhNzA2ZTlhYWFkMzcwODkx;" In some places it makes more
sense to use RCU directly rather than a convenient wrapper; these will be
addressed by later patches.";David Howells;2008-11-13;1;1
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;mm: move migrate_prep out from under mmap_sem;Christoph Lameter;2008-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"Move the migrate_prep outside the mmap_sem for the following system calls
1";Christoph Lameter;2008-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"sys_move_pages
2";Christoph Lameter;2008-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"sys_migrate_pages
3";Christoph Lameter;2008-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"sys_mbind()
It really does not matter when we flush the lru";Christoph Lameter;2008-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;" The system is free to
add pages onto the lru even during migration which will make the page
migration either skip the page (mbind, migrate_pages) or return a busy
state (move_pages)";Christoph Lameter;2008-11-06;1;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;Fixes this lockdep warning (and potential deadlock);Christoph Lameter;2008-11-06;0;1
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"Some VM place has
      mmap_sem -> kevent_wq via lru_add_drain_all()
net/core/dev.c::dev_ioctl()  has
     rtnl_lock  ->  mmap_sem        (*) the ioctl has copy_from_user() and it can do page fault";Christoph Lameter;2008-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODowYWVkYWRmOTFhNzBhMTFjNGEzZTdjN2Q5OWIyMWU1NTI4YWY4ZDVk;"linkwatch_event has
     kevent_wq -> rtnl_lock";Christoph Lameter;2008-11-06;0;0
MDY6Q29tbWl0MjMyNTI5ODpiN2FiZWE5NjMwYmM4ZmZjNjYzYTc1MWU0NjY4MGRiMjVjNGNkZjhk;memcg: make page->mapping NULL before uncharge;KAMEZAWA Hiroyuki;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiN2FiZWE5NjMwYmM4ZmZjNjYzYTc1MWU0NjY4MGRiMjVjNGNkZjhk;"This patch tries to make page->mapping to be NULL before
mem_cgroup_uncharge_cache_page() is called";KAMEZAWA Hiroyuki;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiN2FiZWE5NjMwYmM4ZmZjNjYzYTc1MWU0NjY4MGRiMjVjNGNkZjhk;"""page->mapping == NULL"" is a good check for ""whether the page is still
radix-tree or not""";KAMEZAWA Hiroyuki;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiN2FiZWE5NjMwYmM4ZmZjNjYzYTc1MWU0NjY4MGRiMjVjNGNkZjhk; This patch also adds BUG_ON() to;KAMEZAWA Hiroyuki;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo1ZTlhMGYwMjNiZWUwMmJmYjk0ZTA4NTkwZDk5ODY2MGMwMWY1YTQ5;mm: extract do_pages_move() out of sys_move_pages();Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo1ZTlhMGYwMjNiZWUwMmJmYjk0ZTA4NTkwZDk5ODY2MGMwMWY1YTQ5;"To prepare the chunking, move the sys_move_pages() code that is used when
nodes!=NULL into do_pages_move()";Brice Goglin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo1ZTlhMGYwMjNiZWUwMmJmYjk0ZTA4NTkwZDk5ODY2MGMwMWY1YTQ5;" And rename do_move_pages() into
do_move_page_to_node_array().";Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODoyZjAwN2U3NGJiODViOWZjNGVhYjI4NTI0MDUyMTYxNzAzMzAwZjFh;mm: don't vmalloc a huge page_to_node array for do_pages_stat();Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODoyZjAwN2U3NGJiODViOWZjNGVhYjI4NTI0MDUyMTYxNzAzMzAwZjFh;do_pages_stat() does not need any page_to_node entry for real;Brice Goglin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODoyZjAwN2U3NGJiODViOWZjNGVhYjI4NTI0MDUyMTYxNzAzMzAwZjFh;" Just pass
the pointers to the user-space page address array and to the user-space
status array, and have do_pages_stat() traverse the former and fill the
latter directly.";Brice Goglin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;mm: stop returning -ENOENT from sys_move_pages() if nothing got migrated;Brice Goglin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;A patchset reworking sys_move_pages();Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;" It removes the possibly large
vmalloc by using multiple chunks when migrating large buffers";Brice Goglin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;" It also
dramatically increases the throughput for large buffers since the lookup
in new_page_node() is now limited to a single chunk, causing the quadratic
complexity to have a much slower impact";Brice Goglin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;" There is no need to use any
radix-tree-like structure to improve this lookup";Brice Goglin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"sys_move_pages() duration on a 4-quadcore-opteron 2347HE (1.9Gz),
migrating between nodes #2 and #3";Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"	length		move_pages (us)		move_pages+patch (us)
	4kB		126			98
	40kB		198			168
	400kB		963			937
	4MB		12503			11930
	40MB		246867			11848
Patches #1 and #4 are the important ones";Brice Goglin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"1) stop returning -ENOENT from sys_move_pages() if nothing got migrated
2) don't vmalloc a huge page_to_node array for do_pages_stat()
3) extract do_pages_move() out of sys_move_pages()
4) rework do_pages_move() to work on page_sized chunks
5) move_pages: no need to set pp->page to ZERO_PAGE(0) by default
This patch";Brice Goglin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"There is no point in returning -ENOENT from sys_move_pages() if all pages
were already on the right node, while we return 0 if only 1 page was not";Brice Goglin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"Most application don't know where their pages are allocated, so it's not
an error to try to migrate them anyway";Brice Goglin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;"Just return 0 and let the status array in user-space be checked if the
application needs details";Brice Goglin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODplNzhiYmZhODI2MjQyNDQxN2EyOTM0OWE4MDY0YTUzNTA1MzkxMmI5;It will make the upcoming chunked-move_pages() support much easier.;Brice Goglin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;mlock: mlocked pages are unevictable;Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"Make sure that mlocked pages also live on the unevictable LRU, so kswapd
will not scan them over and over again";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;This is achieved through various strategies;Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"1) add yet another page flag--PG_mlocked--to indicate that
   the page is locked for efficient testing in vmscan and,
   optionally, fault path";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" This allows early culling of
   unevictable pages, preventing them from getting to
   page_referenced()/try_to_unmap()";Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" Also allows separate
   accounting of mlock'd pages, as Nick's original patch
   did";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"   Note:  Nick's original mlock patch used a PG_mlocked
   flag";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" I had removed this in favor of the PG_unevictable
   flag + an mlock_count [new page struct member]";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" I
   restored the PG_mlocked flag to eliminate the new
   count field";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"2) add the mlock/unevictable infrastructure to mm/mlock.c,
   with internal APIs in mm/internal.h";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" This is a rework
   of Nick's original patch to these files, taking into
   account that mlocked pages are now kept on unevictable
   LRU list";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"3) update vmscan.c:page_evictable() to check PageMlocked()
   and, if vma passed in, the vm_flags";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" Note that the vma
   and then only if the ""cull unevictable pages in fault
   path"" patch is included";Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;"4) add try_to_unlock() to rmap.c to walk a page's rmap and
   ClearPageMlocked() if no other vmas have it mlocked";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;   Reuses as much of try_to_unmap() as possible;Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" This
   effectively replaces the use of one of the lru list links
   as an mlock count";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" If this mechanism let's pages in mlocked
   vmas leak through w/o PG_mlocked set [I don't know that it
   does], we should catch them later in try_to_unmap()";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;" One
   hopes this will be rare, as it will be relatively expensive";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjkxZjAwMDM5M2Y1YTBiNjc5MDEyYjM5ZDc5ZmJjODVjMDE4MjMz;Original mm/internal.h, mm/rmap.c and mm/mlock.c changes:;Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;Unevictable LRU Infrastructure;Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"When the system contains lots of mlocked or otherwise unevictable pages,
the pageout code (kswapd) can spend lots of time scanning over these
pages";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" Worse still, the presence of lots of unevictable pages can confuse
kswapd into thinking that more aggressive pageout modes are required,
resulting in all kinds of bad behaviour";Lee Schermerhorn;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"Infrastructure to manage pages excluded from reclaim--i.e., hidden from
vmscan";Lee Schermerhorn;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1; Based on a patch by Larry Woodman of Red Hat;Lee Schermerhorn;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" Reworked to
maintain ""unevictable"" pages on a separate per-zone LRU list, to ""hide""
them from vmscan";Lee Schermerhorn;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"Kosaki Motohiro added the support for the memory controller unevictable
lru list";Lee Schermerhorn;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;Pages on the unevictable list have both PG_unevictable and PG_lru set;Lee Schermerhorn;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"Thus, PG_unevictable is analogous to and mutually exclusive with
PG_active--it specifies which LRU list the page is on";Lee Schermerhorn;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"The unevictable infrastructure is enabled by a new mm Kconfig option
[CONFIG_]UNEVICTABLE_LRU";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"A new function 'page_evictable(page, vma)' in vmscan.c tests whether or
not a page may be evictable";Lee Schermerhorn;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" Subsequent patches will add the various
!evictable tests";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" We'll want to keep these tests light-weight for use in
shrink_active_list() and, possibly, the fault path";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;"To avoid races between tasks putting pages [back] onto an LRU list and
tasks that might be moving the page from non-evictable to evictable state,
the new function 'putback_lru_page()' -- inverse to 'isolate_lru_page()'
-- tests the ""evictability"" of a page after placing it on the LRU, before
dropping the reference";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" If the page has become unevictable,
putback_lru_page() will redo the 'putback', thus moving the page to the
unevictable list";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OTRiYzMxMDQxOWFjOTVmNGZhNDE0MmRjMzY0NDAxYTdlNjA3ZjY1;" This way, we avoid ""stranding"" evictable pages on the
unevictable list.";Lee Schermerhorn;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;define page_file_cache() function;Rik van Riel;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;Define page_file_cache() function to answer the question;Rik van Riel;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;"	is page backed by a file?
Originally part of Rik van Riel's split-lru patch";Rik van Riel;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;" Extracted to make
available for other, independent reclaim patches";Rik van Riel;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;"Moved inline function to linux/mm_inline.h where it will be needed by
subsequent ""split LRU"" and ""noreclaim"" patches";Rik van Riel;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;"Unfortunately this needs to use a page flag, since the PG_swapbacked state
needs to be preserved all the way to the point where the page is last
removed from the LRU";Rik van Riel;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;" Trying to derive the status from other info in the
page resulted in wrong VM statistics in earlier split VM patchsets";Rik van Riel;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMmUxODUzODRmNTM0NzgxZmQyMmY1Y2UxNzBiMmFkMjZmOTdkZjcw;"The total number of page flags in use on a 32 bit machine after this patch
is 19.";Rik van Riel;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODpmMDRlOWViYmU0OTA5ZjlhNDFlZmQ1NTE0OWJjMzUzMjk5ZjRlODNi;swap: use an array for the LRU pagevecs;KOSAKI Motohiro;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpmMDRlOWViYmU0OTA5ZjlhNDFlZmQ1NTE0OWJjMzUzMjk5ZjRlODNi;Turn the pagevecs into an array just like the LRUs;KOSAKI Motohiro;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpmMDRlOWViYmU0OTA5ZjlhNDFlZmQ1NTE0OWJjMzUzMjk5ZjRlODNi;" This significantly
cleans up the source code and reduces the size of the kernel by about 13kB
after all the LRU lists have been created further down in the split VM
patch series.";KOSAKI Motohiro;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;vmscan: move isolate_lru_page() to vmscan.c;Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"On large memory systems, the VM can spend way too much time scanning
through pages that it cannot (or should not) evict from memory";Nick Piggin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;" Not only
does it use up CPU time, but it also provokes lock contention and can
leave large systems under memory presure in a catatonic state";Nick Piggin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;This patch series improves VM scalability by;Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"1) putting filesystem backed, swap backed and unevictable pages
   onto their own LRUs, so the system only scans the pages that it
   can/should evict from memory
2) switching to two handed clock replacement for the anonymous LRUs,
   so the number of pages that need to be scanned when the system
   starts swapping is bound to a reasonable number
3) keeping unevictable pages off the LRU completely, so the
   VM does not waste CPU time scanning them";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"ramfs, ramdisk,
   SHM_LOCKED shared memory segments and mlock()ed VMA pages
   are keept on the unevictable list";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;This patch;Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;isolate_lru_page logically belongs to be in vmscan.c than migrate.c;Nick Piggin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"It is tough, because we don't need that function without memory migration
so there is a valid argument to have it in migrate.c";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;" However a
subsequent patch needs to make use of it in the core mm, so we can happily
move it to vmscan.c";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"Also, make the function a little more generic by not requiring that it
adds an isolated page to a given list";Nick Piggin;2008-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh; Callers can do that;Nick Piggin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;"	Note that we now have '__isolate_lru_page()', that does
	something quite different, visible outside of vmscan.c
	for use with memory controller";Nick Piggin;2008-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;" Methinks we need to
	rationalize these names/purposes";Nick Piggin;2008-10-19;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MjY5NWE4NGViOGYyZTcxOGJmNGRmYjIxNzAwYWZhYTdhMDhlMGVh;--lts;Nick Piggin;2008-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODo1MjlhZTlhYWEwODM3OGNmZTJhNDM1MGJkZWQ3NmYzMmNjOGZmMGNl;mm: rename page trylock;Nick Piggin;2008-08-02;1;0
MDY6Q29tbWl0MjMyNTI5ODo1MjlhZTlhYWEwODM3OGNmZTJhNDM1MGJkZWQ3NmYzMmNjOGZmMGNl;"Converting page lock to new locking bitops requires a change of page flag
operation naming, so we might as well convert it to something nicer
(!TestSetPageLocked_Lock => trylock_page, SetPageLocked => set_page_locked)";Nick Piggin;2008-08-02;1;1
MDY6Q29tbWl0MjMyNTI5ODo1MjlhZTlhYWEwODM3OGNmZTJhNDM1MGJkZWQ3NmYzMmNjOGZmMGNl;This also facilitates lockdeping of page lock.;Nick Piggin;2008-08-02;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZkNjIzMTI3OWJlM2MzYmRkMDJlZDk5ZjliMGViMTk1OTc4MDY0;mm: spinlock tree_lock;Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZkNjIzMTI3OWJlM2MzYmRkMDJlZDk5ZjliMGViMTk1OTc4MDY0;mapping->tree_lock has no read lockers;Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODoxOWZkNjIzMTI3OWJlM2MzYmRkMDJlZDk5ZjliMGViMTk1OTc4MDY0;" convert the lock from an rwlock
to a spinlock.";Nick Piggin;2008-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;mm: speculative page references;Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;"If we can be sure that elevating the page_count on a pagecache page will
pin it, we can speculatively run this operation, and subsequently check to
see if we hit the right page rather than relying on holding a lock or
otherwise pinning a reference to the page";Nick Piggin;2008-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;"This can be done if get_page/put_page behaves consistently throughout the
whole tree (ie";Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;" if we ""get"" the page after it has been used for something
else, we must be able to free it with a put_page)";Nick Piggin;2008-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;"Actually, there is a period where the count behaves differently: when the
page is free or if it is a constituent page of a compound page";Nick Piggin;2008-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;" We need
an atomic_inc_not_zero operation to ensure we don't try to grab the page
in either case";Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;This patch introduces the core locking protocol to the pagecache (ie;Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;"adds page_cache_get_speculative, and tweaks some update-side code to make
it work)";Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjg2NzgxZDVmMmU5Yzg0NmUwMTJhMzk2NTNhMTY2ZTlkMzE3Nzdk;"Thanks to Hugh for pointing out an improvement to the algorithm setting
page_count to zero when we have control of all references, in order to
hold off speculative getters.";Nick Piggin;2008-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;memcg: remove refcnt from page_cgroup;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"memcg: performance improvements
Patch Description
 1/5 ..";KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"remove refcnt fron page_cgroup patch (shmem handling is fixed)
 2/5 ..";KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"swapcache handling patch
 3/5 ..";KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"add helper function for shmem's memory reclaim patch
 4/5 ..";KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"optimize by likely/unlikely ppatch
 5/5 ..";KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"remove redundunt check patch (shmem handling is fixed.)
Unix bench result";KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"== 2.6.26-rc2-mm1 + memory resource controller
Execl Throughput                           2915.4 lps   (29.6 secs, 3 samples)
C Compiler Throughput                      1019.3 lpm   (60.0 secs, 3 samples)
Shell Scripts (1 concurrent)               5796.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (8 concurrent)               1097.7 lpm   (60.0 secs, 3 samples)
Shell Scripts (16 concurrent)               565.3 lpm   (60.0 secs, 3 samples)
File Read 1024 bufsize 2000 maxblocks    1022128.0 KBps  (30.0 secs, 3 samples)
File Write 1024 bufsize 2000 maxblocks   544057.0 KBps  (30.0 secs, 3 samples)
File Copy 1024 bufsize 2000 maxblocks    346481.0 KBps  (30.0 secs, 3 samples)
File Read 256 bufsize 500 maxblocks      319325.0 KBps  (30.0 secs, 3 samples)
File Write 256 bufsize 500 maxblocks     148788.0 KBps  (30.0 secs, 3 samples)
File Copy 256 bufsize 500 maxblocks       99051.0 KBps  (30.0 secs, 3 samples)
File Read 4096 bufsize 8000 maxblocks    2058917.0 KBps  (30.0 secs, 3 samples)
File Write 4096 bufsize 8000 maxblocks   1606109.0 KBps  (30.0 secs, 3 samples)
File Copy 4096 bufsize 8000 maxblocks    854789.0 KBps  (30.0 secs, 3 samples)
Dc: sqrt(2) to 99 decimal places         126145.2 lpm   (30.0 secs, 3 samples)
                     INDEX VALUES
TEST                                        BASELINE     RESULT      INDEX
Execl Throughput                                43.0     2915.4      678.0
File Copy 1024 bufsize 2000 maxblocks         3960.0   346481.0      875.0
File Copy 256 bufsize 500 maxblocks           1655.0    99051.0      598.5
File Copy 4096 bufsize 8000 maxblocks         5800.0   854789.0     1473.8
Shell Scripts (8 concurrent)                     6.0     1097.7     1829.5
     FINAL SCORE                                                     991.3
== 2.6.26-rc2-mm1 + this set ==
Execl Throughput                           3012.9 lps   (29.9 secs, 3 samples)
C Compiler Throughput                       981.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (1 concurrent)               5872.0 lpm   (60.0 secs, 3 samples)
Shell Scripts (8 concurrent)               1120.3 lpm   (60.0 secs, 3 samples)
Shell Scripts (16 concurrent)               578.0 lpm   (60.0 secs, 3 samples)
File Read 1024 bufsize 2000 maxblocks    1003993.0 KBps  (30.0 secs, 3 samples)
File Write 1024 bufsize 2000 maxblocks   550452.0 KBps  (30.0 secs, 3 samples)
File Copy 1024 bufsize 2000 maxblocks    347159.0 KBps  (30.0 secs, 3 samples)
File Read 256 bufsize 500 maxblocks      314644.0 KBps  (30.0 secs, 3 samples)
File Write 256 bufsize 500 maxblocks     151852.0 KBps  (30.0 secs, 3 samples)
File Copy 256 bufsize 500 maxblocks      101000.0 KBps  (30.0 secs, 3 samples)
File Read 4096 bufsize 8000 maxblocks    2033256.0 KBps  (30.0 secs, 3 samples)
File Write 4096 bufsize 8000 maxblocks   1611814.0 KBps  (30.0 secs, 3 samples)
File Copy 4096 bufsize 8000 maxblocks    847979.0 KBps  (30.0 secs, 3 samples)
Dc: sqrt(2) to 99 decimal places         128148.7 lpm   (30.0 secs, 3 samples)
                     INDEX VALUES
TEST                                        BASELINE     RESULT      INDEX
Execl Throughput                                43.0     3012.9      700.7
File Copy 1024 bufsize 2000 maxblocks         3960.0   347159.0      876.7
File Copy 256 bufsize 500 maxblocks           1655.0   101000.0      610.3
File Copy 4096 bufsize 8000 maxblocks         5800.0   847979.0     1462.0
Shell Scripts (8 concurrent)                     6.0     1120.3     1867.2
     FINAL SCORE                                                    1004.6
This patch";KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;Remove refcnt from page_cgroup();KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"After this,
	* Anon page is newly mapped";KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;	* File page is added to mapping->tree;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;	* Anon page is fully unmapped;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;	* File page is removed from LRU;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;There is no change in behavior from user's view;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OTAyOWNkNTUwMjg0ZTMyZGUxM2Q2ZGQyZjc3YjcyM2M4YTBlNDQ0;"This patch also removes unnecessary calls in rmap.c which was used only for
refcnt mangement.";KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;memcg: better migration handling;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;"This patch changes page migration under memory controller to use a
Before";KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1; - page_cgroup is migrated from an old page to a new page;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;After;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1; - a new page is accounted , no reuse of page_cgroup;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;Pros;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1; - We can avoid compliated lock depndencies and races in migration;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;Cons;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1; - new param to mem_cgroup_charge_common();KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1; - mem_cgroup_getref() is added for handling ref_cnt ping-pong;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;"This version simplifies complicated lock dependency in page migraiton
under memory resource controller";KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  new refcnt sequence is following;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;a mapped page;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  prepage_migration() ....;KAMEZAWA Hiroyuki;2008-07-25;0;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;"+1 to NEW page
  try_to_unmap()      ....";KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;all refs to OLD page is gone;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  move_pages()        ....;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;+1 to NEW page if page cache;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  remap..;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;           ....;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;all refs from *map* is added to NEW one;KAMEZAWA Hiroyuki;2008-07-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  end_migration()     ....;KAMEZAWA Hiroyuki;2008-07-25;1;1
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;-1 to New page;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODplODU4OWNjMTg5Zjk2Yjg3MzQ4YWU4M2VhNGRiMzhlYWFjNjI0MTM1;  page's mapcount + (page_is_cache) refs are added to NEW one.;KAMEZAWA Hiroyuki;2008-07-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;mm: make CONFIG_MIGRATION available w/o CONFIG_NUMA;Gerald Schaefer;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;"We'd like to support CONFIG_MEMORY_HOTREMOVE on s390, which depends on
CONFIG_MIGRATION";Gerald Schaefer;2008-07-24;1;0
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;" So far, CONFIG_MIGRATION is only available with NUMA
support";Gerald Schaefer;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;"This patch makes CONFIG_MIGRATION selectable for architectures that define
ARCH_ENABLE_MEMORY_HOTREMOVE";Gerald Schaefer;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;" When MIGRATION is enabled w/o NUMA, the
kernel won't compile because migrate_vmas() does not know about
vm_ops->migrate() and vma_migratable() does not know about policy_zone";Gerald Schaefer;2008-07-24;0;1
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;"To fix this, those two functions can be restricted to '#ifdef CONFIG_NUMA'
because they are not being used w/o NUMA";Gerald Schaefer;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo4M2QxNjc0YTk0NjE0MWMzYzU5ZDQzMGU5NmMyMjRmNzkzN2U2MTU4;" vma_migratable() is moved over
from migrate.h to mempolicy.h.";Gerald Schaefer;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZjVjYTI2NTc4ODk3M2UzZjVhMTEyOWE5NmVlNGE5Y2JmNTg3ZjJi;mm/migrate.c should #include <linux/syscalls.h>;Adrian Bunk;2008-07-24;1;1
MDY6Q29tbWl0MjMyNTI5ODo0ZjVjYTI2NTc4ODk3M2UzZjVhMTEyOWE5NmVlNGE5Y2JmNTg3ZjJi;"Every file should include the headers containing the externs for its
global functions (in this case for sys_move_pages()).";Adrian Bunk;2008-07-24;1;0
MDY6Q29tbWl0MjMyNTI5ODpjZGU1MzUzNTk5MWZiYjVjMzRhMTU2NmYyNTk1NTI5N2MxNDg3Yjhk;Christoph has moved;Christoph Lameter;2008-07-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZGU1MzUzNTk5MWZiYjVjMzRhMTU2NmYyNTk1NTI5N2MxNDg3Yjhk;"Remove all clameter@sgi.com addresses from the kernel tree since they will
become invalid on June 27th";Christoph Lameter;2008-07-04;1;1
MDY6Q29tbWl0MjMyNTI5ODpjZGU1MzUzNTk5MWZiYjVjMzRhMTU2NmYyNTk1NTI5N2MxNDg3Yjhk;" Change my maintainer email address for the
slab allocators to cl@linux-foundation.org (which will be the new email
address for the future).";Christoph Lameter;2008-07-04;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;Reinstate ZERO_PAGE optimization in 'get_user_pages()' and fix XIP;Linus Torvalds;2008-06-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"KAMEZAWA Hiroyuki and Oleg Nesterov point out that since the commit
557ed1fa2620dc119adb86b34c614e152a629a80 (""remove ZERO_PAGE"") removed
the ZERO_PAGE from the VM mappings, any users of get_user_pages() will
generally now populate the VM with real empty pages needlessly";Linus Torvalds;2008-06-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"We used to get the ZERO_PAGE when we did the ""handle_mm_fault()"", but
since fault handling no longer uses ZERO_PAGE for new anonymous pages,
we now need to handle that special case in follow_page() instead";Linus Torvalds;2008-06-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"In particular, the removal of ZERO_PAGE effectively removed the core
file writing optimization where we would skip writing pages that had not
been populated at all, and increased memory pressure a lot by allocating
all those useless newly zeroed pages";Linus Torvalds;2008-06-20;0;0
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"This reinstates the optimization by making the unmapped PTE case the
same as for a non-existent page table, which already did this correctly";Linus Torvalds;2008-06-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"While at it, this also fixes the XIP case for follow_page(), where the
caller could not differentiate between the case of a page that simply
could not be used (because it had no ""struct page"" associated with it)
and a page that just wasn't mapped";Linus Torvalds;2008-06-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;"We do that by simply returning an error pointer for pages that could not
be turned into a ""struct page *""";Linus Torvalds;2008-06-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;" The error is arbitrarily picked to be
EFAULT, since that was what get_user_pages() already used for the
equivalent IO-mapped page case";Linus Torvalds;2008-06-20;0;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;[ Also removed an impossible test for pte_offset_map_lock() failing;Linus Torvalds;2008-06-20;1;1
MDY6Q29tbWl0MjMyNTI5ODo4OWY1YjdkYTJhNmJhZDJlODQ2NzA0MjJhYjgxOTIzODJhNWFlYjlm;  that's not how that function works ];Linus Torvalds;2008-06-20;0;1
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;mm: fix warning on memory offline;Nick Piggin;2008-04-30;1;1
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;"KAMEZAWA Hiroyuki found a warning message in the buffer dirtying code that
is coming from page migration caller";Nick Piggin;2008-04-30;0;0
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;"WARNING: at fs/buffer.c:720 __set_page_dirty+0x330/0x360()
What was happening is that migrate_page_copy wants to transfer the PG_dirty
bit from old page to new page, so what it would do is set_page_dirty(newpage)";Nick Piggin;2008-04-30;0;1
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;"However set_page_dirty() is used to set the entire page dirty, wheras in
this case, only part of the page was dirty, and it also was not uptodate";Nick Piggin;2008-04-30;0;1
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;"Marking the whole page dirty with set_page_dirty would lead to corruption or
unresolvable conditions -- a dirty && !uptodate page and dirty && !uptodate
buffers";Nick Piggin;2008-04-30;0;1
MDY6Q29tbWl0MjMyNTI5ODozYTkwMmM1ZjY4NTFjZDBiNjRjMzNlZmFhM2JkNTdhYTI3YTgyZWZi;however in the interests of keeping the change minimal...;Nick Piggin;2008-04-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;memcg: fix VM_BUG_ON from page migration;Hugh Dickins;2008-03-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;Page migration gave me free_hot_cold_page's VM_BUG_ON page->page_cgroup;Hugh Dickins;2008-03-04;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;"remove_migration_pte was calling mem_cgroup_charge on the new page whenever it
found a swap pte, before it had determined it to be a migration entry";Hugh Dickins;2008-03-04;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;" That
left a surplus reference count on the page_cgroup, so it was still attached
when the page was later freed";Hugh Dickins;2008-03-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;Move that mem_cgroup_charge down to where we're sure it's a migration entry;Hugh Dickins;2008-03-04;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;"We were already under i_mmap_lock or anon_vma->lock, so its GFP_KERNEL was
already inappropriate: change that to GFP_ATOMIC";Hugh Dickins;2008-03-04;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;"It's essential that remove_migration_pte removes all the migration entries,
other crashes follow if not";Hugh Dickins;2008-03-04;1;0
MDY6Q29tbWl0MjMyNTI5ODo5ODgzN2M3ZjgyZWY3OGFhMzhmNDA0NjJhYTJmY2FjNjhmZDNhY2Jm;" So proceed even when the charge fails: normally
it cannot, but after a mem_cgroup_force_empty it might - comment in the code.";Hugh Dickins;2008-03-04;0;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;bugfix for memory cgroup controller: migration under memory controller fix;KAMEZAWA Hiroyuki;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;While using memory control cgroup, page-migration under it works as following;KAMEZAWA Hiroyuki;2008-02-07;0;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5; 1;KAMEZAWA Hiroyuki;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;uncharge all refs at try to unmap;KAMEZAWA Hiroyuki;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5; 2;KAMEZAWA Hiroyuki;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;"charge regs again remove_migration_ptes()
This is simple but has following problems";KAMEZAWA Hiroyuki;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5; The page is uncharged and charged back again if *mapped*;KAMEZAWA Hiroyuki;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;"    - This means that cgroup before migration can be different from one after
      migration
    - If page is not mapped but charged as page cache, charge is just ignored
      (because not mapped, it will not be uncharged before migration)
      This is memory leak";KAMEZAWA Hiroyuki;2008-02-07;0;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;"This patch tries to keep memory cgroup at page migration by increasing
one refcnt during it";KAMEZAWA Hiroyuki;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;3 functions are added;KAMEZAWA Hiroyuki;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;" mem_cgroup_prepare_migration() --- increase refcnt of page->page_cgroup
 mem_cgroup_end_migration()     --- decrease refcnt of page->page_cgroup
 mem_cgroup_page_migration() --- copy page->page_cgroup from old page to
                                 new page";KAMEZAWA Hiroyuki;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;"During migration
  - old page is under PG_locked";KAMEZAWA Hiroyuki;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;  - new page is under PG_locked, too;KAMEZAWA Hiroyuki;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;  - both old page and new page is not on LRU;KAMEZAWA Hiroyuki;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;These 3 facts guarantee that page_cgroup() migration has no race;KAMEZAWA Hiroyuki;2008-02-07;0;1
MDY6Q29tbWl0MjMyNTI5ODphZTQxYmUzNzQyOTNlNzBlMWVkNDQxZDk4NmFmY2M2ZTc0NGVmOWQ5;Tested and worked well in x86_64/fake-NUMA box.;KAMEZAWA Hiroyuki;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;Memory controller: make charging gfp mask aware;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;"Nick Piggin pointed out that swap cache and page cache addition routines
could be called from non GFP_KERNEL contexts";Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;" This patch makes the
charging routine aware of the gfp context";Balbir Singh;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;" Charging might fail if the
cgroup is over it's limit, in which case a suitable error is returned";Balbir Singh;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;This patch was tested on a Powerpc box;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODplMWExY2Q1OTBlM2ZjYjBkMmUyMzAxMjhkYWYyMzM3ZWE1NTM4N2Rj;" I am still looking at being able
to test the path, through which allocations happen in non GFP_KERNEL
contexts.";Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;Memory controller: memory accounting;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;Add the accounting hooks;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;" The accounting is carried out for RSS and Page
Cache (unmapped) pages";Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj; There is now a common limit and accounting for both;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;"The RSS accounting is accounted at page_add_*_rmap() and page_remove_rmap()
time";Balbir Singh;2008-02-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;" Page cache is accounted at add_to_page_cache(),
__delete_from_page_cache()";Balbir Singh;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj; Swap cache is also accounted for;Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;"Each page's page_cgroup is protected with the last bit of the
page_cgroup pointer, this makes handling of race conditions involving
simultaneous mappings of a page easier";Balbir Singh;2008-02-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;" A reference count is kept in the
page_cgroup to deal with cases where a page might be unmapped from the RSS
of all tasks, but still lives in the page cache";Balbir Singh;2008-02-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;"Credits go to Vaidyanathan Srinivasan for helping with reference counting work
of the page cgroup";Balbir Singh;2008-02-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo4YTlmM2NjZDI0NzQxYjUwMjAwYzNmMzNkNjI1MzRjNzI3MWYzZGZj;" Almost all of the page cache accounting code has help
from Vaidyanathan Srinivasan.";Balbir Singh;2008-02-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MmUxYzU1MzAwZjMwNmUwNjQ3OGY0NjBhN2VlZmJhMDg1MjA2ZTBi;page migraton: handle orphaned pages;Shaohua Li;2008-02-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo2MmUxYzU1MzAwZjMwNmUwNjQ3OGY0NjBhN2VlZmJhMDg1MjA2ZTBi;Orphaned page might have fs-private metadata, the page is truncated;Shaohua Li;2008-02-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MmUxYzU1MzAwZjMwNmUwNjQ3OGY0NjBhN2VlZmJhMDg1MjA2ZTBi;" As
the page hasn't mapping, page migration refuse to migrate the page";Shaohua Li;2008-02-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MmUxYzU1MzAwZjMwNmUwNjQ3OGY0NjBhN2VlZmJhMDg1MjA2ZTBi;" It
appears the page is only freed in page reclaim and if zone watermark is
low, the page is never freed, as a result migration always fail";Shaohua Li;2008-02-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo2MmUxYzU1MzAwZjMwNmUwNjQ3OGY0NjBhN2VlZmJhMDg1MjA2ZTBi;" I thought
we could free the metadata so such page can be freed in migration and make
migration more reliable.";Shaohua Li;2008-02-05;1;1
MDY6Q29tbWl0MjMyNTI5ODo2OThkZDRiYTZiMTJlMzRlMWU0MzJjOTQ0YzAxNDc4YzBiMmNkNzcz;maps4: move is_swap_pte;Matt Mackall;2008-02-05;1;0
MDY6Q29tbWl0MjMyNTI5ODo2OThkZDRiYTZiMTJlMzRlMWU0MzJjOTQ0YzAxNDc4YzBiMmNkNzcz;Move is_swap_pte helper function to swapops.h for use by pagemap code;Matt Mackall;2008-02-05;1;1
MDY6Q29tbWl0MjMyNTI5ODplOTUzNGIzZmQ3ODQzZDFiZDVhN2ExZmUyNDc0YTA5ZjcyZDQxYWI4;Typo fixes retrun -> return;Gabriel Craciunescu;2007-10-20;0;1
MDY6Q29tbWl0MjMyNTI5ODplOTUzNGIzZmQ3ODQzZDFiZDVhN2ExZmUyNDc0YTA5ZjcyZDQxYWI4;Typo fixes retrun -> return;Gabriel Craciunescu;2007-10-20;0;1
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;Uninline find_task_by_xxx set of functions;Pavel Emelyanov;2007-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;"The find_task_by_something is a set of macros are used to find task by pid
depending on what kind of pid is proposed - global or virtual one";Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;" All of
them are wrappers above the most generic one - find_task_by_pid_type_ns() -
and just substitute some args for it";Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;"It turned out, that dereferencing the current->nsproxy->pid_ns construction
and pushing one more argument on the stack inline cause kernel text size to
grow";Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;This patch moves all this stuff out-of-line into kernel/pid.c;Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODoyMjhlYmNiZTYzNGEzMGFlYzM1MTMyZWE0Mzc1NzIxYmNjNDFiZWMw;" Together
with the next patch it saves a bit less than 400 bytes from the .text
section.";Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNDg4ODkzYTM5MGVkZmUwMjdiYWU3YTQ2ZTlhZjgwODNlNzQwNjY4;pid namespaces: changes to show virtual ids to user;Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODpiNDg4ODkzYTM5MGVkZmUwMjdiYWU3YTQ2ZTlhZjgwODNlNzQwNjY4;This is the largest patch in the set;Pavel Emelyanov;2007-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDg4ODkzYTM5MGVkZmUwMjdiYWU3YTQ2ZTlhZjgwODNlNzQwNjY4;"Make all (I hope) the places where
the pid is shown to or get from user operate on the virtual pids";Pavel Emelyanov;2007-10-19;1;0
MDY6Q29tbWl0MjMyNTI5ODpiNDg4ODkzYTM5MGVkZmUwMjdiYWU3YTQ2ZTlhZjgwODNlNzQwNjY4;The idea is;Pavel Emelyanov;2007-10-19;0;0
MDY6Q29tbWl0MjMyNTI5ODpiNDg4ODkzYTM5MGVkZmUwMjdiYWU3YTQ2ZTlhZjgwODNlNzQwNjY4;" - all in-kernel data structures must store either struct pid itself
 - when seeking the task from kernel code with the stored id one
 - when showing pid's numerical value to the user the virtual one
   should be used, but however when one shows task's pid outside this
 - when getting the pid from userspace one need to consider this as
   the virtual one and use appropriate task/pid-searching functions.";Pavel Emelyanov;2007-10-19;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;flush icache before set_pte() on ia64: flush icache at set_pte;KAMEZAWA Hiroyuki;2007-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"Current ia64 kernel flushes icache by lazy_mmu_prot_update() *after*
set_pte()";KAMEZAWA Hiroyuki;2007-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk; This is too late;KAMEZAWA Hiroyuki;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;" This patch removes lazy_mmu_prot_update and
add modfied set_pte() for flushing if necessary";KAMEZAWA Hiroyuki;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"This patch flush icache of a page when
	new pte has exec bit";KAMEZAWA Hiroyuki;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"	&& new pte has present bit
	&& new pte is user's page";KAMEZAWA Hiroyuki;2007-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"	&& (old *ptep is not present
            || new pte's pfn is not same to old *ptep's ptn)
	&& new pte's page has no Pg_arch_1 bit";KAMEZAWA Hiroyuki;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;	   Pg_arch_1 is set when a page is cache consistent;KAMEZAWA Hiroyuki;2007-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"I think this condition checks are much easier to understand than considering
""Where sync_icache_dcache() should be inserted ?""";KAMEZAWA Hiroyuki;2007-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;"pte_user() for ia64 was removed by as
clean-up";KAMEZAWA Hiroyuki;2007-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NTRmZmNiMzVmNWFjYTQyODY2MWQyOWI5NmM0ZWVlODJiM2MxOWNk;So, I added it again.;KAMEZAWA Hiroyuki;2007-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo5N2VlMDUyNDYxNDQ2NTI2ZTFkZTcyMzY0OTdlNmYxYjFmZmVkZjhj;flush cache before installing new page at migraton;KAMEZAWA Hiroyuki;2007-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo5N2VlMDUyNDYxNDQ2NTI2ZTFkZTcyMzY0OTdlNmYxYjFmZmVkZjhj;"In migration, a new page should be cache flushed before set_pte() in some
archs which have virtually-tagged cache.";KAMEZAWA Hiroyuki;2007-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;Memoryless nodes: Update memory policy and page migration;Christoph Lameter;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;Online nodes now may have no memory;Christoph Lameter;2007-10-16;1;0
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;" The checks and initialization must
therefore be changed to no longer use the online functions";Christoph Lameter;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;"This will correctly initialize the interleave on bootup to only target nodes
with memory and will make sys_move_pages return an error when a page is to be
moved to a memoryless node";Christoph Lameter;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;" Similarly we will get an error if MPOL_BIND and
MPOL_INTERLEAVE is used on a memoryless node";Christoph Lameter;2007-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;These are somewhat new semantics;Christoph Lameter;2007-10-16;0;0
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;" So far one could specify memoryless nodes
and we would maybe do the right thing and just ignore the node (or we'd do
something strange like with MPOL_INTERLEAVE)";Christoph Lameter;2007-10-16;0;1
MDY6Q29tbWl0MjMyNTI5ODo1NmJiZDY1ZGYwZTkyYTRhOGViNzBjNWYyYjQxNmFlMmI2YzVmYjMx;" If we want to allow the
specification of memoryless nodes via memory policies then we need to keep
checking for online nodes.";Christoph Lameter;2007-10-16;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDk2NmQ0OTVjNGFjMmZjYmQwMTBmNjgxNDI1ZjY3MTQxZjgwYmFk;mm/migrate.c __user annotation;Al Viro;2007-10-14;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ZDk2NmQ0OTVjNGFjMmZjYmQwMTBmNjgxNDI1ZjY3MTQxZjgwYmFk;;Al Viro;2007-10-14;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ODlmODljNTdlNjM2MWU3ZDE2ZmJkOTU3MmI1ZGE3ZDMxM2IwNzNk;fix rcu_read_lock() in page migraton;KAMEZAWA Hiroyuki;2007-08-31;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ODlmODljNTdlNjM2MWU3ZDE2ZmJkOTU3MmI1ZGE3ZDMxM2IwNzNk;In migration fallback path, write_page() or lock_page() will be called;KAMEZAWA Hiroyuki;2007-08-31;0;0
MDY6Q29tbWl0MjMyNTI5ODo5ODlmODljNTdlNjM2MWU3ZDE2ZmJkOTU3MmI1ZGE3ZDMxM2IwNzNk;This causes sleep with holding rcu_read_lock().;KAMEZAWA Hiroyuki;2007-08-31;0;1
MDY6Q29tbWl0MjMyNTI5ODozZGQ5ZmU4YzM5N2RmNjgwODZlNmExYjIxNjA1NzNhYmJlOTQ0ODEz;memory unplug: isolate_lru_page fix;KAMEZAWA Hiroyuki;2007-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODozZGQ5ZmU4YzM5N2RmNjgwODZlNmExYjIxNjA1NzNhYmJlOTQ0ODEz;"release_pages() in mm/swap.c changes page_count() to be 0 without removing
PageLRU flag..";KAMEZAWA Hiroyuki;2007-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODozZGQ5ZmU4YzM5N2RmNjgwODZlNmExYjIxNjA1NzNhYmJlOTQ0ODEz;"This means isolate_lru_page() can see a page, PageLRU() &&
page_count(page)==0.";KAMEZAWA Hiroyuki;2007-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODozZGQ5ZmU4YzM5N2RmNjgwODZlNmExYjIxNjA1NzNhYmJlOTQ0ODEz; This is BUG;KAMEZAWA Hiroyuki;2007-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODozZGQ5ZmU4YzM5N2RmNjgwODZlNmExYjIxNjA1NzNhYmJlOTQ0ODEz;" (get_page() will be called against
count=0 page.)";KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;memory unplug: migration by kernel;KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;In usual, migrate_pages(page,,) is called with holding mm->sem by system call;KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;"(mm here is a mm_struct which maps the migration target page.)
This semaphore helps avoiding some race conditions";KAMEZAWA Hiroyuki;2007-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;"But, if we want to migrate a page by some kernel codes, we have to avoid
some races";KAMEZAWA Hiroyuki;2007-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;This patch adds check code for following race condition;KAMEZAWA Hiroyuki;2007-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;1;KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;A page which page->mapping==NULL can be target of migration;KAMEZAWA Hiroyuki;2007-07-26;0;1
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;"Then, we have
   to check page->mapping before calling try_to_unmap()";KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;2;KAMEZAWA Hiroyuki;2007-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;"anon_vma can be freed while page is unmapped, but page->mapping remains as
   it was";KAMEZAWA Hiroyuki;2007-07-26;1;1
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;We drop page->mapcount to be 0;KAMEZAWA Hiroyuki;2007-07-26;1;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;Then we cannot trust page->mapping;KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODpkYzM4NmQ0ZDFlOThiYjM5ZmI5NjdlZTE1NmNkNDU2YzgwMmZjNjky;"   So, use rcu_read_lock() to prevent anon_vma pointed by page->mapping from
   being freed during migration.";KAMEZAWA Hiroyuki;2007-07-26;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;Add __GFP_MOVABLE for callers to flag allocations from high memory that may be migrated;Mel Gorman;2007-07-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;It is often known at allocation time whether a page may be migrated or not;Mel Gorman;2007-07-17;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;"This patch adds a flag called __GFP_MOVABLE and a new mask called
GFP_HIGH_MOVABLE";Mel Gorman;2007-07-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;" Allocations using the __GFP_MOVABLE can be either migrated
using the page migration mechanism or reclaimed by syncing with backing
storage and discarding";Mel Gorman;2007-07-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;"An API function very similar to alloc_zeroed_user_highpage() is added for
__GFP_MOVABLE allocations called alloc_zeroed_user_highpage_movable()";Mel Gorman;2007-07-17;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;" The
flags used by alloc_zeroed_user_highpage() are not changed because it would
change the semantics of an existing API";Mel Gorman;2007-07-17;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;" After this patch is applied there
are no in-kernel users of alloc_zeroed_user_highpage() so it probably should
be marked deprecated if this patch is merged";Mel Gorman;2007-07-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;"Note that this patch includes a minor cleanup to the use of __GFP_ZERO in
shmem.c to keep all flag modifications to inode->mapping in the
shmem_dir_alloc() helper function";Mel Gorman;2007-07-17;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;" This clean-up suggestion is courtesy of
Hugh Dickens";Mel Gorman;2007-07-17;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;"Additional credit goes to Christoph Lameter and Linus Torvalds for shaping the
concept";Mel Gorman;2007-07-17;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Njk4NDhjMDM4OTViNjNlNTY2MmViN2U0ZWM4YzQ4NjZmN2QwMTgz;" Credit to Hugh Dickens for catching issues with shmem swap vector
and ramfs allocations.";Mel Gorman;2007-07-17;0;0
MDY6Q29tbWl0MjMyNTI5ODowZThjN2QwZmQ1YjQ5OTk2NzVjN2Q1Y2Q5NWQwZWI3MTA2Yjc1NmIz;page migration: fix NR_FILE_PAGES accounting;Christoph Lameter;2007-04-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowZThjN2QwZmQ1YjQ5OTk2NzVjN2Q1Y2Q5NWQwZWI3MTA2Yjc1NmIz;"NR_FILE_PAGES must be accounted for depending on the zone that the page
belongs to";Christoph Lameter;2007-04-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowZThjN2QwZmQ1YjQ5OTk2NzVjN2Q1Y2Q5NWQwZWI3MTA2Yjc1NmIz;" If we replace the page in the radix tree then we may have to
shift the count to another zone.";Christoph Lameter;2007-04-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowZGM5NTJkYzNlNmQ5NmQ1NTRhMTlmYTdiZWUzZjNiMWQ1NWUzY2Zm;[PATCH] Page migration: Fix vma flag checking;Christoph Lameter;2007-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODowZGM5NTJkYzNlNmQ5NmQ1NTRhMTlmYTdiZWUzZjNiMWQ1NWUzY2Zm;"Currently we do not check for vma flags if sys_move_pages is called to move
individual pages";Christoph Lameter;2007-03-05;0;0
MDY6Q29tbWl0MjMyNTI5ODowZGM5NTJkYzNlNmQ5NmQ1NTRhMTlmYTdiZWUzZjNiMWQ1NWUzY2Zm;" If sys_migrate_pages is called to move pages then we
check for vm_flags that indicate a non migratable vma but that still
includes VM_LOCKED and we can migrate mlocked pages";Christoph Lameter;2007-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODowZGM5NTJkYzNlNmQ5NmQ1NTRhMTlmYTdiZWUzZjNiMWQ1NWUzY2Zm;"Extract the vma_migratable check from mm/mempolicy.c, fix it and put it
into migrate.h so that is can be used from both locations";Christoph Lameter;2007-03-05;1;1
MDY6Q29tbWl0MjMyNTI5ODowZGM5NTJkYzNlNmQ5NmQ1NTRhMTlmYTdiZWUzZjNiMWQ1NWUzY2Zm;Problem was spotted by Lee Schermerhorn;Christoph Lameter;2007-03-05;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;[PATCH] radix-tree: RCU lockless readside;Nick Piggin;2006-12-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;Make radix tree lookups safe to be performed without locks;Nick Piggin;2006-12-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;" Readers are
protected against nodes being deleted by using RCU based freeing";Nick Piggin;2006-12-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;" Readers
are protected against new node insertion by using memory barriers to ensure
the node itself will be properly written before it is visible in the radix
tree";Nick Piggin;2006-12-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;Each radix tree node keeps a record of their height (above leaf nodes);Nick Piggin;2006-12-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;"This height does not change after insertion -- when the radix tree is
extended, higher nodes are only inserted in the top";Nick Piggin;2006-12-07;1;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;" So a lookup can take
the pointer to what is *now* the root node, and traverse down it even if
the tree is concurrently extended and this node becomes a subtree of a new
root";Nick Piggin;2006-12-07;1;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;"""Direct"" pointers (tree height of 0, where root->rnode points directly to
the data item) are handled by using the low bit of the pointer to signal
whether rnode is a direct pointer or a pointer to a radix tree node";Nick Piggin;2006-12-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;"When a reader wants to traverse the next branch, they will take a copy of
the pointer";Nick Piggin;2006-12-07;0;0
MDY6Q29tbWl0MjMyNTI5ODo3Y2Y5YzJjNzZjMWExN2IzMmYyZGE4NWI1MGNkNGZlNDY4ZWQ0NGI1;" This pointer will be either NULL (and the branch is empty) or
non-NULL (and will point to a valid node).";Nick Piggin;2006-12-07;0;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2UwODQ2NGQyYzc0OTYxMGE1MmM0ZDZjN2MxMTA4MGE3ZWFhZWYx;[PATCH] Fix sys_move_pages when a NULL node list is passed;Stephen Rothwell;2006-11-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2UwODQ2NGQyYzc0OTYxMGE1MmM0ZDZjN2MxMTA4MGE3ZWFhZWYx;"sys_move_pages() uses vmalloc() to allocate an array of structures that is
fills with information passed from user mode and then passes to
do_stat_pages() (in the case the node list is NULL)";Stephen Rothwell;2006-11-03;0;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2UwODQ2NGQyYzc0OTYxMGE1MmM0ZDZjN2MxMTA4MGE3ZWFhZWYx;" do_stat_pages()
depends on a marker in the node field of the structure to decide how large
the array is and this marker is correctly inserted into the last element of
the array";Stephen Rothwell;2006-11-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo4Y2UwODQ2NGQyYzc0OTYxMGE1MmM0ZDZjN2MxMTA4MGE3ZWFhZWYx;" However, vmalloc() doesn't zero the memory it allocates and if
the user passes NULL for the node list, then the node fields are not filled
in (except for the end marker)";Stephen Rothwell;2006-11-03;0;0
MDY6Q29tbWl0MjMyNTI5ODo4Y2UwODQ2NGQyYzc0OTYxMGE1MmM0ZDZjN2MxMTA4MGE3ZWFhZWYx;" If the memory the vmalloc() returned
happend to have a word with the marker value in it in just the right place,
do_pages_stat will fail to fill the status field of part of the array and
we will return (random) kernel data to user mode.";Stephen Rothwell;2006-11-03;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;[PATCH] BLOCK: Make it possible to disable the block layer [try #6];David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;Make it possible to disable the block layer;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" Not all embedded devices require
it, some can make do with just JFFS2, NFS, ramfs, etc - none of which require
the block layer to be present";David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;This patch does the following;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) Introduces CONFIG_BLOCK to disable the block layer, buffering and blockdev
     support";David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) Adds dependencies on CONFIG_BLOCK to any configuration item that controls
     an item that uses the block layer";David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; This includes;David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) Block I/O tracing;David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) Disk partition code;David Howells;2006-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) All filesystems that are block based, eg: Ext3, ReiserFS, ISOFS;David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) The SCSI layer;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" As far as I can tell, even SCSI chardevs use the
     	 block layer to do scheduling";David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" Some drivers that use SCSI facilities -
     	 such as USB storage - end up disabled indirectly from this";David Howells;2006-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;"     (*) Various block-based device drivers, such as IDE and the old CDROM
     	 drivers";David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) MTD blockdev handling and FTL;David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;"     (*) JFFS - which uses set_bdev_super(), something it could avoid doing by
     	 taking a leaf out of JFFS2's book";David Howells;2006-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) Makes most of the contents of linux/blkdev.h, linux/buffer_head.h and
     linux/elevator.h contingent on CONFIG_BLOCK being set";David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" sector_div() is,
     however, still used in places, and so is still available";David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) Also made contingent are the contents of linux/mpage.h, linux/genhd.h and
     parts of linux/fs.h";David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; (*) Makes a number of files in fs/ contingent on CONFIG_BLOCK;David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; (*) Makes mm/bounce.c (bounce buffering) contingent on CONFIG_BLOCK;David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) set_page_dirty() doesn't call __set_page_dirty_buffers() if CONFIG_BLOCK
     is not enabled";David Howells;2006-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) fs/no-block.c is created to hold out-of-line stubs and things that are
     required when CONFIG_BLOCK is not set";David Howells;2006-09-30;0;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) Default blockdev file operations (to give error ENODEV on opening);David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; (*) Makes some /proc changes;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) /proc/devices does not list any blockdevs;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;     (*) /proc/diskstats and /proc/partitions are contingent on CONFIG_BLOCK;David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; (*) Makes some compat ioctl handling contingent on CONFIG_BLOCK;David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) If CONFIG_BLOCK is not defined, makes sys_quotactl() return -ENODEV if
     given command other than Q_SYNC or if a special device is specified";David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) In init/do_mounts.c, no reference is made to the blockdev routines if
     CONFIG_BLOCK is not defined";David Howells;2006-09-30;1;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3; This does not prohibit NFS roots or JFFS2;David Howells;2006-09-30;0;0
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) The bdflush, ioprio_set and ioprio_get syscalls can now be absent (return
     error ENOSYS by way of cond_syscall if so)";David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODo5MzYxNDAxZWI3NjE5YzAzM2UyMzk0ZTRmOWY2ZDQxMGQ2NzE5YWM3;" (*) The seclvl_bd_claim() and seclvl_bd_release() security calls do nothing if
     CONFIG_BLOCK is not set, since they can't then happen.";David Howells;2006-09-30;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMzk4ZjZiZmY5M2EyNDdkMmE3MDk5ZTkyOTA1Mzc0OTY2ZTQ1NThm;[PATCH] BLOCK: Stop fallback_migrate_page() from using page_has_buffers() [try #6];David Howells;2006-08-29;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMzk4ZjZiZmY5M2EyNDdkMmE3MDk5ZTkyOTA1Mzc0OTY2ZTQ1NThm;"Stop fallback_migrate_page() from using page_has_buffers() since that might not
be available";David Howells;2006-08-29;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMzk4ZjZiZmY5M2EyNDdkMmE3MDk5ZTkyOTA1Mzc0OTY2ZTQ1NThm; Use PagePrivate() instead since that's more general.;David Howells;2006-08-29;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ODAxMjhmMjIzZmEzYzc1ZTNlYmRkZTY1MGM5ZjFiY2FiZDRjMGEy;[PATCH] Define easier to handle GFP_THISNODE;Christoph Lameter;2006-09-26;1;1
MDY6Q29tbWl0MjMyNTI5ODo5ODAxMjhmMjIzZmEzYzc1ZTNlYmRkZTY1MGM5ZjFiY2FiZDRjMGEy;In many places we will need to use the same combination of flags;Christoph Lameter;2006-09-26;0;1
MDY6Q29tbWl0MjMyNTI5ODo5ODAxMjhmMjIzZmEzYzc1ZTNlYmRkZTY1MGM5ZjFiY2FiZDRjMGEy;" Specify
a single GFP_THISNODE definition for ease of use in gfp.h.";Christoph Lameter;2006-09-26;1;0
MDY6Q29tbWl0MjMyNTI5ODozZDk5Y2ZiNWY0NjE5MWZjNjhmMTM0M2ZlZWIyY2Y4MzUwMDFmN2Q3;[PATCH] sys_move_pages: Do not fall back to other nodes;Christoph Lameter;2006-09-26;1;0
MDY6Q29tbWl0MjMyNTI5ODozZDk5Y2ZiNWY0NjE5MWZjNjhmMTM0M2ZlZWIyY2Y4MzUwMDFmN2Q3;"If the user specified a node where we should move the page to then we
really do not want any other node.";Christoph Lameter;2006-09-26;0;1
MDY6Q29tbWl0MjMyNTI5ODplNmExNTMwZDY5MmQ2YTYwY2RmMTVkZmJjZmVhMDdmNTMyNGQ3Yjlm;[PATCH] Allow migration of mlocked pages;Christoph Lameter;2006-06-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplNmExNTMwZDY5MmQ2YTYwY2RmMTVkZmJjZmVhMDdmNTMyNGQ3Yjlm;Hugh clarified the role of VM_LOCKED;Christoph Lameter;2006-06-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplNmExNTMwZDY5MmQ2YTYwY2RmMTVkZmJjZmVhMDdmNTMyNGQ3Yjlm;" So we can now implement page
migration for mlocked pages";Christoph Lameter;2006-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODplNmExNTMwZDY5MmQ2YTYwY2RmMTVkZmJjZmVhMDdmNTMyNGQ3Yjlm;Allow the migration of mlocked pages;Christoph Lameter;2006-06-25;1;0
MDY6Q29tbWl0MjMyNTI5ODplNmExNTMwZDY5MmQ2YTYwY2RmMTVkZmJjZmVhMDdmNTMyNGQ3Yjlm;" This means that try_to_unmap must
unmap mlocked pages in the migration case.";Christoph Lameter;2006-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;[PATCH] page migration: Support a vma migration function;Christoph Lameter;2006-06-25;1;0
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;"Hooks for calling vma specific migration functions
With this patch a vma may define a vma->vm_ops->migrate function";Christoph Lameter;2006-06-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" That
function may perform page migration on its own (some vmas may not contain page
structs and therefore cannot be handled by regular page migration";Christoph Lameter;2006-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" Pages in a
vma may require special preparatory treatment before migration is possible
etc) ";Christoph Lameter;2006-06-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3; Only mmap_sem is held when the migration function is called;Christoph Lameter;2006-06-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" The
migrate() function gets passed two sets of nodemasks describing the source and
the target of the migration";Christoph Lameter;2006-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" The flags parameter either contains
MPOL_MF_MOVE	which means that only pages used exclusively by
		the specified mm should be moved
MPOL_MF_MOVE_ALL which means that pages shared with other processes
		should also be moved";Christoph Lameter;2006-06-25;0;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;The migration function returns 0 on success or an error condition;Christoph Lameter;2006-06-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" An error
condition will prevent regular page migration from occurring";Christoph Lameter;2006-06-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;"On its own this patch cannot be included since there are no users for this
functionality";Christoph Lameter;2006-06-25;1;1
MDY6Q29tbWl0MjMyNTI5ODo3YjIyNTliM2U1M2YxMjhjMTBhOWZkZWQwOTY1ZTY5ZDRhOTQ5ODQ3;" But it seems that the uncached allocator will need this
functionality at some point.";Christoph Lameter;2006-06-25;0;0
MDY6Q29tbWl0MjMyNTI5ODo4NmMzYTc2NDVjMDVhN2QwNmI3MjY1M2FhNGIyYmVhNGU3MjI5ZDFi;[PATCH] SELinux: add security_task_movememory calls to mm code;David Quigley;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NmMzYTc2NDVjMDVhN2QwNmI3MjY1M2FhNGIyYmVhNGU3MjI5ZDFi;"This patch inserts security_task_movememory hook calls into memory management
code to enable security modules to mediate this operation between tasks";David Quigley;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo4NmMzYTc2NDVjMDVhN2QwNmI3MjY1M2FhNGIyYmVhNGU3MjI5ZDFi;"Since the last posting, the hook has been renamed following feedback from
Christoph Lameter.";David Quigley;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;[PATCH] page migration: sys_move_pages(): support moving of individual pages;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;move_pages() is used to move individual pages of a process;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The function can
be used to determine the location of pages and to move them onto the desired
node";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;move_pages() returns status information for each page;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"long move_pages(pid, number_of_pages_to_move,
		addresses_of_pages[],
		nodes[] or NULL,
		status[],
The addresses of pages is an array of void * pointing to the
pages to be moved";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The nodes array contains the node numbers that the pages should be moved
to";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"If a NULL is passed instead of an array then no pages are moved but
the status array is updated";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The status request may be used to determine
the page state before issuing another move_pages() to move pages";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The status array will contain the state of all individual page migration
attempts when the function terminates";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The status array is only valid if
move_pages() completed successfullly";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;Possible page states in status[];Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;0..MAX_NUMNODES	The page is now on the indicated node;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"-ENOENT		Page is not present
-EACCES		Page is mapped by multiple processes and can only
		be moved if MPOL_MF_MOVE_ALL is specified";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"-EPERM		The page has been mlocked by a process/driver and
		cannot be moved";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EBUSY		Page is busy and cannot be moved;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;Try again later;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EFAULT		Invalid address (no VMA or zero page);Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-ENOMEM		Unable to allocate memory on target node;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EIO		Unable to write back page;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The page must be written
		back in order to move it since the page is dirty and the
		filesystem does not provide a migration function that
		would allow the moving of dirty pages";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EINVAL		A dirty page cannot be moved;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"The filesystem does not provide
		a migration function and has no ability to write back pages";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;The flags parameter indicates what types of pages to move;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;MPOL_MF_MOVE	Move pages that are only mapped by the process;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;MPOL_MF_MOVE_ALL Also move pages that are mapped by multiple processes;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;		Requires sufficient capabilities;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"Possible return codes from move_pages()
-ENOENT		No pages found that would require moving";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"All pages
		are either already on the target node, not present, had an
		invalid address or could not be moved because they were
		mapped by multiple processes";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"-EINVAL		Flags other than MPOL_MF_MOVE(_ALL) specified or an attempt
		to migrate pages in a kernel thread";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EPERM		MPOL_MF_MOVE_ALL specified without sufficient priviledges;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;		or an attempt to move a process belonging to another user;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EACCES		One of the target nodes is not allowed by the current cpuset;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-ENODEV		One of the target nodes is not online;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-ESRCH		Process does not exist;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-E2BIG		Too many pages to move;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-ENOMEM		Not enough memory to allocate control array;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;-EFAULT		Parameters could not be accessed;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;"A test program for move_pages() may be found with the patches
on ftp.kernel.org:/pub/linux/kernel/people/christoph/pmig/patches-2.6.17-rc4-mm3
From: Christoph Lameter <clameter@sgi.com>
  Detailed results for sys_move_pages()
  Pass a pointer to an integer to get_new_page() that may be used to
  indicate where the completion status of a migration operation should be
  placed";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;" This allows sys_move_pags() to report back exactly what happened to
  each page";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;  Wish there would be a better way to do this;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo3NDI3NTVhMWQ4Y2UyYjU0ODQyOGY3YWFjZjE3NThiNGJiYTUwMDgw;Looks a bit hacky.;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl;[PATCH] page migration: use allocator function for migrate_pages();Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl;"Instead of passing a list of new pages, pass a function to allocate a new
page";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl;" This allows the correct placement of MPOL_INTERLEAVE pages during page
migration";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl; It also further simplifies the callers of migrate pages;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl;"migrate_pages() becomes similar to migrate_pages_to() so drop
migrate_pages_to()";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo5NWE0MDJjMzg0N2NjMTZmNGJhMDMwMTNjZDAxNDA0ZmEwZjE0YzJl; The batching of new page allocations becomes unnecessary.;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODphYWE5OTRiMzAwYTE3MmFmYWZhYjQ3OTM4ODA0ODM2YjkyM2U1ZWY3;[PATCH] page migration: handle freeing of pages in migrate_pages();Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODphYWE5OTRiMzAwYTE3MmFmYWZhYjQ3OTM4ODA0ODM2YjkyM2U1ZWY3;Do not leave pages on the lists passed to migrate_pages();Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODphYWE5OTRiMzAwYTE3MmFmYWZhYjQ3OTM4ODA0ODM2YjkyM2U1ZWY3;" Seems that we will
not need any postprocessing of pages";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODphYWE5OTRiMzAwYTE3MmFmYWZhYjQ3OTM4ODA0ODM2YjkyM2U1ZWY3;" This will simplify the handling of
pages by the callers of migrate_pages().";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;[PATCH] page migration: simplify migrate_pages();Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;Currently migrate_pages() is mess with lots of goto;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;" Extract two functions
from migrate_pages() and get rid of the gotos";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;"Plus we can just unconditionally set the locked bit on the new page since we
are the only one holding a reference";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;" Locking is to stop others from
accessing the page once we establish references to the new page";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODplMjRmMGI4Zjc2Y2MzZGQ5NmYzNmY1YjZhOWYwMjBmNmMzZmNlMTk4;"Remove the list_del from move_to_lru in order to have finer control over list
processing.";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;[PATCH] More page migration: use migration entries for file pages;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;"This implements the use of migration entries to preserve ptes of file backed
pages during migration";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;" Processes can therefore be migrated back and forth
without loosing their connection to pagecache pages";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;Note that we implement the migration entries only for linear mappings;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;Nonlinear mappings still require the unmapping of the ptes for migration;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;And another writepage() ugliness shows up;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;" writepage() can drop the page
lock";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNGU2MmEyOWJmMTU3Y2UxZWRkMTY4ZjJiNzFiNTMzYzgwZDEzNjI4;" Therefore we have to remove migration ptes before calling writepages()
in order to avoid having migration entries point to unlocked pages.";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo0NDJjOTEzN2RlOGQ3NjkwNTNlODFkMzI1NzA5ZGNhNzJmMGI1ZTQ0;[PATCH] More page migration: do not inc/dec rss counters;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NDJjOTEzN2RlOGQ3NjkwNTNlODFkMzI1NzA5ZGNhNzJmMGI1ZTQ0;"If we install a migration entry then the rss not really decreases since the
page is just moved somewhere else";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo0NDJjOTEzN2RlOGQ3NjkwNTNlODFkMzI1NzA5ZGNhNzJmMGI1ZTQ0;" We can save ourselves the work of
decrementing and later incrementing which will just eventually cause cacheline
bouncing.";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;[PATCH] Swapless page migration: modify core logic;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;"Use the migration entries for page migration
This modifies the migration code to use the new migration entries";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;" It now
becomes possible to migrate anonymous pages without having to add a swap
entry";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;"We add a couple of new functions to replace migration entries with the proper
ptes";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;We cannot take the tree_lock for migrating anonymous pages anymore;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo2YzUyNDBhZTdmNDhjODNmY2FhOGUyNGZhNjNlN2ViMDlhYmE1NjUx;" However,
we know that we hold the only remaining reference to the page when the page
count reaches 1.";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;[PATCH] Swapless page migration: rip out swap based logic;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;Rip the page migration logic out;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;Remove all code that has to do with swapping during page migration;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;This also guts the ability to migrate pages to swap;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;" No one used that so lets
let it go for good";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpkNzVhMGZjZGEyY2ZjNzFiNTBlMTZkYzg5ZTBjMzJjNTdkNDI3ZTg1;Page migration should be a bit broken after this patch.;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;[PATCH] Swapless page migration: add R/W migration entries;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"Implement read/write migration ptes
We take the upper two swapfiles for the two types of migration ptes and define
a series of macros in swapops.h";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;The VM is modified to handle the migration entries;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" migration entries can
only be encountered when the page they are pointing to is locked";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" This limits
the number of places one has to fix";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" We also check in copy_pte_range and in
mprotect_pte_range() for migration ptes";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"We check for migration ptes in do_swap_cache and call a function that will
then wait on the page lock";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" This allows us to effectively stop all accesses
to apge";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"Migration entries are created by try_to_unmap if called for migration and
removed by local functions in migrate.c
From: Hugh Dickins <hugh@veritas.com>
  Several times while testing swapless page migration (I've no NUMA, just
  hacking it up to migrate recklessly while running load), I've hit the
  BUG_ON(!PageLocked(p)) in migration_entry_to_page";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"  This comes from an orphaned migration entry, unrelated to the current
  correctly locked migration, but hit by remove_anon_migration_ptes as it
  checks an address in each vma of the anon_vma list";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;  Such an orphan may be left behind if an earlier migration raced with fork;Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"  copy_one_pte can duplicate a migration entry from parent to child, after
  remove_anon_migration_ptes has checked the child vma, but before it has
  removed it from the parent vma";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" (If the process were later to fault on this
  orphaned entry, it would hit the same BUG from migration_entry_wait.)
  This could be fixed by locking anon_vma in copy_one_pte, but we'd rather
  not";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" There's no such problem with file pages, because vma_prio_tree_add
  adds child vma after parent vma, and the page table locking at each end is
  enough to serialize";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" Follow that example with anon_vma: add new vmas to the
  tail instead of the head";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"  (There's no corresponding problem when inserting migration entries,
  because a missed pte will leave the page count and mapcount high, which is
  allowed for";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" And there's no corresponding problem when migrating via swap,
  because a leftover swap entry will be correctly faulted";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;" But the swapless
  method has no refcounting of its entries.)
From: Ingo Molnar <mingo@elte.hu>
  pte_unmap_unlock() takes the pte pointer as an argument";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"From: Hugh Dickins <hugh@veritas.com>
  Several times while testing swapless page migration, gcc has tried to exec
  a pointer instead of a string: smells like COW mappings are not being
  properly write-protected on fork";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"  The protection in copy_one_pte looks very convincing, until at last you
  realize that the second arg to make_migration_entry is a boolean ""write"",
  and SWP_MIGRATION_READ is 30";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"  Anyway, it's better done like in change_pte_range, using
  is_write_migration_entry and make_migration_entry_read";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODowNjk3MjEyYTQxMWMxZGFlMDNjMjc4NDVmMmRlMmYzYWRiMzJjMzMx;"From: Hugh Dickins <hugh@veritas.com>
  Remove unnecessary obfuscation from sys_swapon's range check on swap type,
  which blew up causing memory corruption once swapless migration made
  MAX_SWAPFILES no longer 2 ^ MAX_SWAPFILES_SHIFT.";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzUxYTZlNDc4NTIxOGEyYjAzYzE0MmJlOTI5MjZiYWZmOTViYTVj;[PATCH] page migration cleanup: move fallback handling into special function;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo4MzUxYTZlNDc4NTIxOGEyYjAzYzE0MmJlOTI5MjZiYWZmOTViYTVj;"Move the fallback code into a new fallback function and make the function
behave like any other migration function";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODo4MzUxYTZlNDc4NTIxOGEyYjAzYzE0MmJlOTI5MjZiYWZmOTViYTVj;" This requires retaking the lock if
pageout() drops it.";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;"[PATCH] page migration cleanup: pass ""mapping"" to migration functions";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;Change handling of address spaces;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;"Pass a pointer to the address space in which the page is migrated to all
migration function";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;" This avoids repeatedly having to retrieve the address
space pointer from the page and checking it for validity";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;" The old page
mapping will change once migration has gone to a certain step, so it is less
confusing to have the pointer always available";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODoyZDFkYjNiMTE3MGRiNGU4YmYwNTMxZGQ2MzY3NDIyNjljMmNmNTc5;"Move the setting of the mapping and index for the new page into
migrate_pages().";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;[PATCH] page migration cleanup: extract try_to_unmap from migration functions;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;"Extract try_to_unmap and rename remove_references -> move_mapping
try_to_unmap() may significantly change the page state by for example setting
the dirty bit";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;" It is therefore best to unmap in migrate_pages() before
calling any migration functions";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;"migrate_page_remove_references() will then only move the new page in place of
the old page in the mapping";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;" Rename the function to
migrate_page_move_mapping()";Christoph Lameter;2006-06-23;1;0
MDY6Q29tbWl0MjMyNTI5ODpjM2ZjZjhhNWRhYWNmMzUwZjA2MzJlMTM3OTQxNGMwMWYzNGVlZWEz;This allows us to get rid of the special unmapping for the fallback path.;Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YjVjNzEyMGUyMTU0MjM5ODM3ZmFkNWUzYzdiN2I3ODEwOTJiMTlj;[PATCH] page migration cleanup: drop nr_refs in remove_references();Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YjVjNzEyMGUyMTU0MjM5ODM3ZmFkNWUzYzdiN2I3ODEwOTJiMTlj;"Drop nr_refs parameter from migrate_page_remove_references()
The nr_refs parameter is not really useful since the number of remaining
references is always
1 for anonymous pages without a mapping
2 for pages with a mapping
3 for pages with a mapping and PagePrivate set";Christoph Lameter;2006-06-23;0;1
MDY6Q29tbWl0MjMyNTI5ODo1YjVjNzEyMGUyMTU0MjM5ODM3ZmFkNWUzYzdiN2I3ODEwOTJiMTlj;"Remove the early check for the number of references since we are checking
page_mapcount() earlier";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODo1YjVjNzEyMGUyMTU0MjM5ODM3ZmFkNWUzYzdiN2I3ODEwOTJiMTlj;" Ultimately only the refcount matters after the
tree_lock has been obtained.";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODplNzM0MGY3MzMwN2FiZWQ5MjgzZDBhMDc1NzBkMDZlMjI4YzIwNWRk;[PATCH] page migration cleanup: remove useless definitions;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzM0MGY3MzMwN2FiZWQ5MjgzZDBhMDc1NzBkMDZlMjI4YzIwNWRk;"Remove the export for migrate_page_remove_references() and migrate_page_copy()
that are unlikely to be used directly by filesystems implementing migration";Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODplNzM0MGY3MzMwN2FiZWQ5MjgzZDBhMDc1NzBkMDZlMjI4YzIwNWRk;"The export was useful when buffer_migrate_page() lived in fs/buffer.c but it
has now been moved to migrate.c in the migration reorg.";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODoxZDhiODVjY2YxZWQ1M2E3MWIwOTJmYjVkODA3ZWRmMWVhN2RhYmRk;[PATCH] page migration cleanup: group functions;Christoph Lameter;2006-06-23;1;1
MDY6Q29tbWl0MjMyNTI5ODoxZDhiODVjY2YxZWQ1M2E3MWIwOTJmYjVkODA3ZWRmMWVhN2RhYmRk;Reorder functions in migrate.c;Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODoxZDhiODVjY2YxZWQ1M2E3MWIwOTJmYjVkODA3ZWRmMWVhN2RhYmRk;" Group all migration functions for struct
address_space_operations together.";Christoph Lameter;2006-06-23;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;[PATCH] page migration: Fix fallback behavior for dirty pages;Christoph Lameter;2006-05-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;"Currently we check PageDirty() in order to make the decision to swap out
the page";Christoph Lameter;2006-05-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" However, the dirty information may be only be contained in the
ptes pointing to the page";Christoph Lameter;2006-05-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" We need to first unmap the ptes before checking
for PageDirty()";Christoph Lameter;2006-05-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" If unmap is successful then the page count of the page
will also be decreased so that pageout() works properly";Christoph Lameter;2006-05-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;This is a fix necessary for 2.6.17;Christoph Lameter;2006-05-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" Without this fix we may migrate dirty
pages for filesystems without migration functions";Christoph Lameter;2006-05-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" Filesystems may keep
pointers to dirty pages";Christoph Lameter;2006-05-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" Migration of dirty pages can result in the
filesystem keeping pointers to freed pages";Christoph Lameter;2006-05-01;0;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;"Unmapping is currently not be separated out from removing all the
references to a page and moving the mapping";Christoph Lameter;2006-05-01;1;1
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" Therefore try_to_unmap will
be called again in migrate_page() if the writeout is successful";Christoph Lameter;2006-05-01;0;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;" However,
it wont do anything since the ptes are already removed";Christoph Lameter;2006-05-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo0YzI4ZjgxMTkzYjY3NzhmN2I0OTA5MDkzMGQ4OGU2ZDEyYmNiOTI4;"The coming updates to the page migration code will restructure the code
so that this is no longer necessary.";Christoph Lameter;2006-05-01;1;0
MDY6Q29tbWl0MjMyNTI5ODo2NGEzY2E1ZjdlYzI2MDZiMDNiZTRhNjU3MzYxNjRhNTM3MzczMmVk;[PATCH] mm/migrate.c: don't export a static function;Adrian Bunk;2006-04-11;0;0
MDY6Q29tbWl0MjMyNTI5ODo2NGEzY2E1ZjdlYzI2MDZiMDNiZTRhNjU3MzYxNjRhNTM3MzczMmVk;EXPORT_SYMBOL'ing of a static function is not a good idea.;Adrian Bunk;2006-04-11;0;1
MDY6Q29tbWl0MjMyNTI5ODplMjNjYTAwYmYxYjFjNmMwZjA0NzAyY2I0ZDI5ZTI3NWFiOGRjMzMw;[PATCH] Some page migration fixups;Christoph Lameter;2006-04-11;1;0
MDY6Q29tbWl0MjMyNTI5ODplMjNjYTAwYmYxYjFjNmMwZjA0NzAyY2I0ZDI5ZTI3NWFiOGRjMzMw;"- Remove sparse comment
- Remove duplicated include
- Return the correct error condition in migrate_page_remove_references().";Christoph Lameter;2006-04-11;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;[PATCH] page migration reorg;Christoph Lameter;2006-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Centralize the page migration functions in anticipation of additional
tinkering";Christoph Lameter;2006-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;" Creates a new file mm/migrate.c
1";Christoph Lameter;2006-03-22;1;1
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Extract buffer_migrate_page() from fs/buffer.c
2";Christoph Lameter;2006-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Extract central migration code from vmscan.c
3";Christoph Lameter;2006-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Extract some components from mempolicy.c
4";Christoph Lameter;2006-03-22;0;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Export pageout() and remove_from_swap() from vmscan.c
5";Christoph Lameter;2006-03-22;1;0
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;"Make it possible to configure NUMA systems without page migration
   and non-NUMA systems with page migration";Christoph Lameter;2006-03-22;0;1
MDY6Q29tbWl0MjMyNTI5ODpiMjBhMzUwMzVmOTgzZjRhYzdlMjljNGE2OGYzMGU0MzUxMDAwN2Uw;I had to so some #ifdeffing in mempolicy.c that may need a cleanup.;Christoph Lameter;2006-03-22;0;1
